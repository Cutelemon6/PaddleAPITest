[Worker 1] Started on GPU 1
[Worker 1] Processing Task 1: paddle.all(Tensor([228170138, 10],"bool"), axis=-1, )
W0520 11:11:16.027863  8631 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 11:11:16.028615  8631 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.all(Tensor([228170138, 10],"bool"), axis=-1, )
[Worker 1] Completed Task 1
[Worker 1] Processing Task 5: paddle.amax(Tensor([22817014, 10, 10],"float32"), axis=list[-1,-2,], keepdim=False, )
[Pass] paddle.amax(Tensor([22817014, 10, 10],"float32"), axis=list[-1,-2,], keepdim=False, )
[Worker 1] Completed Task 5
[Worker 1] Processing Task 9: paddle.amax(Tensor([3, 2, 95070891, 4],"float32"), axis=tuple(1,2,), keepdim=True, )
[Pass] paddle.amax(Tensor([3, 2, 95070891, 4],"float32"), axis=tuple(1,2,), keepdim=True, )
[Worker 1] Completed Task 9
[Worker 1] Processing Task 12: paddle.amax(Tensor([3, 38028357, 5, 4],"float32"), axis=tuple(1,2,), keepdim=True, )
[Pass] paddle.amax(Tensor([3, 38028357, 5, 4],"float32"), axis=tuple(1,2,), keepdim=True, )
[Worker 1] Completed Task 12
[Worker 1] Processing Task 15: paddle.amax(Tensor([57042535, 2, 5, 4],"float32"), axis=tuple(1,2,), keepdim=True, )
[Pass] paddle.amax(Tensor([57042535, 2, 5, 4],"float32"), axis=tuple(1,2,), keepdim=True, )
[Worker 1] Completed Task 15
[Worker 1] Processing Task 20: paddle.amin(Tensor([3, 2, 4, 95070891],"float32"), axis=-1, keepdim=True, )
[Pass] paddle.amin(Tensor([3, 2, 4, 95070891],"float32"), axis=-1, keepdim=True, )
[Worker 1] Completed Task 20
[Worker 1] Processing Task 24: paddle.amin(Tensor([3, 38028357, 4, 5],"float32"), axis=-1, keepdim=True, )
[Pass] paddle.amin(Tensor([3, 38028357, 4, 5],"float32"), axis=-1, keepdim=True, )
[Worker 1] Completed Task 24
[Worker 1] Processing Task 28: paddle.amin(Tensor([57042535, 2, 5, 4],"float32"), axis=2, keepdim=True, )
[Pass] paddle.amin(Tensor([57042535, 2, 5, 4],"float32"), axis=2, keepdim=True, )
[Worker 1] Completed Task 28
[Worker 1] Processing Task 33: paddle.argmax(Tensor([1, 4294967297],"float16"), axis=-1, )
[Pass] paddle.argmax(Tensor([1, 4294967297],"float16"), axis=-1, )
[Worker 1] Completed Task 33
[Worker 1] Processing Task 38: paddle.argmax(Tensor([2, 536870913, 4],"float16"), axis=-1, keepdim=True, )
[cuda error] paddle.argmax(Tensor([2, 536870913, 4],"float16"), axis=-1, keepdim=True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747711475 (unix time) try "date -d @1747711475" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x21b7) received by PID 8631 (TID 0x7f66bfd56740) from PID 8631 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 44: paddle.argmax(Tensor([3, 3, 3, 3, 17674763, 3],"float16"), axis=0, )
W0520 11:26:03.132102  9160 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 11:26:03.132911  9160 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.argmax(Tensor([3, 3, 3, 3, 17674763, 3],"float16"), axis=0, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747711564 (unix time) try "date -d @1747711564" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x23c8) received by PID 9160 (TID 0x7f66bfd56740) from PID 9160 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 50: paddle.argmax(Tensor([4, 4, 16777217, 4, 4],"float16"), axis=0, )
W0520 11:27:45.358985  9460 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 11:27:45.359798  9460 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.argmax(Tensor([4, 4, 16777217, 4, 4],"float16"), axis=0, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747711667 (unix time) try "date -d @1747711667" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x24f4) received by PID 9460 (TID 0x7f66bfd56740) from PID 9460 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 54: paddle.argmax(Tensor([456340276, 5],"float32"), keepdim=True, )
W0520 11:28:37.461835  9758 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 11:28:37.462543  9758 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.argmax(Tensor([456340276, 5],"float32"), keepdim=True, )
[Worker 1] Completed Task 54
[Worker 1] Processing Task 57: paddle.argmax(x=Tensor([3, 3, 477218589],"float16"), )
[Pass] paddle.argmax(x=Tensor([3, 3, 477218589],"float16"), )
[Worker 1] Completed Task 57
[Worker 1] Processing Task 61: paddle.argmax(x=Tensor([477218589, 3, 3],"float16"), )
[Pass] paddle.argmax(x=Tensor([477218589, 3, 3],"float16"), )
[Worker 1] Completed Task 61
[Worker 1] Processing Task 66: paddle.argmin(Tensor([3, 3, 17674763, 3, 3, 3],"float16"), axis=0, )
[cuda error] paddle.argmin(Tensor([3, 3, 17674763, 3, 3, 3],"float16"), axis=0, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747711926 (unix time) try "date -d @1747711926" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x261e) received by PID 9758 (TID 0x7f66bfd56740) from PID 9758 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 70: paddle.argmin(Tensor([3, 760567127],"float32"), )
W0520 11:32:50.297587 10211 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 11:32:50.298327 10211 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.argmin(Tensor([3, 760567127],"float32"), )
[Worker 1] Completed Task 70
[Worker 1] Processing Task 73: paddle.argmin(Tensor([4, 4, 16777217, 4, 4],"float16"), axis=0, )
[cuda error] paddle.argmin(Tensor([4, 4, 16777217, 4, 4],"float16"), axis=0, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747712050 (unix time) try "date -d @1747712050" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x27e3) received by PID 10211 (TID 0x7f66bfd56740) from PID 10211 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 77: paddle.argmin(Tensor([456340276, 5],"float32"), keepdim=True, )
W0520 11:34:50.810002 10658 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 11:34:50.810715 10658 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.argmin(Tensor([456340276, 5],"float32"), keepdim=True, )
[Worker 1] Completed Task 77
[Worker 1] Processing Task 81: paddle.argmin(x=Tensor([3, 477218589, 3],"float16"), )
[Pass] paddle.argmin(x=Tensor([3, 477218589, 3],"float16"), )
[Worker 1] Completed Task 81
[Worker 1] Processing Task 84: paddle.bmm(x=Tensor([2, 715827883, 3],"float16"), y=Tensor([2, 3, 2],"float16"), )
/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:823: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:180.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[cuda error] paddle.bmm(x=Tensor([2, 715827883, 3],"float16"), y=Tensor([2, 3, 2],"float16"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747712554 (unix time) try "date -d @1747712554" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x29a2) received by PID 10658 (TID 0x7f66bfd56740) from PID 10658 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 93: paddle.copysign(Tensor([12, 178956971, 2],"float16"), Tensor([12, 178956971, 2],"float16"), )
W0520 11:46:32.587040 11115 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 11:46:32.588009 11115 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.copysign(Tensor([12, 178956971, 2],"float16"), Tensor([12, 178956971, 2],"float16"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747713220 (unix time) try "date -d @1747713220" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2b6b) received by PID 11115 (TID 0x7f66bfd56740) from PID 11115 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 99: paddle.copysign(Tensor([2, 3, 143165577, 5],"float16"), Tensor([2, 3, 143165577, 5],"float16"), )
W0520 11:57:39.716253 11567 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 11:57:39.717211 11567 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.copysign(Tensor([2, 3, 143165577, 5],"float16"), Tensor([2, 3, 143165577, 5],"float16"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747713885 (unix time) try "date -d @1747713885" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2d2f) received by PID 11567 (TID 0x7f66bfd56740) from PID 11567 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 102: paddle.copysign(Tensor([214748365, 4, 5],"float16"), Tensor([4, 5],"float16"), )
W0520 12:07:16.378932 11797 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 12:07:16.380347 11797 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.copysign(Tensor([214748365, 4, 5],"float16"), Tensor([4, 5],"float16"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747714460 (unix time) try "date -d @1747714460" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2e15) received by PID 11797 (TID 0x7f66bfd56740) from PID 11797 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 108: paddle.copysign(Tensor([71582789, 3, 4, 5],"float16"), Tensor([71582789, 3, 4, 5],"float16"), )
W0520 12:18:18.748309 12251 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 12:18:18.749352 12251 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.copysign(Tensor([71582789, 3, 4, 5],"float16"), Tensor([71582789, 3, 4, 5],"float16"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747715133 (unix time) try "date -d @1747715133" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2fdb) received by PID 12251 (TID 0x7f66bfd56740) from PID 12251 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 112: paddle.copysign(Tensor([8, 2556529, 5, 6, 7],"float16"), Tensor([8, 2556529, 5, 6, 7],"float16"), )
W0520 12:29:43.434161 12557 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 12:29:43.435206 12557 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.copysign(Tensor([8, 2556529, 5, 6, 7],"float16"), Tensor([8, 2556529, 5, 6, 7],"float16"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747715824 (unix time) try "date -d @1747715824" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x310d) received by PID 12557 (TID 0x7f66bfd56740) from PID 12557 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 118: paddle.cross(x=Tensor([3, 1431655766],"float16"), y=Tensor([3, 1431655766],"float16"), )
/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py:104: UserWarning: Using torch.cross without specifying the dim arg is deprecated.
Please either pass the dim explicitly or simply use torch.linalg.cross.
The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at /pytorch/aten/src/ATen/native/Cross.cpp:62.)
  return func(*args, **kwargs)
W0520 12:40:57.448280 12861 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 12:40:57.449240 12861 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.cross(x=Tensor([3, 1431655766],"float16"), y=Tensor([3, 1431655766],"float16"), )
[Worker 1] Completed Task 118
[Worker 1] Processing Task 122: paddle.cross(x=Tensor([3, 477218589, 3],"float16"), y=Tensor([3, 477218589, 3],"float16"), axis=0, )
[Pass] paddle.cross(x=Tensor([3, 477218589, 3],"float16"), y=Tensor([3, 477218589, 3],"float16"), axis=0, )
[Worker 1] Completed Task 122
[Worker 1] Processing Task 126: paddle.cross(x=Tensor([477218589, 3, 3],"float16"), y=Tensor([477218589, 3, 3],"float16"), axis=1, )
[Pass] paddle.cross(x=Tensor([477218589, 3, 3],"float16"), y=Tensor([477218589, 3, 3],"float16"), axis=1, )
[Worker 1] Completed Task 126
[Worker 1] Processing Task 145: paddle.cumsum(Tensor([3, 2, 380283564],"float32"), axis=1, )
[Pass] paddle.cumsum(Tensor([3, 2, 380283564],"float32"), axis=1, )
[Worker 1] Completed Task 145
[Worker 1] Processing Task 151: paddle.cumsum(Tensor([50704476, 45],"int64"), axis=1, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.cumsum(Tensor([50704476, 45],"int64"), axis=1, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.610352GB memory has been allocated and available memory is only 9.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 1] Completed Task 151
[Worker 1] Processing Task 154: paddle.cumsum(Tensor([570425345, 4],"int64"), axis=-2, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.cumsum(Tensor([570425345, 4],"int64"), axis=-2, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.610352GB memory has been allocated and available memory is only 9.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 1] Completed Task 154
[Worker 1] Processing Task 159: paddle.cumsum(x=Tensor([1, 16, 8388609, 32],"float16"), axis=2, )
[accuracy error] paddle.cumsum(x=Tensor([1, 16, 8388609, 32],"float16"), axis=2, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4170604822 / 4294967808 (97.1%)
Max absolute difference: 2154.
Max relative difference: inf
 x: array([[[[-4.1040e-01, -4.7388e-01, -3.6597e-01, ...,  3.6938e-01,
          -7.8491e-02, -1.2646e-01],
         [-4.7632e-01, -5.0635e-01, -7.3828e-01, ...,  3.7769e-01,...
 y: array([[[[-4.1040e-01, -4.7388e-01, -3.6597e-01, ...,  3.6938e-01,
          -7.8491e-02, -1.2646e-01],
         [-4.7632e-01, -5.0635e-01, -7.3828e-01, ...,  3.7769e-01,...
[Worker 1] Completed Task 159
[Worker 1] Processing Task 161: paddle.cumsum(x=Tensor([1, 2, 715827883, 3],"float16"), axis=3, )
[Pass] paddle.cumsum(x=Tensor([1, 2, 715827883, 3],"float16"), axis=3, )
[Worker 1] Completed Task 161
[Worker 1] Processing Task 165: paddle.cumulative_trapezoid(y=Tensor([1073741825, 4],"float16"), x=Tensor([1073741825, 4],"float16"), )
W0520 14:44:05.505486 12861 backward.cc:441] While running Node (GatherGradNode) raises a std::exception: paddle::memory::allocation::BadAlloc
[paddle error] paddle.cumulative_trapezoid(y=Tensor([1073741825, 4],"float16"), x=Tensor([1073741825, 4],"float16"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   GatherGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::gather_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, paddle::Tensor*)
4   void phi::funcs::GatherV2GradCUDAFunction<phi::dtype::float16, long>(phi::DenseTensor const*, phi::DenseTensor const*, int, phi::DenseTensor*, phi::GPUContext const&)
5   phi::dtype::float16* phi::DeviceContext::Alloc<phi::dtype::float16>(phi::TensorBase*, unsigned long, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::Allocator::Allocate(unsigned long)
10  paddle::memory::allocation::Allocator::Allocate(unsigned long)
11  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
12  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 8.000000GB memory on GPU 0, 77.625977GB memory has been allocated and available memory is only 1.558899GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747723450 (unix time) try "date -d @1747723450" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x323d) received by PID 12861 (TID 0x7f66bfd56740) from PID 12861 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 171: paddle.diff(Tensor([4, 1073741825],"float16"), axis=1, )
W0520 14:46:01.741824 13015 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 14:46:01.742606 13015 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.diff(Tensor([4, 1073741825],"float16"), axis=1, )
[Worker 1] Completed Task 171
[Worker 1] Processing Task 174: paddle.diff(x=Tensor([67108865, 4, 4, 4],"float16"), )
[Pass] paddle.diff(x=Tensor([67108865, 4, 4, 4],"float16"), )
[Worker 1] Completed Task 174
[Worker 1] Processing Task 177: paddle.dist(Tensor([1140850690, 2],"float32"), Tensor([1140850690, 2],"float32"), 0, )
Expected a proper Tensor but got None (or an undefined Tensor in C++) for argument #0 'self'
[cuda error] paddle.dist(Tensor([1140850690, 2],"float32"), Tensor([1140850690, 2],"float32"), 0, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

[Worker 1] Error on Task 177: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[Worker 1] OOM on Task 177: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 180: paddle.dist(x=Tensor([1073741825, 4],"float16"), y=Tensor([1073741825, 4],"float16"), p=1, )
W0520 15:02:46.173277 13091 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 15:02:46.174283 13091 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.dist(x=Tensor([1073741825, 4],"float16"), y=Tensor([1073741825, 4],"float16"), p=1, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747724572 (unix time) try "date -d @1747724572" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x3323) received by PID 13091 (TID 0x7f66bfd56740) from PID 13091 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 185: paddle.dist(x=Tensor([4, 570425345],"float32"), y=Tensor([4, 570425345],"float32"), )
W0520 15:04:45.860507 13391 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 15:04:45.861472 13391 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.dist(x=Tensor([4, 570425345],"float32"), y=Tensor([4, 570425345],"float32"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747724689 (unix time) try "date -d @1747724689" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x344f) received by PID 13391 (TID 0x7f66bfd56740) from PID 13391 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 189: paddle.frac(Tensor([10, 228170138, 1],"float32"), )
W0520 15:06:37.800000 13693 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 15:06:37.800997 13693 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.frac(Tensor([10, 228170138, 1],"float32"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747724799 (unix time) try "date -d @1747724799" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x357d) received by PID 13693 (TID 0x7f66bfd56740) from PID 13693 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 191: paddle.frac(Tensor([2, 1140850690],"float32"), )
W0520 15:08:00.883074 13848 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 15:08:00.884380 13848 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.frac(Tensor([2, 1140850690],"float32"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747724882 (unix time) try "date -d @1747724882" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x3618) received by PID 13848 (TID 0x7f66bfd56740) from PID 13848 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 194: paddle.isfinite(Tensor([11, 17, 12201612],"int32"), )
W0520 15:08:33.967181 14077 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 15:08:33.967984 14077 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.isfinite(Tensor([11, 17, 12201612],"int32"), )
[Worker 1] Completed Task 194
[Worker 1] Processing Task 197: paddle.isfinite(Tensor([134217729, 17],"float32"), )
[Pass] paddle.isfinite(Tensor([134217729, 17],"float32"), )
[Worker 1] Completed Task 197
[Worker 1] Processing Task 201: paddle.isfinite(Tensor([2, 3, 4, 89478486],"float64"), )
[Pass] paddle.isfinite(Tensor([2, 3, 4, 89478486],"float64"), )
[Worker 1] Completed Task 201
[Worker 1] Processing Task 203: paddle.isfinite(Tensor([2, 53687092, 4, 5],"float64"), )
[Pass] paddle.isfinite(Tensor([2, 53687092, 4, 5],"float64"), )
[Worker 1] Completed Task 203
[Worker 1] Processing Task 208: paddle.isfinite(Tensor([35791395, 3, 4, 5],"float64"), )
[Pass] paddle.isfinite(Tensor([35791395, 3, 4, 5],"float64"), )
[Worker 1] Completed Task 208
[Worker 1] Processing Task 211: paddle.isfinite(Tensor([4, 280, 27164, 25, 3],"float32"), )
[Pass] paddle.isfinite(Tensor([4, 280, 27164, 25, 3],"float32"), )
[Worker 1] Completed Task 211
[Worker 1] Processing Task 215: paddle.isfinite(Tensor([78050, 94, 311],"float32"), )
[Pass] paddle.isfinite(Tensor([78050, 94, 311],"float32"), )
[Worker 1] Completed Task 215
[Worker 1] Processing Task 219: paddle.isin(Tensor([285212673, 8],"int32"), Tensor([2, 3],"int32"), False, True, )
[Pass] paddle.isin(Tensor([285212673, 8],"int32"), Tensor([2, 3],"int32"), False, True, )
[Worker 1] Completed Task 219
[Worker 1] Processing Task 223: paddle.isin(Tensor([4, 570425345],"int64"), Tensor([2, 3],"int64"), False, True, )
[Pass] paddle.isin(Tensor([4, 570425345],"int64"), Tensor([2, 3],"int64"), False, True, )
[Worker 1] Completed Task 223
[Worker 1] Processing Task 227: paddle.isinf(Tensor([1073741825, 2],"float64"), )
[Pass] paddle.isinf(Tensor([1073741825, 2],"float64"), )
[Worker 1] Completed Task 227
[Worker 1] Processing Task 231: paddle.isinf(Tensor([1140850690, 2],"float32"), )
[Pass] paddle.isinf(Tensor([1140850690, 2],"float32"), )
[Worker 1] Completed Task 231
[Worker 1] Processing Task 235: paddle.isinf(Tensor([14, 40744668, 4],"float32"), )
[Pass] paddle.isinf(Tensor([14, 40744668, 4],"float32"), )
[Worker 1] Completed Task 235
[Worker 1] Processing Task 239: paddle.isinf(Tensor([178956971, 12],"float64"), )
[Pass] paddle.isinf(Tensor([178956971, 12],"float64"), )
[Worker 1] Completed Task 239
[Worker 1] Processing Task 243: paddle.isinf(Tensor([2, 3, 4, 89478486],"float64"), )
[Pass] paddle.isinf(Tensor([2, 3, 4, 89478486],"float64"), )
[Worker 1] Completed Task 243
[Worker 1] Processing Task 247: paddle.isinf(Tensor([2147483649],"int64"), )
[Pass] paddle.isinf(Tensor([2147483649],"int64"), )
[Worker 1] Completed Task 247
[Worker 1] Processing Task 251: paddle.isinf(Tensor([2281701379],"int64"), )
[Pass] paddle.isinf(Tensor([2281701379],"int64"), )
[Worker 1] Completed Task 251
[Worker 1] Processing Task 255: paddle.isinf(Tensor([4, 5, 107374183],"float64"), )
[Pass] paddle.isinf(Tensor([4, 5, 107374183],"float64"), )
[Worker 1] Completed Task 255
[Worker 1] Processing Task 258: paddle.isinf(Tensor([4294967295],"uint8"), )
[Pass] paddle.isinf(Tensor([4294967295],"uint8"), )
[Worker 1] Completed Task 258
[Worker 1] Processing Task 263: paddle.isnan(Tensor([11, 17, 12201612],"int32"), )
[Pass] paddle.isnan(Tensor([11, 17, 12201612],"int32"), )
[Worker 1] Completed Task 263
[Worker 1] Processing Task 265: paddle.isnan(Tensor([11, 20742740, 10],"int32"), )
[Pass] paddle.isnan(Tensor([11, 20742740, 10],"int32"), )
[Worker 1] Completed Task 265
[Worker 1] Processing Task 267: paddle.isnan(Tensor([134217729, 17],"float32"), )
[Pass] paddle.isnan(Tensor([134217729, 17],"float32"), )
[Worker 1] Completed Task 267
[Worker 1] Processing Task 272: paddle.isnan(Tensor([14, 40744668, 4],"float32"), )
[Pass] paddle.isnan(Tensor([14, 40744668, 4],"float32"), )
[Worker 1] Completed Task 272
[Worker 1] Processing Task 276: paddle.isnan(Tensor([1431655765, 3],"float32"), )
[Pass] paddle.isnan(Tensor([1431655765, 3],"float32"), )
[Worker 1] Completed Task 276
[Worker 1] Processing Task 281: paddle.isnan(Tensor([2, 1073741824, 2],"float32"), )
[Pass] paddle.isnan(Tensor([2, 1073741824, 2],"float32"), )
[Worker 1] Completed Task 281
[Worker 1] Processing Task 286: paddle.isnan(Tensor([2, 3, 76056713, 5],"float32"), )
[Pass] paddle.isnan(Tensor([2, 3, 76056713, 5],"float32"), )
[Worker 1] Completed Task 286
[Worker 1] Processing Task 290: paddle.isnan(Tensor([2, 53687092, 4, 5],"float64"), )
[Pass] paddle.isnan(Tensor([2, 53687092, 4, 5],"float64"), )
[Worker 1] Completed Task 290
[Worker 1] Processing Task 294: paddle.isnan(Tensor([2147483649],"float64"), )
[Pass] paddle.isnan(Tensor([2147483649],"float64"), )
[Worker 1] Completed Task 294
[Worker 1] Processing Task 298: paddle.isnan(Tensor([2281701379],"int64"), )
[Pass] paddle.isnan(Tensor([2281701379],"int64"), )
[Worker 1] Completed Task 298
[Worker 1] Processing Task 302: paddle.isnan(Tensor([286331153, 5, 3],"float32"), )
[Pass] paddle.isnan(Tensor([286331153, 5, 3],"float32"), )
[Worker 1] Completed Task 302
[Worker 1] Processing Task 306: paddle.isnan(Tensor([3, 357913942, 2],"int64"), )
[Pass] paddle.isnan(Tensor([3, 357913942, 2],"int64"), )
[Worker 1] Completed Task 306
[Worker 1] Processing Task 310: paddle.isnan(Tensor([3292499, 7, 99],"float32"), )
[Pass] paddle.isnan(Tensor([3292499, 7, 99],"float32"), )
[Worker 1] Completed Task 310
[Worker 1] Processing Task 313: paddle.isnan(Tensor([35791395, 3, 4, 5],"float64"), )
[Pass] paddle.isnan(Tensor([35791395, 3, 4, 5],"float64"), )
[Worker 1] Completed Task 313
[Worker 1] Processing Task 317: paddle.isnan(Tensor([4, 3, 153094, 1242],"float32"), )
[Pass] paddle.isnan(Tensor([4, 3, 153094, 1242],"float32"), )
[Worker 1] Completed Task 317
[Worker 1] Processing Task 321: paddle.isnan(Tensor([400, 5368710],"float64"), )
[Pass] paddle.isnan(Tensor([400, 5368710],"float64"), )
[Worker 1] Completed Task 321
[Worker 1] Processing Task 325: paddle.isnan(Tensor([5, 429496730],"float64"), )
[Pass] paddle.isnan(Tensor([5, 429496730],"float64"), )
[Worker 1] Completed Task 325
[Worker 1] Processing Task 327: paddle.isnan(Tensor([536870912, 4, 2],"float32"), )
[Pass] paddle.isnan(Tensor([536870912, 4, 2],"float32"), )
[Worker 1] Completed Task 327
[Worker 1] Processing Task 332: paddle.isnan(Tensor([8912897, 64, 4],"float32"), )
[Pass] paddle.isnan(Tensor([8912897, 64, 4],"float32"), )
[Worker 1] Completed Task 332
[Worker 1] Processing Task 334: paddle.isneginf(Tensor([11, 207427399],"float32"), )
[Pass] paddle.isneginf(Tensor([11, 207427399],"float32"), )
[Worker 1] Completed Task 334
[Worker 1] Processing Task 338: paddle.isneginf(Tensor([4294967295],"uint8"), )
[Pass] paddle.isneginf(Tensor([4294967295],"uint8"), )
[Worker 1] Completed Task 338
[Worker 1] Processing Task 345: paddle.linalg.cond(Tensor([2, 126761188, 3, 3],"float32"), 2, )
[torch error] paddle.linalg.cond(Tensor([2, 126761188, 3, 3],"float32"), 2, )
Traceback (most recent call last):
  File "/luozeyu01_api_test/PaddleAPITest/tester/accuracy.py", line 84, in test
    exec(code.core_compiled, exec_globals, exec_locals)
  File "<string>", line 1, in <module>
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[Worker 1] Error on Task 345: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[Worker 1] OOM on Task 345: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 348: paddle.linalg.cond(Tensor([63380594, 4, 3, 3],"float32"), 2, )
[torch error] paddle.linalg.cond(Tensor([63380594, 4, 3, 3],"float32"), 2, )
Traceback (most recent call last):
  File "/luozeyu01_api_test/PaddleAPITest/tester/accuracy.py", line 84, in test
    exec(code.core_compiled, exec_globals, exec_locals)
  File "<string>", line 1, in <module>
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[Worker 1] Error on Task 348: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[Worker 1] OOM on Task 348: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 353: paddle.linalg.lstsq(Tensor([9, 253522376],"float32"), Tensor([9, 5],"float32"), rcond=1e-15, driver="gels", )
[torch error] paddle.linalg.lstsq(Tensor([9, 253522376],"float32"), Tensor([9, 5],"float32"), rcond=1e-15, driver="gels", )
Traceback (most recent call last):
  File "/luozeyu01_api_test/PaddleAPITest/tester/accuracy.py", line 84, in test
    exec(code.core_compiled, exec_globals, exec_locals)
  File "<string>", line 1, in <module>
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[Worker 1] Error on Task 353: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[Worker 1] OOM on Task 353: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 354: paddle.linalg.matrix_rank(Tensor([114085069, 4, 5],"float32"), )
[torch error] paddle.linalg.matrix_rank(Tensor([114085069, 4, 5],"float32"), )
Traceback (most recent call last):
  File "/luozeyu01_api_test/PaddleAPITest/tester/accuracy.py", line 84, in test
    exec(code.core_compiled, exec_globals, exec_locals)
  File "<string>", line 1, in <module>
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[Worker 1] Error on Task 354: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[Worker 1] OOM on Task 354: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 358: paddle.linalg.norm(Tensor([5, 5, 91268056],"float32"), p="fro", axis=list[0,1,], keepdim=True, )
W0520 15:58:31.203711 14976 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 15:58:31.204738 14976 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.linalg.norm(Tensor([5, 5, 91268056],"float32"), p="fro", axis=list[0,1,], keepdim=True, )
[Worker 1] Completed Task 358
[Worker 1] Processing Task 362: paddle.linalg.norm(x=Tensor([2, 536870913, 4],"float16"), p=1, axis=list[0,1,], keepdim=True, )
[accuracy error] paddle.linalg.norm(x=Tensor([2, 536870913, 4],"float16"), p=1, axis=list[0,1,], keepdim=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4 / 4 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0., 0., 0., 0.]]], dtype=float16)
 y: array([[[1., 1., 1., 1.]]], dtype=float16)
[Worker 1] Completed Task 362
[Worker 1] Processing Task 366: paddle.mean(Tensor([1, 1, 2, 4, 536870912],"float16"), )
[Pass] paddle.mean(Tensor([1, 1, 2, 4, 536870912],"float16"), )
[Worker 1] Completed Task 366
[Worker 1] Processing Task 370: paddle.mean(Tensor([1, 16777216, 8, 32],"float16"), )
[Pass] paddle.mean(Tensor([1, 16777216, 8, 32],"float16"), )
[Worker 1] Completed Task 370
[Worker 1] Processing Task 374: paddle.mean(Tensor([1, 3, 1, 1, 1431655765],"float16"), )
[Pass] paddle.mean(Tensor([1, 3, 1, 1, 1431655765],"float16"), )
[Worker 1] Completed Task 374
[Worker 1] Processing Task 378: paddle.mean(Tensor([1, 4, 32, 33554432],"float16"), )
[Pass] paddle.mean(Tensor([1, 4, 32, 33554432],"float16"), )
[Worker 1] Completed Task 378
[Worker 1] Processing Task 382: paddle.mean(Tensor([1, 4294967295, 1, 1, 1],"float16"), )
[Pass] paddle.mean(Tensor([1, 4294967295, 1, 1, 1],"float16"), )
[Worker 1] Completed Task 382
[Worker 1] Processing Task 386: paddle.mean(Tensor([1073741824, 1, 2, 2],"float16"), )
[Pass] paddle.mean(Tensor([1073741824, 1, 2, 2],"float16"), )
[Worker 1] Completed Task 386
[Worker 1] Processing Task 390: paddle.mean(Tensor([1431655765, 1, 1, 3],"float16"), )
[Pass] paddle.mean(Tensor([1431655765, 1, 1, 3],"float16"), )
[Worker 1] Completed Task 390
[Worker 1] Processing Task 394: paddle.mean(Tensor([1431655765, 3],"float16"), )
[Pass] paddle.mean(Tensor([1431655765, 3],"float16"), )
[Worker 1] Completed Task 394
[Worker 1] Processing Task 398: paddle.mean(Tensor([2, 1, 1, 2147483648],"float16"), )
[Pass] paddle.mean(Tensor([2, 1, 1, 2147483648],"float16"), )
[Worker 1] Completed Task 398
[Worker 1] Processing Task 401: paddle.mean(Tensor([2, 1, 2147483648, 1, 1],"float16"), )
[Pass] paddle.mean(Tensor([2, 1, 2147483648, 1, 1],"float16"), )
[Worker 1] Completed Task 401
[Worker 1] Processing Task 405: paddle.mean(Tensor([2, 2147483648, 1, 1, 1],"float16"), )
[Pass] paddle.mean(Tensor([2, 2147483648, 1, 1, 1],"float16"), )
[Worker 1] Completed Task 405
[Worker 1] Processing Task 409: paddle.mean(Tensor([2, 4, 536870912],"float16"), )
[Pass] paddle.mean(Tensor([2, 4, 536870912],"float16"), )
[Worker 1] Completed Task 409
[Worker 1] Processing Task 412: paddle.mean(Tensor([2147483648, 1, 1, 2],"float16"), )
[Pass] paddle.mean(Tensor([2147483648, 1, 1, 2],"float16"), )
[Worker 1] Completed Task 412
[Worker 1] Processing Task 417: paddle.mean(Tensor([286331153, 15],"float16"), )
[Pass] paddle.mean(Tensor([286331153, 15],"float16"), )
[Worker 1] Completed Task 417
[Worker 1] Processing Task 421: paddle.mean(Tensor([4, 1, 1, 1, 1073741824],"float16"), )
[Pass] paddle.mean(Tensor([4, 1, 1, 1, 1073741824],"float16"), )
[Worker 1] Completed Task 421
[Worker 1] Processing Task 425: paddle.mean(Tensor([4, 1, 1073741824, 1],"float16"), )
[Pass] paddle.mean(Tensor([4, 1, 1073741824, 1],"float16"), )
[Worker 1] Completed Task 425
[Worker 1] Processing Task 429: paddle.mean(Tensor([4, 1, 2, 536870912],"float16"), )
[Pass] paddle.mean(Tensor([4, 1, 2, 536870912],"float16"), )
[Worker 1] Completed Task 429
[Worker 1] Processing Task 433: paddle.mean(Tensor([4, 1073741824, 1, 1],"float16"), )
[Pass] paddle.mean(Tensor([4, 1073741824, 1, 1],"float16"), )
[Worker 1] Completed Task 433
[Worker 1] Processing Task 437: paddle.mean(Tensor([4, 268435456, 2, 2],"float16"), )
[Pass] paddle.mean(Tensor([4, 268435456, 2, 2],"float16"), )
[Worker 1] Completed Task 437
[Worker 1] Processing Task 441: paddle.mean(Tensor([4, 67108864, 4, 4],"float16"), )
[Pass] paddle.mean(Tensor([4, 67108864, 4, 4],"float16"), )
[Worker 1] Completed Task 441
[Worker 1] Processing Task 445: paddle.mean(Tensor([4294967295],"float16"), )
[Pass] paddle.mean(Tensor([4294967295],"float16"), )
[Worker 1] Completed Task 445
[Worker 1] Processing Task 449: paddle.mean(Tensor([524288, 64, 128],"float16"), )
[Pass] paddle.mean(Tensor([524288, 64, 128],"float16"), )
[Worker 1] Completed Task 449
[Worker 1] Processing Task 453: paddle.mean(Tensor([64, 64, 1048576],"float16"), )
[Pass] paddle.mean(Tensor([64, 64, 1048576],"float16"), )
[Worker 1] Completed Task 453
[Worker 1] Processing Task 457: paddle.mean(Tensor([858993459, 5, 1],"float16"), )
[Pass] paddle.mean(Tensor([858993459, 5, 1],"float16"), )
[Worker 1] Completed Task 457
[Worker 1] Processing Task 461: paddle.mean(x=Tensor([8, 536870912],"float16"), )
[Pass] paddle.mean(x=Tensor([8, 536870912],"float16"), )
[Worker 1] Completed Task 461
[Worker 1] Processing Task 465: paddle.nansum(Tensor([1431655765, 3],"float32"), axis=None, keepdim=True, name=None, )
[Worker 1] Started on GPU 1
[Worker 1] Processing Task 469: paddle.nansum(Tensor([3, 1431655765],"float32"), keepdim=True, )
W0520 19:55:11.850946 15586 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 19:55:11.852030 15586 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Worker 1] Started on GPU 1
[Worker 1] Processing Task 473: paddle.nansum(Tensor([858993459, 5],"float32"), keepdim=True, )
W0520 20:25:23.189905 15890 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 20:25:23.190914 15890 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Worker 1] Started on GPU 1
[Worker 1] Processing Task 487: paddle.nn.functional.grid_sample(Tensor([56, 9948, 64, 64],"float32"), Tensor([56, 64, 64, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, )
W0520 20:55:48.590727 16954 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 20:55:48.591790 16954 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.grid_sample(Tensor([56, 9948, 64, 64],"float32"), Tensor([56, 64, 64, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747745809 (unix time) try "date -d @1747745809" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x423a) received by PID 16954 (TID 0x7f66bfd56740) from PID 16954 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 491: paddle.nn.functional.grid_sample(Tensor([727584, 4, 28, 28],"float32"), Tensor([727584, 28, 28, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )
W0520 20:58:58.889993 17258 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 20:58:58.891009 17258 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.grid_sample(Tensor([727584, 4, 28, 28],"float32"), Tensor([727584, 28, 28, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747745999 (unix time) try "date -d @1747745999" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x436a) received by PID 17258 (TID 0x7f66bfd56740) from PID 17258 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 496: paddle.nn.functional.layer_norm(Tensor([69633, 128, 256],"float32"), list[256,], weight=Tensor([256],"float32"), bias=Tensor([256],"float32"), epsilon=1e-05, )
W0520 21:01:48.345520 17635 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 21:01:48.346580 17635 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.layer_norm(Tensor([69633, 128, 256],"float32"), list[256,], weight=Tensor([256],"float32"), bias=Tensor([256],"float32"), epsilon=1e-05, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747746162 (unix time) try "date -d @1747746162" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x44e3) received by PID 17635 (TID 0x7f66bfd56740) from PID 17635 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 499: paddle.nn.functional.normalize(Tensor([1, 128, 557057, 32],"float32"), axis=1, )
W0520 21:04:09.919436 17863 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 21:04:09.920852 17863 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.normalize(Tensor([1, 128, 557057, 32],"float32"), axis=1, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747746310 (unix time) try "date -d @1747746310" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x45c7) received by PID 17863 (TID 0x7f66bfd56740) from PID 17863 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 503: paddle.nn.functional.normalize(Tensor([1, 557057, 64, 64],"float32"), axis=1, )
W0520 21:06:57.802779 18169 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 21:06:57.803889 18169 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.normalize(Tensor([1, 557057, 64, 64],"float32"), axis=1, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747746472 (unix time) try "date -d @1747746472" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x46f9) received by PID 18169 (TID 0x7f66bfd56740) from PID 18169 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 506: paddle.nn.functional.normalize(Tensor([1, 8912897, 16, 16],"float32"), axis=1, )
W0520 21:09:11.875696 18397 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 21:09:11.876672 18397 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.normalize(Tensor([1, 8912897, 16, 16],"float32"), axis=1, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747746614 (unix time) try "date -d @1747746614" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x47dd) received by PID 18397 (TID 0x7f66bfd56740) from PID 18397 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 511: paddle.nn.functional.normalize(Tensor([11883862, 192],"float32"), axis=1, )
W0520 21:12:09.076428 18775 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 21:12:09.077489 18775 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.normalize(Tensor([11883862, 192],"float32"), axis=1, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747746799 (unix time) try "date -d @1747746799" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x4957) received by PID 18775 (TID 0x7f66bfd56740) from PID 18775 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 513: paddle.nn.functional.normalize(Tensor([17409, 128, 32, 32],"float32"), axis=1, )
W0520 21:14:42.610720 18928 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 21:14:42.611699 18928 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.normalize(Tensor([17409, 128, 32, 32],"float32"), axis=1, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747746937 (unix time) try "date -d @1747746937" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x49f0) received by PID 18928 (TID 0x7f66bfd56740) from PID 18928 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 516: paddle.nn.functional.normalize(Tensor([2, 8, 7, 20372334],"float32"), axis=1, )
W0520 21:17:30.194454 19158 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 21:17:30.195477 19158 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.normalize(Tensor([2, 8, 7, 20372334],"float32"), axis=1, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747747106 (unix time) try "date -d @1747747106" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x4ad6) received by PID 19158 (TID 0x7f66bfd56740) from PID 19158 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 520: paddle.nn.functional.normalize(Tensor([228170138, 10],"float32"), p=1.5, )
W0520 21:20:14.372933 19462 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 21:20:14.373895 19462 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.normalize(Tensor([228170138, 10],"float32"), p=1.5, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747747266 (unix time) try "date -d @1747747266" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x4c06) received by PID 19462 (TID 0x7f66bfd56740) from PID 19462 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 523: paddle.nn.functional.normalize(Tensor([325957340, 7],"float32"), axis=0, )
W0520 21:22:55.924283 19690 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 21:22:55.925278 19690 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.normalize(Tensor([325957340, 7],"float32"), axis=0, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747747607 (unix time) try "date -d @1747747607" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x4cea) received by PID 19690 (TID 0x7f66bfd56740) from PID 19690 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 531: paddle.nn.functional.normalize(Tensor([4456449, 512],"float32"), )
W0520 21:28:34.995893 20296 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 21:28:34.996882 20296 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.normalize(Tensor([4456449, 512],"float32"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747747768 (unix time) try "date -d @1747747768" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x4f48) received by PID 20296 (TID 0x7f66bfd56740) from PID 20296 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 534: paddle.nn.functional.normalize(Tensor([570425345, 4],"float32"), axis=0, )
W0520 21:31:20.181594 20526 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 21:31:20.182619 20526 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.normalize(Tensor([570425345, 4],"float32"), axis=0, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747748195 (unix time) try "date -d @1747748195" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x502e) received by PID 20526 (TID 0x7f66bfd56740) from PID 20526 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 538: paddle.nn.functional.normalize(Tensor([8388609, 512],"float16"), )
W0520 21:39:23.997423 20830 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 21:39:23.998493 20830 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.normalize(Tensor([8388609, 512],"float16"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747748813 (unix time) try "date -d @1747748813" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x515e) received by PID 20830 (TID 0x7f66bfd56740) from PID 20830 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 543: paddle.nn.functional.normalize(x=Tensor([20452226, 5, 6, 7],"float16"), )
W0520 21:49:59.435750 21210 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 21:49:59.437028 21210 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.normalize(x=Tensor([20452226, 5, 6, 7],"float16"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747749427 (unix time) try "date -d @1747749427" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x52da) received by PID 21210 (TID 0x7f66bfd56740) from PID 21210 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 549: paddle.nn.functional.normalize(x=Tensor([4, 5, 214748365],"float16"), )
W0520 22:00:03.382371 21664 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 22:00:03.383338 21664 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.normalize(x=Tensor([4, 5, 214748365],"float16"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747750020 (unix time) try "date -d @1747750020" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x54a0) received by PID 21664 (TID 0x7f66bfd56740) from PID 21664 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 551: paddle.nn.functional.normalize(x=Tensor([4, 5, 30678338, 7],"float16"), p=1, )
W0520 22:10:00.352246 21818 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 22:10:00.353252 21818 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.normalize(x=Tensor([4, 5, 30678338, 7],"float16"), p=1, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747750628 (unix time) try "date -d @1747750628" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x553a) received by PID 21818 (TID 0x7f66bfd56740) from PID 21818 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 555: paddle.nn.functional.normalize(x=Tensor([570425345, 4],"float32"), )
W0520 22:19:02.418973 22122 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 22:19:02.420008 22122 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.normalize(x=Tensor([570425345, 4],"float32"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747750811 (unix time) try "date -d @1747750811" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x566a) received by PID 22122 (TID 0x7f66bfd56740) from PID 22122 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 558: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), 2, 1e-06, False, None, )
W0520 22:21:30.180294 22348 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 22:21:30.181262 22348 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), 2, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 42.582153
Max relative difference: 0.02183664
 x: array([1908.0511, 1908.1268, 1907.58  , 1907.9276, 1907.6008, 1907.9865,
       1907.4379, 1907.2494, 1907.6245, 1907.4144, 1907.3505, 1907.64  ,
       1907.2959, 1907.5405, 1907.9407, 1907.538 , 1907.9069, 1907.7578,...
 y: array([1950.5225, 1950.5486, 1949.9794, 1950.331 , 1950.022 , 1950.4689,
       1949.9487, 1949.6821, 1950.0718, 1949.7959, 1949.7949, 1950.0443,
       1949.7332, 1950.004 , 1950.2974, 1950.0109, 1950.2976, 1950.1687,...
[Worker 1] Completed Task 558
[Worker 1] Processing Task 561: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), math.inf, 1e-06, False, None, )
[cuda error] paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), math.inf, 1e-06, False, None, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747750977 (unix time) try "date -d @1747750977" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x574c) received by PID 22348 (TID 0x7f66bfd56740) from PID 22348 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 566: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), -1, 1e-06, True, None, )
W0520 22:25:57.608929 22728 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 22:25:57.609959 22728 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), -1, 1e-06, True, None, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747751194 (unix time) try "date -d @1747751194" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x58c8) received by PID 22728 (TID 0x7f66bfd56740) from PID 22728 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 569: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), math.inf, 1e-06, False, None, )
W0520 22:29:24.168694 22956 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 22:29:24.169677 22956 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), math.inf, 1e-06, False, None, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747751373 (unix time) try "date -d @1747751373" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x59ac) received by PID 22956 (TID 0x7f66bfd56740) from PID 22956 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 574: paddle.nn.functional.pairwise_distance(Tensor([1431655766, 3],"float16"), Tensor([1431655766, 3],"float16"), 2, 1e-06, False, None, )
W0520 22:32:19.133093 23111 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 22:32:19.134080 23111 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.pairwise_distance(Tensor([1431655766, 3],"float16"), Tensor([1431655766, 3],"float16"), 2, 1e-06, False, None, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747751679 (unix time) try "date -d @1747751679" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x5a47) received by PID 23111 (TID 0x7f66bfd56740) from PID 23111 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 580: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), -1, 1e-06, False, None, )
W0520 22:36:20.077754 23566 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 22:36:20.079435 23566 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), -1, 1e-06, False, None, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747751785 (unix time) try "date -d @1747751785" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x5c0e) received by PID 23566 (TID 0x7f66bfd56740) from PID 23566 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 586: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), -math.inf, 1e-06, False, None, )
W0520 22:38:12.786820 23940 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 22:38:12.788445 23940 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), -math.inf, 1e-06, False, None, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747751895 (unix time) try "date -d @1747751895" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x5d84) received by PID 23940 (TID 0x7f66bfd56740) from PID 23940 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 589: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), 1, 1e-06, False, None, )
W0520 22:40:35.626922 24170 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 22:40:35.627954 24170 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), 1, 1e-06, False, None, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747752045 (unix time) try "date -d @1747752045" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x5e6a) received by PID 24170 (TID 0x7f66bfd56740) from PID 24170 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 594: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), 2, 1e-06, True, None, )
W0520 22:43:39.068934 24548 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 22:43:39.069949 24548 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), 2, 1e-06, True, None, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747752229 (unix time) try "date -d @1747752229" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x5fe4) received by PID 24548 (TID 0x7f66bfd56740) from PID 24548 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 598: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), -math.inf, 1e-06, True, None, )
W0520 22:46:51.542470 24852 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 22:46:51.543504 24852 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), -math.inf, 1e-06, True, None, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747752424 (unix time) try "date -d @1747752424" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x6114) received by PID 24852 (TID 0x7f66bfd56740) from PID 24852 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 604: paddle.nn.functional.pairwise_distance(x=Tensor([22817014, 100],"float32"), y=Tensor([22817014, 1],"float32"), p=2.0, epsilon=1e-06, keepdim=False, )
W0520 22:48:25.466557 25308 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 22:48:25.467581 25308 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.nn.functional.pairwise_distance(x=Tensor([22817014, 100],"float32"), y=Tensor([22817014, 1],"float32"), p=2.0, epsilon=1e-06, keepdim=False, )
[Worker 1] Completed Task 604
[Worker 1] Processing Task 607: paddle.nn.functional.rrelu(Tensor([2, 57042535, 4, 5],"float32"), 0.1, 0.3, training=False, )
[cuda error] paddle.nn.functional.rrelu(Tensor([2, 57042535, 4, 5],"float32"), 0.1, 0.3, training=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1388)

[Worker 1] Completed Task 607
[Worker 1] Processing Task 610: paddle.std(Tensor([1, 3, 143165577, 10],"float16"), 2, True, False, )
W0520 22:52:37.079269 25308 dygraph_functions.cc:85067] got different data type, run type promotion automatically, this may cause data type been changed.
W0520 22:52:37.079511 25308 dygraph_functions.cc:87101] got different data type, run type promotion automatically, this may cause data type been changed.
W0520 22:52:37.081926 25308 dygraph_functions.cc:82416] got different data type, run type promotion automatically, this may cause data type been changed.
[Pass] paddle.std(Tensor([1, 3, 143165577, 10],"float16"), 2, True, False, )
[Worker 1] Completed Task 610
[Worker 1] Processing Task 614: paddle.std(Tensor([35791395, 3, 4, 10],"float16"), 2, True, False, )
[accuracy error] backward  paddle.std(Tensor([35791395, 3, 4, 10],"float16"), 2, True, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 33 / 4294967400 (7.68e-07%)
Max absolute difference: 0.0216
Max relative difference: 288.
 x: array([[[[-1.0529e-02,  8.1665e-02, -7.6233e-02, ..., -5.1758e-02,
           7.0923e-02, -1.2497e-02],
         [-7.4768e-02,  7.2510e-02, -1.6125e-01, ..., -7.6233e-02,...
 y: array([[[[-1.0490e-02,  8.1665e-02, -7.6233e-02, ..., -5.1788e-02,
           7.0923e-02, -1.2497e-02],
         [-7.4707e-02,  7.2510e-02, -1.6125e-01, ..., -7.6294e-02,...
[Worker 1] Completed Task 614
[Worker 1] Processing Task 619: paddle.std(x=Tensor([3, 3, 477218589],"float16"), axis=0, )
[accuracy error] backward  paddle.std(x=Tensor([3, 3, 477218589],"float16"), axis=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[-0.02693 ,  0.1918  , -0.00625 , ..., -0.2076  ,  0.01854 ,
         -0.1865  ],
        [-0.0418  ,  0.05502 , -0.10583 , ...,  0.01968 , -0.1032  ,...
 y: array([[[-0.02695 ,  0.1918  , -0.006252, ..., -0.2076  ,  0.01851 ,
         -0.1865  ],
        [-0.04178 ,  0.05502 , -0.1059  , ...,  0.01968 , -0.1034  ,...
[Worker 1] Completed Task 619
[Worker 1] Processing Task 620: paddle.std(x=Tensor([3, 3, 477218589],"float16"), axis=0, unbiased=False, )
[accuracy error] backward  paddle.std(x=Tensor([3, 3, 477218589],"float16"), axis=0, unbiased=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[ 0.133   , -0.02933 ,  0.05383 , ...,  0.1843  ,  0.08704 ,
         -0.1108  ],
        [ 0.1584  ,  0.02226 , -0.005657, ..., -0.1622  , -0.1616  ,...
 y: array([[[ 0.133   , -0.02933 ,  0.05383 , ...,  0.1843  ,  0.08704 ,
         -0.1108  ],
        [ 0.1584  ,  0.02226 , -0.00566 , ..., -0.1622  , -0.1616  ,...
[Worker 1] Completed Task 620
[Worker 1] Processing Task 624: paddle.std(x=Tensor([3, 477218589, 3],"float16"), axis=0, )
[accuracy error] backward  paddle.std(x=Tensor([3, 477218589, 3],"float16"), axis=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[ 1.0950e-01, -3.0624e-02, -1.0779e-01],
        [-2.6627e-02, -1.1267e-01,  5.8197e-02],
        [-1.2128e-01,  1.4258e-01, -1.2769e-01],...
 y: array([[[ 1.0950e-01, -3.0624e-02, -1.0779e-01],
        [-2.6642e-02, -1.1261e-01,  5.8319e-02],
        [-1.2128e-01,  1.4246e-01, -1.2769e-01],...
[Worker 1] Completed Task 624
[Worker 1] Processing Task 625: paddle.std(x=Tensor([3, 477218589, 3],"float16"), axis=0, unbiased=False, )
[accuracy error] backward  paddle.std(x=Tensor([3, 477218589, 3],"float16"), axis=0, unbiased=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[ 0.1793  ,  0.0751  , -0.04013 ],
        [ 0.04062 ,  0.12366 ,  0.1259  ],
        [-0.1945  ,  0.003944,  0.0988  ],...
 y: array([[[ 0.1793  ,  0.0751  , -0.04013 ],
        [ 0.04065 ,  0.1237  ,  0.1257  ],
        [-0.1945  ,  0.00395 ,  0.0988  ],...
[Worker 1] Completed Task 625
[Worker 1] Processing Task 627: paddle.std(x=Tensor([3, 477218589, 3],"float16"), axis=tuple(0,1,), )
[Pass] paddle.std(x=Tensor([3, 477218589, 3],"float16"), axis=tuple(0,1,), )
[Worker 1] Completed Task 627
[Worker 1] Processing Task 631: paddle.std(x=Tensor([477218589, 3, 3],"float16"), axis=list[0,1,], )
[Pass] paddle.std(x=Tensor([477218589, 3, 3],"float16"), axis=list[0,1,], )
[Worker 1] Completed Task 631
[Worker 1] Processing Task 635: paddle.sum(Tensor([1, 2281701379],"int64"), )
element 0 of tensors does not require grad and does not have a grad_fn
[Pass] paddle.sum(Tensor([1, 2281701379],"int64"), )
[Worker 1] Completed Task 635
[Worker 1] Processing Task 637: paddle.sum(Tensor([2, 2147483649],"float16"), 1, keepdim=False, dtype=None, )
[Pass] paddle.sum(Tensor([2, 2147483649],"float16"), 1, keepdim=False, dtype=None, )
[Worker 1] Completed Task 637
[Worker 1] Processing Task 642: paddle.take_along_axis(Tensor([2, 302, 768],"bfloat16"), axis=1, indices=Tensor([2, 1485483, 768],"int64"), )
One of the differentiated Tensors does not require grad
[Pass] paddle.take_along_axis(Tensor([2, 302, 768],"bfloat16"), axis=1, indices=Tensor([2, 1485483, 768],"int64"), )
[Worker 1] Completed Task 642
[Worker 1] Processing Task 646: paddle.Tensor.amax(Tensor([3, 2, 76056713, 5],"float32"), axis=-1, keepdim=True, )
[Pass] paddle.Tensor.amax(Tensor([3, 2, 76056713, 5],"float32"), axis=-1, keepdim=True, )
[Worker 1] Completed Task 646
[Worker 1] Processing Task 651: paddle.Tensor.amax(Tensor([3, 38028357, 5, 4],"float32"), axis=tuple(1,2,), keepdim=True, )
[Pass] paddle.Tensor.amax(Tensor([3, 38028357, 5, 4],"float32"), axis=tuple(1,2,), keepdim=True, )
[Worker 1] Completed Task 651
[Worker 1] Processing Task 654: paddle.Tensor.amax(Tensor([57042535, 2, 5, 4],"float32"), axis=tuple(1,2,), keepdim=True, )
[Pass] paddle.Tensor.amax(Tensor([57042535, 2, 5, 4],"float32"), axis=tuple(1,2,), keepdim=True, )
[Worker 1] Completed Task 654
[Worker 1] Processing Task 659: paddle.Tensor.amin(Tensor([3, 38028357, 4, 5],"float32"), axis=-1, keepdim=True, )
[Pass] paddle.Tensor.amin(Tensor([3, 38028357, 4, 5],"float32"), axis=-1, keepdim=True, )
[Worker 1] Completed Task 659
[Worker 1] Processing Task 663: paddle.Tensor.amin(Tensor([57042535, 2, 5, 4],"float32"), axis=2, keepdim=True, )
[Pass] paddle.Tensor.amin(Tensor([57042535, 2, 5, 4],"float32"), axis=2, keepdim=True, )
[Worker 1] Completed Task 663
[Worker 1] Processing Task 668: paddle.Tensor.argmax(Tensor([4, 285212673, 2],"float32"), axis=-1, )
[cuda error] paddle.Tensor.argmax(Tensor([4, 285212673, 2],"float32"), axis=-1, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747757468 (unix time) try "date -d @1747757468" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x62dc) received by PID 25308 (TID 0x7f66bfd56740) from PID 25308 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 673: paddle.Tensor.argmax(Tensor([93991, 1, 24276],"float32"), axis=-2, )
W0521 00:12:17.346709 25689 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0521 00:12:17.347517 25689 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.Tensor.argmax(Tensor([93991, 1, 24276],"float32"), axis=-2, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747757538 (unix time) try "date -d @1747757538" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x6459) received by PID 25689 (TID 0x7f66bfd56740) from PID 25689 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 677: paddle.Tensor.cumsum(Tensor([1, 12, 190141782],"float32"), 1, )
W0521 00:13:32.752560 25915 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0521 00:13:32.753567 25915 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.Tensor.cumsum(Tensor([1, 12, 190141782],"float32"), 1, )
[Worker 1] Completed Task 677
[Worker 1] Processing Task 680: paddle.Tensor.cumsum(Tensor([1, 15845149, 144],"float32"), 1, )
[accuracy error] paddle.Tensor.cumsum(Tensor([1, 15845149, 144],"float32"), 1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2831965 / 2281701456 (0.124%)
Max absolute difference: 0.40478516
Max relative difference: 26746.656
 x: array([[[-3.475469e-01, -4.771953e-02, -4.874659e-01, ...,
         -2.354844e-01, -1.936555e-01,  3.684275e-01],
        [-2.815731e-01, -2.993821e-01, -2.926146e-01, ...,...
 y: array([[[-3.475469e-01, -4.771953e-02, -4.874659e-01, ...,
         -2.354844e-01, -1.936555e-01,  3.684275e-01],
        [-2.815731e-01, -2.993821e-01, -2.926146e-01, ...,...
[Worker 1] Completed Task 680
[Worker 1] Processing Task 684: paddle.Tensor.cumsum(Tensor([1, 192, 11883862],"float32"), 1, )
[Pass] paddle.Tensor.cumsum(Tensor([1, 192, 11883862],"float32"), 1, )
[Worker 1] Completed Task 684
[Worker 1] Processing Task 689: paddle.Tensor.cumsum(Tensor([1, 285212673, 8],"float32"), 2, )
[Pass] paddle.Tensor.cumsum(Tensor([1, 285212673, 8],"float32"), 2, )
[Worker 1] Completed Task 689
[Worker 1] Processing Task 691: paddle.Tensor.cumsum(Tensor([1, 91268056, 25],"float32"), 2, )
[Pass] paddle.Tensor.cumsum(Tensor([1, 91268056, 25],"float32"), 2, )
[Worker 1] Completed Task 691
[Worker 1] Processing Task 694: paddle.Tensor.cumsum(Tensor([162978670, 14],"int32"), -1, )
element 0 of tensors does not require grad and does not have a grad_fn
[Pass] paddle.Tensor.cumsum(Tensor([162978670, 14],"int32"), -1, )
[Worker 1] Completed Task 694
[Worker 1] Processing Task 698: paddle.Tensor.cumsum(Tensor([285212673, 4, 2],"int64"), axis=0, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.cumsum(Tensor([285212673, 4, 2],"int64"), axis=0, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.610352GB memory has been allocated and available memory is only 9.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 1] Completed Task 698
[Worker 1] Processing Task 704: paddle.Tensor.cumsum(Tensor([3, 380283564, 2],"int64"), axis=1, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.cumsum(Tensor([3, 380283564, 2],"int64"), axis=1, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.610352GB memory has been allocated and available memory is only 9.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 1] Completed Task 704
[Worker 1] Processing Task 709: paddle.Tensor.cumsum(Tensor([5, 456340276],"int64"), axis=0, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.cumsum(Tensor([5, 456340276],"int64"), axis=0, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.610352GB memory has been allocated and available memory is only 9.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 1] Completed Task 709
[Worker 1] Processing Task 713: paddle.Tensor.cumsum(Tensor([570425345, 4],"int64"), axis=1, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.cumsum(Tensor([570425345, 4],"int64"), axis=1, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.610352GB memory has been allocated and available memory is only 9.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 1] Completed Task 713
[Worker 1] Processing Task 714: paddle.Tensor.cumsum(Tensor([760567127, 3],"float32"), axis=-1, )
[Pass] paddle.Tensor.cumsum(Tensor([760567127, 3],"float32"), axis=-1, )
[Worker 1] Completed Task 714
[Worker 1] Processing Task 718: paddle.Tensor.cumsum(Tensor([82527, 192, 144],"float32"), 2, )
[Pass] paddle.Tensor.cumsum(Tensor([82527, 192, 144],"float32"), 2, )
[Worker 1] Completed Task 718
[Worker 1] Processing Task 722: paddle.Tensor.diff(x=Tensor([67108865, 4, 4, 4],"float16"), axis=2, )
[Pass] paddle.Tensor.diff(x=Tensor([67108865, 4, 4, 4],"float16"), axis=2, )
[Worker 1] Completed Task 722
[Worker 1] Processing Task 726: paddle.trapezoid(Tensor([2281701379],"float32"), dx=2.0, )
[paddle error] paddle.trapezoid(Tensor([2281701379],"float32"), dx=2.0, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at /paddle/paddle/phi/core/dense_tensor.cc:113)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747759908 (unix time) try "date -d @1747759908" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x653b) received by PID 25915 (TID 0x7f66bfd56740) from PID 25915 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 727: paddle.trapezoid(y=Tensor([1073741825, 4],"float16"), x=Tensor([1073741825, 4],"float16"), )
W0521 00:54:44.323302 26070 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0521 00:54:44.324267 26070 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SliceGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::slice_grad(paddle::Tensor const&, paddle::Tensor const&, std::vector<long, std::allocator<long> > const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, paddle::Tensor*)
4   void phi::SliceGradStridedKernel<phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, std::vector<long, std::allocator<long> > const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, std::vector<long, std::allocator<long> > const&, std::vector<long, std::allocator<long> > const&, phi::DenseTensor*)
5   phi::DeviceContext::Alloc(phi::TensorBase*, phi::DataType, unsigned long, bool, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
9   paddle::memory::allocation::RetryAllocator::AllocateImpl(unsigned long)
10  paddle::memory::allocation::StreamSafeCUDAAllocator::AllocateImpl(unsigned long)
11  paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
12  paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747760187 (unix time) try "date -d @1747760187" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x65d6) received by PID 26070 (TID 0x7f66bfd56740) from PID 26070 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 731: paddle.trunc(input=Tensor([3, 6, 6, 6628036, 6],"float16"), )
W0521 00:59:26.316223 26221 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0521 00:59:26.317301 26221 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.trunc(input=Tensor([3, 6, 6, 6628036, 6],"float16"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747760367 (unix time) try "date -d @1747760367" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x666d) received by PID 26221 (TID 0x7f66bfd56740) from PID 26221 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 734: paddle.trunc(input=Tensor([3314018, 6, 6, 6, 6],"float16"), )
W0521 01:02:24.956382 26448 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0521 01:02:24.957324 26448 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.trunc(input=Tensor([3314018, 6, 6, 6, 6],"float16"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747760546 (unix time) try "date -d @1747760546" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x6750) received by PID 26448 (TID 0x7f66bfd56740) from PID 26448 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 738: paddle.trunc(input=Tensor([6, 380283564],"int32"), )
element 0 of tensors does not require grad and does not have a grad_fn
W0521 01:03:44.843242 26750 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0521 01:03:44.844213 26750 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.trunc(input=Tensor([6, 380283564],"int32"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

[Worker 1] Error on Task 738: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[Worker 1] OOM on Task 738: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 739: paddle.trunc(input=Tensor([6, 6, 119304648],"float16"), )
W0521 01:06:06.489102 26827 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0521 01:06:06.490100 26827 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.trunc(input=Tensor([6, 6, 119304648],"float16"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747760767 (unix time) try "date -d @1747760767" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x68cb) received by PID 26827 (TID 0x7f66bfd56740) from PID 26827 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 742: paddle.trunc(Tensor([10, 20, 11408507],"float32"), )
W0521 01:07:55.751973 27055 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0521 01:07:55.752974 27055 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.trunc(Tensor([10, 20, 11408507],"float32"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747760877 (unix time) try "date -d @1747760877" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x69af) received by PID 27055 (TID 0x7f66bfd56740) from PID 27055 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 746: paddle.trunc(Tensor([20, 114085069],"float32"), )
W0521 01:09:41.442435 27357 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0521 01:09:41.443421 27357 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.trunc(Tensor([20, 114085069],"float32"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747760982 (unix time) try "date -d @1747760982" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x6add) received by PID 27357 (TID 0x7f66bfd56740) from PID 27357 ***]

[Worker 1] Started on GPU 1
[Worker 1] Processing Task 748: paddle.var(Tensor([1, 107374183, 4, 10],"float16"), 2, True, False, )
W0521 01:11:25.077777 27511 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0521 01:11:25.078756 27511 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
W0521 01:11:25.136936 27511 dygraph_functions.cc:85067] got different data type, run type promotion automatically, this may cause data type been changed.
W0521 01:11:25.137158 27511 dygraph_functions.cc:87101] got different data type, run type promotion automatically, this may cause data type been changed.
W0521 01:11:25.139508 27511 dygraph_functions.cc:82416] got different data type, run type promotion automatically, this may cause data type been changed.
[Pass] paddle.var(Tensor([1, 107374183, 4, 10],"float16"), 2, True, False, )
[Worker 1] Completed Task 748
[Worker 1] Processing Task 754: paddle.var(Tensor([16, 142606337, 1, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Pass] paddle.var(Tensor([16, 142606337, 1, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Worker 1] Completed Task 754
[Worker 1] Processing Task 755: paddle.var(Tensor([16, 268435457],"float16"), axis=-1, keepdim=True, )
[Pass] paddle.var(Tensor([16, 268435457],"float16"), axis=-1, keepdim=True, )
[Worker 1] Completed Task 755
[Worker 1] Processing Task 760: paddle.var(Tensor([192, 11883862, 1, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Pass] paddle.var(Tensor([192, 11883862, 1, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Worker 1] Completed Task 760
[Worker 1] Processing Task 763: paddle.var(Tensor([192, 96, 1, 123791],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Pass] paddle.var(Tensor([192, 96, 1, 123791],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Worker 1] Completed Task 763
[Worker 1] Processing Task 766: paddle.var(Tensor([35791395, 3, 4, 10],"float16"), list[1,2,], True, False, )
[Pass] paddle.var(Tensor([35791395, 3, 4, 10],"float16"), list[1,2,], True, False, )
[Worker 1] Completed Task 766
[Worker 1] Processing Task 770: paddle.var(x=Tensor([13, 175515491, 1],"float32"), axis=tuple(1,), keepdim=True, unbiased=False, )
[Pass] paddle.var(x=Tensor([13, 175515491, 1],"float32"), axis=tuple(1,), keepdim=True, unbiased=False, )
[Worker 1] Completed Task 770
[Worker 1] Processing Task 771: paddle.var(x=Tensor([16, 142606337, 1],"float32"), axis=tuple(1,), keepdim=True, unbiased=False, )
[Pass] paddle.var(x=Tensor([16, 142606337, 1],"float32"), axis=tuple(1,), keepdim=True, unbiased=False, )
[Worker 1] Completed Task 771
[Worker 1] Processing Task 773: paddle.var(x=Tensor([2, 570425345, 2],"float32"), axis=tuple(1,), keepdim=True, unbiased=False, )
[Pass] paddle.var(x=Tensor([2, 570425345, 2],"float32"), axis=tuple(1,), keepdim=True, unbiased=False, )
[Worker 1] Completed Task 773
[Worker 1] Processing Task 775: paddle.var(x=Tensor([3, 3, 477218589],"float16"), axis=0, unbiased=False, )
[Pass] paddle.var(x=Tensor([3, 3, 477218589],"float16"), axis=0, unbiased=False, )
[Worker 1] Completed Task 775
[Worker 1] Processing Task 779: paddle.var(x=Tensor([3, 477218589, 3],"float16"), axis=0, )
[Pass] paddle.var(x=Tensor([3, 477218589, 3],"float16"), axis=0, )
[Worker 1] Completed Task 779
[Worker 1] Processing Task 783: paddle.var(x=Tensor([3, 477218589, 3],"float16"), axis=tuple(0,1,), keepdim=True, )
[Pass] paddle.var(x=Tensor([3, 477218589, 3],"float16"), axis=tuple(0,1,), keepdim=True, )
[Worker 1] Completed Task 783
[Worker 1] Processing Task 787: paddle.var(x=Tensor([477218589, 3, 3],"float16"), axis=tuple(0,1,), )
[Pass] paddle.var(x=Tensor([477218589, 3, 3],"float16"), axis=tuple(0,1,), )
[Worker 1] Completed Task 787
[Worker 1] Received stop signal.
