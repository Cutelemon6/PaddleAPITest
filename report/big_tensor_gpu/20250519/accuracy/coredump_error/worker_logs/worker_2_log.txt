[Worker 2] Started on GPU 2
[Worker 2] Processing Task 2: paddle.amax(Tensor([10, 10, 22817014],"float32"), axis=list[0,1,], keepdim=False, )
W0520 11:11:23.089002  8632 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 11:11:23.089983  8632 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.amax(Tensor([10, 10, 22817014],"float32"), axis=list[0,1,], keepdim=False, )
[Worker 2] Completed Task 2
[Worker 2] Processing Task 6: paddle.amax(Tensor([3, 2, 4, 95070891],"float32"), axis=-1, keepdim=True, )
[Pass] paddle.amax(Tensor([3, 2, 4, 95070891],"float32"), axis=-1, keepdim=True, )
[Worker 2] Completed Task 6
[Worker 2] Processing Task 10: paddle.amax(Tensor([3, 38028357, 4, 5],"float32"), axis=-1, keepdim=True, )
[Pass] paddle.amax(Tensor([3, 38028357, 4, 5],"float32"), axis=-1, keepdim=True, )
[Worker 2] Completed Task 10
[Worker 2] Processing Task 14: paddle.amax(Tensor([57042535, 2, 5, 4],"float32"), axis=2, keepdim=True, )
[Pass] paddle.amax(Tensor([57042535, 2, 5, 4],"float32"), axis=2, keepdim=True, )
[Worker 2] Completed Task 14
[Worker 2] Processing Task 18: paddle.amin(Tensor([22817014, 10, 10],"float32"), axis=list[0,1,], keepdim=False, )
[Pass] paddle.amin(Tensor([22817014, 10, 10],"float32"), axis=list[0,1,], keepdim=False, )
[Worker 2] Completed Task 18
[Worker 2] Processing Task 22: paddle.amin(Tensor([3, 2, 95070891, 4],"float32"), axis=2, keepdim=True, )
[Pass] paddle.amin(Tensor([3, 2, 95070891, 4],"float32"), axis=2, keepdim=True, )
[Worker 2] Completed Task 22
[Worker 2] Processing Task 26: paddle.amin(Tensor([3, 38028357, 5, 4],"float32"), axis=tuple(1,2,), keepdim=True, )
[Pass] paddle.amin(Tensor([3, 38028357, 5, 4],"float32"), axis=tuple(1,2,), keepdim=True, )
[Worker 2] Completed Task 26
[Worker 2] Processing Task 30: paddle.any(Tensor([2, 1140850690],"bool"), list[0,], )
[Pass] paddle.any(Tensor([2, 1140850690],"bool"), list[0,], )
[Worker 2] Completed Task 30
[Worker 2] Processing Task 32: paddle.argmax(Tensor([1, 2281701379],"float32"), axis=-1, )
[Pass] paddle.argmax(Tensor([1, 2281701379],"float32"), axis=-1, )
[Worker 2] Completed Task 32
[Worker 2] Processing Task 35: paddle.argmax(Tensor([13, 1371215, 4, 16, 2],"float32"), axis=-1, )
[cuda error] paddle.argmax(Tensor([13, 1371215, 4, 16, 2],"float32"), axis=-1, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747711396 (unix time) try "date -d @1747711396" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x21b8) received by PID 8632 (TID 0x7f66bfd56740) from PID 8632 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 40: paddle.argmax(Tensor([228170138, 10],"float32"), )
W0520 11:24:09.922510  8936 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 11:24:09.923249  8936 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.argmax(Tensor([228170138, 10],"float32"), )
[Worker 2] Completed Task 40
[Worker 2] Processing Task 43: paddle.argmax(Tensor([3, 3, 3, 17674763, 3, 3],"float16"), axis=0, )
[cuda error] paddle.argmax(Tensor([3, 3, 3, 17674763, 3, 3],"float16"), axis=0, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747711524 (unix time) try "date -d @1747711524" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x22e8) received by PID 8936 (TID 0x7f66bfd56740) from PID 8936 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 46: paddle.argmax(Tensor([3, 760567127],"float32"), )
W0520 11:26:02.213624  9308 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 11:26:02.214346  9308 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.argmax(Tensor([3, 760567127],"float32"), )
[Worker 2] Completed Task 46
[Worker 2] Processing Task 48: paddle.argmax(Tensor([357913942, 3, 4],"float16"), axis=-1, keepdim=True, )
[cuda error] paddle.argmax(Tensor([357913942, 3, 4],"float16"), axis=-1, keepdim=True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747711634 (unix time) try "date -d @1747711634" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x245c) received by PID 9308 (TID 0x7f66bfd56740) from PID 9308 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 53: paddle.argmax(Tensor([456340276, 5],"float32"), )
W0520 11:28:10.820431  9609 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 11:28:10.821152  9609 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.argmax(Tensor([456340276, 5],"float32"), )
[Worker 2] Completed Task 53
[Worker 2] Processing Task 55: paddle.argmax(Tensor([8912897, 2, 4, 16, 2],"float32"), axis=-1, )
[cuda error] paddle.argmax(Tensor([8912897, 2, 4, 16, 2],"float32"), axis=-1, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747711729 (unix time) try "date -d @1747711729" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2589) received by PID 9609 (TID 0x7f66bfd56740) from PID 9609 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 58: paddle.argmax(x=Tensor([3, 3, 477218589],"float16"), axis=1, keepdim=False, )
W0520 11:30:13.767267  9910 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 11:30:13.768092  9910 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.argmax(x=Tensor([3, 3, 477218589],"float16"), axis=1, keepdim=False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747711815 (unix time) try "date -d @1747711815" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x26b6) received by PID 9910 (TID 0x7f66bfd56740) from PID 9910 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 63: paddle.argmin(Tensor([2281701379],"float32"), 0, )
W0520 11:30:54.692349 10060 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 11:30:54.693149 10060 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.argmin(Tensor([2281701379],"float32"), 0, )
[Worker 2] Completed Task 63
[Worker 2] Processing Task 67: paddle.argmin(Tensor([3, 3, 3, 17674763, 3, 3],"float16"), axis=0, )
[cuda error] paddle.argmin(Tensor([3, 3, 3, 17674763, 3, 3],"float16"), axis=0, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747711937 (unix time) try "date -d @1747711937" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x274c) received by PID 10060 (TID 0x7f66bfd56740) from PID 10060 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 71: paddle.argmin(Tensor([3, 760567127],"float32"), keepdim=True, )
W0520 11:32:56.306238 10358 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 11:32:56.306972 10358 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.argmin(Tensor([3, 760567127],"float32"), keepdim=True, )
[Worker 2] Completed Task 71
[Worker 2] Processing Task 74: paddle.argmin(Tensor([4, 4, 4, 16777217, 4],"float16"), axis=0, )
[cuda error] paddle.argmin(Tensor([4, 4, 4, 16777217, 4],"float16"), axis=0, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747712048 (unix time) try "date -d @1747712048" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2876) received by PID 10358 (TID 0x7f66bfd56740) from PID 10358 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 78: paddle.argmin(x=Tensor([2281701379],"int64"), axis=-1, keepdim=True, )
W0520 11:34:46.159513 10659 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 11:34:46.160210 10659 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.argmin(x=Tensor([2281701379],"int64"), axis=-1, keepdim=True, )
[Worker 2] Completed Task 78
[Worker 2] Processing Task 80: paddle.argmin(x=Tensor([3, 3, 477218589],"float16"), axis=1, keepdim=False, )
[cuda error] paddle.argmin(x=Tensor([3, 3, 477218589],"float16"), axis=1, keepdim=False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747712159 (unix time) try "date -d @1747712159" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x29a3) received by PID 10659 (TID 0x7f66bfd56740) from PID 10659 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 85: paddle.broadcast_to(Tensor([10140896, 1, 15, 15],"bool"), list[10,8,15,15,], )
[torch error] paddle.broadcast_to(Tensor([10140896, 1, 15, 15],"bool"), list[10,8,15,15,], )
Traceback (most recent call last):
  File "/luozeyu01_api_test/PaddleAPITest/tester/accuracy.py", line 84, in test
    exec(code.core_compiled, exec_globals, exec_locals)
  File "<string>", line 1, in <module>
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
RuntimeError: The expanded size of the tensor (10) must match the existing size (10140896) at non-singleton dimension 0.  Target sizes: [10, 8, 15, 15].  Tensor sizes: [10140896, 1, 15, 15]
W0520 11:36:45.853257 10884 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 11:36:45.854022 10884 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Worker 2] Completed Task 85
[Worker 2] Processing Task 87: paddle.cdist(Tensor([300, 7605672],"float32"), Tensor([1, 7605672],"float32"), p=1, )
[Pass] paddle.cdist(Tensor([300, 7605672],"float32"), Tensor([1, 7605672],"float32"), p=1, )
[Worker 2] Completed Task 87
[Worker 2] Processing Task 90: paddle.copysign(Tensor([10, 429496730],"float16"), Tensor([10, 429496730],"float16"), )
[cuda error] paddle.copysign(Tensor([10, 429496730],"float16"), Tensor([10, 429496730],"float16"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747712920 (unix time) try "date -d @1747712920" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2a84) received by PID 10884 (TID 0x7f66bfd56740) from PID 10884 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 95: paddle.copysign(Tensor([12, 20, 9507090],"float32"), Tensor([12, 20, 9507090],"float32"), )
W0520 11:51:07.645200 11267 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 11:51:07.646317 11267 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.copysign(Tensor([12, 20, 9507090],"float32"), Tensor([12, 20, 9507090],"float32"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747713141 (unix time) try "date -d @1747713141" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2c03) received by PID 11267 (TID 0x7f66bfd56740) from PID 11267 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 97: paddle.copysign(Tensor([1203073, 17, 5, 6, 7],"float16"), Tensor([1203073, 17, 5, 6, 7],"float16"), )
W0520 11:56:36.379266 11419 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 11:56:36.380538 11419 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.copysign(Tensor([1203073, 17, 5, 6, 7],"float16"), Tensor([1203073, 17, 5, 6, 7],"float16"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747713816 (unix time) try "date -d @1747713816" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2c9b) received by PID 11419 (TID 0x7f66bfd56740) from PID 11419 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 101: paddle.copysign(Tensor([214748365, 20],"float16"), Tensor([214748365, 20],"float16"), )
W0520 12:07:46.289839 11723 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 12:07:46.290839 11723 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.copysign(Tensor([214748365, 20],"float16"), Tensor([214748365, 20],"float16"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747714485 (unix time) try "date -d @1747714485" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2dcb) received by PID 11723 (TID 0x7f66bfd56740) from PID 11723 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 107: paddle.copysign(Tensor([57042535, 20, 2],"float32"), Tensor([57042535, 20, 2],"float32"), )
W0520 12:16:34.574846 12177 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 12:16:34.575852 12177 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.copysign(Tensor([57042535, 20, 2],"float32"), Tensor([57042535, 20, 2],"float32"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747714652 (unix time) try "date -d @1747714652" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2f91) received by PID 12177 (TID 0x7f66bfd56740) from PID 12177 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 109: paddle.copysign(Tensor([8, 17, 5, 6, 1052689],"float16"), Tensor([8, 17, 5, 6, 1052689],"float16"), )
W0520 12:21:24.271116 12331 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 12:21:24.272069 12331 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.copysign(Tensor([8, 17, 5, 6, 1052689],"float16"), Tensor([8, 17, 5, 6, 1052689],"float16"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747715318 (unix time) try "date -d @1747715318" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x302b) received by PID 12331 (TID 0x7f66bfd56740) from PID 12331 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 113: paddle.count_nonzero(Tensor([2281701379],"float32"), axis=0, )
element 0 of tensors does not require grad and does not have a grad_fn
W0520 12:29:52.523286 12634 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 12:29:52.524219 12634 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.count_nonzero(Tensor([2281701379],"float32"), axis=0, )
[Worker 2] Completed Task 113
[Worker 2] Processing Task 114: paddle.cross(Tensor([760567127, 3],"float32"), Tensor([760567127, 3],"float32"), axis=1, )
[Pass] paddle.cross(Tensor([760567127, 3],"float32"), Tensor([760567127, 3],"float32"), axis=1, )
[Worker 2] Completed Task 114
[Worker 2] Processing Task 116: paddle.cross(x=Tensor([1431655766, 3],"float16"), y=Tensor([1431655766, 3],"float16"), axis=1, )
[Pass] paddle.cross(x=Tensor([1431655766, 3],"float16"), y=Tensor([1431655766, 3],"float16"), axis=1, )
[Worker 2] Completed Task 116
[Worker 2] Processing Task 120: paddle.cross(x=Tensor([3, 3, 477218589],"float16"), y=Tensor([3, 3, 477218589],"float16"), axis=0, )
[Pass] paddle.cross(x=Tensor([3, 3, 477218589],"float16"), y=Tensor([3, 3, 477218589],"float16"), axis=0, )
[Worker 2] Completed Task 120
[Worker 2] Processing Task 124: paddle.cross(x=Tensor([3, 760567127],"float32"), y=Tensor([3, 760567127],"float32"), )
/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py:104: UserWarning: Using torch.cross without specifying the dim arg is deprecated.
Please either pass the dim explicitly or simply use torch.linalg.cross.
The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at /pytorch/aten/src/ATen/native/Cross.cpp:62.)
  return func(*args, **kwargs)
[Pass] paddle.cross(x=Tensor([3, 760567127],"float32"), y=Tensor([3, 760567127],"float32"), )
[Worker 2] Completed Task 124
[Worker 2] Processing Task 128: paddle.cross(x=Tensor([760567127, 3],"float32"), y=Tensor([760567127, 3],"float32"), )
[Pass] paddle.cross(x=Tensor([760567127, 3],"float32"), y=Tensor([760567127, 3],"float32"), )
[Worker 2] Completed Task 128
[Worker 2] Processing Task 129: paddle.cross(x=Tensor([760567127, 3],"int32"), y=Tensor([760567127, 3],"int32"), )
element 0 of tensors does not require grad and does not have a grad_fn
[Pass] paddle.cross(x=Tensor([760567127, 3],"int32"), y=Tensor([760567127, 3],"int32"), )
[Worker 2] Completed Task 129
[Worker 2] Processing Task 130: paddle.cumsum(Tensor([1, 2281701379],"float32"), axis=0, )
[torch error] paddle.cumsum(Tensor([1, 2281701379],"float32"), axis=0, )
Traceback (most recent call last):
  File "/luozeyu01_api_test/PaddleAPITest/tester/accuracy.py", line 84, in test
    exec(code.core_compiled, exec_globals, exec_locals)
  File "<string>", line 1, in <module>
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
RuntimeError: CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[Worker 2] Error on Task 130: CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[Worker 2] OOM on Task 130: CUDA error: invalid configuration argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 131: paddle.cumsum(Tensor([114085069, 20],"int64"), axis=1, )
element 0 of tensors does not require grad and does not have a grad_fn
W0520 13:40:28.952098 12939 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 13:40:28.953079 12939 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.cumsum(Tensor([114085069, 20],"int64"), axis=1, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.610352GB memory has been allocated and available memory is only 9.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 2] Completed Task 131
[Worker 2] Processing Task 132: paddle.cumsum(Tensor([114085069, 20],"int64"), axis=-1, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.cumsum(Tensor([114085069, 20],"int64"), axis=-1, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.610352GB memory has been allocated and available memory is only 9.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 2] Completed Task 132
[Worker 2] Processing Task 133: paddle.cumsum(Tensor([1140850690, 2],"int64"), axis=-1, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.cumsum(Tensor([1140850690, 2],"int64"), axis=-1, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.610352GB memory has been allocated and available memory is only 9.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 2] Completed Task 133
[Worker 2] Processing Task 134: paddle.cumsum(Tensor([120, 19014179],"int32"), axis=0, )
element 0 of tensors does not require grad and does not have a grad_fn
[Pass] paddle.cumsum(Tensor([120, 19014179],"int32"), axis=0, )
[Worker 2] Completed Task 134
[Worker 2] Processing Task 135: paddle.cumsum(Tensor([16, 142606337],"int32"), axis=0, )
element 0 of tensors does not require grad and does not have a grad_fn
[Pass] paddle.cumsum(Tensor([16, 142606337],"int32"), axis=0, )
[Worker 2] Completed Task 135
[Worker 2] Processing Task 136: paddle.cumsum(Tensor([207427399, 11],"int64"), axis=-1, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.cumsum(Tensor([207427399, 11],"int64"), axis=-1, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.610352GB memory has been allocated and available memory is only 9.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 2] Completed Task 136
[Worker 2] Processing Task 137: paddle.cumsum(Tensor([21939437, 104],"int64"), 1, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.cumsum(Tensor([21939437, 104],"int64"), 1, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000001GB memory on GPU 0, 69.610352GB memory has been allocated and available memory is only 9.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 2] Completed Task 137
[Worker 2] Processing Task 139: paddle.cumsum(Tensor([22369622, 102],"int64"), 1, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.cumsum(Tensor([22369622, 102],"int64"), 1, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000001GB memory on GPU 0, 69.610352GB memory has been allocated and available memory is only 9.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 2] Completed Task 139
[Worker 2] Processing Task 141: paddle.cumsum(Tensor([22817014, 100],"int64"), 1, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.cumsum(Tensor([22817014, 100],"int64"), 1, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.610352GB memory has been allocated and available memory is only 9.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 2] Completed Task 141
[Worker 2] Processing Task 143: paddle.cumsum(Tensor([285212673, 2, 4],"float32"), axis=1, )
[Pass] paddle.cumsum(Tensor([285212673, 2, 4],"float32"), axis=1, )
[Worker 2] Completed Task 143
[Worker 2] Processing Task 146: paddle.cumsum(Tensor([3, 760567127],"int64"), axis=0, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.cumsum(Tensor([3, 760567127],"int64"), axis=0, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.610352GB memory has been allocated and available memory is only 9.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 2] Completed Task 146
[Worker 2] Processing Task 148: paddle.cumsum(Tensor([325957340, 7],"int32"), axis=1, )
element 0 of tensors does not require grad and does not have a grad_fn
[Pass] paddle.cumsum(Tensor([325957340, 7],"int32"), axis=1, )
[Worker 2] Completed Task 148
[Worker 2] Processing Task 152: paddle.cumsum(Tensor([51856850, 44],"int64"), axis=1, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.cumsum(Tensor([51856850, 44],"int64"), axis=1, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.610352GB memory has been allocated and available memory is only 9.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 2] Completed Task 152
[Worker 2] Processing Task 156: paddle.cumsum(Tensor([76057, 30000],"float32"), axis=-1, )
[Pass] paddle.cumsum(Tensor([76057, 30000],"float32"), axis=-1, )
[Worker 2] Completed Task 156
[Worker 2] Processing Task 158: paddle.cumsum(x=Tensor([1, 1431655766, 1, 3],"float16"), axis=3, )
[Pass] paddle.cumsum(x=Tensor([1, 1431655766, 1, 3],"float16"), axis=3, )
[Worker 2] Completed Task 158
[Worker 2] Processing Task 164: paddle.cumsum(x=Tensor([87382, 16, 96, 32],"float16"), axis=2, )
[accuracy error] paddle.cumsum(x=Tensor([87382, 16, 96, 32],"float16"), axis=2, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 115313 / 4295000064 (0.00268%)
Max absolute difference: 7.246
Max relative difference: 19504.
 x: array([[[[-7.7393e-02,  2.0251e-01,  1.7639e-01, ...,  4.3115e-01,
          -4.7729e-01,  4.9463e-01],
         [-3.5327e-01,  1.4185e-01, -1.5735e-01, ...,  9.2529e-02,...
 y: array([[[[-7.7393e-02,  2.0251e-01,  1.7639e-01, ...,  4.3115e-01,
          -4.7729e-01,  4.9463e-01],
         [-3.5327e-01,  1.4185e-01, -1.5735e-01, ...,  9.2529e-02,...
[Worker 2] Completed Task 164
[Worker 2] Processing Task 166: paddle.diag(x=Tensor([2, 2147483649],"float16"), offset=0, )
Warning: The core code of paddle.diag is too complex.
[Pass] paddle.diag(x=Tensor([2, 2147483649],"float16"), offset=0, )
[Worker 2] Completed Task 166
[Worker 2] Processing Task 169: paddle.diagonal(x=Tensor([29826162, 6, 6, 2, 2],"float16"), axis1=3, axis2=4, )
[Pass] paddle.diagonal(x=Tensor([29826162, 6, 6, 2, 2],"float16"), axis1=3, axis2=4, )
[Worker 2] Completed Task 169
[Worker 2] Processing Task 176: paddle.diff(x=Tensor([67108865, 4, 4, 4],"float16"), axis=-2, )
[Pass] paddle.diff(x=Tensor([67108865, 4, 4, 4],"float16"), axis=-2, )
[Worker 2] Completed Task 176
[Worker 2] Processing Task 182: paddle.dist(x=Tensor([2147483649, 2],"float16"), y=Tensor([2147483649, 2],"float16"), p=0, )
Expected a proper Tensor but got None (or an undefined Tensor in C++) for argument #0 'self'
[cuda error] paddle.dist(x=Tensor([2147483649, 2],"float16"), y=Tensor([2147483649, 2],"float16"), p=0, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

[Worker 2] Error on Task 182: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[Worker 2] OOM on Task 182: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 186: paddle.dist(x=Tensor([4294967297],"float16"), y=Tensor([4294967297],"float16"), )
W0520 15:07:04.123490 13466 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 15:07:04.124490 13466 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.dist(x=Tensor([4294967297],"float16"), y=Tensor([4294967297],"float16"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747724830 (unix time) try "date -d @1747724830" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x349a) received by PID 13466 (TID 0x7f66bfd56740) from PID 13466 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 193: paddle.isclose(x=Tensor([2281701379],"float32"), y=Tensor([2281701379],"float32"), )
W0520 15:08:46.607969 14000 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 15:08:46.608711 14000 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.isclose(x=Tensor([2281701379],"float32"), y=Tensor([2281701379],"float32"), )
[Worker 2] Completed Task 193
[Worker 2] Processing Task 198: paddle.isfinite(Tensor([13421773, 17, 10],"int32"), )
[Pass] paddle.isfinite(Tensor([13421773, 17, 10],"int32"), )
[Worker 2] Completed Task 198
[Worker 2] Processing Task 202: paddle.isfinite(Tensor([2, 3, 71582789, 5],"float64"), )
[Pass] paddle.isfinite(Tensor([2, 3, 71582789, 5],"float64"), )
[Worker 2] Completed Task 202
[Worker 2] Processing Task 204: paddle.isfinite(Tensor([2147483649],"int64"), )
[Pass] paddle.isfinite(Tensor([2147483649],"int64"), )
[Worker 2] Completed Task 204
[Worker 2] Processing Task 207: paddle.isfinite(Tensor([289, 280, 376, 25, 3],"float32"), )
[Pass] paddle.isfinite(Tensor([289, 280, 376, 25, 3],"float32"), )
[Worker 2] Completed Task 207
[Worker 2] Processing Task 210: paddle.isfinite(Tensor([4, 20228, 376, 25, 3],"float32"), )
[Pass] paddle.isfinite(Tensor([4, 20228, 376, 25, 3],"float32"), )
[Worker 2] Completed Task 210
[Worker 2] Processing Task 214: paddle.isfinite(Tensor([4, 94, 6068355],"float32"), )
[Pass] paddle.isfinite(Tensor([4, 94, 6068355],"float32"), )
[Worker 2] Completed Task 214
[Worker 2] Processing Task 218: paddle.isin(Tensor([285212673, 8],"int32"), Tensor([2, 3],"int32"), False, False, )
[Pass] paddle.isin(Tensor([285212673, 8],"int32"), Tensor([2, 3],"int32"), False, False, )
[Worker 2] Completed Task 218
[Worker 2] Processing Task 222: paddle.isin(Tensor([4, 570425345],"int64"), Tensor([2, 3],"int64"), False, False, )
[Pass] paddle.isin(Tensor([4, 570425345],"int64"), Tensor([2, 3],"int64"), False, False, )
[Worker 2] Completed Task 222
[Worker 2] Processing Task 226: paddle.isinf(Tensor([10186167, 7, 32],"float32"), )
[Pass] paddle.isinf(Tensor([10186167, 7, 32],"float32"), )
[Worker 2] Completed Task 226
[Worker 2] Processing Task 230: paddle.isinf(Tensor([11, 20742740, 10],"int32"), )
[Pass] paddle.isinf(Tensor([11, 20742740, 10],"int32"), )
[Worker 2] Completed Task 230
[Worker 2] Processing Task 234: paddle.isinf(Tensor([14, 10186167, 16],"float32"), )
[Pass] paddle.isinf(Tensor([14, 10186167, 16],"float32"), )
[Worker 2] Completed Task 234
[Worker 2] Processing Task 238: paddle.isinf(Tensor([14, 7, 23282668],"float32"), )
[Pass] paddle.isinf(Tensor([14, 7, 23282668],"float32"), )
[Worker 2] Completed Task 238
[Worker 2] Processing Task 242: paddle.isinf(Tensor([2, 1140850690],"float32"), )
[Pass] paddle.isinf(Tensor([2, 1140850690],"float32"), )
[Worker 2] Completed Task 242
[Worker 2] Processing Task 246: paddle.isinf(Tensor([2147483649],"float64"), )
[Pass] paddle.isinf(Tensor([2147483649],"float64"), )
[Worker 2] Completed Task 246
[Worker 2] Processing Task 250: paddle.isinf(Tensor([2281701379],"float32"), )
[Pass] paddle.isinf(Tensor([2281701379],"float32"), )
[Worker 2] Completed Task 250
[Worker 2] Processing Task 254: paddle.isinf(Tensor([4, 268435457, 2],"float64"), )
[Pass] paddle.isinf(Tensor([4, 268435457, 2],"float64"), )
[Worker 2] Completed Task 254
[Worker 2] Processing Task 257: paddle.isinf(Tensor([4294967295],"float32"), )
[Pass] paddle.isinf(Tensor([4294967295],"float32"), )
[Worker 2] Completed Task 257
[Worker 2] Processing Task 262: paddle.isnan(Tensor([1073741824, 4],"float32"), )
[Pass] paddle.isnan(Tensor([1073741824, 4],"float32"), )
[Worker 2] Completed Task 262
[Worker 2] Processing Task 269: paddle.isnan(Tensor([1393, 64, 160, 160],"float32"), )
[Pass] paddle.isnan(Tensor([1393, 64, 160, 160],"float32"), )
[Worker 2] Completed Task 269
[Worker 2] Processing Task 273: paddle.isnan(Tensor([14, 5093084, 32],"float32"), )
[Pass] paddle.isnan(Tensor([14, 5093084, 32],"float32"), )
[Worker 2] Completed Task 273
[Worker 2] Processing Task 277: paddle.isnan(Tensor([16, 134217729],"float64"), )
[Pass] paddle.isnan(Tensor([16, 134217729],"float64"), )
[Worker 2] Completed Task 277
[Worker 2] Processing Task 280: paddle.isnan(Tensor([1633, 3, 375, 1242],"float32"), )
[Pass] paddle.isnan(Tensor([1633, 3, 375, 1242],"float32"), )
[Worker 2] Completed Task 280
[Worker 2] Processing Task 283: paddle.isnan(Tensor([2, 3, 4, 89478486],"float64"), )
[Pass] paddle.isnan(Tensor([2, 3, 4, 89478486],"float64"), )
[Worker 2] Completed Task 283
[Worker 2] Processing Task 285: paddle.isnan(Tensor([2, 3, 71582789, 5],"float64"), )
[Pass] paddle.isnan(Tensor([2, 3, 71582789, 5],"float64"), )
[Worker 2] Completed Task 285
[Worker 2] Processing Task 289: paddle.isnan(Tensor([2, 536870913, 2],"float64"), )
[Pass] paddle.isnan(Tensor([2, 536870913, 2],"float64"), )
[Worker 2] Completed Task 289
[Worker 2] Processing Task 292: paddle.isnan(Tensor([2147483648, 2],"float32"), )
[Pass] paddle.isnan(Tensor([2147483648, 2],"float32"), )
[Worker 2] Completed Task 292
[Worker 2] Processing Task 299: paddle.isnan(Tensor([252645135, 17],"float32"), )
[Pass] paddle.isnan(Tensor([252645135, 17],"float32"), )
[Worker 2] Completed Task 299
[Worker 2] Processing Task 305: paddle.isnan(Tensor([3, 357913942, 2],"float64"), )
[Pass] paddle.isnan(Tensor([3, 357913942, 2],"float64"), )
[Worker 2] Completed Task 305
[Worker 2] Processing Task 307: paddle.isnan(Tensor([3, 4, 178956971],"float64"), )
[Pass] paddle.isnan(Tensor([3, 4, 178956971],"float64"), )
[Worker 2] Completed Task 307
[Worker 2] Processing Task 311: paddle.isnan(Tensor([33554433, 64],"float64"), )
[Pass] paddle.isnan(Tensor([33554433, 64],"float64"), )
[Worker 2] Completed Task 311
[Worker 2] Processing Task 315: paddle.isnan(Tensor([4, 1225, 375, 1242],"float32"), )
[Pass] paddle.isnan(Tensor([4, 1225, 375, 1242],"float32"), )
[Worker 2] Completed Task 315
[Worker 2] Processing Task 319: paddle.isnan(Tensor([4, 64, 160, 55706],"float32"), )
[Pass] paddle.isnan(Tensor([4, 64, 160, 55706],"float32"), )
[Worker 2] Completed Task 319
[Worker 2] Processing Task 323: paddle.isnan(Tensor([4294967295],"float32"), )
[Pass] paddle.isnan(Tensor([4294967295],"float32"), )
[Worker 2] Completed Task 323
[Worker 2] Processing Task 329: paddle.isnan(Tensor([536870913, 4],"float64"), )
[Pass] paddle.isnan(Tensor([536870913, 4],"float64"), )
[Worker 2] Completed Task 329
[Worker 2] Processing Task 330: paddle.isnan(Tensor([67108864, 64],"float32"), )
[Pass] paddle.isnan(Tensor([67108864, 64],"float32"), )
[Worker 2] Completed Task 330
[Worker 2] Processing Task 335: paddle.isneginf(Tensor([11, 20742740, 10],"int32"), )
[Pass] paddle.isneginf(Tensor([11, 20742740, 10],"int32"), )
[Worker 2] Completed Task 335
[Worker 2] Processing Task 339: paddle.isposinf(Tensor([11, 17, 12201612],"int32"), )
[Pass] paddle.isposinf(Tensor([11, 17, 12201612],"int32"), )
[Worker 2] Completed Task 339
[Worker 2] Processing Task 342: paddle.isposinf(Tensor([134217729, 17],"float32"), )
[Pass] paddle.isposinf(Tensor([134217729, 17],"float32"), )
[Worker 2] Completed Task 342
[Worker 2] Processing Task 346: paddle.linalg.cond(Tensor([2, 126761188, 3, 3],"float32"), None, )
[torch error] paddle.linalg.cond(Tensor([2, 126761188, 3, 3],"float32"), None, )
Traceback (most recent call last):
  File "/luozeyu01_api_test/PaddleAPITest/tester/accuracy.py", line 84, in test
    exec(code.core_compiled, exec_globals, exec_locals)
  File "<string>", line 1, in <module>
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[Worker 2] Error on Task 346: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[Worker 2] OOM on Task 346: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 349: paddle.linalg.cond(Tensor([63380594, 4, 3, 3],"float32"), -2, )
[torch error] paddle.linalg.cond(Tensor([63380594, 4, 3, 3],"float32"), -2, )
Traceback (most recent call last):
  File "/luozeyu01_api_test/PaddleAPITest/tester/accuracy.py", line 84, in test
    exec(code.core_compiled, exec_globals, exec_locals)
  File "<string>", line 1, in <module>
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[Worker 2] Error on Task 349: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[Worker 2] OOM on Task 349: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 356: paddle.linalg.matrix_rank(Tensor([19014179, 4, 5, 6],"float32"), Tensor([3, 4],"float32"), False, )
[torch error] paddle.linalg.matrix_rank(Tensor([19014179, 4, 5, 6],"float32"), Tensor([3, 4],"float32"), False, )
Traceback (most recent call last):
  File "/luozeyu01_api_test/PaddleAPITest/tester/accuracy.py", line 84, in test
    exec(code.core_compiled, exec_globals, exec_locals)
  File "<string>", line 1, in <module>
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py", line 104, in __torch_function__
    return func(*args, **kwargs)
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[Worker 2] Error on Task 356: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[Worker 2] OOM on Task 356: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 360: paddle.linalg.norm(Tensor([91268056, 5, 5],"float32"), p="fro", axis=list[0,1,], keepdim=True, )
W0520 15:58:49.028286 15125 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 15:58:49.029574 15125 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.linalg.norm(Tensor([91268056, 5, 5],"float32"), p="fro", axis=list[0,1,], keepdim=True, )
[Worker 2] Completed Task 360
[Worker 2] Processing Task 364: paddle.mean(Tensor([1, 1, 134217728, 32],"float16"), )
[Pass] paddle.mean(Tensor([1, 1, 134217728, 32],"float16"), )
[Worker 2] Completed Task 364
[Worker 2] Processing Task 368: paddle.mean(Tensor([1, 1, 8, 536870912],"float16"), )
[Pass] paddle.mean(Tensor([1, 1, 8, 536870912],"float16"), )
[Worker 2] Completed Task 368
[Worker 2] Processing Task 372: paddle.mean(Tensor([1, 2097152, 64, 32],"float16"), )
[Pass] paddle.mean(Tensor([1, 2097152, 64, 32],"float16"), )
[Worker 2] Completed Task 372
[Worker 2] Processing Task 376: paddle.mean(Tensor([1, 3, 1431655765, 1, 1],"float16"), )
[Pass] paddle.mean(Tensor([1, 3, 1431655765, 1, 1],"float16"), )
[Worker 2] Completed Task 376
[Worker 2] Processing Task 380: paddle.mean(Tensor([1, 4, 4, 268435456],"float16"), )
[Pass] paddle.mean(Tensor([1, 4, 4, 268435456],"float16"), )
[Worker 2] Completed Task 380
[Worker 2] Processing Task 384: paddle.mean(Tensor([1048576, 4, 32, 32],"float16"), )
[Pass] paddle.mean(Tensor([1048576, 4, 32, 32],"float16"), )
[Worker 2] Completed Task 384
[Worker 2] Processing Task 388: paddle.mean(Tensor([128, 524288, 64],"float16"), )
[Pass] paddle.mean(Tensor([128, 524288, 64],"float16"), )
[Worker 2] Completed Task 388
[Worker 2] Processing Task 393: paddle.mean(Tensor([1431655765, 3, 1],"float16"), )
[Pass] paddle.mean(Tensor([1431655765, 3, 1],"float16"), )
[Worker 2] Completed Task 393
[Worker 2] Processing Task 397: paddle.mean(Tensor([2, 1, 1, 2147483648, 1],"float16"), )
[Pass] paddle.mean(Tensor([2, 1, 1, 2147483648, 1],"float16"), )
[Worker 2] Completed Task 397
[Worker 2] Processing Task 402: paddle.mean(Tensor([2, 1, 536870912, 4],"float16"), )
[Pass] paddle.mean(Tensor([2, 1, 536870912, 4],"float16"), )
[Worker 2] Completed Task 402
[Worker 2] Processing Task 406: paddle.mean(Tensor([2, 2147483648, 1, 1],"float16"), )
[Pass] paddle.mean(Tensor([2, 2147483648, 1, 1],"float16"), )
[Worker 2] Completed Task 406
[Worker 2] Processing Task 410: paddle.mean(Tensor([2, 536870912, 4],"float16"), )
[Pass] paddle.mean(Tensor([2, 536870912, 4],"float16"), )
[Worker 2] Completed Task 410
[Worker 2] Processing Task 414: paddle.mean(Tensor([2147483648, 2],"float16"), )
[Pass] paddle.mean(Tensor([2147483648, 2],"float16"), )
[Worker 2] Completed Task 414
[Worker 2] Processing Task 418: paddle.mean(Tensor([3, 1, 1431655765],"float16"), )
[Pass] paddle.mean(Tensor([3, 1, 1431655765],"float16"), )
[Worker 2] Completed Task 418
[Worker 2] Processing Task 422: paddle.mean(Tensor([4, 1, 1, 1073741824, 1],"float16"), )
[Pass] paddle.mean(Tensor([4, 1, 1, 1073741824, 1],"float16"), )
[Worker 2] Completed Task 422
[Worker 2] Processing Task 426: paddle.mean(Tensor([4, 1, 1073741824],"float16"), )
[Pass] paddle.mean(Tensor([4, 1, 1073741824],"float16"), )
[Worker 2] Completed Task 426
[Worker 2] Processing Task 430: paddle.mean(Tensor([4, 1, 268435456, 2, 2],"float16"), )
[Pass] paddle.mean(Tensor([4, 1, 268435456, 2, 2],"float16"), )
[Worker 2] Completed Task 430
[Worker 2] Processing Task 434: paddle.mean(Tensor([4, 1073741824, 1],"float16"), )
[Pass] paddle.mean(Tensor([4, 1073741824, 1],"float16"), )
[Worker 2] Completed Task 434
[Worker 2] Processing Task 438: paddle.mean(Tensor([4, 4, 134217728, 2],"float16"), )
[Pass] paddle.mean(Tensor([4, 4, 134217728, 2],"float16"), )
[Worker 2] Completed Task 438
[Worker 2] Processing Task 442: paddle.mean(Tensor([4294967295, 1, 1, 1, 1],"float16"), )
[Pass] paddle.mean(Tensor([4294967295, 1, 1, 1, 1],"float16"), )
[Worker 2] Completed Task 442
[Worker 2] Processing Task 446: paddle.mean(Tensor([5, 858993459, 1],"float16"), )
[Pass] paddle.mean(Tensor([5, 858993459, 1],"float16"), )
[Worker 2] Completed Task 446
[Worker 2] Processing Task 450: paddle.mean(Tensor([536870912, 1, 2, 2, 2],"float16"), )
[Pass] paddle.mean(Tensor([536870912, 1, 2, 2, 2],"float16"), )
[Worker 2] Completed Task 450
[Worker 2] Processing Task 454: paddle.mean(Tensor([64, 67108864],"float16"), )
[Pass] paddle.mean(Tensor([64, 67108864],"float16"), )
[Worker 2] Completed Task 454
[Worker 2] Processing Task 458: paddle.mean(Tensor([858993459, 5],"float16"), )
[Pass] paddle.mean(Tensor([858993459, 5],"float16"), )
[Worker 2] Completed Task 458
[Worker 2] Processing Task 462: paddle.nansum(Tensor([1073741824, 4],"float32"), )
[Worker 2] Started on GPU 2
[Worker 2] Processing Task 466: paddle.nansum(Tensor([3, 1431655765],"float32"), axis=None, )
W0520 19:47:20.507719 15358 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 19:47:20.508739 15358 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Worker 2] Started on GPU 2
[Worker 2] Processing Task 470: paddle.nansum(Tensor([858993459, 5],"float32"), axis=None, )
W0520 20:17:24.585847 15662 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 20:17:24.586836 15662 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Worker 2] Started on GPU 2
[Worker 2] Processing Task 474: paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 67108864, 32],"float32"), 16, False, None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

[Worker 2] Error on Task 474: (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

[Worker 2] OOM on Task 474: (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 476: paddle.nn.functional.adaptive_max_pool2d(Tensor([29217465, 3, 7, 7],"float32"), output_size=list[2,5,], return_mask=False, name=None, )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

[Worker 2] Error on Task 476: (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

[Worker 2] OOM on Task 476: (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 479: paddle.nn.functional.adaptive_max_pool2d(x=Tensor([29217465, 3, 7, 7],"float32"), output_size=list[2,5,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

[Worker 2] Error on Task 479: (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

[Worker 2] OOM on Task 479: (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 481: paddle.nn.functional.adaptive_max_pool2d(x=Tensor([29217465, 3, 7, 7],"float32"), return_mask=False, output_size=list[3,3,], )
(External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

[Worker 2] Error on Task 481: (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

[Worker 2] OOM on Task 481: (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 484: paddle.nn.functional.grid_sample(Tensor([56, 3, 848848, 16],"float32"), Tensor([56, 16, 16, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, )
W0520 20:52:39.563957 16724 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 20:52:39.564955 16724 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.grid_sample(Tensor([56, 3, 848848, 16],"float32"), Tensor([56, 16, 16, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747745560 (unix time) try "date -d @1747745560" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x4154) received by PID 16724 (TID 0x7f66bfd56740) from PID 16724 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 486: paddle.nn.functional.grid_sample(Tensor([56, 39790, 32, 32],"float32"), Tensor([56, 32, 32, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, )
W0520 20:54:12.132973 16876 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 20:54:12.134024 16876 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.grid_sample(Tensor([56, 39790, 32, 32],"float32"), Tensor([56, 32, 32, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747745712 (unix time) try "date -d @1747745712" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x41ec) received by PID 16876 (TID 0x7f66bfd56740) from PID 16876 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 490: paddle.nn.functional.grid_sample(Tensor([5821, 4, 280, 350],"float32"), Tensor([5821, 368, 416, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, )
W0520 20:57:17.782838 17177 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 20:57:17.783870 17177 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.grid_sample(Tensor([5821, 4, 280, 350],"float32"), Tensor([5821, 368, 416, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747745938 (unix time) try "date -d @1747745938" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x4319) received by PID 17177 (TID 0x7f66bfd56740) from PID 17177 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 494: paddle.nn.functional.grid_sample(Tensor([742742, 3, 32, 32],"float32"), Tensor([742742, 32, 32, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, )
W0520 21:01:24.683490 17485 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 21:01:24.684564 17485 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.grid_sample(Tensor([742742, 3, 32, 32],"float32"), Tensor([742742, 32, 32, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747746156 (unix time) try "date -d @1747746156" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x444d) received by PID 17485 (TID 0x7f66bfd56740) from PID 17485 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 500: paddle.nn.functional.normalize(Tensor([1, 2228225, 32, 32],"float32"), axis=1, )
W0520 21:04:36.456244 17939 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 21:04:36.457268 17939 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.normalize(Tensor([1, 2228225, 32, 32],"float32"), axis=1, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747746339 (unix time) try "date -d @1747746339" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x4613) received by PID 17939 (TID 0x7f66bfd56740) from PID 17939 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 504: paddle.nn.functional.normalize(Tensor([1, 64, 557057, 64],"float32"), axis=1, )
W0520 21:07:02.104990 18170 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 21:07:02.105955 18170 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.normalize(Tensor([1, 64, 557057, 64],"float32"), axis=1, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747746476 (unix time) try "date -d @1747746476" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x46fa) received by PID 18170 (TID 0x7f66bfd56740) from PID 18170 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 507: paddle.nn.functional.normalize(Tensor([10, 228170138],"float32"), )
W0520 21:09:21.049435 18471 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 21:09:21.050539 18471 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.normalize(Tensor([10, 228170138],"float32"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747746742 (unix time) try "date -d @1747746742" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x4827) received by PID 18471 (TID 0x7f66bfd56740) from PID 18471 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 512: paddle.nn.functional.normalize(Tensor([12, 190141782],"float32"), axis=-1, )
W0520 21:14:09.226754 18854 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 21:14:09.227792 18854 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.normalize(Tensor([12, 190141782],"float32"), axis=-1, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747747006 (unix time) try "date -d @1747747006" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x49a6) received by PID 18854 (TID 0x7f66bfd56740) from PID 18854 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 519: paddle.nn.functional.normalize(Tensor([228170138, 10],"float32"), axis=0, )
W0520 21:18:49.749542 19383 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 21:18:49.750551 19383 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.normalize(Tensor([228170138, 10],"float32"), axis=0, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747747332 (unix time) try "date -d @1747747332" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x4bb7) received by PID 19383 (TID 0x7f66bfd56740) from PID 19383 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 524: paddle.nn.functional.normalize(Tensor([325957340, 7],"float32"), axis=1, )
W0520 21:23:34.099913 19765 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 21:23:34.100934 19765 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.normalize(Tensor([325957340, 7],"float32"), axis=1, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747747471 (unix time) try "date -d @1747747471" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x4d35) received by PID 19765 (TID 0x7f66bfd56740) from PID 19765 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 528: paddle.nn.functional.normalize(Tensor([4, 570425345],"float32"), axis=0, )
W0520 21:26:20.983661 20068 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 21:26:20.984999 20068 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.normalize(Tensor([4, 570425345],"float32"), axis=0, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747747640 (unix time) try "date -d @1747747640" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x4e64) received by PID 20068 (TID 0x7f66bfd56740) from PID 20068 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 532: paddle.nn.functional.normalize(Tensor([45, 50704476],"float32"), axis=0, )
W0520 21:29:09.635967 20370 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 21:29:09.637091 20370 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.normalize(Tensor([45, 50704476],"float32"), axis=0, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747747811 (unix time) try "date -d @1747747811" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x4f92) received by PID 20370 (TID 0x7f66bfd56740) from PID 20370 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 535: paddle.nn.functional.normalize(Tensor([60, 38028357],"float32"), axis=0, )
W0520 21:31:59.940958 20601 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 21:31:59.941962 20601 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.normalize(Tensor([60, 38028357],"float32"), axis=0, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747747972 (unix time) try "date -d @1747747972" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x5079) received by PID 20601 (TID 0x7f66bfd56740) from PID 20601 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 536: paddle.nn.functional.normalize(Tensor([760567127, 3],"float32"), axis=0, )
W0520 21:34:48.969056 20678 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 21:34:48.970387 20678 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.normalize(Tensor([760567127, 3],"float32"), axis=0, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747748457 (unix time) try "date -d @1747748457" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x50c6) received by PID 20678 (TID 0x7f66bfd56740) from PID 20678 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 542: paddle.nn.functional.normalize(x=Tensor([2, 1140850690],"float32"), )
W0520 21:42:47.752321 21133 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 21:42:47.753275 21133 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.normalize(x=Tensor([2, 1140850690],"float32"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747749057 (unix time) try "date -d @1747749057" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x528d) received by PID 21133 (TID 0x7f66bfd56740) from PID 21133 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 546: paddle.nn.functional.normalize(x=Tensor([2970966, 768],"float32"), axis=-1, )
W0520 21:52:52.343887 21437 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 21:52:52.344888 21437 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.normalize(x=Tensor([2970966, 768],"float32"), axis=-1, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747749224 (unix time) try "date -d @1747749224" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x53bd) received by PID 21437 (TID 0x7f66bfd56740) from PID 21437 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 547: paddle.nn.functional.normalize(x=Tensor([4, 1073741825],"float16"), p=1.2, )
W0520 21:56:39.717952 21514 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 21:56:39.718909 21514 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.normalize(x=Tensor([4, 1073741825],"float16"), p=1.2, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747750257 (unix time) try "date -d @1747750257" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x540a) received by PID 21514 (TID 0x7f66bfd56740) from PID 21514 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 554: paddle.nn.functional.normalize(x=Tensor([4, 5, 6, 35791395],"float16"), p=1, )
W0520 22:14:04.545773 22045 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 22:14:04.547075 22045 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.normalize(x=Tensor([4, 5, 6, 35791395],"float16"), p=1, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747750893 (unix time) try "date -d @1747750893" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x561d) received by PID 22045 (TID 0x7f66bfd56740) from PID 22045 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 563: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), math.inf, 1e-06, True, None, )
W0520 22:23:23.534845 22576 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 22:23:23.535877 22576 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), math.inf, 1e-06, True, None, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747751007 (unix time) try "date -d @1747751007" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x5830) received by PID 22576 (TID 0x7f66bfd56740) from PID 22576 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 567: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), 2, 1e-06, False, None, )
W0520 22:26:20.260453 22802 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 22:26:20.261471 22802 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), 2, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 2630.
Max relative difference: 0.983
 x: array([45.25, 45.25, 45.25, 45.25, 45.25, 45.25, 45.25, 45.25, 45.25,
       45.25, 45.25, 45.25, 45.25, 45.25, 45.25, 45.25, 45.25, 45.25,
       45.25, 45.25, 45.25, 45.25, 45.25, 45.25, 45.25, 45.25, 45.25,...
 y: array([2676., 2676., 2676., 2676., 2676., 2676., 2676., 2676., 2676.,
       2676., 2676., 2676., 2676., 2676., 2676., 2676., 2676., 2676.,
       2676., 2676., 2676., 2676., 2676., 2676., 2676., 2676., 2676.,...
[Worker 2] Completed Task 567
[Worker 2] Processing Task 570: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), -math.inf, 1e-06, False, None, )
[cuda error] paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), -math.inf, 1e-06, False, None, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747751367 (unix time) try "date -d @1747751367" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x5912) received by PID 22802 (TID 0x7f66bfd56740) from PID 22802 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 575: paddle.nn.functional.pairwise_distance(Tensor([2, 1140850690],"float32"), Tensor([2, 1140850690],"float32"), math.inf, 1e-06, False, None, )
W0520 22:31:12.928880 23258 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 22:31:12.929852 23258 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.pairwise_distance(Tensor([2, 1140850690],"float32"), Tensor([2, 1140850690],"float32"), math.inf, 1e-06, False, None, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747751535 (unix time) try "date -d @1747751535" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x5ada) received by PID 23258 (TID 0x7f66bfd56740) from PID 23258 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 577: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([100],"float32"), 2.0, 1e-06, False, None, )
W0520 22:33:27.404958 23414 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 22:33:27.405952 23414 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([100],"float32"), 2.0, 1e-06, False, None, )
[Worker 2] Completed Task 577
[Worker 2] Processing Task 579: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), 1, 1e-06, False, None, )
[cuda error] paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), 1, 1e-06, False, None, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747751734 (unix time) try "date -d @1747751734" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x5b76) received by PID 23414 (TID 0x7f66bfd56740) from PID 23414 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 583: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), 2, 1e-06, False, None, )
W0520 22:37:20.764039 23716 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 22:37:20.765031 23716 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), 2, 1e-06, False, None, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747751844 (unix time) try "date -d @1747751844" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x5ca4) received by PID 23716 (TID 0x7f66bfd56740) from PID 23716 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 587: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), math.inf, 1e-06, True, None, )
W0520 22:39:14.699810 24021 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 22:39:14.700784 24021 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), math.inf, 1e-06, True, None, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747751957 (unix time) try "date -d @1747751957" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x5dd5) received by PID 24021 (TID 0x7f66bfd56740) from PID 24021 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 592: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), -1, 1e-06, True, None, )
W0520 22:42:12.383199 24396 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 22:42:12.384214 24396 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), -1, 1e-06, True, None, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747752145 (unix time) try "date -d @1747752145" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x5f4c) received by PID 24396 (TID 0x7f66bfd56740) from PID 24396 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 595: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), math.inf, 1e-06, False, None, )
W0520 22:45:09.734879 24626 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 22:45:09.735872 24626 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), math.inf, 1e-06, False, None, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747752321 (unix time) try "date -d @1747752321" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x6032) received by PID 24626 (TID 0x7f66bfd56740) from PID 24626 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 599: paddle.nn.functional.pairwise_distance(Tensor([760567127, 3],"float32"), Tensor([760567127, 3],"float32"), 1, 1e-06, False, None, )
W0520 22:46:47.425069 24930 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 22:46:47.426049 24930 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.nn.functional.pairwise_distance(Tensor([760567127, 3],"float32"), Tensor([760567127, 3],"float32"), 1, 1e-06, False, None, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747752428 (unix time) try "date -d @1747752428" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x6162) received by PID 24930 (TID 0x7f66bfd56740) from PID 24930 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 602: paddle.nn.functional.pairwise_distance(x=Tensor([100, 22817014],"float32"), y=Tensor([100, 22817014],"float32"), p=2.0, epsilon=1e-06, keepdim=False, )
W0520 22:48:33.820824 25160 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0520 22:48:33.821766 25160 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.nn.functional.pairwise_distance(x=Tensor([100, 22817014],"float32"), y=Tensor([100, 22817014],"float32"), p=2.0, epsilon=1e-06, keepdim=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 42.57202
Max relative difference: 0.02183421
 x: array([1907.4491, 1907.6361, 1907.2861, 1908.1038, 1907.4648, 1907.8242,
       1907.5741, 1907.7722, 1907.445 , 1907.434 , 1907.791 , 1907.7051,
       1907.201 , 1907.3218, 1907.7052, 1907.0494, 1907.4783, 1907.681 ,...
 y: array([1949.9088, 1949.9734, 1949.7076, 1950.5201, 1949.8757, 1950.1951,
       1950.1334, 1950.2035, 1949.952 , 1949.8347, 1950.2418, 1950.0923,
       1949.742 , 1949.8057, 1950.2404, 1949.4932, 1949.8438, 1950.0902,...
[Worker 2] Completed Task 602
[Worker 2] Processing Task 605: paddle.nn.functional.rrelu(Tensor([2, 3, 4, 95070891],"float32"), 0.1, 0.3, training=False, )
[cuda error] paddle.nn.functional.rrelu(Tensor([2, 3, 4, 95070891],"float32"), 0.1, 0.3, training=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1388)

[Worker 2] Completed Task 605
[Worker 2] Processing Task 608: paddle.nn.functional.rrelu(Tensor([38028357, 3, 4, 5],"float32"), 0.1, 0.3, training=False, )
[cuda error] paddle.nn.functional.rrelu(Tensor([38028357, 3, 4, 5],"float32"), 0.1, 0.3, training=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1388)

[Worker 2] Completed Task 608
[Worker 2] Processing Task 612: paddle.std(Tensor([1, 3, 4, 357913942],"float16"), 2, True, False, )
W0520 22:54:12.996814 25160 dygraph_functions.cc:85067] got different data type, run type promotion automatically, this may cause data type been changed.
W0520 22:54:12.997051 25160 dygraph_functions.cc:87101] got different data type, run type promotion automatically, this may cause data type been changed.
W0520 22:54:12.999454 25160 dygraph_functions.cc:82416] got different data type, run type promotion automatically, this may cause data type been changed.
[accuracy error] backward  paddle.std(Tensor([1, 3, 4, 357913942],"float16"), 2, True, False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 41 / 4294967304 (9.55e-07%)
Max absolute difference: 0.04688
Max relative difference: 192.4
 x: array([[[[-0.04855 ,  0.0959  , -0.02184 , ...,  0.06158 , -0.03583 ,
           0.05878 ],
         [-0.03815 , -0.05795 , -0.004696, ...,  0.02731 ,  0.1501  ,...
 y: array([[[[-0.04855 ,  0.0959  , -0.02185 , ...,  0.06158 , -0.03586 ,
           0.05872 ],
         [-0.03815 , -0.05795 , -0.004715, ...,  0.0273  ,  0.1501  ,...
[Worker 2] Completed Task 612
[Worker 2] Processing Task 616: paddle.std(Tensor([35791395, 3, 4, 10],"float16"), list[1,3,], False, False, )
[Pass] paddle.std(Tensor([35791395, 3, 4, 10],"float16"), list[1,3,], False, False, )
[Worker 2] Completed Task 616
[Worker 2] Processing Task 621: paddle.std(x=Tensor([3, 3, 477218589],"float16"), axis=list[0,1,], )
[Pass] paddle.std(x=Tensor([3, 3, 477218589],"float16"), axis=list[0,1,], )
[Worker 2] Completed Task 621
[Worker 2] Processing Task 626: paddle.std(x=Tensor([3, 477218589, 3],"float16"), axis=list[0,1,], )
[Pass] paddle.std(x=Tensor([3, 477218589, 3],"float16"), axis=list[0,1,], )
[Worker 2] Completed Task 626
[Worker 2] Processing Task 630: paddle.std(x=Tensor([477218589, 3, 3],"float16"), axis=0, unbiased=False, )
[Pass] paddle.std(x=Tensor([477218589, 3, 3],"float16"), axis=0, unbiased=False, )
[Worker 2] Completed Task 630
[Worker 2] Processing Task 634: paddle.sum(Tensor([1, 2281701379, 1],"float32"), 1, )
[Pass] paddle.sum(Tensor([1, 2281701379, 1],"float32"), 1, )
[Worker 2] Completed Task 634
[Worker 2] Processing Task 636: paddle.sum(Tensor([1, 2281701379],"int64"), axis=1, )
element 0 of tensors does not require grad and does not have a grad_fn
[Pass] paddle.sum(Tensor([1, 2281701379],"int64"), axis=1, )
[Worker 2] Completed Task 636
[Worker 2] Processing Task 638: paddle.sum(Tensor([2, 2147483649],"float16"), -1, keepdim=True, dtype=None, )
[Pass] paddle.sum(Tensor([2, 2147483649],"float16"), -1, keepdim=True, dtype=None, )
[Worker 2] Completed Task 638
[Worker 2] Processing Task 641: paddle.take_along_axis(Tensor([13, 4, 7, 14],"float32"), axis=-1, indices=Tensor([13, 4, 7, 6268411],"int64"), )
One of the differentiated Tensors does not require grad
[Pass] paddle.take_along_axis(Tensor([13, 4, 7, 14],"float32"), axis=-1, indices=Tensor([13, 4, 7, 6268411],"int64"), )
[Worker 2] Completed Task 641
[Worker 2] Processing Task 645: paddle.Tensor.amax(Tensor([3, 2, 4, 95070891],"float32"), axis=-1, keepdim=True, )
[Pass] paddle.Tensor.amax(Tensor([3, 2, 4, 95070891],"float32"), axis=-1, keepdim=True, )
[Worker 2] Completed Task 645
[Worker 2] Processing Task 649: paddle.Tensor.amax(Tensor([3, 38028357, 4, 5],"float32"), axis=-1, keepdim=True, )
[Pass] paddle.Tensor.amax(Tensor([3, 38028357, 4, 5],"float32"), axis=-1, keepdim=True, )
[Worker 2] Completed Task 649
[Worker 2] Processing Task 653: paddle.Tensor.amax(Tensor([57042535, 2, 5, 4],"float32"), axis=2, keepdim=True, )
[Pass] paddle.Tensor.amax(Tensor([57042535, 2, 5, 4],"float32"), axis=2, keepdim=True, )
[Worker 2] Completed Task 653
[Worker 2] Processing Task 657: paddle.Tensor.amin(Tensor([3, 2, 95070891, 4],"float32"), axis=2, keepdim=True, )
[Pass] paddle.Tensor.amin(Tensor([3, 2, 95070891, 4],"float32"), axis=2, keepdim=True, )
[Worker 2] Completed Task 657
[Worker 2] Processing Task 661: paddle.Tensor.amin(Tensor([3, 38028357, 5, 4],"float32"), axis=tuple(1,2,), keepdim=True, )
[Pass] paddle.Tensor.amin(Tensor([3, 38028357, 5, 4],"float32"), axis=tuple(1,2,), keepdim=True, )
[Worker 2] Completed Task 661
[Worker 2] Processing Task 664: paddle.Tensor.amin(Tensor([57042535, 2, 5, 4],"float32"), axis=tuple(1,2,), keepdim=True, )
[Pass] paddle.Tensor.amin(Tensor([57042535, 2, 5, 4],"float32"), axis=tuple(1,2,), keepdim=True, )
[Worker 2] Completed Task 664
[Worker 2] Processing Task 670: paddle.Tensor.argmax(Tensor([7225, 157920, 2],"float32"), axis=-1, )
[cuda error] paddle.Tensor.argmax(Tensor([7225, 157920, 2],"float32"), axis=-1, ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747757495 (unix time) try "date -d @1747757495" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x6248) received by PID 25160 (TID 0x7f66bfd56740) from PID 25160 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 674: paddle.Tensor.cumsum(Tensor([1, 10, 228170138],"float32"), 1, )
W0521 00:13:03.231575 25764 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0521 00:13:03.232590 25764 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.Tensor.cumsum(Tensor([1, 10, 228170138],"float32"), 1, )
[Worker 2] Completed Task 674
[Worker 2] Processing Task 678: paddle.Tensor.cumsum(Tensor([1, 144, 15845149],"float32"), 1, )
[Pass] paddle.Tensor.cumsum(Tensor([1, 144, 15845149],"float32"), 1, )
[Worker 2] Completed Task 678
[Worker 2] Processing Task 683: paddle.Tensor.cumsum(Tensor([1, 18, 126761188],"float32"), 2, )
[accuracy error] paddle.Tensor.cumsum(Tensor([1, 18, 126761188],"float32"), 2, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 395328 / 2281701384 (0.0173%)
Max absolute difference: 0.375
Max relative difference: 2276.2266
 x: array([[[-3.010522e-01, -9.120835e-02,  1.285962e-02, ...,
          5.178796e+03,  5.178639e+03,  5.178180e+03],
        [ 1.837195e-01, -3.001863e-01, -2.991787e-01, ...,...
 y: array([[[-3.010522e-01, -9.120835e-02,  1.285962e-02, ...,
          5.178829e+03,  5.178671e+03,  5.178212e+03],
        [ 1.837195e-01, -3.001863e-01, -2.991787e-01, ...,...
[Worker 2] Completed Task 683
[Worker 2] Processing Task 686: paddle.Tensor.cumsum(Tensor([1, 253522376, 9],"float32"), 1, )
[accuracy error] paddle.Tensor.cumsum(Tensor([1, 253522376, 9],"float32"), 1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 7966117 / 2281701384 (0.349%)
Max absolute difference: 2.7182617
Max relative difference: 157633.97
 x: array([[[ 3.508059e-01, -3.945816e-01,  2.769890e-01, ...,
          1.309120e-03, -3.394884e-01,  1.488597e-01],
        [ 8.404341e-02, -5.936796e-01,  4.527132e-01, ...,...
 y: array([[[ 3.508059e-01, -3.945816e-01,  2.769890e-01, ...,
          1.309120e-03, -3.394884e-01,  1.488597e-01],
        [ 8.404341e-02, -5.936796e-01,  4.527132e-01, ...,...
[Worker 2] Completed Task 686
[Worker 2] Processing Task 692: paddle.Tensor.cumsum(Tensor([114085069, 20],"float32"), axis=-1, )
[Pass] paddle.Tensor.cumsum(Tensor([114085069, 20],"float32"), axis=-1, )
[Worker 2] Completed Task 692
[Worker 2] Processing Task 696: paddle.Tensor.cumsum(Tensor([21126865, 12, 9],"float32"), 2, )
[Pass] paddle.Tensor.cumsum(Tensor([21126865, 12, 9],"float32"), 2, )
[Worker 2] Completed Task 696
[Worker 2] Processing Task 700: paddle.Tensor.cumsum(Tensor([285212673, 4, 2],"int64"), axis=2, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.cumsum(Tensor([285212673, 4, 2],"int64"), axis=2, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.610352GB memory has been allocated and available memory is only 9.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 2] Completed Task 700
[Worker 2] Processing Task 703: paddle.Tensor.cumsum(Tensor([3, 380283564, 2],"int64"), axis=0, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.cumsum(Tensor([3, 380283564, 2],"int64"), axis=0, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.610352GB memory has been allocated and available memory is only 9.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 2] Completed Task 703
[Worker 2] Processing Task 705: paddle.Tensor.cumsum(Tensor([3, 380283564, 2],"int64"), axis=2, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.cumsum(Tensor([3, 380283564, 2],"int64"), axis=2, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.610352GB memory has been allocated and available memory is only 9.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 2] Completed Task 705
[Worker 2] Processing Task 708: paddle.Tensor.cumsum(Tensor([3, 760567127],"int64"), axis=0, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.cumsum(Tensor([3, 760567127],"int64"), axis=0, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.610352GB memory has been allocated and available memory is only 9.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 2] Completed Task 708
[Worker 2] Processing Task 712: paddle.Tensor.cumsum(Tensor([570425345, 4],"int64"), axis=0, )
element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.cumsum(Tensor([570425345, 4],"int64"), axis=0, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
4   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
5   paddle::memory::allocation::Allocator::Allocate(unsigned long)
6   paddle::memory::allocation::StatAllocator::AllocateImpl(unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.610352GB memory has been allocated and available memory is only 9.574524GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at /paddle/paddle/phi/core/memory/allocation/cuda_allocator.cc:71)

[Worker 2] Completed Task 712
[Worker 2] Processing Task 717: paddle.Tensor.cumsum(Tensor([82527, 192, 144],"float32"), 1, )
[Pass] paddle.Tensor.cumsum(Tensor([82527, 192, 144],"float32"), 1, )
[Worker 2] Completed Task 717
[Worker 2] Processing Task 721: paddle.Tensor.diff(x=Tensor([67108865, 4, 4, 4],"float16"), )
[Pass] paddle.Tensor.diff(x=Tensor([67108865, 4, 4, 4],"float16"), )
[Worker 2] Completed Task 721
[Worker 2] Processing Task 723: paddle.Tensor.diff(x=Tensor([67108865, 4, 4, 4],"float16"), axis=-2, )
[Pass] paddle.Tensor.diff(x=Tensor([67108865, 4, 4, 4],"float16"), axis=-2, )
[Worker 2] Completed Task 723
[Worker 2] Processing Task 730: paddle.trunc(input=Tensor([3, 6, 6, 6, 6628036],"float16"), )
[cuda error] paddle.trunc(input=Tensor([3, 6, 6, 6, 6628036],"float16"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747760302 (unix time) try "date -d @1747760302" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x64a4) received by PID 25764 (TID 0x7f66bfd56740) from PID 25764 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 733: paddle.trunc(input=Tensor([3, 6628036, 6, 6, 6],"float16"), )
W0521 01:01:22.961335 26372 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0521 01:01:22.962339 26372 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.trunc(input=Tensor([3, 6628036, 6, 6, 6],"float16"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747760484 (unix time) try "date -d @1747760484" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x6704) received by PID 26372 (TID 0x7f66bfd56740) from PID 26372 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 737: paddle.trunc(input=Tensor([6, 19884108, 6, 6],"float16"), )
W0521 01:04:19.549625 26674 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0521 01:04:19.550614 26674 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.trunc(input=Tensor([6, 19884108, 6, 6],"float16"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747760660 (unix time) try "date -d @1747760660" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x6832) received by PID 26674 (TID 0x7f66bfd56740) from PID 26674 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 740: paddle.trunc(input=Tensor([6, 6, 19884108, 6],"float16"), )
W0521 01:06:59.744829 26902 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0521 01:06:59.745812 26902 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.trunc(input=Tensor([6, 6, 19884108, 6],"float16"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747760821 (unix time) try "date -d @1747760821" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x6916) received by PID 26902 (TID 0x7f66bfd56740) from PID 26902 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 743: paddle.trunc(Tensor([10, 228170138, 1],"float32"), )
W0521 01:08:17.005264 27130 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0521 01:08:17.006240 27130 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.trunc(Tensor([10, 228170138, 1],"float32"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747760898 (unix time) try "date -d @1747760898" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x69fa) received by PID 27130 (TID 0x7f66bfd56740) from PID 27130 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 745: paddle.trunc(Tensor([114085069, 20],"float32"), )
W0521 01:09:34.315028 27283 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0521 01:09:34.315999 27283 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.trunc(Tensor([114085069, 20],"float32"), ) 
 (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/fluid/pybind/eager_functions.cc:1385)

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /paddle/paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::EmptyCache()
1   paddle::memory::allocation::StreamSafeCUDAAllocator::ReleaseImpl(phi::Place const&)
2   paddle::memory::allocation::AutoGrowthBestFitAllocator::FreeIdleChunks()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1747760975 (unix time) try "date -d @1747760975" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x6a93) received by PID 27283 (TID 0x7f66bfd56740) from PID 27283 ***]

[Worker 2] Started on GPU 2
[Worker 2] Processing Task 749: paddle.var(Tensor([1, 107374183, 4, 10],"float16"), list[1,2,], True, False, )
W0521 01:11:27.227896 27585 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0521 01:11:27.228829 27585 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
W0521 01:11:27.273051 27585 dygraph_functions.cc:85067] got different data type, run type promotion automatically, this may cause data type been changed.
W0521 01:11:27.273283 27585 dygraph_functions.cc:87101] got different data type, run type promotion automatically, this may cause data type been changed.
W0521 01:11:27.275691 27585 dygraph_functions.cc:82416] got different data type, run type promotion automatically, this may cause data type been changed.
[Pass] paddle.var(Tensor([1, 107374183, 4, 10],"float16"), list[1,2,], True, False, )
[Worker 2] Completed Task 749
[Worker 2] Processing Task 751: paddle.var(Tensor([1, 3, 143165577, 10],"float16"), list[1,2,], True, False, )
[Pass] paddle.var(Tensor([1, 3, 143165577, 10],"float16"), list[1,2,], True, False, )
[Worker 2] Completed Task 751
[Worker 2] Processing Task 756: paddle.var(Tensor([16, 32, 4456449, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Pass] paddle.var(Tensor([16, 32, 4456449, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Worker 2] Completed Task 756
[Worker 2] Processing Task 757: paddle.var(Tensor([16, 8, 17825793, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Pass] paddle.var(Tensor([16, 8, 17825793, 1],"float32"), axis=list[1,2,3,], keepdim=True, unbiased=False, )
[Worker 2] Completed Task 757
[Worker 2] Processing Task 758: paddle.var(Tensor([16777217, 256],"float16"), axis=-1, keepdim=True, )
[Pass] paddle.var(Tensor([16777217, 256],"float16"), axis=-1, keepdim=True, )
[Worker 2] Completed Task 758
[Worker 2] Processing Task 768: paddle.var(Tensor([35791395, 3, 4, 10],"float16"), list[1,3,], True, False, )
[Pass] paddle.var(Tensor([35791395, 3, 4, 10],"float16"), list[1,3,], True, False, )
[Worker 2] Completed Task 768
[Worker 2] Processing Task 776: paddle.var(x=Tensor([3, 3, 477218589],"float16"), axis=list[0,1,], )
[Pass] paddle.var(x=Tensor([3, 3, 477218589],"float16"), axis=list[0,1,], )
[Worker 2] Completed Task 776
[Worker 2] Processing Task 780: paddle.var(x=Tensor([3, 477218589, 3],"float16"), axis=0, unbiased=False, )
[Pass] paddle.var(x=Tensor([3, 477218589, 3],"float16"), axis=0, unbiased=False, )
[Worker 2] Completed Task 780
[Worker 2] Processing Task 785: paddle.var(x=Tensor([477218589, 3, 3],"float16"), axis=0, unbiased=False, )
[Pass] paddle.var(x=Tensor([477218589, 3, 3],"float16"), axis=0, unbiased=False, )
[Worker 2] Completed Task 785
[Worker 2] Received stop signal.
