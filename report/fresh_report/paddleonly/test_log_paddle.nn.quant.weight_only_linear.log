paddle.nn.quant.weight_only_linear(Tensor([1, 1, 64],"float16"), Tensor([128, 64],"int8"), bias=Tensor([256],"float16"), weight_scale=Tensor([256],"float16"), weight_dtype="int4", group_size=-1, )

[paddle error] paddle.nn.quant.weight_only_linear(Tensor([1, 1, 64],"float16"), Tensor([128, 64],"int8"), bias=Tensor([256],"float16"), weight_scale=Tensor([256],"float16"), weight_dtype="int4", group_size=-1, ) 
 (InvalidArgument) Currently weightonly linear grad only support arch = 80 or 86. 
  [Hint: Expected ((arch == 80) || (arch == 86)) == true, but received ((arch == 80) || (arch == 86)):0 != true:1.] (at ../paddle/phi/infermeta/backward.cc:1765)

paddle.nn.quant.weight_only_linear(Tensor([1, 1, 64],"float16"), Tensor([256, 64],"int8"), bias=Tensor([256],"float16"), weight_scale=Tensor([256],"float16"), weight_dtype="int8", group_size=-1, )

[paddle error] paddle.nn.quant.weight_only_linear(Tensor([1, 1, 64],"float16"), Tensor([256, 64],"int8"), bias=Tensor([256],"float16"), weight_scale=Tensor([256],"float16"), weight_dtype="int8", group_size=-1, ) 
 (InvalidArgument) Currently weightonly linear grad only support arch = 80 or 86. 
  [Hint: Expected ((arch == 80) || (arch == 86)) == true, but received ((arch == 80) || (arch == 86)):0 != true:1.] (at ../paddle/phi/infermeta/backward.cc:1765)