paddle.clip(Tensor([10, 2],"float64"), min=1e-06, )
paddle.clip(Tensor([5, 2],"float64"), min=0.0, )
paddle.clip(Tensor([5],"float64"), min=0, )
paddle.clip(Tensor([5],"float64"), min=0.0, )
paddle.clip(x=Tensor([1, 2, 3],"float64"), min=None, max=1, )
paddle.clip(x=Tensor([3, 3, 3],"float64"), min=None, max=5, )
paddle.clip(x=Tensor([3, 3],"float64"), min=1.0, max=None, )
paddle.clip(x=Tensor([3, 3],"float64"), min=5, max=None, )
paddle.clip(x=Tensor([3, 3],"float64"), min=Tensor([1],"float64"), max=None, )
paddle.clip(x=Tensor([3],"float64"), )
paddle.clip(Tensor([16, 256],"float64"), -127, 127, )
paddle.clip(Tensor([2, 20],"float64"), min=-2.0, max=2.0, )
paddle.clip(Tensor([2, 25],"float64"), min=-2.0, max=2.0, )
paddle.clip(Tensor([2, 30],"float64"), min=-2.0, max=2.0, )
paddle.clip(Tensor([20, 1],"float64"), min=-2.0, max=2.0, )
paddle.clip(Tensor([20, 20],"float64"), min=-2.0, max=2.0, )
paddle.clip(Tensor([24, 17, 128, 128],"float64"), min=0, max=2, )
paddle.clip(Tensor([24, 17, 256, 256],"float64"), min=0, max=2, )
paddle.clip(Tensor([25, 1],"float64"), min=-2.0, max=2.0, )
paddle.clip(Tensor([25, 25],"float64"), min=-2.0, max=2.0, )
paddle.clip(Tensor([30, 1],"float64"), min=-2.0, max=2.0, )
paddle.clip(Tensor([30, 30],"float64"), min=-2.0, max=2.0, )
paddle.clip(Tensor([4, 4],"float64"), -1, 1, )
paddle.clip(Tensor([5, 5],"float64"), -1, 1, )
paddle.clip(Tensor([],"float64"), min=0, )
paddle.clip(x=Tensor([1, 2],"float64"), min=Tensor([1],"float64"), max=Tensor([1],"float64"), )
paddle.clip(x=Tensor([10, 10],"float64"), min=-1.0, max=1.0, )
paddle.clip(x=Tensor([10, 10],"float64"), min=-2.0, max=-1.0, )
paddle.clip(x=Tensor([10, 10],"float64"), min=2.0, max=2.0, )
paddle.clip(x=Tensor([2, 2],"float64"), min=Tensor([1],"float64"), max=Tensor([1],"float64"), )
paddle.clip(x=Tensor([2],"float64"), min=Tensor([1],"float64"), max=Tensor([1],"float64"), name="test name", )
paddle.clip(x=Tensor([3, 3],"float64"), min=-5.0, max=5.0, )
paddle.clip(x=Tensor([3, 3],"float64"), min=0.0, max=5, )
paddle.clip(x=Tensor([3, 3],"float64"), min=1, max=Tensor([1],"float64"), )
paddle.clip(x=Tensor([3, 3],"float64"), min=Tensor([1],"float64"), max=Tensor([1],"float64"), )
paddle.clip(x=Tensor([3],"float64"), min=2.0, max=2.0, )
paddle.clip(x=Tensor([3],"float64"), min=None, max=-1, )
paddle.clip(x=Tensor([3],"float64"), min=Tensor([1],"float64"), max=Tensor([1],"float64"), )
paddle.clip(x=Tensor([4, 10, 10],"float64"), min=-1.0, max=1.0, )

grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
2025-05-16 05:34:02.472655 test begin: paddle.clip(Tensor([10, 2],"float64"), min=1e-06, )
W0516 05:34:08.466921 11831 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 6.1, Driver API Version: 12.0, Runtime API Version: 11.8
W0516 05:34:08.468230 11831 gpu_resources.cc:164] device: 0, cuDNN Version: 8.8.
tensor([[-8.9715e+307, -2.5460e+307],
        [ 3.8859e+307, -1.1867e+307],
        [ 7.0429e+307,  2.9944e+307],
        [ 4.3879e+307, -8.2192e+307],
        [ 5.1010e+307,  7.2600e+307],
        [-4.2717e+307,  5.7567e+307],
        [-6.1908e+307,  1.7015e+307],
        [-3.6913e+307, -5.4656e+307],
        [-5.3328e+307,  5.1012e+305],
        [ 3.5358e+307,  8.0198e+307]], device='cuda:0', dtype=torch.float64,
       requires_grad=True)
-----------
tensor([[ 1.0000e-06,  1.0000e-06],
        [3.8859e+307,  1.0000e-06],
        [7.0429e+307, 2.9944e+307],
        [4.3879e+307,  1.0000e-06],
        [5.1010e+307, 7.2600e+307],
        [ 1.0000e-06, 5.7567e+307],
        [ 1.0000e-06, 1.7015e+307],
        [ 1.0000e-06,  1.0000e-06],
        [ 1.0000e-06, 5.1012e+305],
        [3.5358e+307, 8.0198e+307]], device='cuda:0', dtype=torch.float64,
       grad_fn=<ClampBackward1>)
Tensor(shape=[10, 2], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [[0.00000100                              ,
         0.00000100                              ],
        [340282346638528859811704183484516925440.,
         0.00000100                              ],
        [340282346638528859811704183484516925440.,
         340282346638528859811704183484516925440.],
        [340282346638528859811704183484516925440.,
         0.00000100                              ],
        [340282346638528859811704183484516925440.,
         340282346638528859811704183484516925440.],
        [0.00000100                              ,
         340282346638528859811704183484516925440.],
        [0.00000100                              ,
         340282346638528859811704183484516925440.],
        [0.00000100                              ,
         0.00000100                              ],
        [0.00000100                              ,
         340282346638528859811704183484516925440.],
        [340282346638528859811704183484516925440.,
         340282346638528859811704183484516925440.]])
[accuracy error] paddle.clip(Tensor([10, 2],"float64"), min=1e-06, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 11 / 20 (55%)
Max absolute difference: 8.01982245e+307
Max relative difference: 1.
 x: array([[1.000000e-06, 1.000000e-06],
       [3.402823e+38, 1.000000e-06],
       [3.402823e+38, 3.402823e+38],...
 y: array([[1.000000e-006, 1.000000e-006],
       [3.885869e+307, 1.000000e-006],
       [7.042903e+307, 2.994414e+307],...
2025-05-16 05:34:38.446332 test begin: paddle.clip(Tensor([16, 256],"float64"), -127, 127, )
tensor([[ 7.3860e+307, -6.7369e+307,  5.9068e+307,  ..., -6.6299e+307,
          9.1327e+306,  2.0225e+307],
        [ 4.9928e+307,  1.8003e+306, -4.5309e+307,  ..., -2.8173e+307,
         -8.7876e+307, -3.3152e+307],
        [ 1.6626e+307,  8.1921e+307,  9.5710e+306,  ..., -7.8746e+307,
         -6.7137e+307, -2.7886e+307],
        ...,
        [ 7.8842e+307,  1.9915e+307, -1.2455e+307,  ..., -4.0788e+307,
          3.2757e+307,  3.1259e+306],
        [ 1.9590e+307,  8.9140e+307, -5.7474e+307,  ..., -4.2169e+307,
          6.6896e+306, -7.3147e+307],
        [-7.1151e+307, -5.4357e+307, -6.5135e+305,  ..., -1.7641e+307,
          3.3766e+307,  7.1516e+306]], device='cuda:0', dtype=torch.float64,
       requires_grad=True)
-----------
tensor([[ 127., -127.,  127.,  ..., -127.,  127.,  127.],
        [ 127.,  127., -127.,  ..., -127., -127., -127.],
        [ 127.,  127.,  127.,  ..., -127., -127., -127.],
        ...,
        [ 127.,  127., -127.,  ..., -127.,  127.,  127.],
        [ 127.,  127., -127.,  ..., -127.,  127., -127.],
        [-127., -127., -127.,  ..., -127.,  127.,  127.]], device='cuda:0',
       dtype=torch.float64, grad_fn=<ClampBackward1>)
Tensor(shape=[16, 256], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [[ 127., -127.,  127., ..., -127.,  127.,  127.],
        [ 127.,  127., -127., ..., -127., -127., -127.],
        [ 127.,  127.,  127., ..., -127., -127., -127.],
        ...,
        [ 127.,  127., -127., ..., -127.,  127.,  127.],
        [ 127.,  127., -127., ..., -127.,  127., -127.],
        [-127., -127., -127., ..., -127.,  127.,  127.]])
[Pass] paddle.clip(Tensor([16, 256],"float64"), -127, 127, )
2025-05-16 05:34:38.732236 test begin: paddle.clip(Tensor([2, 20],"float64"), min=-2.0, max=2.0, )
tensor([[ 4.4619e+307, -2.3255e+307, -5.6172e+307,  8.5518e+307,  2.0265e+307,
         -4.0208e+307, -8.8045e+307,  6.2913e+306,  2.4246e+307,  4.9466e+307,
          6.1679e+307,  6.5546e+307,  8.0297e+307,  9.1948e+306, -7.7243e+307,
          4.2663e+307, -7.8650e+307, -8.5309e+307,  2.8782e+307,  2.7092e+307],
        [ 4.9942e+307, -2.5814e+307, -4.6981e+307, -5.5945e+307, -4.0963e+307,
         -2.8189e+307, -2.8541e+307, -1.2806e+307, -7.5437e+307,  5.7486e+307,
          1.1590e+307, -2.6103e+307,  2.7899e+307, -8.2134e+307, -4.0599e+307,
          7.6276e+307, -5.6795e+306, -8.4341e+307,  4.3047e+307, -8.7403e+307]],
       device='cuda:0', dtype=torch.float64, requires_grad=True)
-----------
tensor([[ 2., -2., -2.,  2.,  2., -2., -2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,
         -2.,  2., -2., -2.,  2.,  2.],
        [ 2., -2., -2., -2., -2., -2., -2., -2., -2.,  2.,  2., -2.,  2., -2.,
         -2.,  2., -2., -2.,  2., -2.]], device='cuda:0', dtype=torch.float64,
       grad_fn=<ClampBackward1>)
Tensor(shape=[2, 20], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [[ 2., -2., -2.,  2.,  2., -2., -2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,
         -2.,  2., -2., -2.,  2.,  2.],
        [ 2., -2., -2., -2., -2., -2., -2., -2., -2.,  2.,  2., -2.,  2., -2.,
         -2.,  2., -2., -2.,  2., -2.]])
[Pass] paddle.clip(Tensor([2, 20],"float64"), min=-2.0, max=2.0, )
2025-05-16 05:34:38.903473 test begin: paddle.clip(Tensor([2, 25],"float64"), min=-2.0, max=2.0, )
tensor([[ 8.6259e+307, -2.6109e+307, -8.1967e+307, -4.9910e+307,  2.6860e+307,
          8.3903e+307,  8.5194e+307, -8.8766e+307,  8.3696e+307,  4.3596e+306,
         -2.6492e+307, -8.5444e+307,  1.2561e+306,  1.8877e+307,  4.2714e+306,
          2.0911e+307,  5.0910e+307, -5.1037e+307,  2.1664e+307,  3.8239e+306,
         -4.7839e+307, -4.1280e+307, -7.0922e+307,  1.3767e+307,  7.7458e+307],
        [ 3.3308e+306,  8.2497e+307, -5.3795e+307, -7.2349e+306, -2.0156e+307,
          6.6724e+307,  2.4428e+307, -4.3645e+307, -8.6341e+306, -5.6522e+307,
          8.3433e+307,  6.9360e+306, -9.9350e+306, -2.0685e+306,  8.7609e+307,
         -8.6754e+307,  1.6007e+307, -4.6066e+307, -1.7632e+307, -8.7673e+307,
          1.2827e+307, -3.3345e+305, -4.3170e+307,  1.1471e+307,  2.0594e+307]],
       device='cuda:0', dtype=torch.float64, requires_grad=True)
-----------
tensor([[ 2., -2., -2., -2.,  2.,  2.,  2., -2.,  2.,  2., -2., -2.,  2.,  2.,
          2.,  2.,  2., -2.,  2.,  2., -2., -2., -2.,  2.,  2.],
        [ 2.,  2., -2., -2., -2.,  2.,  2., -2., -2., -2.,  2.,  2., -2., -2.,
          2., -2.,  2., -2., -2., -2.,  2., -2., -2.,  2.,  2.]],
       device='cuda:0', dtype=torch.float64, grad_fn=<ClampBackward1>)
Tensor(shape=[2, 25], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [[ 2., -2., -2., -2.,  2.,  2.,  2., -2.,  2.,  2., -2., -2.,  2.,  2.,
          2.,  2.,  2., -2.,  2.,  2., -2., -2., -2.,  2.,  2.],
        [ 2.,  2., -2., -2., -2.,  2.,  2., -2., -2., -2.,  2.,  2., -2., -2.,
          2., -2.,  2., -2., -2., -2.,  2., -2., -2.,  2.,  2.]])
[Pass] paddle.clip(Tensor([2, 25],"float64"), min=-2.0, max=2.0, )
2025-05-16 05:34:39.068391 test begin: paddle.clip(Tensor([2, 30],"float64"), min=-2.0, max=2.0, )
tensor([[-2.6942e+307, -6.9983e+307, -5.0158e+307,  5.3177e+307,  1.7841e+307,
         -4.6417e+307,  8.9261e+307,  3.0647e+307, -2.0692e+307, -4.3618e+307,
          5.6987e+307,  2.6331e+307,  2.4207e+307,  2.1815e+307,  4.2311e+307,
         -6.8455e+307, -2.2014e+307,  6.9751e+307, -4.7981e+307,  6.0285e+307,
         -4.9890e+307,  5.4609e+307, -3.2671e+306,  1.2974e+307,  5.8689e+306,
         -6.9200e+307,  8.9054e+307,  7.5144e+307,  8.1028e+306,  5.9179e+307],
        [-6.7949e+307, -8.8484e+307,  1.6752e+307, -3.4206e+307, -8.1940e+307,
         -3.7876e+306,  1.0674e+307,  6.8014e+307, -9.7665e+306, -8.3455e+307,
         -1.9978e+307,  1.4898e+307,  4.1452e+307, -2.5191e+307,  5.4349e+307,
         -5.8521e+307, -1.0734e+307, -6.7968e+307, -7.8294e+307, -3.5741e+307,
         -8.0624e+307,  1.0727e+307, -5.5845e+307,  2.8737e+307,  4.1429e+307,
          8.6845e+307,  2.5141e+307,  7.3017e+307,  5.7632e+307, -7.8999e+307]],
       device='cuda:0', dtype=torch.float64, requires_grad=True)
-----------
tensor([[-2., -2., -2.,  2.,  2., -2.,  2.,  2., -2., -2.,  2.,  2.,  2.,  2.,
          2., -2., -2.,  2., -2.,  2., -2.,  2., -2.,  2.,  2., -2.,  2.,  2.,
          2.,  2.],
        [-2., -2.,  2., -2., -2., -2.,  2.,  2., -2., -2., -2.,  2.,  2., -2.,
          2., -2., -2., -2., -2., -2., -2.,  2., -2.,  2.,  2.,  2.,  2.,  2.,
          2., -2.]], device='cuda:0', dtype=torch.float64,
       grad_fn=<ClampBackward1>)
Tensor(shape=[2, 30], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [[-2., -2., -2.,  2.,  2., -2.,  2.,  2., -2., -2.,  2.,  2.,  2.,  2.,
          2., -2., -2.,  2., -2.,  2., -2.,  2., -2.,  2.,  2., -2.,  2.,  2.,
          2.,  2.],
        [-2., -2.,  2., -2., -2., -2.,  2.,  2., -2., -2., -2.,  2.,  2., -2.,
          2., -2., -2., -2., -2., -2., -2.,  2., -2.,  2.,  2.,  2.,  2.,  2.,
          2., -2.]])
[Pass] paddle.clip(Tensor([2, 30],"float64"), min=-2.0, max=2.0, )
2025-05-16 05:34:39.235599 test begin: paddle.clip(Tensor([20, 1],"float64"), min=-2.0, max=2.0, )
tensor([[ 3.9686e+306],
        [ 8.1160e+307],
        [ 1.3924e+307],
        [ 4.2279e+307],
        [-3.6439e+307],
        [-7.1796e+307],
        [-5.7154e+307],
        [-7.6050e+307],
        [-5.7382e+307],
        [ 3.9095e+307],
        [ 1.7352e+307],
        [ 4.9688e+307],
        [-7.4959e+307],
        [ 3.7710e+307],
        [-5.2070e+307],
        [ 2.0973e+307],
        [-1.1838e+307],
        [-2.0307e+307],
        [ 5.3293e+307],
        [-8.2206e+307]], device='cuda:0', dtype=torch.float64,
       requires_grad=True)
-----------
tensor([[ 2.],
        [ 2.],
        [ 2.],
        [ 2.],
        [-2.],
        [-2.],
        [-2.],
        [-2.],
        [-2.],
        [ 2.],
        [ 2.],
        [ 2.],
        [-2.],
        [ 2.],
        [-2.],
        [ 2.],
        [-2.],
        [-2.],
        [ 2.],
        [-2.]], device='cuda:0', dtype=torch.float64, grad_fn=<ClampBackward1>)
Tensor(shape=[20, 1], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [[ 2.],
        [ 2.],
        [ 2.],
        [ 2.],
        [-2.],
        [-2.],
        [-2.],
        [-2.],
        [-2.],
        [ 2.],
        [ 2.],
        [ 2.],
        [-2.],
        [ 2.],
        [-2.],
        [ 2.],
        [-2.],
        [-2.],
        [ 2.],
        [-2.]])
[Pass] paddle.clip(Tensor([20, 1],"float64"), min=-2.0, max=2.0, )
2025-05-16 05:34:39.405995 test begin: paddle.clip(Tensor([20, 20],"float64"), min=-2.0, max=2.0, )
tensor([[ 8.1945e+307, -2.9449e+306,  2.9228e+307,  2.5584e+306,  5.8890e+307,
         -1.1521e+307, -3.1930e+307,  4.5028e+307, -2.1722e+307, -5.5321e+307,
          4.5259e+307, -6.7745e+306, -7.5486e+307, -5.5544e+307,  3.6613e+307,
          4.7961e+307,  2.5366e+307, -1.8610e+307, -6.1446e+307, -8.8260e+307],
        [-8.7073e+307,  5.5622e+307, -1.1452e+307, -7.2499e+307, -8.1257e+307,
          2.0748e+307,  1.5383e+307, -4.3127e+307,  5.2281e+307,  6.6072e+307,
         -2.4526e+307, -3.9849e+307,  1.9261e+307, -6.3548e+307,  8.7611e+307,
          4.7312e+307,  8.3522e+307,  4.6734e+307,  1.1748e+307, -7.0922e+306],
        [ 3.9671e+307, -8.5247e+307,  5.1946e+307,  1.1476e+307,  6.9709e+307,
         -2.2195e+307, -7.9983e+307, -1.8627e+307, -3.5402e+306, -1.3916e+307,
         -6.6689e+307, -8.0984e+307, -4.5855e+307,  4.2818e+307,  8.7993e+307,
         -3.7362e+306, -4.7139e+307, -5.8374e+307,  2.9242e+307,  5.1539e+307],
        [-5.3645e+307, -6.9775e+307, -4.9283e+307, -4.0279e+307, -2.1823e+307,
          8.3478e+307, -1.2869e+307,  5.9405e+306,  7.5075e+307, -2.1730e+307,
         -7.2189e+307,  4.0899e+307,  8.1349e+307, -8.6966e+307, -1.2947e+307,
         -3.9536e+307,  7.0811e+307,  9.8969e+306, -7.1802e+307,  8.3316e+307],
        [-5.5887e+307,  7.9338e+307,  4.7862e+306,  6.2058e+307, -3.0892e+306,
         -7.0801e+307, -9.7229e+306, -2.8420e+306, -5.8640e+307, -4.5590e+307,
          1.7981e+307, -5.9680e+307, -7.0315e+307,  1.3498e+307, -3.4500e+307,
          1.8019e+307,  4.0005e+307, -1.7041e+307,  4.6267e+307,  8.2101e+307],
        [-8.2813e+307, -3.9925e+307, -8.7507e+306,  1.7774e+307,  8.7904e+307,
         -2.3848e+307,  3.4789e+307, -6.0532e+307, -6.3607e+307,  8.4952e+307,
          6.2725e+307, -2.5857e+307, -8.2721e+307, -7.7494e+307, -3.7030e+307,
         -9.4436e+306, -3.3086e+307, -5.2049e+307, -1.0737e+307, -8.3707e+307],
        [-7.4049e+307, -6.3213e+307,  2.8512e+306, -8.5741e+307,  7.8800e+307,
          2.7607e+307, -6.4858e+307,  5.4369e+307,  1.3641e+307, -5.3770e+307,
         -4.4433e+306, -8.3426e+307, -8.3319e+307, -1.7174e+307, -2.8182e+306,
          3.3492e+307,  7.7321e+307,  7.4094e+307, -6.4866e+307,  8.5784e+307],
        [ 5.2547e+307,  8.0025e+307, -6.8776e+307, -7.0056e+307,  4.2387e+307,
         -2.2205e+307, -1.5557e+307,  6.4380e+307,  8.6530e+307,  1.5109e+307,
         -7.6812e+307, -3.4067e+306,  7.2734e+307,  8.3748e+306, -6.3655e+307,
         -5.0704e+307,  6.3709e+307, -3.0338e+307, -8.0081e+307,  7.9195e+307],
        [ 2.0371e+307, -4.8055e+307, -1.1676e+307,  1.5454e+307, -8.7030e+307,
          4.3344e+307,  1.4176e+307,  3.8493e+307,  7.9644e+307, -2.4135e+307,
         -6.3658e+307,  7.4901e+307, -3.4210e+307,  9.5595e+305,  2.6527e+306,
         -2.9163e+307,  1.7857e+306, -1.3808e+307, -6.1167e+306, -1.6431e+307],
        [ 2.1366e+307, -8.4241e+307,  2.6736e+307, -8.7150e+307,  7.8917e+307,
         -5.8219e+307, -3.8294e+307,  4.2028e+307, -7.1906e+307,  8.5020e+307,
         -3.9209e+307, -2.5908e+307,  7.2720e+306,  7.2072e+307,  3.8902e+307,
         -4.1036e+307,  2.5435e+307, -7.3918e+307, -4.7146e+307, -1.4954e+307],
        [-7.3812e+307, -4.3729e+307, -8.9148e+307,  5.6880e+307, -8.0081e+307,
         -1.2706e+307, -1.7760e+307, -8.5348e+307,  7.0752e+307, -7.4128e+307,
          1.8475e+307,  6.8780e+307,  3.4946e+307, -7.9643e+307, -8.9900e+306,
         -8.4765e+307, -7.8762e+307, -7.7660e+307,  4.2972e+307, -6.4665e+307],
        [-4.4510e+307, -5.2452e+307, -5.6762e+306, -4.4766e+307, -2.3792e+307,
          2.7545e+307, -1.0692e+307, -2.8796e+307,  1.1608e+307, -3.1979e+307,
         -6.0497e+307, -4.9231e+307, -5.0819e+307, -7.7079e+307,  3.3655e+307,
         -2.8110e+307, -7.1836e+307,  4.3952e+307, -6.8121e+307,  2.3501e+307],
        [-3.0525e+307,  3.1106e+307,  2.8346e+307,  8.0661e+307,  7.6484e+306,
          6.7803e+307,  8.9748e+307, -6.7973e+307,  8.6939e+307, -7.5536e+307,
          6.5756e+307,  6.7810e+307, -8.2665e+306, -2.6980e+307, -5.3744e+306,
          5.0516e+306, -4.9352e+307, -6.6689e+307, -3.2823e+307, -8.8486e+307],
        [-1.6828e+307, -2.9440e+307, -6.6055e+307, -8.4178e+306,  3.2762e+307,
          6.8330e+306,  7.3485e+307, -5.5319e+307, -5.2436e+307,  6.0215e+307,
          1.9967e+307,  3.3092e+307, -5.9027e+307, -3.6987e+307,  3.5324e+307,
          2.4026e+307, -1.1174e+306, -4.1600e+307, -4.1595e+307, -2.6012e+307],
        [-4.0558e+307, -8.3911e+307, -6.1867e+307,  1.6875e+307,  6.6777e+307,
          2.7853e+307,  3.1152e+307,  1.8213e+307, -5.3107e+307,  7.1735e+307,
         -6.8315e+306,  3.6942e+307,  4.9091e+307,  8.6465e+307, -8.3423e+307,
          8.5912e+307,  3.6344e+306,  2.5490e+307,  2.4121e+307, -8.6477e+307],
        [ 8.3130e+307,  4.2346e+307, -2.6092e+307,  8.1003e+307,  7.9912e+307,
         -2.0699e+307,  7.8578e+307, -5.7144e+307,  2.4128e+307, -6.9928e+307,
         -3.8263e+307,  5.6360e+307,  4.1713e+307,  7.6776e+307,  2.2381e+307,
         -6.9759e+307, -1.1355e+307, -3.9170e+307,  3.9052e+306,  3.8605e+307],
        [-3.9429e+307, -8.8160e+307, -6.7480e+307,  2.1842e+307, -1.5906e+307,
          3.7018e+307, -7.8860e+307,  1.8000e+306,  1.0350e+307,  2.3597e+307,
          7.2772e+307,  8.7425e+306,  3.9229e+307,  1.3153e+306, -3.1258e+307,
         -4.5382e+307, -4.2637e+306,  4.8531e+307, -1.4299e+307,  8.6238e+307],
        [ 6.9879e+307,  6.4329e+307,  5.5399e+307,  5.1873e+307,  1.7450e+307,
          9.1358e+306, -3.1549e+307, -8.8849e+307,  4.9266e+307,  6.4522e+307,
          8.9250e+307, -2.9163e+306, -6.4083e+307,  4.7973e+307, -5.0100e+307,
         -1.1441e+307,  5.6119e+307,  6.6831e+306, -4.3147e+307,  1.8669e+307],
        [-7.4657e+307,  4.6691e+307, -4.8160e+307, -3.5167e+307, -4.6567e+307,
         -1.8066e+307, -8.9142e+307, -6.8256e+307, -8.9846e+307,  5.1256e+307,
         -4.2368e+307,  3.0830e+307,  6.7438e+307, -7.5399e+307,  2.2816e+307,
         -6.6534e+306,  6.7315e+307,  1.3757e+307, -7.0309e+307,  6.7089e+307],
        [ 7.1124e+307,  7.2387e+307,  1.4254e+307,  3.0372e+307,  6.5892e+307,
          6.4323e+307, -3.9241e+307,  5.6633e+307, -3.6234e+306, -2.0751e+307,
          8.3993e+307, -1.7777e+307, -7.1087e+307, -7.4948e+307, -1.8248e+307,
         -5.6390e+307,  3.5728e+307,  5.6810e+307,  3.2353e+307, -7.1695e+307]],
       device='cuda:0', dtype=torch.float64, requires_grad=True)
-----------
tensor([[ 2., -2.,  2.,  2.,  2., -2., -2.,  2., -2., -2.,  2., -2., -2., -2.,
          2.,  2.,  2., -2., -2., -2.],
        [-2.,  2., -2., -2., -2.,  2.,  2., -2.,  2.,  2., -2., -2.,  2., -2.,
          2.,  2.,  2.,  2.,  2., -2.],
        [ 2., -2.,  2.,  2.,  2., -2., -2., -2., -2., -2., -2., -2., -2.,  2.,
          2., -2., -2., -2.,  2.,  2.],
        [-2., -2., -2., -2., -2.,  2., -2.,  2.,  2., -2., -2.,  2.,  2., -2.,
         -2., -2.,  2.,  2., -2.,  2.],
        [-2.,  2.,  2.,  2., -2., -2., -2., -2., -2., -2.,  2., -2., -2.,  2.,
         -2.,  2.,  2., -2.,  2.,  2.],
        [-2., -2., -2.,  2.,  2., -2.,  2., -2., -2.,  2.,  2., -2., -2., -2.,
         -2., -2., -2., -2., -2., -2.],
        [-2., -2.,  2., -2.,  2.,  2., -2.,  2.,  2., -2., -2., -2., -2., -2.,
         -2.,  2.,  2.,  2., -2.,  2.],
        [ 2.,  2., -2., -2.,  2., -2., -2.,  2.,  2.,  2., -2., -2.,  2.,  2.,
         -2., -2.,  2., -2., -2.,  2.],
        [ 2., -2., -2.,  2., -2.,  2.,  2.,  2.,  2., -2., -2.,  2., -2.,  2.,
          2., -2.,  2., -2., -2., -2.],
        [ 2., -2.,  2., -2.,  2., -2., -2.,  2., -2.,  2., -2., -2.,  2.,  2.,
          2., -2.,  2., -2., -2., -2.],
        [-2., -2., -2.,  2., -2., -2., -2., -2.,  2., -2.,  2.,  2.,  2., -2.,
         -2., -2., -2., -2.,  2., -2.],
        [-2., -2., -2., -2., -2.,  2., -2., -2.,  2., -2., -2., -2., -2., -2.,
          2., -2., -2.,  2., -2.,  2.],
        [-2.,  2.,  2.,  2.,  2.,  2.,  2., -2.,  2., -2.,  2.,  2., -2., -2.,
         -2.,  2., -2., -2., -2., -2.],
        [-2., -2., -2., -2.,  2.,  2.,  2., -2., -2.,  2.,  2.,  2., -2., -2.,
          2.,  2., -2., -2., -2., -2.],
        [-2., -2., -2.,  2.,  2.,  2.,  2.,  2., -2.,  2., -2.,  2.,  2.,  2.,
         -2.,  2.,  2.,  2.,  2., -2.],
        [ 2.,  2., -2.,  2.,  2., -2.,  2., -2.,  2., -2., -2.,  2.,  2.,  2.,
          2., -2., -2., -2.,  2.,  2.],
        [-2., -2., -2.,  2., -2.,  2., -2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,
         -2., -2., -2.,  2., -2.,  2.],
        [ 2.,  2.,  2.,  2.,  2.,  2., -2., -2.,  2.,  2.,  2., -2., -2.,  2.,
         -2., -2.,  2.,  2., -2.,  2.],
        [-2.,  2., -2., -2., -2., -2., -2., -2., -2.,  2., -2.,  2.,  2., -2.,
          2., -2.,  2.,  2., -2.,  2.],
        [ 2.,  2.,  2.,  2.,  2.,  2., -2.,  2., -2., -2.,  2., -2., -2., -2.,
         -2., -2.,  2.,  2.,  2., -2.]], device='cuda:0', dtype=torch.float64,
       grad_fn=<ClampBackward1>)
Tensor(shape=[20, 20], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [[ 2., -2.,  2.,  2.,  2., -2., -2.,  2., -2., -2.,  2., -2., -2., -2.,
          2.,  2.,  2., -2., -2., -2.],
        [-2.,  2., -2., -2., -2.,  2.,  2., -2.,  2.,  2., -2., -2.,  2., -2.,
          2.,  2.,  2.,  2.,  2., -2.],
        [ 2., -2.,  2.,  2.,  2., -2., -2., -2., -2., -2., -2., -2., -2.,  2.,
          2., -2., -2., -2.,  2.,  2.],
        [-2., -2., -2., -2., -2.,  2., -2.,  2.,  2., -2., -2.,  2.,  2., -2.,
         -2., -2.,  2.,  2., -2.,  2.],
        [-2.,  2.,  2.,  2., -2., -2., -2., -2., -2., -2.,  2., -2., -2.,  2.,
         -2.,  2.,  2., -2.,  2.,  2.],
        [-2., -2., -2.,  2.,  2., -2.,  2., -2., -2.,  2.,  2., -2., -2., -2.,
         -2., -2., -2., -2., -2., -2.],
        [-2., -2.,  2., -2.,  2.,  2., -2.,  2.,  2., -2., -2., -2., -2., -2.,
         -2.,  2.,  2.,  2., -2.,  2.],
        [ 2.,  2., -2., -2.,  2., -2., -2.,  2.,  2.,  2., -2., -2.,  2.,  2.,
         -2., -2.,  2., -2., -2.,  2.],
        [ 2., -2., -2.,  2., -2.,  2.,  2.,  2.,  2., -2., -2.,  2., -2.,  2.,
          2., -2.,  2., -2., -2., -2.],
        [ 2., -2.,  2., -2.,  2., -2., -2.,  2., -2.,  2., -2., -2.,  2.,  2.,
          2., -2.,  2., -2., -2., -2.],
        [-2., -2., -2.,  2., -2., -2., -2., -2.,  2., -2.,  2.,  2.,  2., -2.,
         -2., -2., -2., -2.,  2., -2.],
        [-2., -2., -2., -2., -2.,  2., -2., -2.,  2., -2., -2., -2., -2., -2.,
          2., -2., -2.,  2., -2.,  2.],
        [-2.,  2.,  2.,  2.,  2.,  2.,  2., -2.,  2., -2.,  2.,  2., -2., -2.,
         -2.,  2., -2., -2., -2., -2.],
        [-2., -2., -2., -2.,  2.,  2.,  2., -2., -2.,  2.,  2.,  2., -2., -2.,
          2.,  2., -2., -2., -2., -2.],
        [-2., -2., -2.,  2.,  2.,  2.,  2.,  2., -2.,  2., -2.,  2.,  2.,  2.,
         -2.,  2.,  2.,  2.,  2., -2.],
        [ 2.,  2., -2.,  2.,  2., -2.,  2., -2.,  2., -2., -2.,  2.,  2.,  2.,
          2., -2., -2., -2.,  2.,  2.],
        [-2., -2., -2.,  2., -2.,  2., -2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,
         -2., -2., -2.,  2., -2.,  2.],
        [ 2.,  2.,  2.,  2.,  2.,  2., -2., -2.,  2.,  2.,  2., -2., -2.,  2.,
         -2., -2.,  2.,  2., -2.,  2.],
        [-2.,  2., -2., -2., -2., -2., -2., -2., -2.,  2., -2.,  2.,  2., -2.,
          2., -2.,  2.,  2., -2.,  2.],
        [ 2.,  2.,  2.,  2.,  2.,  2., -2.,  2., -2., -2.,  2., -2., -2., -2.,
         -2., -2.,  2.,  2.,  2., -2.]])
[Pass] paddle.clip(Tensor([20, 20],"float64"), min=-2.0, max=2.0, )
2025-05-16 05:34:39.637161 test begin: paddle.clip(Tensor([24, 17, 128, 128],"float64"), min=0, max=2, )
tensor([[[[ 6.9128e+307, -7.1766e+306, -6.9930e+307,  ..., -5.0042e+306,
            2.5128e+307,  1.9363e+307],
          [-4.6348e+307, -7.7590e+307, -4.9506e+306,  ...,  6.3680e+307,
           -6.7581e+307, -3.1553e+307],
          [ 4.7654e+307,  5.0644e+307,  2.4940e+307,  ...,  8.4297e+307,
           -7.9545e+307,  5.9877e+306],
          ...,
          [-8.6785e+307, -8.0517e+307, -8.2052e+307,  ..., -2.1248e+307,
           -1.7204e+307, -3.4811e+307],
          [-8.2648e+307,  1.9445e+307,  8.3941e+306,  ..., -5.3024e+307,
           -7.8618e+307,  4.7336e+307],
          [ 7.9791e+307, -2.4553e+307, -2.7932e+307,  ..., -7.4885e+307,
            5.3264e+307, -8.0961e+307]],

         [[ 8.1599e+307,  7.0845e+307, -5.7605e+307,  ...,  7.8989e+307,
            1.7284e+307, -4.5196e+307],
          [ 8.7685e+307, -8.0243e+307, -4.2653e+307,  ..., -5.3497e+307,
            2.1372e+307,  8.0881e+307],
          [-6.1456e+307,  3.9232e+307,  2.3324e+307,  ...,  5.3356e+307,
           -3.4598e+306,  8.7814e+307],
          ...,
          [-4.8717e+307,  4.1068e+307, -1.4799e+307,  ..., -6.9797e+307,
           -2.8912e+307,  4.8604e+307],
          [ 7.1740e+307, -8.5543e+307,  2.5351e+306,  ..., -5.4715e+307,
           -2.7057e+307,  5.6315e+307],
          [ 5.9803e+307, -3.0873e+307,  9.5289e+306,  ..., -2.6194e+307,
           -7.3028e+307,  3.7490e+307]],

         [[-1.3231e+307, -6.3233e+307,  6.3596e+307,  ...,  2.9410e+307,
           -3.6513e+307,  4.6199e+307],
          [-1.5542e+307,  7.7498e+307,  3.9767e+307,  ...,  4.0427e+306,
            3.6067e+307,  5.2558e+307],
          [ 5.2282e+307, -6.9235e+307, -7.1554e+307,  ...,  1.9286e+307,
           -9.5212e+306, -8.8715e+306],
          ...,
          [ 4.7655e+307,  7.3574e+307, -8.4658e+307,  ...,  6.0993e+307,
            2.4533e+307,  1.6793e+307],
          [-1.8787e+307,  3.0646e+307, -4.9207e+307,  ..., -7.8361e+307,
            4.5415e+307, -6.2495e+306],
          [-2.4108e+307, -5.2990e+307,  4.2307e+307,  ...,  7.2687e+307,
            9.3143e+306, -1.3652e+307]],

         ...,

         [[-1.0649e+307,  4.6932e+306, -4.1285e+306,  ...,  3.2202e+307,
            3.3777e+306,  4.1546e+307],
          [-2.3233e+306, -2.1290e+306, -1.8292e+307,  ..., -7.2831e+307,
           -4.3732e+307, -2.0241e+307],
          [ 7.7579e+307,  8.3657e+307,  5.1700e+307,  ..., -5.8303e+307,
            8.7192e+307,  4.6936e+307],
          ...,
          [ 4.4814e+307, -3.0133e+307, -7.7264e+307,  ...,  2.2878e+306,
            3.6306e+307, -6.1217e+307],
          [ 3.1633e+307,  5.6020e+307,  6.2535e+307,  ...,  7.3223e+307,
           -4.9234e+307, -3.2632e+307],
          [ 2.1352e+307,  1.5576e+306,  4.8566e+307,  ..., -7.8130e+307,
           -7.6664e+307,  6.0194e+307]],

         [[ 6.2539e+307,  7.0117e+307, -5.7149e+307,  ...,  3.3794e+306,
           -3.0790e+307, -6.7652e+307],
          [-4.3461e+307,  5.2295e+306, -6.4547e+307,  ...,  5.2768e+307,
           -1.9318e+307, -4.2946e+306],
          [-5.3151e+307,  2.2223e+307,  1.9247e+307,  ..., -8.0230e+307,
            1.9643e+306, -6.4553e+307],
          ...,
          [ 7.3798e+307, -8.7857e+306, -6.3374e+307,  ..., -5.3470e+307,
            8.6797e+307,  4.7702e+307],
          [ 3.7060e+307, -4.4275e+307, -3.2577e+307,  ...,  3.8904e+307,
           -3.2805e+306,  3.5050e+307],
          [-4.0769e+306,  6.6706e+307, -2.3760e+307,  ..., -7.4936e+306,
            2.7883e+307, -5.5237e+307]],

         [[ 7.2007e+307,  1.8648e+307,  2.6628e+306,  ...,  2.9179e+306,
            4.6356e+307, -3.4596e+307],
          [ 8.5455e+307,  8.7139e+306, -5.4973e+307,  ..., -7.4801e+306,
            1.5513e+307,  8.6279e+305],
          [ 5.9096e+307, -6.9682e+307,  8.4391e+307,  ...,  7.6881e+307,
           -7.6532e+307,  2.6259e+307],
          ...,
          [-2.1514e+306, -8.5289e+307,  7.4254e+307,  ...,  4.3991e+307,
           -3.4127e+307, -1.5308e+307],
          [-5.3652e+307, -2.4851e+307, -1.1015e+306,  ...,  2.6119e+307,
            8.2629e+307, -4.6357e+307],
          [ 4.0973e+307, -3.1962e+307,  4.3321e+307,  ..., -7.8932e+307,
            7.4634e+307, -8.4062e+307]]],


        [[[-1.6573e+307,  3.6139e+307,  6.5476e+306,  ...,  1.1509e+307,
            1.5109e+307, -5.4747e+307],
          [-2.3521e+307, -5.7225e+307, -3.9600e+307,  ...,  2.6256e+307,
           -2.9287e+307, -4.1702e+307],
          [-8.7806e+307,  1.5940e+306, -4.0038e+307,  ..., -5.0192e+307,
           -8.7215e+307,  2.4806e+307],
          ...,
          [-4.2157e+307,  5.3793e+307,  5.6399e+307,  ...,  3.4178e+307,
            5.3335e+307, -7.2057e+307],
          [-8.8030e+307, -5.5647e+307,  1.8648e+306,  ..., -5.7271e+307,
            5.9424e+307, -8.8921e+307],
          [ 5.9402e+307, -4.6252e+307,  1.8912e+307,  ...,  5.6200e+307,
           -8.3562e+307,  3.5708e+307]],

         [[ 1.9880e+307, -4.8324e+307, -3.8958e+307,  ...,  2.9722e+307,
            5.0004e+307,  3.3533e+307],
          [ 3.0255e+307,  5.9860e+307,  3.4661e+307,  ..., -1.5019e+307,
            4.2335e+307, -6.3082e+307],
          [-6.4054e+307,  2.8084e+307,  1.2322e+307,  ..., -7.8510e+306,
            4.2779e+307, -7.3077e+307],
          ...,
          [ 7.8941e+306, -7.1065e+307,  3.9592e+307,  ..., -6.7195e+307,
            7.4980e+307,  2.0184e+307],
          [-3.5160e+307,  3.8002e+307,  8.3179e+307,  ...,  2.5372e+307,
            2.3259e+307, -7.7938e+307],
          [-2.8716e+307,  3.5678e+307, -6.2095e+307,  ...,  5.5273e+307,
           -5.8262e+307, -4.6741e+307]],

         [[-6.6677e+307,  7.1542e+307,  2.5996e+307,  ..., -3.3241e+307,
           -5.7431e+307, -1.9573e+307],
          [-1.6475e+307, -1.9073e+307,  1.6065e+306,  ...,  5.4177e+307,
            6.1996e+307,  2.2681e+307],
          [-3.6832e+307, -4.4304e+307, -8.0907e+307,  ..., -3.0788e+307,
           -8.0526e+306,  4.8865e+307],
          ...,
          [-4.6589e+307, -9.6022e+306, -2.4627e+307,  ...,  2.8966e+306,
           -2.8021e+307,  8.5100e+307],
          [-4.5068e+307, -4.3547e+307,  3.7094e+307,  ...,  5.9535e+307,
            4.4362e+306,  2.8219e+307],
          [-6.6618e+307,  8.3409e+307,  3.0736e+307,  ..., -6.5727e+307,
           -5.9484e+307, -2.9103e+306]],

         ...,

         [[-1.0243e+307,  4.7014e+307, -2.9510e+307,  ..., -3.8467e+307,
           -6.5000e+307,  3.3654e+307],
          [ 4.9532e+307,  8.4000e+307, -4.7974e+307,  ..., -7.3927e+307,
            7.1852e+307,  4.5763e+307],
          [ 8.1378e+307, -8.4447e+307, -6.7825e+306,  ...,  7.5897e+307,
            3.2201e+307,  8.4881e+307],
          ...,
          [-8.3567e+307, -8.8319e+307, -4.2801e+307,  ..., -8.7609e+307,
           -4.9953e+307,  4.9044e+307],
          [ 3.8402e+307,  4.9349e+307, -6.9146e+307,  ..., -1.4637e+307,
           -3.1073e+306,  6.2443e+307],
          [-4.9413e+307,  4.7978e+307, -5.7399e+307,  ...,  4.3732e+307,
           -6.8530e+307, -8.2660e+307]],

         [[-5.7159e+307, -3.9524e+307, -6.3587e+307,  ...,  7.4075e+306,
            4.3033e+307,  4.8610e+307],
          [-3.1410e+307, -1.1224e+307,  7.1718e+307,  ..., -7.1623e+307,
            5.5711e+307,  8.2457e+306],
          [-8.5608e+306,  1.8022e+307, -7.1625e+307,  ..., -4.3639e+306,
            8.7313e+307, -4.7958e+307],
          ...,
          [-7.1116e+307,  8.4555e+307,  5.4466e+307,  ..., -4.4763e+307,
            6.8823e+307,  4.3826e+307],
          [-2.1915e+307, -3.6741e+307, -8.1382e+306,  ..., -3.7777e+307,
            4.4817e+307, -1.0052e+307],
          [-7.5555e+307, -1.0021e+307, -5.9829e+307,  ...,  5.7755e+307,
           -7.7314e+307, -5.1289e+307]],

         [[ 7.8771e+307,  5.4030e+307, -1.9848e+307,  ...,  2.1636e+307,
            7.8520e+307,  2.5643e+307],
          [-4.3391e+307,  9.3252e+306, -2.2765e+307,  ...,  7.3755e+307,
           -1.3564e+307, -3.4182e+306],
          [-8.1933e+307,  2.9689e+307,  5.9436e+307,  ..., -2.8506e+307,
           -2.3560e+307,  3.6619e+307],
          ...,
          [-6.8813e+307, -6.8119e+306,  5.9440e+307,  ..., -5.4177e+307,
            6.3858e+307,  6.2642e+307],
          [-5.2068e+307, -2.9322e+306, -4.6267e+307,  ..., -3.4479e+307,
           -8.7493e+307,  6.1436e+307],
          [ 4.7068e+307, -5.5106e+304,  2.8592e+306,  ..., -2.8348e+307,
           -6.8266e+307,  4.1170e+307]]],


        [[[ 5.2816e+307, -3.3040e+307,  8.3433e+307,  ..., -3.8947e+306,
            5.0110e+307,  1.0108e+307],
          [-9.2167e+306,  7.6179e+307,  7.0358e+307,  ...,  4.6252e+307,
           -7.8092e+307, -1.0814e+307],
          [ 3.3224e+306,  4.6235e+307,  2.9432e+307,  ..., -1.3582e+307,
            6.2809e+307,  2.8688e+307],
          ...,
          [ 6.0122e+307,  6.2509e+307,  8.5004e+307,  ...,  5.1453e+307,
            3.3887e+305,  4.8832e+307],
          [ 2.0701e+307,  7.0563e+307,  7.4372e+307,  ..., -5.3105e+307,
            8.0570e+307, -8.7729e+307],
          [-3.8098e+307,  8.6749e+307, -8.8583e+307,  ..., -7.4237e+307,
           -1.0108e+307,  1.5317e+307]],

         [[-4.2115e+307, -5.9367e+307, -7.8969e+307,  ...,  3.8554e+307,
           -5.9690e+307,  3.3336e+307],
          [ 4.4563e+307,  1.6542e+307, -6.0868e+307,  ..., -8.4611e+307,
           -6.4630e+307,  5.2305e+307],
          [ 5.7288e+306,  4.1151e+307, -8.1858e+307,  ..., -5.9345e+307,
            4.6936e+307,  7.0428e+307],
          ...,
          [-7.0721e+307,  8.9873e+307,  6.2449e+306,  ...,  3.5171e+307,
           -4.6166e+307, -9.5321e+306],
          [ 8.3378e+307, -6.5727e+307, -2.0780e+307,  ...,  8.6436e+307,
            5.7668e+307,  3.0621e+307],
          [-7.7288e+307, -2.1116e+307, -2.3927e+307,  ...,  3.3155e+307,
           -7.7726e+307,  6.8690e+307]],

         [[ 3.6714e+307, -1.6404e+307,  6.0226e+306,  ..., -4.6363e+307,
            6.0437e+307, -3.6897e+307],
          [ 2.9019e+307, -2.1321e+307, -4.6366e+307,  ...,  8.1369e+307,
            8.9552e+307, -7.3809e+307],
          [ 5.9674e+307, -3.8000e+307, -3.5842e+307,  ...,  8.7164e+307,
            2.6338e+307, -6.6042e+307],
          ...,
          [-2.0623e+307,  3.0983e+307,  5.8479e+307,  ...,  7.0530e+307,
           -9.6731e+306,  3.9610e+307],
          [-2.9584e+307, -5.9985e+306,  2.9662e+307,  ...,  2.2891e+307,
            5.8137e+306, -8.7130e+306],
          [-8.2442e+307, -4.3983e+306, -7.0598e+307,  ...,  7.1598e+307,
            4.7457e+307,  3.8616e+306]],

         ...,

         [[ 8.6973e+307,  8.3553e+307, -7.5634e+307,  ...,  5.6177e+307,
            6.6613e+307,  7.7397e+307],
          [-1.0054e+307,  5.9645e+306,  1.1844e+307,  ..., -9.1899e+306,
            6.8106e+307,  5.6656e+307],
          [ 2.9567e+307, -9.2215e+306,  4.7063e+307,  ..., -4.5689e+307,
            6.8148e+307,  6.2491e+307],
          ...,
          [ 8.0825e+307, -7.5394e+307,  7.0972e+307,  ...,  6.4126e+307,
           -7.7376e+307, -6.1584e+307],
          [-8.8679e+307,  5.1386e+307, -7.0583e+306,  ..., -6.1448e+307,
            5.4334e+307, -6.4967e+307],
          [-7.1516e+307,  7.4750e+306,  6.2677e+307,  ..., -4.5783e+307,
            4.3699e+307,  7.9367e+307]],

         [[-4.7683e+307,  4.8694e+307, -2.9510e+307,  ...,  2.2040e+307,
           -4.3060e+307, -5.3251e+307],
          [-5.6241e+307, -3.1063e+306, -6.5957e+307,  ...,  5.3182e+307,
            6.6297e+307, -3.0791e+307],
          [-4.0845e+305,  3.6530e+307, -7.9660e+307,  ...,  1.8210e+307,
            6.9438e+307, -7.8613e+307],
          ...,
          [-4.0722e+307,  5.3154e+307,  4.7484e+307,  ...,  8.7063e+307,
           -1.8316e+307,  5.7251e+307],
          [-8.0538e+307, -7.5574e+307,  1.7890e+307,  ...,  2.8276e+307,
           -2.3110e+307,  7.6189e+307],
          [ 1.0121e+307, -8.1156e+307,  3.4778e+307,  ..., -3.8879e+307,
           -7.3138e+307,  3.9002e+307]],

         [[-3.3490e+307, -2.2831e+306,  8.2129e+307,  ...,  5.7420e+307,
            4.5367e+307,  6.7272e+307],
          [-6.9193e+307,  6.1402e+306,  1.9226e+307,  ...,  4.6797e+307,
            2.2717e+307,  6.7829e+307],
          [ 5.1051e+307,  7.2483e+307,  5.4298e+306,  ..., -1.8613e+307,
            7.4742e+307, -2.5310e+306],
          ...,
          [-7.9489e+307,  4.0043e+307, -6.9991e+307,  ..., -1.8969e+307,
            5.3645e+307,  3.5168e+307],
          [ 5.8329e+307,  7.1622e+307, -5.7577e+307,  ..., -4.9830e+307,
           -4.7993e+307,  2.4077e+306],
          [ 8.4233e+307,  6.9262e+307,  2.8673e+307,  ..., -6.9295e+307,
            5.3189e+307,  7.4144e+307]]],


        ...,


        [[[ 4.9753e+306, -4.7531e+306,  6.3203e+307,  ..., -2.2896e+307,
           -4.8943e+307, -3.0777e+307],
          [ 4.0478e+307,  4.2911e+307, -4.8047e+306,  ..., -6.8039e+307,
            6.2000e+307,  3.9522e+307],
          [-3.7510e+307,  4.1303e+307,  4.1077e+306,  ..., -3.0972e+307,
           -5.8804e+307, -8.5467e+307],
          ...,
          [ 4.3608e+307,  6.5845e+307, -4.0469e+307,  ..., -4.1989e+307,
            8.5727e+307,  4.8427e+307],
          [ 6.0885e+307, -7.3452e+307, -4.6433e+307,  ...,  1.0594e+306,
           -2.6420e+307,  6.8485e+307],
          [-3.4544e+307, -1.2414e+307,  7.3762e+307,  ..., -5.2814e+307,
            2.6599e+307,  8.6460e+307]],

         [[-5.1815e+307, -4.2108e+307, -2.6637e+306,  ...,  6.2058e+307,
           -3.9045e+307, -5.3176e+307],
          [ 5.3567e+307, -2.0930e+307, -7.2317e+307,  ...,  1.3655e+307,
            6.2757e+307,  7.1695e+307],
          [ 4.9915e+307,  1.4141e+306,  8.4897e+307,  ..., -7.1156e+307,
            3.1447e+307,  3.8299e+307],
          ...,
          [-1.3869e+307,  8.3847e+307, -7.5786e+307,  ..., -5.8517e+307,
           -4.2338e+307,  1.7914e+307],
          [-7.5779e+307,  6.6710e+306,  6.8101e+307,  ..., -9.9317e+306,
            8.7817e+307, -1.8337e+307],
          [-5.4024e+306, -7.0299e+307, -5.6065e+307,  ..., -1.9736e+307,
            3.2584e+307, -7.9315e+307]],

         [[-5.1568e+307, -8.3731e+307,  7.9482e+307,  ..., -3.8799e+307,
           -8.6107e+307,  1.7840e+307],
          [-3.1805e+307,  8.8691e+307,  1.7620e+307,  ..., -3.3770e+307,
            1.2350e+307, -7.8834e+306],
          [-7.4950e+306,  2.4046e+306, -7.5913e+307,  ..., -6.1524e+307,
            4.3729e+307, -7.9632e+306],
          ...,
          [-5.5872e+306, -2.3254e+306, -4.3121e+306,  ...,  5.4888e+307,
           -4.6236e+307,  1.8667e+307],
          [-6.8298e+307,  3.2150e+307,  5.3418e+307,  ...,  4.6358e+307,
            2.6580e+307,  7.6111e+306],
          [-5.8885e+306,  7.8876e+307,  4.9211e+307,  ...,  8.5677e+307,
           -3.6834e+306, -6.6638e+307]],

         ...,

         [[-6.8052e+307, -4.2188e+307,  4.8499e+307,  ..., -5.1001e+307,
            2.8054e+307,  4.8817e+307],
          [ 2.7556e+307,  6.3259e+307,  1.3009e+307,  ...,  1.7496e+307,
            6.5127e+307,  3.2578e+307],
          [-6.8640e+307, -2.9232e+307, -4.5261e+307,  ..., -3.2188e+307,
           -8.4196e+307,  5.3127e+307],
          ...,
          [-7.0969e+307, -6.7797e+307, -8.0951e+307,  ...,  4.7508e+307,
           -7.9499e+307, -8.6101e+307],
          [ 4.6755e+307,  8.8157e+307,  2.3362e+307,  ..., -4.1272e+307,
            1.6105e+307, -2.1653e+307],
          [-8.7967e+307,  8.1461e+307,  7.0077e+307,  ..., -3.7226e+307,
           -6.6768e+307, -2.0176e+307]],

         [[-1.3790e+307,  2.5849e+307, -1.0716e+307,  ..., -7.2912e+307,
            3.8519e+307,  2.1649e+307],
          [-1.0617e+307,  2.6992e+307, -5.0004e+307,  ..., -5.4309e+307,
           -5.1964e+307, -8.8419e+307],
          [ 2.8381e+307, -6.4158e+307,  2.3209e+307,  ...,  4.6729e+307,
            7.2811e+307, -1.8396e+307],
          ...,
          [ 6.1802e+307,  3.2919e+306, -8.6984e+307,  ..., -7.0465e+307,
            5.0301e+307,  8.8409e+307],
          [-7.9610e+307,  6.9536e+307,  4.5371e+307,  ..., -7.5167e+307,
           -6.0913e+307,  4.0844e+307],
          [-4.9592e+306, -3.4255e+306, -3.1759e+307,  ...,  5.0187e+307,
            1.4181e+307,  2.8198e+307]],

         [[-8.5191e+306,  6.5648e+307,  8.6017e+307,  ..., -8.4961e+307,
           -8.6194e+307, -1.9796e+307],
          [-4.2714e+305,  3.6974e+307, -7.5379e+307,  ..., -7.1609e+307,
           -5.1696e+307, -2.2060e+307],
          [ 3.1870e+307, -7.2856e+307, -8.7058e+307,  ..., -3.5085e+307,
            8.4023e+307, -3.0087e+307],
          ...,
          [-6.5052e+307,  4.2816e+307, -5.1310e+307,  ..., -4.5816e+307,
            7.5591e+307, -7.1214e+307],
          [ 5.0042e+307, -3.4907e+307, -7.6487e+307,  ...,  8.5543e+307,
            2.9187e+307,  2.8854e+307],
          [ 5.0037e+306, -3.4695e+307, -8.2329e+307,  ..., -3.0124e+307,
           -6.3984e+307,  3.6526e+307]]],


        [[[ 8.5393e+307,  3.9595e+306, -1.6321e+307,  ..., -5.4966e+307,
           -3.8551e+307,  6.8982e+307],
          [ 4.8706e+307, -6.2981e+307, -6.7284e+307,  ..., -3.7277e+307,
           -6.9589e+307,  1.9704e+307],
          [-6.3560e+307, -7.6142e+307, -1.5912e+307,  ..., -5.7701e+307,
            6.3547e+307, -2.7452e+307],
          ...,
          [-5.8653e+307, -8.9069e+307, -1.2818e+307,  ..., -2.4266e+307,
           -2.5663e+307, -7.8578e+307],
          [-2.0570e+306, -3.0142e+307,  4.3405e+306,  ..., -2.7543e+307,
            4.1189e+307, -7.7493e+307],
          [-1.5346e+306, -7.5658e+307, -7.4577e+307,  ..., -7.7556e+307,
            8.4648e+307, -8.7467e+307]],

         [[ 1.4632e+306, -2.5300e+307, -8.1593e+307,  ...,  7.0285e+307,
            8.1405e+307, -4.0017e+306],
          [ 2.5459e+307,  7.7062e+306,  4.9461e+307,  ...,  3.9452e+307,
           -9.8210e+306, -6.7831e+307],
          [ 5.5127e+307, -8.7838e+307, -1.9090e+307,  ...,  3.8759e+307,
           -4.7296e+307,  5.4996e+307],
          ...,
          [-9.8028e+306,  3.4913e+307,  7.5210e+307,  ..., -5.2554e+307,
           -8.9469e+307,  3.2395e+307],
          [-6.0406e+307, -1.7870e+307, -5.2055e+307,  ...,  1.2619e+307,
            5.9032e+307, -6.3744e+307],
          [ 2.0314e+307, -7.1294e+307,  1.7812e+307,  ...,  5.2462e+307,
           -6.2037e+307,  4.2074e+307]],

         [[-4.3374e+307, -7.3296e+307, -5.4411e+307,  ...,  6.8729e+307,
            7.4925e+307, -5.0980e+307],
          [-7.1576e+307,  3.6514e+307, -8.4623e+307,  ..., -1.3189e+306,
            7.3610e+306,  7.3150e+307],
          [ 5.8157e+307, -2.0393e+307,  5.0946e+306,  ...,  7.7338e+307,
           -2.5337e+307, -2.3671e+307],
          ...,
          [ 3.4962e+306, -4.7660e+307,  5.0759e+307,  ..., -1.7323e+307,
           -9.6157e+306, -5.0518e+307],
          [ 4.6140e+307,  6.1996e+307,  4.4629e+307,  ..., -5.8248e+307,
           -2.9978e+307,  8.5388e+307],
          [-3.5084e+307,  6.2073e+307, -8.6706e+307,  ..., -2.6321e+307,
            8.5573e+307,  2.1264e+307]],

         ...,

         [[ 3.6258e+307,  1.4489e+307,  1.9604e+307,  ...,  1.7099e+307,
           -5.8991e+307, -7.0207e+307],
          [ 7.9812e+307,  3.1546e+307, -5.5922e+307,  ...,  1.1286e+307,
            6.2695e+307,  1.9732e+307],
          [-1.1142e+307,  2.2286e+307,  6.2858e+306,  ...,  6.1943e+306,
           -2.4298e+307, -1.0313e+307],
          ...,
          [-1.6064e+307, -7.2362e+307,  2.3071e+306,  ..., -2.6717e+307,
           -6.8560e+307,  6.0363e+307],
          [ 7.4931e+307, -4.9949e+307,  6.2777e+306,  ..., -3.2295e+307,
            3.9178e+307, -7.6558e+307],
          [ 4.9928e+307, -9.3097e+306,  4.5254e+307,  ...,  6.1383e+307,
            7.5879e+307,  8.6837e+307]],

         [[ 8.0725e+307, -6.4391e+306,  8.6718e+307,  ...,  7.8093e+307,
            3.7218e+307, -2.0430e+307],
          [-1.7572e+307,  4.1318e+307,  4.6802e+307,  ..., -5.1973e+307,
           -5.5170e+307, -5.3672e+307],
          [-8.5590e+307,  3.9794e+307,  7.0759e+307,  ..., -7.3237e+307,
            4.3080e+307,  4.3735e+307],
          ...,
          [ 6.5131e+307, -7.9032e+307, -5.4340e+307,  ..., -1.8941e+307,
            1.9072e+307,  5.7454e+307],
          [-4.7194e+307, -5.8535e+307,  4.1565e+307,  ...,  5.6955e+307,
           -8.1916e+307,  3.6571e+307],
          [ 2.3978e+307,  5.6286e+307, -1.2564e+307,  ..., -4.5946e+307,
            4.7961e+307, -7.4746e+307]],

         [[-6.1964e+307, -6.7125e+307, -4.3779e+307,  ...,  5.7986e+307,
           -8.0242e+307, -5.2349e+307],
          [ 2.4629e+307,  4.2328e+307, -3.9692e+307,  ...,  5.9986e+307,
           -2.0106e+307,  6.3505e+307],
          [-8.3766e+307,  3.8892e+307, -3.9536e+307,  ..., -3.8046e+305,
            8.3376e+307,  6.0671e+307],
          ...,
          [-3.2462e+307, -1.2528e+306,  2.6186e+307,  ...,  1.0928e+306,
            3.3511e+307,  7.8227e+307],
          [-1.0295e+307, -1.9246e+307, -3.7841e+307,  ..., -1.5817e+307,
           -3.6533e+307, -3.5820e+307],
          [ 6.3654e+307,  1.3238e+307, -2.9179e+307,  ...,  8.5004e+307,
           -8.6959e+307, -5.2079e+307]]],


        [[[ 8.1178e+306,  2.3994e+307, -5.8348e+307,  ...,  9.0444e+306,
           -2.8970e+307,  5.2470e+307],
          [ 5.8689e+307, -7.8362e+307, -3.7629e+307,  ...,  1.4972e+307,
            3.5077e+307,  5.0525e+306],
          [-2.5862e+307, -5.3205e+307,  2.2162e+307,  ...,  4.4750e+307,
            4.8964e+307, -4.2933e+307],
          ...,
          [ 7.9794e+307, -5.5487e+307,  5.1975e+307,  ..., -7.4754e+307,
            6.7547e+307,  2.8449e+306],
          [ 8.4692e+307, -5.7547e+307,  1.5649e+307,  ...,  7.9111e+307,
            6.9502e+307, -1.8945e+307],
          [-8.8154e+307, -2.8922e+306,  3.7263e+306,  ...,  1.2983e+306,
            6.6163e+307,  5.5229e+307]],

         [[-5.2096e+307,  5.1335e+307,  3.0556e+307,  ...,  8.8937e+307,
           -2.8542e+307,  1.6323e+307],
          [ 7.1904e+307, -3.0641e+307,  6.3352e+307,  ...,  7.9539e+307,
           -1.3106e+307,  4.6182e+307],
          [ 7.1772e+307,  5.6880e+307,  2.0211e+307,  ..., -6.2277e+307,
           -4.4795e+307,  7.8848e+307],
          ...,
          [ 3.9779e+307,  7.1863e+307, -3.2290e+307,  ..., -1.0312e+307,
           -7.6648e+307,  5.3563e+307],
          [ 4.2282e+307, -2.5087e+307, -7.8100e+307,  ...,  1.4930e+307,
            3.3270e+307, -5.7304e+307],
          [ 1.0763e+307,  1.8947e+307, -4.0045e+307,  ..., -7.6536e+307,
            3.5401e+307,  7.4188e+307]],

         [[ 1.1984e+307, -4.1735e+307,  5.9503e+307,  ..., -5.1198e+307,
            7.8538e+307, -3.2940e+307],
          [ 3.9252e+307,  1.4677e+307, -4.1220e+307,  ..., -2.4529e+307,
            2.7876e+307,  1.0531e+306],
          [ 2.9149e+307,  8.9649e+307, -8.5631e+307,  ..., -7.8865e+307,
           -1.9324e+307, -4.8423e+307],
          ...,
          [-3.9654e+307, -6.2189e+307, -3.1852e+307,  ..., -1.4906e+307,
           -2.7261e+307,  7.1095e+307],
          [ 6.7168e+307,  6.8402e+307,  2.0551e+307,  ..., -8.1546e+307,
            6.1094e+307, -7.3443e+307],
          [ 3.2520e+305, -5.8994e+307,  5.7909e+307,  ...,  6.0248e+306,
            6.0428e+307,  6.9934e+307]],

         ...,

         [[ 7.1621e+307, -7.4803e+307, -9.0716e+306,  ..., -7.9891e+307,
           -5.9469e+307,  5.1736e+305],
          [ 8.7156e+307, -2.8587e+307,  8.7857e+307,  ..., -6.3347e+307,
           -1.6248e+307, -8.4317e+306],
          [-3.3836e+307, -5.3021e+307,  5.7166e+307,  ...,  6.8129e+307,
           -4.9294e+307, -5.9566e+307],
          ...,
          [ 2.6138e+307, -7.0488e+307, -5.3688e+307,  ..., -4.1948e+307,
           -7.9654e+307,  4.7023e+307],
          [ 4.0082e+307,  4.5479e+307,  4.9345e+307,  ..., -5.5877e+307,
           -5.4645e+307, -7.4317e+307],
          [ 4.3710e+306,  4.3703e+307, -8.6793e+307,  ...,  6.4262e+307,
           -3.4678e+307,  6.9769e+307]],

         [[ 5.1353e+307,  5.4533e+307, -7.4684e+307,  ..., -5.6661e+307,
            5.5187e+307, -7.1107e+307],
          [ 2.1626e+306,  2.4219e+307, -6.1777e+307,  ..., -3.0298e+307,
            7.7947e+307,  2.1235e+306],
          [ 3.7442e+307,  5.4873e+307, -1.9408e+307,  ..., -7.5238e+307,
            2.8860e+307,  5.5280e+306],
          ...,
          [ 2.1756e+307, -7.7535e+307,  2.3251e+307,  ...,  6.6680e+306,
            4.9138e+307, -6.1878e+307],
          [ 7.0776e+307,  2.1946e+307, -4.4725e+307,  ...,  4.6524e+307,
            1.0153e+307, -1.3643e+307],
          [ 2.5405e+307, -3.8056e+307,  8.3478e+307,  ..., -7.2420e+307,
            5.8591e+307, -8.4516e+307]],

         [[-5.9384e+307,  8.1748e+307,  1.7263e+306,  ..., -6.0844e+307,
            2.8874e+307, -7.3843e+307],
          [-4.3079e+307, -6.7851e+307, -3.4741e+307,  ..., -3.8855e+307,
            6.8345e+307,  3.6922e+307],
          [ 7.4989e+307,  5.8617e+306, -3.3012e+307,  ...,  2.9104e+307,
           -8.3415e+307, -3.0248e+307],
          ...,
          [-4.3676e+307, -2.8666e+307, -6.0026e+307,  ..., -6.9505e+307,
           -5.6876e+307,  2.6208e+307],
          [ 2.1249e+307,  3.7123e+307, -5.1548e+307,  ..., -5.7473e+307,
            8.5206e+307, -4.9590e+307],
          [ 4.6107e+307, -5.7056e+307, -7.5884e+306,  ..., -1.2847e+307,
           -5.9324e+307,  4.6402e+307]]]], device='cuda:0',
       dtype=torch.float64, requires_grad=True)
-----------
tensor([[[[2., 0., 0.,  ..., 0., 2., 2.],
          [0., 0., 0.,  ..., 2., 0., 0.],
          [2., 2., 2.,  ..., 2., 0., 2.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 2., 2.,  ..., 0., 0., 2.],
          [2., 0., 0.,  ..., 0., 2., 0.]],

         [[2., 2., 0.,  ..., 2., 2., 0.],
          [2., 0., 0.,  ..., 0., 2., 2.],
          [0., 2., 2.,  ..., 2., 0., 2.],
          ...,
          [0., 2., 0.,  ..., 0., 0., 2.],
          [2., 0., 2.,  ..., 0., 0., 2.],
          [2., 0., 2.,  ..., 0., 0., 2.]],

         [[0., 0., 2.,  ..., 2., 0., 2.],
          [0., 2., 2.,  ..., 2., 2., 2.],
          [2., 0., 0.,  ..., 2., 0., 0.],
          ...,
          [2., 2., 0.,  ..., 2., 2., 2.],
          [0., 2., 0.,  ..., 0., 2., 0.],
          [0., 0., 2.,  ..., 2., 2., 0.]],

         ...,

         [[0., 2., 0.,  ..., 2., 2., 2.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [2., 2., 2.,  ..., 0., 2., 2.],
          ...,
          [2., 0., 0.,  ..., 2., 2., 0.],
          [2., 2., 2.,  ..., 2., 0., 0.],
          [2., 2., 2.,  ..., 0., 0., 2.]],

         [[2., 2., 0.,  ..., 2., 0., 0.],
          [0., 2., 0.,  ..., 2., 0., 0.],
          [0., 2., 2.,  ..., 0., 2., 0.],
          ...,
          [2., 0., 0.,  ..., 0., 2., 2.],
          [2., 0., 0.,  ..., 2., 0., 2.],
          [0., 2., 0.,  ..., 0., 2., 0.]],

         [[2., 2., 2.,  ..., 2., 2., 0.],
          [2., 2., 0.,  ..., 0., 2., 2.],
          [2., 0., 2.,  ..., 2., 0., 2.],
          ...,
          [0., 0., 2.,  ..., 2., 0., 0.],
          [0., 0., 0.,  ..., 2., 2., 0.],
          [2., 0., 2.,  ..., 0., 2., 0.]]],


        [[[0., 2., 2.,  ..., 2., 2., 0.],
          [0., 0., 0.,  ..., 2., 0., 0.],
          [0., 2., 0.,  ..., 0., 0., 2.],
          ...,
          [0., 2., 2.,  ..., 2., 2., 0.],
          [0., 0., 2.,  ..., 0., 2., 0.],
          [2., 0., 2.,  ..., 2., 0., 2.]],

         [[2., 0., 0.,  ..., 2., 2., 2.],
          [2., 2., 2.,  ..., 0., 2., 0.],
          [0., 2., 2.,  ..., 0., 2., 0.],
          ...,
          [2., 0., 2.,  ..., 0., 2., 2.],
          [0., 2., 2.,  ..., 2., 2., 0.],
          [0., 2., 0.,  ..., 2., 0., 0.]],

         [[0., 2., 2.,  ..., 0., 0., 0.],
          [0., 0., 2.,  ..., 2., 2., 2.],
          [0., 0., 0.,  ..., 0., 0., 2.],
          ...,
          [0., 0., 0.,  ..., 2., 0., 2.],
          [0., 0., 2.,  ..., 2., 2., 2.],
          [0., 2., 2.,  ..., 0., 0., 0.]],

         ...,

         [[0., 2., 0.,  ..., 0., 0., 2.],
          [2., 2., 0.,  ..., 0., 2., 2.],
          [2., 0., 0.,  ..., 2., 2., 2.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 2.],
          [2., 2., 0.,  ..., 0., 0., 2.],
          [0., 2., 0.,  ..., 2., 0., 0.]],

         [[0., 0., 0.,  ..., 2., 2., 2.],
          [0., 0., 2.,  ..., 0., 2., 2.],
          [0., 2., 0.,  ..., 0., 2., 0.],
          ...,
          [0., 2., 2.,  ..., 0., 2., 2.],
          [0., 0., 0.,  ..., 0., 2., 0.],
          [0., 0., 0.,  ..., 2., 0., 0.]],

         [[2., 2., 0.,  ..., 2., 2., 2.],
          [0., 2., 0.,  ..., 2., 0., 0.],
          [0., 2., 2.,  ..., 0., 0., 2.],
          ...,
          [0., 0., 2.,  ..., 0., 2., 2.],
          [0., 0., 0.,  ..., 0., 0., 2.],
          [2., 0., 2.,  ..., 0., 0., 2.]]],


        [[[2., 0., 2.,  ..., 0., 2., 2.],
          [0., 2., 2.,  ..., 2., 0., 0.],
          [2., 2., 2.,  ..., 0., 2., 2.],
          ...,
          [2., 2., 2.,  ..., 2., 2., 2.],
          [2., 2., 2.,  ..., 0., 2., 0.],
          [0., 2., 0.,  ..., 0., 0., 2.]],

         [[0., 0., 0.,  ..., 2., 0., 2.],
          [2., 2., 0.,  ..., 0., 0., 2.],
          [2., 2., 0.,  ..., 0., 2., 2.],
          ...,
          [0., 2., 2.,  ..., 2., 0., 0.],
          [2., 0., 0.,  ..., 2., 2., 2.],
          [0., 0., 0.,  ..., 2., 0., 2.]],

         [[2., 0., 2.,  ..., 0., 2., 0.],
          [2., 0., 0.,  ..., 2., 2., 0.],
          [2., 0., 0.,  ..., 2., 2., 0.],
          ...,
          [0., 2., 2.,  ..., 2., 0., 2.],
          [0., 0., 2.,  ..., 2., 2., 0.],
          [0., 0., 0.,  ..., 2., 2., 2.]],

         ...,

         [[2., 2., 0.,  ..., 2., 2., 2.],
          [0., 2., 2.,  ..., 0., 2., 2.],
          [2., 0., 2.,  ..., 0., 2., 2.],
          ...,
          [2., 0., 2.,  ..., 2., 0., 0.],
          [0., 2., 0.,  ..., 0., 2., 0.],
          [0., 2., 2.,  ..., 0., 2., 2.]],

         [[0., 2., 0.,  ..., 2., 0., 0.],
          [0., 0., 0.,  ..., 2., 2., 0.],
          [0., 2., 0.,  ..., 2., 2., 0.],
          ...,
          [0., 2., 2.,  ..., 2., 0., 2.],
          [0., 0., 2.,  ..., 2., 0., 2.],
          [2., 0., 2.,  ..., 0., 0., 2.]],

         [[0., 0., 2.,  ..., 2., 2., 2.],
          [0., 2., 2.,  ..., 2., 2., 2.],
          [2., 2., 2.,  ..., 0., 2., 0.],
          ...,
          [0., 2., 0.,  ..., 0., 2., 2.],
          [2., 2., 0.,  ..., 0., 0., 2.],
          [2., 2., 2.,  ..., 0., 2., 2.]]],


        ...,


        [[[2., 0., 2.,  ..., 0., 0., 0.],
          [2., 2., 0.,  ..., 0., 2., 2.],
          [0., 2., 2.,  ..., 0., 0., 0.],
          ...,
          [2., 2., 0.,  ..., 0., 2., 2.],
          [2., 0., 0.,  ..., 2., 0., 2.],
          [0., 0., 2.,  ..., 0., 2., 2.]],

         [[0., 0., 0.,  ..., 2., 0., 0.],
          [2., 0., 0.,  ..., 2., 2., 2.],
          [2., 2., 2.,  ..., 0., 2., 2.],
          ...,
          [0., 2., 0.,  ..., 0., 0., 2.],
          [0., 2., 2.,  ..., 0., 2., 0.],
          [0., 0., 0.,  ..., 0., 2., 0.]],

         [[0., 0., 2.,  ..., 0., 0., 2.],
          [0., 2., 2.,  ..., 0., 2., 0.],
          [0., 2., 0.,  ..., 0., 2., 0.],
          ...,
          [0., 0., 0.,  ..., 2., 0., 2.],
          [0., 2., 2.,  ..., 2., 2., 2.],
          [0., 2., 2.,  ..., 2., 0., 0.]],

         ...,

         [[0., 0., 2.,  ..., 0., 2., 2.],
          [2., 2., 2.,  ..., 2., 2., 2.],
          [0., 0., 0.,  ..., 0., 0., 2.],
          ...,
          [0., 0., 0.,  ..., 2., 0., 0.],
          [2., 2., 2.,  ..., 0., 2., 0.],
          [0., 2., 2.,  ..., 0., 0., 0.]],

         [[0., 2., 0.,  ..., 0., 2., 2.],
          [0., 2., 0.,  ..., 0., 0., 0.],
          [2., 0., 2.,  ..., 2., 2., 0.],
          ...,
          [2., 2., 0.,  ..., 0., 2., 2.],
          [0., 2., 2.,  ..., 0., 0., 2.],
          [0., 0., 0.,  ..., 2., 2., 2.]],

         [[0., 2., 2.,  ..., 0., 0., 0.],
          [0., 2., 0.,  ..., 0., 0., 0.],
          [2., 0., 0.,  ..., 0., 2., 0.],
          ...,
          [0., 2., 0.,  ..., 0., 2., 0.],
          [2., 0., 0.,  ..., 2., 2., 2.],
          [2., 0., 0.,  ..., 0., 0., 2.]]],


        [[[2., 2., 0.,  ..., 0., 0., 2.],
          [2., 0., 0.,  ..., 0., 0., 2.],
          [0., 0., 0.,  ..., 0., 2., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 2.,  ..., 0., 2., 0.],
          [0., 0., 0.,  ..., 0., 2., 0.]],

         [[2., 0., 0.,  ..., 2., 2., 0.],
          [2., 2., 2.,  ..., 2., 0., 0.],
          [2., 0., 0.,  ..., 2., 0., 2.],
          ...,
          [0., 2., 2.,  ..., 0., 0., 2.],
          [0., 0., 0.,  ..., 2., 2., 0.],
          [2., 0., 2.,  ..., 2., 0., 2.]],

         [[0., 0., 0.,  ..., 2., 2., 0.],
          [0., 2., 0.,  ..., 0., 2., 2.],
          [2., 0., 2.,  ..., 2., 0., 0.],
          ...,
          [2., 0., 2.,  ..., 0., 0., 0.],
          [2., 2., 2.,  ..., 0., 0., 2.],
          [0., 2., 0.,  ..., 0., 2., 2.]],

         ...,

         [[2., 2., 2.,  ..., 2., 0., 0.],
          [2., 2., 0.,  ..., 2., 2., 2.],
          [0., 2., 2.,  ..., 2., 0., 0.],
          ...,
          [0., 0., 2.,  ..., 0., 0., 2.],
          [2., 0., 2.,  ..., 0., 2., 0.],
          [2., 0., 2.,  ..., 2., 2., 2.]],

         [[2., 0., 2.,  ..., 2., 2., 0.],
          [0., 2., 2.,  ..., 0., 0., 0.],
          [0., 2., 2.,  ..., 0., 2., 2.],
          ...,
          [2., 0., 0.,  ..., 0., 2., 2.],
          [0., 0., 2.,  ..., 2., 0., 2.],
          [2., 2., 0.,  ..., 0., 2., 0.]],

         [[0., 0., 0.,  ..., 2., 0., 0.],
          [2., 2., 0.,  ..., 2., 0., 2.],
          [0., 2., 0.,  ..., 0., 2., 2.],
          ...,
          [0., 0., 2.,  ..., 2., 2., 2.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [2., 2., 0.,  ..., 2., 0., 0.]]],


        [[[2., 2., 0.,  ..., 2., 0., 2.],
          [2., 0., 0.,  ..., 2., 2., 2.],
          [0., 0., 2.,  ..., 2., 2., 0.],
          ...,
          [2., 0., 2.,  ..., 0., 2., 2.],
          [2., 0., 2.,  ..., 2., 2., 0.],
          [0., 0., 2.,  ..., 2., 2., 2.]],

         [[0., 2., 2.,  ..., 2., 0., 2.],
          [2., 0., 2.,  ..., 2., 0., 2.],
          [2., 2., 2.,  ..., 0., 0., 2.],
          ...,
          [2., 2., 0.,  ..., 0., 0., 2.],
          [2., 0., 0.,  ..., 2., 2., 0.],
          [2., 2., 0.,  ..., 0., 2., 2.]],

         [[2., 0., 2.,  ..., 0., 2., 0.],
          [2., 2., 0.,  ..., 0., 2., 2.],
          [2., 2., 0.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 2.],
          [2., 2., 2.,  ..., 0., 2., 0.],
          [2., 0., 2.,  ..., 2., 2., 2.]],

         ...,

         [[2., 0., 0.,  ..., 0., 0., 2.],
          [2., 0., 2.,  ..., 0., 0., 0.],
          [0., 0., 2.,  ..., 2., 0., 0.],
          ...,
          [2., 0., 0.,  ..., 0., 0., 2.],
          [2., 2., 2.,  ..., 0., 0., 0.],
          [2., 2., 0.,  ..., 2., 0., 2.]],

         [[2., 2., 0.,  ..., 0., 2., 0.],
          [2., 2., 0.,  ..., 0., 2., 2.],
          [2., 2., 0.,  ..., 0., 2., 2.],
          ...,
          [2., 0., 2.,  ..., 2., 2., 0.],
          [2., 2., 0.,  ..., 2., 2., 0.],
          [2., 0., 2.,  ..., 0., 2., 0.]],

         [[0., 2., 2.,  ..., 0., 2., 0.],
          [0., 0., 0.,  ..., 0., 2., 2.],
          [2., 2., 0.,  ..., 2., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 2.],
          [2., 2., 0.,  ..., 0., 2., 0.],
          [2., 0., 0.,  ..., 0., 0., 2.]]]], device='cuda:0',
       dtype=torch.float64, grad_fn=<ClampBackward1>)
Tensor(shape=[24, 17, 128, 128], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [[[[2., 0., 0., ..., 0., 2., 2.],
          [0., 0., 0., ..., 2., 0., 0.],
          [2., 2., 2., ..., 2., 0., 2.],
          ...,
          [0., 0., 0., ..., 0., 0., 0.],
          [0., 2., 2., ..., 0., 0., 2.],
          [2., 0., 0., ..., 0., 2., 0.]],

         [[2., 2., 0., ..., 2., 2., 0.],
          [2., 0., 0., ..., 0., 2., 2.],
          [0., 2., 2., ..., 2., 0., 2.],
          ...,
          [0., 2., 0., ..., 0., 0., 2.],
          [2., 0., 2., ..., 0., 0., 2.],
          [2., 0., 2., ..., 0., 0., 2.]],

         [[0., 0., 2., ..., 2., 0., 2.],
          [0., 2., 2., ..., 2., 2., 2.],
          [2., 0., 0., ..., 2., 0., 0.],
          ...,
          [2., 2., 0., ..., 2., 2., 2.],
          [0., 2., 0., ..., 0., 2., 0.],
          [0., 0., 2., ..., 2., 2., 0.]],

         ...,

         [[0., 2., 0., ..., 2., 2., 2.],
          [0., 0., 0., ..., 0., 0., 0.],
          [2., 2., 2., ..., 0., 2., 2.],
          ...,
          [2., 0., 0., ..., 2., 2., 0.],
          [2., 2., 2., ..., 2., 0., 0.],
          [2., 2., 2., ..., 0., 0., 2.]],

         [[2., 2., 0., ..., 2., 0., 0.],
          [0., 2., 0., ..., 2., 0., 0.],
          [0., 2., 2., ..., 0., 2., 0.],
          ...,
          [2., 0., 0., ..., 0., 2., 2.],
          [2., 0., 0., ..., 2., 0., 2.],
          [0., 2., 0., ..., 0., 2., 0.]],

         [[2., 2., 2., ..., 2., 2., 0.],
          [2., 2., 0., ..., 0., 2., 2.],
          [2., 0., 2., ..., 2., 0., 2.],
          ...,
          [0., 0., 2., ..., 2., 0., 0.],
          [0., 0., 0., ..., 2., 2., 0.],
          [2., 0., 2., ..., 0., 2., 0.]]],


        [[[0., 2., 2., ..., 2., 2., 0.],
          [0., 0., 0., ..., 2., 0., 0.],
          [0., 2., 0., ..., 0., 0., 2.],
          ...,
          [0., 2., 2., ..., 2., 2., 0.],
          [0., 0., 2., ..., 0., 2., 0.],
          [2., 0., 2., ..., 2., 0., 2.]],

         [[2., 0., 0., ..., 2., 2., 2.],
          [2., 2., 2., ..., 0., 2., 0.],
          [0., 2., 2., ..., 0., 2., 0.],
          ...,
          [2., 0., 2., ..., 0., 2., 2.],
          [0., 2., 2., ..., 2., 2., 0.],
          [0., 2., 0., ..., 2., 0., 0.]],

         [[0., 2., 2., ..., 0., 0., 0.],
          [0., 0., 2., ..., 2., 2., 2.],
          [0., 0., 0., ..., 0., 0., 2.],
          ...,
          [0., 0., 0., ..., 2., 0., 2.],
          [0., 0., 2., ..., 2., 2., 2.],
          [0., 2., 2., ..., 0., 0., 0.]],

         ...,

         [[0., 2., 0., ..., 0., 0., 2.],
          [2., 2., 0., ..., 0., 2., 2.],
          [2., 0., 0., ..., 2., 2., 2.],
          ...,
          [0., 0., 0., ..., 0., 0., 2.],
          [2., 2., 0., ..., 0., 0., 2.],
          [0., 2., 0., ..., 2., 0., 0.]],

         [[0., 0., 0., ..., 2., 2., 2.],
          [0., 0., 2., ..., 0., 2., 2.],
          [0., 2., 0., ..., 0., 2., 0.],
          ...,
          [0., 2., 2., ..., 0., 2., 2.],
          [0., 0., 0., ..., 0., 2., 0.],
          [0., 0., 0., ..., 2., 0., 0.]],

         [[2., 2., 0., ..., 2., 2., 2.],
          [0., 2., 0., ..., 2., 0., 0.],
          [0., 2., 2., ..., 0., 0., 2.],
          ...,
          [0., 0., 2., ..., 0., 2., 2.],
          [0., 0., 0., ..., 0., 0., 2.],
          [2., 0., 2., ..., 0., 0., 2.]]],


        [[[2., 0., 2., ..., 0., 2., 2.],
          [0., 2., 2., ..., 2., 0., 0.],
          [2., 2., 2., ..., 0., 2., 2.],
          ...,
          [2., 2., 2., ..., 2., 2., 2.],
          [2., 2., 2., ..., 0., 2., 0.],
          [0., 2., 0., ..., 0., 0., 2.]],

         [[0., 0., 0., ..., 2., 0., 2.],
          [2., 2., 0., ..., 0., 0., 2.],
          [2., 2., 0., ..., 0., 2., 2.],
          ...,
          [0., 2., 2., ..., 2., 0., 0.],
          [2., 0., 0., ..., 2., 2., 2.],
          [0., 0., 0., ..., 2., 0., 2.]],

         [[2., 0., 2., ..., 0., 2., 0.],
          [2., 0., 0., ..., 2., 2., 0.],
          [2., 0., 0., ..., 2., 2., 0.],
          ...,
          [0., 2., 2., ..., 2., 0., 2.],
          [0., 0., 2., ..., 2., 2., 0.],
          [0., 0., 0., ..., 2., 2., 2.]],

         ...,

         [[2., 2., 0., ..., 2., 2., 2.],
          [0., 2., 2., ..., 0., 2., 2.],
          [2., 0., 2., ..., 0., 2., 2.],
          ...,
          [2., 0., 2., ..., 2., 0., 0.],
          [0., 2., 0., ..., 0., 2., 0.],
          [0., 2., 2., ..., 0., 2., 2.]],

         [[0., 2., 0., ..., 2., 0., 0.],
          [0., 0., 0., ..., 2., 2., 0.],
          [0., 2., 0., ..., 2., 2., 0.],
          ...,
          [0., 2., 2., ..., 2., 0., 2.],
          [0., 0., 2., ..., 2., 0., 2.],
          [2., 0., 2., ..., 0., 0., 2.]],

         [[0., 0., 2., ..., 2., 2., 2.],
          [0., 2., 2., ..., 2., 2., 2.],
          [2., 2., 2., ..., 0., 2., 0.],
          ...,
          [0., 2., 0., ..., 0., 2., 2.],
          [2., 2., 0., ..., 0., 0., 2.],
          [2., 2., 2., ..., 0., 2., 2.]]],


        ...,


        [[[2., 0., 2., ..., 0., 0., 0.],
          [2., 2., 0., ..., 0., 2., 2.],
          [0., 2., 2., ..., 0., 0., 0.],
          ...,
          [2., 2., 0., ..., 0., 2., 2.],
          [2., 0., 0., ..., 2., 0., 2.],
          [0., 0., 2., ..., 0., 2., 2.]],

         [[0., 0., 0., ..., 2., 0., 0.],
          [2., 0., 0., ..., 2., 2., 2.],
          [2., 2., 2., ..., 0., 2., 2.],
          ...,
          [0., 2., 0., ..., 0., 0., 2.],
          [0., 2., 2., ..., 0., 2., 0.],
          [0., 0., 0., ..., 0., 2., 0.]],

         [[0., 0., 2., ..., 0., 0., 2.],
          [0., 2., 2., ..., 0., 2., 0.],
          [0., 2., 0., ..., 0., 2., 0.],
          ...,
          [0., 0., 0., ..., 2., 0., 2.],
          [0., 2., 2., ..., 2., 2., 2.],
          [0., 2., 2., ..., 2., 0., 0.]],

         ...,

         [[0., 0., 2., ..., 0., 2., 2.],
          [2., 2., 2., ..., 2., 2., 2.],
          [0., 0., 0., ..., 0., 0., 2.],
          ...,
          [0., 0., 0., ..., 2., 0., 0.],
          [2., 2., 2., ..., 0., 2., 0.],
          [0., 2., 2., ..., 0., 0., 0.]],

         [[0., 2., 0., ..., 0., 2., 2.],
          [0., 2., 0., ..., 0., 0., 0.],
          [2., 0., 2., ..., 2., 2., 0.],
          ...,
          [2., 2., 0., ..., 0., 2., 2.],
          [0., 2., 2., ..., 0., 0., 2.],
          [0., 0., 0., ..., 2., 2., 2.]],

         [[0., 2., 2., ..., 0., 0., 0.],
          [0., 2., 0., ..., 0., 0., 0.],
          [2., 0., 0., ..., 0., 2., 0.],
          ...,
          [0., 2., 0., ..., 0., 2., 0.],
          [2., 0., 0., ..., 2., 2., 2.],
          [2., 0., 0., ..., 0., 0., 2.]]],


        [[[2., 2., 0., ..., 0., 0., 2.],
          [2., 0., 0., ..., 0., 0., 2.],
          [0., 0., 0., ..., 0., 2., 0.],
          ...,
          [0., 0., 0., ..., 0., 0., 0.],
          [0., 0., 2., ..., 0., 2., 0.],
          [0., 0., 0., ..., 0., 2., 0.]],

         [[2., 0., 0., ..., 2., 2., 0.],
          [2., 2., 2., ..., 2., 0., 0.],
          [2., 0., 0., ..., 2., 0., 2.],
          ...,
          [0., 2., 2., ..., 0., 0., 2.],
          [0., 0., 0., ..., 2., 2., 0.],
          [2., 0., 2., ..., 2., 0., 2.]],

         [[0., 0., 0., ..., 2., 2., 0.],
          [0., 2., 0., ..., 0., 2., 2.],
          [2., 0., 2., ..., 2., 0., 0.],
          ...,
          [2., 0., 2., ..., 0., 0., 0.],
          [2., 2., 2., ..., 0., 0., 2.],
          [0., 2., 0., ..., 0., 2., 2.]],

         ...,

         [[2., 2., 2., ..., 2., 0., 0.],
          [2., 2., 0., ..., 2., 2., 2.],
          [0., 2., 2., ..., 2., 0., 0.],
          ...,
          [0., 0., 2., ..., 0., 0., 2.],
          [2., 0., 2., ..., 0., 2., 0.],
          [2., 0., 2., ..., 2., 2., 2.]],

         [[2., 0., 2., ..., 2., 2., 0.],
          [0., 2., 2., ..., 0., 0., 0.],
          [0., 2., 2., ..., 0., 2., 2.],
          ...,
          [2., 0., 0., ..., 0., 2., 2.],
          [0., 0., 2., ..., 2., 0., 2.],
          [2., 2., 0., ..., 0., 2., 0.]],

         [[0., 0., 0., ..., 2., 0., 0.],
          [2., 2., 0., ..., 2., 0., 2.],
          [0., 2., 0., ..., 0., 2., 2.],
          ...,
          [0., 0., 2., ..., 2., 2., 2.],
          [0., 0., 0., ..., 0., 0., 0.],
          [2., 2., 0., ..., 2., 0., 0.]]],


        [[[2., 2., 0., ..., 2., 0., 2.],
          [2., 0., 0., ..., 2., 2., 2.],
          [0., 0., 2., ..., 2., 2., 0.],
          ...,
          [2., 0., 2., ..., 0., 2., 2.],
          [2., 0., 2., ..., 2., 2., 0.],
          [0., 0., 2., ..., 2., 2., 2.]],

         [[0., 2., 2., ..., 2., 0., 2.],
          [2., 0., 2., ..., 2., 0., 2.],
          [2., 2., 2., ..., 0., 0., 2.],
          ...,
          [2., 2., 0., ..., 0., 0., 2.],
          [2., 0., 0., ..., 2., 2., 0.],
          [2., 2., 0., ..., 0., 2., 2.]],

         [[2., 0., 2., ..., 0., 2., 0.],
          [2., 2., 0., ..., 0., 2., 2.],
          [2., 2., 0., ..., 0., 0., 0.],
          ...,
          [0., 0., 0., ..., 0., 0., 2.],
          [2., 2., 2., ..., 0., 2., 0.],
          [2., 0., 2., ..., 2., 2., 2.]],

         ...,

         [[2., 0., 0., ..., 0., 0., 2.],
          [2., 0., 2., ..., 0., 0., 0.],
          [0., 0., 2., ..., 2., 0., 0.],
          ...,
          [2., 0., 0., ..., 0., 0., 2.],
          [2., 2., 2., ..., 0., 0., 0.],
          [2., 2., 0., ..., 2., 0., 2.]],

         [[2., 2., 0., ..., 0., 2., 0.],
          [2., 2., 0., ..., 0., 2., 2.],
          [2., 2., 0., ..., 0., 2., 2.],
          ...,
          [2., 0., 2., ..., 2., 2., 0.],
          [2., 2., 0., ..., 2., 2., 0.],
          [2., 0., 2., ..., 0., 2., 0.]],

         [[0., 2., 2., ..., 0., 2., 0.],
          [0., 0., 0., ..., 0., 2., 2.],
          [2., 2., 0., ..., 2., 0., 0.],
          ...,
          [0., 0., 0., ..., 0., 0., 2.],
          [2., 2., 0., ..., 0., 2., 0.],
          [2., 0., 0., ..., 0., 0., 2.]]]])
[Pass] paddle.clip(Tensor([24, 17, 128, 128],"float64"), min=0, max=2, )
2025-05-16 05:34:41.211989 test begin: paddle.clip(Tensor([24, 17, 256, 256],"float64"), min=0, max=2, )
tensor([[[[-1.6026e+307,  6.0406e+307,  8.8666e+307,  ...,  1.4146e+307,
           -6.9581e+307,  5.4520e+307],
          [-8.0500e+307, -6.3300e+307,  5.2114e+306,  ..., -7.9961e+307,
           -7.6261e+306, -8.2661e+307],
          [ 6.5935e+307, -2.4088e+307,  1.3094e+307,  ...,  1.8020e+307,
           -3.1199e+307,  5.6736e+307],
          ...,
          [-5.7018e+307, -5.1229e+307,  2.6627e+307,  ...,  8.8146e+307,
           -6.6768e+307,  3.6065e+306],
          [ 6.8684e+307, -1.6476e+307, -8.1495e+307,  ..., -5.9139e+307,
           -8.0743e+306, -7.3889e+305],
          [-2.4389e+307,  7.9492e+307, -3.6078e+307,  ..., -2.0500e+307,
            7.6200e+307, -4.4628e+307]],

         [[ 1.3104e+307, -4.5844e+307,  6.5961e+306,  ...,  7.2913e+307,
           -3.6001e+307, -5.0013e+307],
          [-2.0546e+307,  4.5170e+307,  5.6727e+307,  ...,  6.4997e+307,
            9.2874e+306,  7.5169e+307],
          [-2.5708e+307, -1.9006e+307,  4.6149e+307,  ...,  6.5243e+307,
           -7.3833e+307,  6.5963e+307],
          ...,
          [ 2.0874e+307,  5.5305e+307,  4.3236e+307,  ...,  2.2670e+306,
           -3.6561e+307,  6.8480e+307],
          [ 1.7698e+307,  2.9399e+307, -2.4037e+306,  ..., -2.2339e+307,
            4.8233e+307, -7.7077e+307],
          [-6.7824e+307, -5.2194e+307, -5.2719e+307,  ..., -3.0387e+307,
            7.8651e+307, -3.5106e+307]],

         [[ 7.1084e+307,  7.7009e+307, -8.4371e+307,  ..., -2.6856e+307,
            2.7783e+307, -8.2105e+307],
          [-6.2383e+307, -8.6838e+307, -7.7158e+307,  ...,  4.9620e+307,
           -7.6276e+307, -1.8598e+307],
          [-7.8645e+307,  2.6484e+307, -2.6944e+307,  ..., -2.1625e+307,
            7.7421e+307, -7.8016e+307],
          ...,
          [-8.3473e+307,  8.5066e+307,  1.4333e+307,  ...,  8.7429e+307,
            7.5746e+307, -7.3680e+306],
          [-5.1799e+306,  3.4946e+307,  1.8189e+307,  ..., -5.4462e+307,
           -2.3032e+307,  2.9265e+306],
          [-2.5156e+307, -3.9276e+307, -3.3942e+307,  ..., -3.2786e+307,
           -9.0842e+306, -4.8610e+307]],

         ...,

         [[-4.4995e+307,  6.2710e+307, -3.4559e+307,  ..., -2.4651e+307,
            1.2440e+307,  1.4437e+307],
          [ 8.2579e+307, -1.6429e+307, -8.8432e+307,  ..., -3.0650e+307,
            7.9154e+307, -4.2916e+307],
          [-8.7074e+307,  4.3267e+307,  5.5317e+307,  ..., -2.5337e+306,
            4.4608e+307, -6.9134e+307],
          ...,
          [-1.8654e+307,  7.3625e+307,  3.2516e+307,  ...,  7.1986e+307,
           -1.9758e+306, -8.3245e+307],
          [ 1.6347e+306, -3.2869e+307, -8.4723e+307,  ...,  4.2615e+307,
            4.4357e+307,  3.1326e+307],
          [ 3.8659e+307,  2.3257e+307, -2.9561e+307,  ..., -1.1758e+307,
            2.4469e+307, -6.2838e+307]],

         [[-5.7655e+306, -1.7416e+307,  8.0601e+307,  ...,  5.7732e+306,
           -5.4897e+307,  7.1072e+307],
          [ 4.6658e+307,  5.0323e+307,  3.6233e+307,  ..., -5.1772e+307,
            7.1528e+307, -6.1508e+307],
          [ 2.9091e+307, -8.5672e+306,  5.1153e+307,  ...,  5.0779e+307,
            6.4031e+307,  7.8732e+307],
          ...,
          [ 7.1593e+307, -4.7786e+307,  2.0368e+307,  ..., -2.0819e+307,
            8.8603e+307, -4.7682e+307],
          [-4.7616e+306, -6.5773e+307, -3.9727e+307,  ...,  3.3863e+307,
            3.0820e+307, -7.5723e+307],
          [ 6.1513e+307,  5.2544e+307,  1.5887e+305,  ...,  8.2940e+307,
           -1.5114e+307,  4.8151e+307]],

         [[-2.3961e+307, -3.8818e+306,  2.5981e+307,  ...,  8.5411e+307,
           -7.4496e+306,  1.7008e+307],
          [ 7.5848e+307,  8.0823e+307,  2.1390e+307,  ..., -2.0004e+307,
            1.5596e+307, -2.9953e+307],
          [ 2.0324e+307, -3.9404e+307, -2.0349e+307,  ...,  4.2915e+306,
           -4.6106e+307, -7.8143e+307],
          ...,
          [ 7.9763e+307, -4.6896e+307,  7.0858e+307,  ..., -2.7109e+307,
            3.8063e+307,  8.6312e+307],
          [ 7.8782e+307,  7.2719e+307,  8.1900e+307,  ...,  1.1825e+307,
            4.5044e+307,  3.5133e+307],
          [-4.7987e+307,  8.8801e+307,  9.8251e+306,  ...,  5.6707e+307,
           -4.9564e+307, -7.8358e+307]]],


        [[[ 2.7567e+307, -3.0074e+307, -7.2014e+306,  ..., -3.4805e+307,
           -7.1452e+307,  4.2024e+307],
          [ 4.9207e+307, -1.5768e+307,  7.6434e+307,  ..., -1.5712e+307,
            7.1495e+307,  2.8143e+307],
          [ 1.7941e+307,  5.6928e+307,  6.2410e+307,  ...,  1.0163e+307,
           -1.2889e+307,  3.6503e+307],
          ...,
          [-1.0870e+307, -7.7507e+306, -7.8065e+306,  ..., -2.7502e+307,
           -8.3452e+306,  3.7676e+307],
          [ 8.6382e+307,  8.2594e+307, -7.5687e+307,  ...,  4.8075e+307,
            5.1269e+307,  7.4927e+307],
          [ 8.4017e+307, -8.8545e+307, -8.6554e+306,  ..., -4.8643e+307,
           -4.4242e+307,  8.1892e+307]],

         [[-2.3631e+307, -3.2997e+307, -8.6556e+307,  ..., -4.7535e+307,
           -4.5382e+307,  9.1397e+306],
          [-5.4095e+307, -8.3715e+307,  1.4280e+307,  ..., -2.2832e+307,
           -1.8762e+307,  4.1311e+307],
          [-2.6575e+307,  4.3983e+307,  5.1154e+307,  ...,  2.7230e+307,
           -2.5506e+307, -8.4448e+307],
          ...,
          [ 7.1949e+307, -7.3528e+307, -4.5726e+306,  ..., -1.6815e+306,
           -2.8416e+307,  6.4140e+307],
          [ 4.8379e+307, -1.3653e+307,  7.5617e+306,  ...,  5.2475e+307,
           -5.3178e+305,  3.1573e+307],
          [ 8.7197e+307,  5.8523e+307, -5.8655e+307,  ...,  5.6635e+307,
           -5.3996e+307, -8.5873e+307]],

         [[-2.1491e+307,  1.6006e+307, -2.9693e+307,  ...,  8.2492e+307,
            5.8376e+307,  4.4340e+307],
          [ 2.9921e+307, -7.0514e+307, -3.0784e+307,  ..., -2.2295e+307,
           -3.1381e+307, -8.9854e+307],
          [-7.5405e+307, -6.6607e+307, -3.6284e+307,  ...,  7.7120e+307,
            1.5930e+306, -8.6043e+307],
          ...,
          [ 5.9676e+307,  2.3570e+307,  4.7801e+307,  ..., -5.5475e+307,
            2.4313e+306,  3.5822e+307],
          [ 7.2674e+306, -5.1457e+307,  4.0655e+306,  ...,  9.9488e+306,
           -4.9508e+307, -2.5840e+307],
          [-9.7690e+306,  3.3948e+307,  6.7077e+307,  ..., -2.6517e+307,
           -4.3695e+307,  1.8405e+307]],

         ...,

         [[ 4.4486e+307, -7.9705e+307,  1.3877e+307,  ..., -7.5777e+307,
           -1.8538e+306,  8.0939e+307],
          [-8.7701e+307,  6.3939e+306,  1.9993e+306,  ..., -6.1683e+307,
            7.4360e+307, -1.4240e+307],
          [ 4.8437e+307, -6.0582e+306, -3.2444e+307,  ...,  5.3066e+307,
           -6.6029e+307,  3.2133e+306],
          ...,
          [-4.7568e+306, -8.5969e+307,  3.5897e+307,  ...,  8.9062e+306,
           -7.1820e+307,  5.9316e+307],
          [ 8.1118e+307,  3.0430e+307,  2.3403e+307,  ..., -6.0200e+307,
           -8.6796e+307, -4.9892e+307],
          [-8.8708e+307,  6.5647e+307, -4.1917e+307,  ...,  8.0254e+307,
           -1.9549e+307, -3.3589e+307]],

         [[-1.8504e+307,  7.8065e+307, -1.2527e+307,  ...,  8.4354e+305,
           -7.1417e+307, -7.6322e+307],
          [ 6.0033e+307,  8.6320e+307, -1.2657e+307,  ..., -8.2841e+306,
           -3.5425e+307, -3.5753e+307],
          [ 2.3645e+307,  7.8509e+307,  1.2392e+307,  ...,  8.9398e+307,
           -3.5813e+307, -8.2184e+307],
          ...,
          [ 8.7188e+306,  6.6964e+307,  7.9630e+307,  ..., -8.7998e+307,
            3.1068e+307,  3.2011e+307],
          [ 5.4673e+307, -1.2198e+307,  6.6587e+307,  ...,  1.5028e+307,
           -5.9597e+306, -6.6290e+307],
          [-3.7535e+307, -2.9543e+307, -1.8487e+307,  ...,  5.5015e+307,
            8.7062e+307, -6.8238e+307]],

         [[-7.6068e+307,  1.8497e+307,  4.5760e+307,  ..., -5.0024e+307,
           -4.1303e+307, -5.1365e+307],
          [ 6.3980e+307,  6.8873e+307,  3.8421e+307,  ..., -5.5135e+307,
           -6.6621e+307,  4.4405e+307],
          [ 3.9378e+307,  2.3847e+307,  4.8780e+307,  ...,  5.6623e+307,
           -5.5694e+306,  4.5504e+307],
          ...,
          [-8.7221e+307, -2.5781e+307,  3.9651e+307,  ...,  2.9742e+307,
           -7.2821e+307, -3.8153e+307],
          [ 2.3542e+307, -3.3709e+307, -6.3771e+307,  ...,  4.1571e+307,
           -6.2152e+306,  1.9571e+307],
          [ 1.0430e+307, -4.6749e+307, -6.3387e+307,  ..., -6.6149e+307,
            3.7363e+307,  7.0781e+307]]],


        [[[ 6.2431e+307,  4.1782e+307,  8.5980e+307,  ..., -7.1301e+307,
           -4.3557e+307,  7.8687e+307],
          [-4.9640e+307, -3.9879e+307,  7.3226e+307,  ...,  3.1389e+306,
           -2.6895e+307,  6.0631e+306],
          [-1.9089e+307, -5.7610e+307, -1.3147e+307,  ...,  2.3729e+307,
           -1.2644e+307, -2.5041e+307],
          ...,
          [ 1.3257e+307,  6.9017e+307,  3.2502e+307,  ..., -5.4884e+306,
           -1.2157e+307, -2.7380e+307],
          [-4.5474e+307,  3.0286e+307,  8.3162e+307,  ...,  5.9493e+307,
           -3.5315e+307,  7.8534e+307],
          [-4.4053e+306, -6.0140e+307,  2.9788e+307,  ...,  5.0606e+307,
           -4.3600e+306,  5.2234e+307]],

         [[-2.3312e+307, -8.7377e+307,  5.1914e+307,  ...,  4.6587e+307,
           -5.4206e+307, -4.6526e+307],
          [-6.5404e+307, -5.2833e+307, -3.3412e+307,  ...,  5.9581e+307,
           -7.1463e+307, -7.2822e+307],
          [ 7.5710e+307,  1.7161e+307, -3.1687e+306,  ..., -5.6903e+307,
           -1.2172e+307, -7.3881e+307],
          ...,
          [ 3.9364e+307, -1.1199e+307,  2.6708e+307,  ..., -4.7902e+307,
            2.6247e+307, -7.4157e+307],
          [-7.4215e+307, -4.3483e+307,  2.9990e+307,  ...,  2.7109e+307,
            2.2615e+307,  7.5589e+307],
          [-2.6238e+307,  5.3458e+307, -5.9169e+307,  ..., -5.9657e+307,
           -6.4520e+307,  1.0145e+306]],

         [[-2.6912e+307, -5.3654e+307, -6.3983e+307,  ..., -1.0136e+307,
           -2.1108e+307,  6.1670e+307],
          [-1.7600e+307,  6.9924e+307, -1.9182e+306,  ...,  3.4427e+307,
           -8.9772e+307, -4.6058e+307],
          [ 5.8063e+307,  6.8947e+305, -2.0794e+307,  ...,  4.8624e+307,
           -5.1962e+307,  8.7763e+307],
          ...,
          [ 4.8765e+307, -2.9129e+307, -3.1579e+307,  ...,  8.6744e+307,
           -3.9062e+307,  2.6871e+307],
          [ 1.5945e+307, -8.2286e+306, -6.3135e+306,  ..., -1.2793e+307,
            4.4577e+307, -5.7141e+307],
          [ 4.4367e+307,  8.3404e+306, -5.0023e+307,  ..., -7.3408e+307,
            4.3445e+307, -8.4193e+307]],

         ...,

         [[-5.1414e+307, -1.0792e+307, -8.4702e+306,  ...,  4.9589e+306,
            1.3645e+307,  5.2434e+307],
          [-7.9788e+307, -4.8766e+306, -3.7401e+307,  ..., -5.4345e+307,
           -8.7960e+307,  8.8538e+307],
          [ 7.4817e+307, -6.1591e+307,  2.9259e+307,  ...,  5.1622e+307,
            4.3154e+307, -2.4682e+307],
          ...,
          [-1.6555e+307,  1.2035e+307,  1.5145e+307,  ..., -8.7021e+307,
            6.8348e+307, -7.2001e+307],
          [-5.9053e+307, -1.5448e+306, -8.8332e+307,  ...,  8.3714e+307,
            3.2343e+307,  2.8579e+307],
          [ 5.6463e+307,  8.2411e+307, -6.7969e+307,  ...,  7.9971e+307,
           -7.5450e+307,  7.9355e+307]],

         [[ 1.7457e+306,  8.3163e+307, -5.5716e+307,  ..., -2.1626e+307,
            7.4210e+307,  3.1662e+307],
          [ 7.2897e+306,  8.8361e+307,  2.4115e+307,  ...,  2.9975e+307,
           -3.9689e+305,  6.3838e+307],
          [ 6.3009e+307, -9.4079e+306,  1.4973e+306,  ...,  5.6239e+307,
            9.0276e+306, -1.2457e+307],
          ...,
          [ 7.7738e+307,  8.3374e+307,  3.5836e+307,  ...,  7.3639e+307,
            7.7315e+307,  8.0590e+307],
          [-2.1126e+307, -5.2236e+307, -7.2899e+307,  ...,  7.5027e+307,
           -2.3631e+307,  2.9945e+307],
          [ 5.0995e+307, -5.9189e+307,  4.6640e+307,  ...,  8.1480e+307,
           -7.5946e+307,  4.5932e+307]],

         [[ 7.8063e+307, -6.6694e+307, -1.9429e+307,  ...,  4.9484e+307,
            6.1921e+307, -1.7383e+307],
          [ 8.7406e+307,  7.1084e+307,  4.1850e+306,  ..., -7.9547e+307,
           -3.3888e+307, -3.6633e+307],
          [-3.0381e+307,  2.5150e+307, -4.7831e+307,  ..., -4.7694e+307,
            2.3371e+307, -4.6437e+306],
          ...,
          [-6.8148e+307, -7.9315e+307, -5.1671e+307,  ..., -1.9870e+307,
           -8.8255e+307,  3.1535e+306],
          [-2.4650e+306, -7.4114e+307,  4.5145e+307,  ...,  4.8087e+307,
            5.8111e+306,  1.9828e+307],
          [ 2.7134e+307, -8.5254e+307, -6.8652e+307,  ..., -2.8411e+307,
           -9.7820e+306,  8.8250e+307]]],


        ...,


        [[[-4.6270e+307, -8.2102e+307, -2.4388e+307,  ..., -3.0497e+306,
           -3.3251e+307, -6.3511e+307],
          [ 1.8438e+307, -5.7936e+307,  3.3634e+307,  ...,  2.4286e+307,
            7.5766e+307, -3.2438e+307],
          [-1.1780e+307,  5.6150e+307, -8.7803e+307,  ..., -8.6574e+307,
            4.4697e+307, -1.2007e+307],
          ...,
          [ 3.1493e+307,  6.3390e+307,  3.7433e+307,  ..., -7.3331e+307,
            8.0736e+307,  3.6716e+307],
          [ 2.5843e+307,  2.0668e+307,  5.9437e+307,  ...,  2.6182e+307,
            1.2502e+307, -7.1211e+307],
          [ 6.0937e+307, -1.8120e+307,  1.6942e+307,  ...,  6.0178e+306,
           -2.2792e+307, -4.8709e+307]],

         [[ 4.9423e+307, -4.5110e+307, -5.0580e+307,  ..., -7.7742e+307,
           -2.9321e+307,  4.8957e+307],
          [-3.6579e+307, -7.3928e+307,  1.3525e+306,  ...,  5.9330e+306,
            1.2348e+307, -1.0587e+307],
          [-8.9661e+307,  5.9453e+306, -6.9930e+307,  ..., -6.8278e+307,
            2.5211e+307,  7.3594e+307],
          ...,
          [-7.2139e+307, -7.1907e+307, -5.1544e+307,  ...,  7.8344e+306,
           -4.6796e+307,  7.2737e+307],
          [ 3.5745e+307, -1.0564e+307, -8.9496e+307,  ...,  5.9014e+307,
            7.8248e+307, -2.0262e+307],
          [ 5.5349e+307, -5.9857e+306,  1.6204e+307,  ..., -4.4282e+307,
           -4.9636e+307,  6.6401e+307]],

         [[ 8.2981e+307, -2.4122e+307, -6.7104e+306,  ...,  8.7985e+307,
           -7.4732e+307,  1.2300e+307],
          [-6.5233e+307, -8.1809e+307,  8.0896e+307,  ..., -1.3800e+307,
           -3.3193e+307,  6.9917e+307],
          [-2.3325e+307, -7.8837e+307,  5.7222e+307,  ...,  5.8090e+307,
           -7.2484e+307, -4.7564e+307],
          ...,
          [-7.7220e+307, -4.5554e+307, -2.9342e+307,  ...,  7.7109e+307,
           -1.5432e+307, -4.3088e+307],
          [-4.4908e+307, -2.8629e+306,  1.5820e+307,  ...,  8.7124e+307,
           -3.0896e+307, -7.3161e+307],
          [-7.8168e+307,  2.1123e+307, -4.4422e+306,  ..., -5.2928e+307,
           -6.7428e+307, -1.4605e+307]],

         ...,

         [[ 2.5258e+307,  6.7726e+307,  2.4604e+307,  ..., -3.1994e+307,
           -1.0083e+307, -1.6334e+307],
          [ 6.7891e+306,  2.9382e+307, -6.7736e+307,  ..., -5.5452e+307,
            1.7619e+307, -3.8208e+307],
          [-2.8073e+307, -1.1772e+307, -2.3879e+307,  ..., -8.2784e+307,
           -3.1636e+307,  6.9447e+306],
          ...,
          [ 4.9811e+307,  2.2996e+307,  6.6339e+307,  ..., -8.5613e+307,
            7.3966e+307, -2.7444e+307],
          [ 4.2430e+307, -2.7595e+307, -1.1056e+307,  ...,  5.6046e+307,
            8.4340e+307, -2.7751e+307],
          [-6.1460e+307,  4.7452e+306, -6.7943e+307,  ..., -1.4083e+307,
            5.0833e+307, -6.5655e+307]],

         [[ 2.9918e+306,  3.3800e+307, -7.3133e+307,  ..., -7.9623e+304,
            5.9484e+307,  2.0811e+307],
          [ 2.2227e+307,  7.4620e+307,  1.9782e+307,  ...,  1.3996e+307,
            4.8226e+307,  3.2287e+307],
          [-5.1896e+307,  3.7818e+307,  6.7883e+307,  ..., -5.6655e+307,
           -3.7163e+307, -5.8650e+307],
          ...,
          [-7.6267e+307, -6.2553e+307,  4.6921e+307,  ...,  3.5771e+307,
            6.3830e+307,  4.2278e+307],
          [-3.8241e+307, -6.7046e+307, -7.2536e+307,  ..., -1.3451e+307,
            7.9472e+307, -6.1563e+307],
          [ 6.9769e+307,  6.2371e+307, -7.0598e+307,  ...,  7.6277e+307,
            8.1859e+307, -7.3629e+307]],

         [[ 4.0482e+307, -4.8260e+307,  8.8973e+307,  ..., -7.4875e+307,
            2.2136e+307, -8.8136e+307],
          [ 4.3312e+307,  7.3689e+307, -6.4969e+307,  ...,  7.4273e+307,
           -1.0808e+306,  3.8980e+306],
          [ 2.3775e+307,  4.2004e+307,  7.3070e+307,  ..., -1.5104e+307,
           -8.4054e+307, -6.5012e+307],
          ...,
          [-5.1008e+307, -8.2539e+307, -8.0237e+307,  ...,  2.3529e+307,
            2.5344e+307, -3.7017e+307],
          [-2.8021e+307,  5.7218e+307,  2.9973e+307,  ..., -6.7747e+307,
           -1.6329e+307, -8.0578e+307],
          [ 2.5356e+307, -5.3927e+307, -7.2006e+306,  ..., -7.9545e+307,
            4.2730e+307,  5.6390e+307]]],


        [[[-6.5365e+307, -6.5720e+307, -1.3962e+307,  ...,  7.3912e+307,
           -5.6034e+307,  4.4333e+307],
          [ 2.8635e+307,  7.4388e+307, -6.7344e+307,  ..., -2.8392e+307,
           -4.3505e+307, -8.0629e+307],
          [ 6.2051e+307,  5.8944e+307, -8.8446e+307,  ..., -7.5457e+307,
            3.3079e+306,  3.3164e+307],
          ...,
          [ 4.3539e+307, -1.3239e+307,  8.4739e+307,  ..., -4.8775e+307,
            8.7420e+307, -2.7500e+307],
          [ 2.6056e+307,  7.0806e+307, -1.0650e+307,  ..., -8.6478e+307,
           -8.5139e+307, -2.3008e+307],
          [ 8.7311e+307,  1.0748e+307, -3.8117e+306,  ...,  6.4381e+306,
            3.7334e+306, -2.8398e+306]],

         [[-4.8659e+306, -4.6051e+307, -2.7532e+307,  ...,  8.1795e+307,
           -1.2447e+307, -1.5426e+307],
          [ 3.5310e+307,  6.5434e+306,  4.3119e+307,  ...,  3.7024e+304,
           -7.1389e+306,  3.0659e+307],
          [ 4.0613e+307,  6.8266e+307,  6.8392e+307,  ..., -3.4561e+307,
           -5.1298e+305,  2.4474e+307],
          ...,
          [-3.6891e+306,  3.1245e+307, -7.9418e+305,  ..., -1.8138e+307,
            6.7367e+307, -4.0168e+307],
          [-3.4431e+307,  6.4461e+307,  6.6277e+307,  ...,  1.0381e+307,
            4.0045e+307, -7.2034e+307],
          [ 5.2260e+307, -7.7756e+307, -7.3496e+307,  ..., -5.1318e+307,
            4.4423e+307, -5.1746e+307]],

         [[-1.5949e+306, -3.8820e+307,  7.3377e+307,  ..., -7.8014e+305,
            4.7436e+307,  7.7208e+307],
          [ 8.1768e+305,  8.9412e+307,  2.1737e+307,  ..., -6.9165e+307,
            4.9854e+307, -5.0198e+307],
          [-8.3434e+307,  1.1550e+307, -4.8150e+307,  ...,  6.5252e+306,
            7.2010e+307, -4.2916e+307],
          ...,
          [-5.6284e+307,  4.8467e+307, -7.8469e+307,  ...,  5.8606e+307,
            7.0323e+307, -7.7799e+307],
          [-4.5383e+307, -3.0003e+307,  4.3207e+307,  ...,  6.5062e+307,
           -1.0370e+307,  2.2079e+307],
          [-4.5155e+306,  4.6287e+306,  1.3795e+307,  ...,  6.0263e+307,
            5.1821e+307, -6.7877e+306]],

         ...,

         [[ 7.5167e+306,  4.8865e+307,  8.0359e+307,  ...,  2.7370e+307,
           -1.9882e+307,  2.9509e+307],
          [ 7.5569e+307,  8.2554e+307, -6.6636e+307,  ..., -8.7837e+307,
           -4.9919e+307, -4.7092e+307],
          [ 3.2506e+307,  6.8751e+307, -5.7038e+307,  ...,  3.9167e+307,
            5.4459e+307, -5.5167e+305],
          ...,
          [-6.8770e+306, -6.9044e+307, -8.4430e+307,  ...,  3.6293e+307,
           -8.5404e+306, -3.6681e+307],
          [ 2.5325e+307,  5.8635e+307, -2.2200e+307,  ..., -6.4184e+307,
           -5.8655e+307, -3.2596e+306],
          [-4.4873e+307,  2.9303e+307,  6.5692e+307,  ..., -1.1956e+306,
           -6.2350e+307,  2.2700e+307]],

         [[-2.1555e+306, -8.5618e+307, -8.5005e+307,  ..., -7.5241e+306,
           -4.4079e+307, -3.8336e+307],
          [-6.8618e+307, -7.1456e+307,  8.5378e+307,  ...,  1.7107e+307,
            1.1851e+307,  4.8339e+307],
          [-6.9077e+306, -6.6967e+307, -3.4126e+307,  ..., -8.5082e+307,
            4.7557e+307, -1.6763e+307],
          ...,
          [ 3.2440e+307, -7.0718e+307,  7.5660e+307,  ...,  4.6734e+307,
            6.4776e+307,  2.4224e+307],
          [ 6.9110e+306,  4.6553e+307, -8.4270e+307,  ...,  4.3769e+307,
            8.8683e+307, -8.2592e+307],
          [-2.2145e+307,  5.1797e+307,  2.9604e+307,  ...,  5.1083e+307,
           -5.1608e+307,  1.2548e+307]],

         [[-6.8815e+307,  1.2395e+307,  3.1379e+307,  ...,  3.0391e+307,
           -6.9919e+307, -4.9469e+307],
          [-5.1600e+307, -2.1218e+307,  2.1231e+307,  ...,  6.3440e+307,
           -3.2368e+307, -4.7661e+307],
          [ 6.8583e+307, -8.9141e+307, -7.1260e+307,  ...,  8.1082e+307,
            8.7802e+306, -5.9048e+307],
          ...,
          [ 3.2329e+306,  7.6844e+307,  3.5088e+306,  ..., -3.6373e+307,
           -8.4085e+307,  1.3471e+307],
          [ 1.0941e+306,  5.8316e+307, -4.5212e+307,  ..., -3.1092e+307,
            4.7601e+307, -4.9531e+307],
          [-1.0917e+307, -2.1531e+307, -4.3170e+307,  ..., -1.0166e+307,
            8.0388e+306, -7.1303e+307]]],


        [[[ 8.6263e+307,  7.1693e+307,  4.4132e+307,  ...,  2.4244e+307,
           -1.6440e+307, -3.0311e+307],
          [-1.0092e+307, -1.6445e+306, -5.9488e+307,  ...,  2.1589e+307,
            5.2370e+307, -1.8068e+307],
          [ 4.4821e+307,  8.9740e+307, -3.6160e+307,  ..., -6.4231e+307,
           -4.4639e+307,  7.6395e+307],
          ...,
          [-1.4169e+307,  6.8378e+307,  5.3667e+307,  ..., -3.8644e+306,
           -7.7045e+306,  5.1597e+307],
          [-2.6363e+307,  8.3526e+307, -5.6923e+307,  ...,  7.5075e+307,
           -2.1281e+307,  1.1587e+307],
          [-6.9306e+307, -2.0814e+307,  3.2459e+307,  ...,  3.4134e+307,
           -3.1798e+307,  2.3655e+307]],

         [[-1.8890e+307, -1.0727e+307,  3.8143e+307,  ..., -1.9308e+307,
           -8.4691e+307,  3.2920e+307],
          [-8.0086e+307,  4.2200e+306,  6.4225e+307,  ..., -3.3924e+307,
            2.9778e+307,  4.1862e+307],
          [-9.7857e+306,  9.5843e+306,  8.8633e+307,  ...,  7.3116e+307,
           -1.5118e+307, -3.2394e+307],
          ...,
          [ 4.4886e+307,  2.5048e+307, -1.4801e+307,  ..., -5.7628e+307,
           -3.4679e+306, -1.0082e+307],
          [-3.3871e+307, -7.8565e+307, -1.4183e+307,  ..., -8.8750e+307,
            3.3052e+306,  1.1821e+306],
          [-2.7429e+307, -3.5436e+307,  5.7624e+307,  ..., -7.7892e+307,
            6.0690e+306, -2.1823e+307]],

         [[ 8.4568e+307,  2.8482e+307, -4.8375e+307,  ..., -7.1059e+307,
            7.7899e+307,  6.1324e+307],
          [-7.4117e+307,  7.9629e+306, -8.2269e+307,  ...,  4.6962e+307,
           -8.9536e+307,  2.5320e+307],
          [ 6.2811e+307,  5.3825e+307,  6.8843e+307,  ...,  7.9771e+307,
           -7.5801e+307,  8.8238e+307],
          ...,
          [-2.9952e+307, -5.8436e+307, -2.0017e+307,  ..., -8.8118e+307,
            6.7480e+307, -1.8223e+307],
          [ 6.7567e+307, -3.4650e+307,  2.0983e+307,  ..., -3.5446e+307,
            8.0324e+307, -8.9442e+307],
          [ 4.8221e+307,  5.7907e+307, -7.9178e+307,  ...,  6.1936e+307,
            3.3803e+307, -3.5499e+307]],

         ...,

         [[ 6.8843e+307, -7.7669e+307, -3.1177e+307,  ..., -4.7951e+307,
            3.1785e+307,  4.4077e+307],
          [-2.5723e+307,  5.3215e+307, -5.1465e+304,  ...,  4.5251e+307,
            1.0322e+307, -3.6096e+306],
          [-3.5949e+307, -6.7615e+307, -5.8891e+307,  ..., -4.1631e+307,
           -3.0488e+307, -7.9564e+307],
          ...,
          [ 8.3168e+307, -4.8768e+307,  7.3434e+306,  ...,  3.4569e+307,
            3.2453e+307,  8.0141e+307],
          [ 7.4183e+307,  5.7907e+306, -2.4224e+307,  ...,  6.5634e+307,
           -7.2031e+307,  8.6225e+307],
          [-8.5021e+307, -4.3696e+307, -4.3154e+307,  ...,  1.8111e+307,
           -4.5967e+306,  6.3779e+307]],

         [[ 3.8278e+306,  2.5761e+307, -6.8645e+307,  ..., -2.3893e+307,
           -2.7534e+307, -4.1209e+306],
          [-1.6943e+307, -5.1299e+307,  5.6814e+307,  ..., -1.3986e+307,
           -3.2598e+307,  3.4427e+307],
          [-7.5430e+307,  6.6753e+307, -1.4109e+307,  ...,  4.8731e+307,
            4.7241e+307, -8.4075e+307],
          ...,
          [-1.4377e+307, -1.6772e+307,  3.0229e+307,  ..., -4.2218e+307,
           -3.2927e+307, -1.9859e+307],
          [-5.6254e+307, -1.3247e+306, -1.6886e+307,  ..., -5.9925e+307,
            2.6283e+306,  6.5696e+307],
          [-8.4604e+307, -3.0938e+307,  3.7958e+307,  ...,  8.5298e+307,
           -3.1265e+306, -2.4792e+306]],

         [[-2.3983e+307, -8.0554e+307,  8.0915e+307,  ...,  8.9282e+307,
           -6.3556e+307, -8.2447e+306],
          [-3.0400e+307,  5.4911e+307, -7.0327e+307,  ...,  6.4583e+307,
            7.1308e+307,  5.4533e+307],
          [ 6.8347e+307,  1.7843e+307, -5.6294e+307,  ..., -4.4687e+307,
            3.3287e+307, -7.3686e+307],
          ...,
          [ 1.3568e+307, -3.1633e+307, -4.1466e+307,  ...,  1.8233e+307,
            3.4678e+307,  9.6203e+305],
          [-6.1314e+307, -3.4302e+307,  3.4798e+307,  ...,  4.7115e+307,
            8.6543e+307,  3.4003e+307],
          [-6.9852e+307,  2.1480e+307,  5.4149e+307,  ...,  6.2598e+307,
            5.1683e+307,  3.4692e+307]]]], device='cuda:0',
       dtype=torch.float64, requires_grad=True)
-----------
tensor([[[[0., 2., 2.,  ..., 2., 0., 2.],
          [0., 0., 2.,  ..., 0., 0., 0.],
          [2., 0., 2.,  ..., 2., 0., 2.],
          ...,
          [0., 0., 2.,  ..., 2., 0., 2.],
          [2., 0., 0.,  ..., 0., 0., 0.],
          [0., 2., 0.,  ..., 0., 2., 0.]],

         [[2., 0., 2.,  ..., 2., 0., 0.],
          [0., 2., 2.,  ..., 2., 2., 2.],
          [0., 0., 2.,  ..., 2., 0., 2.],
          ...,
          [2., 2., 2.,  ..., 2., 0., 2.],
          [2., 2., 0.,  ..., 0., 2., 0.],
          [0., 0., 0.,  ..., 0., 2., 0.]],

         [[2., 2., 0.,  ..., 0., 2., 0.],
          [0., 0., 0.,  ..., 2., 0., 0.],
          [0., 2., 0.,  ..., 0., 2., 0.],
          ...,
          [0., 2., 2.,  ..., 2., 2., 0.],
          [0., 2., 2.,  ..., 0., 0., 2.],
          [0., 0., 0.,  ..., 0., 0., 0.]],

         ...,

         [[0., 2., 0.,  ..., 0., 2., 2.],
          [2., 0., 0.,  ..., 0., 2., 0.],
          [0., 2., 2.,  ..., 0., 2., 0.],
          ...,
          [0., 2., 2.,  ..., 2., 0., 0.],
          [2., 0., 0.,  ..., 2., 2., 2.],
          [2., 2., 0.,  ..., 0., 2., 0.]],

         [[0., 0., 2.,  ..., 2., 0., 2.],
          [2., 2., 2.,  ..., 0., 2., 0.],
          [2., 0., 2.,  ..., 2., 2., 2.],
          ...,
          [2., 0., 2.,  ..., 0., 2., 0.],
          [0., 0., 0.,  ..., 2., 2., 0.],
          [2., 2., 2.,  ..., 2., 0., 2.]],

         [[0., 0., 2.,  ..., 2., 0., 2.],
          [2., 2., 2.,  ..., 0., 2., 0.],
          [2., 0., 0.,  ..., 2., 0., 0.],
          ...,
          [2., 0., 2.,  ..., 0., 2., 2.],
          [2., 2., 2.,  ..., 2., 2., 2.],
          [0., 2., 2.,  ..., 2., 0., 0.]]],


        [[[2., 0., 0.,  ..., 0., 0., 2.],
          [2., 0., 2.,  ..., 0., 2., 2.],
          [2., 2., 2.,  ..., 2., 0., 2.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 2.],
          [2., 2., 0.,  ..., 2., 2., 2.],
          [2., 0., 0.,  ..., 0., 0., 2.]],

         [[0., 0., 0.,  ..., 0., 0., 2.],
          [0., 0., 2.,  ..., 0., 0., 2.],
          [0., 2., 2.,  ..., 2., 0., 0.],
          ...,
          [2., 0., 0.,  ..., 0., 0., 2.],
          [2., 0., 2.,  ..., 2., 0., 2.],
          [2., 2., 0.,  ..., 2., 0., 0.]],

         [[0., 2., 0.,  ..., 2., 2., 2.],
          [2., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 2., 2., 0.],
          ...,
          [2., 2., 2.,  ..., 0., 2., 2.],
          [2., 0., 2.,  ..., 2., 0., 0.],
          [0., 2., 2.,  ..., 0., 0., 2.]],

         ...,

         [[2., 0., 2.,  ..., 0., 0., 2.],
          [0., 2., 2.,  ..., 0., 2., 0.],
          [2., 0., 0.,  ..., 2., 0., 2.],
          ...,
          [0., 0., 2.,  ..., 2., 0., 2.],
          [2., 2., 2.,  ..., 0., 0., 0.],
          [0., 2., 0.,  ..., 2., 0., 0.]],

         [[0., 2., 0.,  ..., 2., 0., 0.],
          [2., 2., 0.,  ..., 0., 0., 0.],
          [2., 2., 2.,  ..., 2., 0., 0.],
          ...,
          [2., 2., 2.,  ..., 0., 2., 2.],
          [2., 0., 2.,  ..., 2., 0., 0.],
          [0., 0., 0.,  ..., 2., 2., 0.]],

         [[0., 2., 2.,  ..., 0., 0., 0.],
          [2., 2., 2.,  ..., 0., 0., 2.],
          [2., 2., 2.,  ..., 2., 0., 2.],
          ...,
          [0., 0., 2.,  ..., 2., 0., 0.],
          [2., 0., 0.,  ..., 2., 0., 2.],
          [2., 0., 0.,  ..., 0., 2., 2.]]],


        [[[2., 2., 2.,  ..., 0., 0., 2.],
          [0., 0., 2.,  ..., 2., 0., 2.],
          [0., 0., 0.,  ..., 2., 0., 0.],
          ...,
          [2., 2., 2.,  ..., 0., 0., 0.],
          [0., 2., 2.,  ..., 2., 0., 2.],
          [0., 0., 2.,  ..., 2., 0., 2.]],

         [[0., 0., 2.,  ..., 2., 0., 0.],
          [0., 0., 0.,  ..., 2., 0., 0.],
          [2., 2., 0.,  ..., 0., 0., 0.],
          ...,
          [2., 0., 2.,  ..., 0., 2., 0.],
          [0., 0., 2.,  ..., 2., 2., 2.],
          [0., 2., 0.,  ..., 0., 0., 2.]],

         [[0., 0., 0.,  ..., 0., 0., 2.],
          [0., 2., 0.,  ..., 2., 0., 0.],
          [2., 2., 0.,  ..., 2., 0., 2.],
          ...,
          [2., 0., 0.,  ..., 2., 0., 2.],
          [2., 0., 0.,  ..., 0., 2., 0.],
          [2., 2., 0.,  ..., 0., 2., 0.]],

         ...,

         [[0., 0., 0.,  ..., 2., 2., 2.],
          [0., 0., 0.,  ..., 0., 0., 2.],
          [2., 0., 2.,  ..., 2., 2., 0.],
          ...,
          [0., 2., 2.,  ..., 0., 2., 0.],
          [0., 0., 0.,  ..., 2., 2., 2.],
          [2., 2., 0.,  ..., 2., 0., 2.]],

         [[2., 2., 0.,  ..., 0., 2., 2.],
          [2., 2., 2.,  ..., 2., 0., 2.],
          [2., 0., 2.,  ..., 2., 2., 0.],
          ...,
          [2., 2., 2.,  ..., 2., 2., 2.],
          [0., 0., 0.,  ..., 2., 0., 2.],
          [2., 0., 2.,  ..., 2., 0., 2.]],

         [[2., 0., 0.,  ..., 2., 2., 0.],
          [2., 2., 2.,  ..., 0., 0., 0.],
          [0., 2., 0.,  ..., 0., 2., 0.],
          ...,
          [0., 0., 0.,  ..., 0., 0., 2.],
          [0., 0., 2.,  ..., 2., 2., 2.],
          [2., 0., 0.,  ..., 0., 0., 2.]]],


        ...,


        [[[0., 0., 0.,  ..., 0., 0., 0.],
          [2., 0., 2.,  ..., 2., 2., 0.],
          [0., 2., 0.,  ..., 0., 2., 0.],
          ...,
          [2., 2., 2.,  ..., 0., 2., 2.],
          [2., 2., 2.,  ..., 2., 2., 0.],
          [2., 0., 2.,  ..., 2., 0., 0.]],

         [[2., 0., 0.,  ..., 0., 0., 2.],
          [0., 0., 2.,  ..., 2., 2., 0.],
          [0., 2., 0.,  ..., 0., 2., 2.],
          ...,
          [0., 0., 0.,  ..., 2., 0., 2.],
          [2., 0., 0.,  ..., 2., 2., 0.],
          [2., 0., 2.,  ..., 0., 0., 2.]],

         [[2., 0., 0.,  ..., 2., 0., 2.],
          [0., 0., 2.,  ..., 0., 0., 2.],
          [0., 0., 2.,  ..., 2., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 2., 0., 0.],
          [0., 0., 2.,  ..., 2., 0., 0.],
          [0., 2., 0.,  ..., 0., 0., 0.]],

         ...,

         [[2., 2., 2.,  ..., 0., 0., 0.],
          [2., 2., 0.,  ..., 0., 2., 0.],
          [0., 0., 0.,  ..., 0., 0., 2.],
          ...,
          [2., 2., 2.,  ..., 0., 2., 0.],
          [2., 0., 0.,  ..., 2., 2., 0.],
          [0., 2., 0.,  ..., 0., 2., 0.]],

         [[2., 2., 0.,  ..., 0., 2., 2.],
          [2., 2., 2.,  ..., 2., 2., 2.],
          [0., 2., 2.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 2.,  ..., 2., 2., 2.],
          [0., 0., 0.,  ..., 0., 2., 0.],
          [2., 2., 0.,  ..., 2., 2., 0.]],

         [[2., 0., 2.,  ..., 0., 2., 0.],
          [2., 2., 0.,  ..., 2., 0., 2.],
          [2., 2., 2.,  ..., 0., 0., 0.],
          ...,
          [0., 0., 0.,  ..., 2., 2., 0.],
          [0., 2., 2.,  ..., 0., 0., 0.],
          [2., 0., 0.,  ..., 0., 2., 2.]]],


        [[[0., 0., 0.,  ..., 2., 0., 2.],
          [2., 2., 0.,  ..., 0., 0., 0.],
          [2., 2., 0.,  ..., 0., 2., 2.],
          ...,
          [2., 0., 2.,  ..., 0., 2., 0.],
          [2., 2., 0.,  ..., 0., 0., 0.],
          [2., 2., 0.,  ..., 2., 2., 0.]],

         [[0., 0., 0.,  ..., 2., 0., 0.],
          [2., 2., 2.,  ..., 2., 0., 2.],
          [2., 2., 2.,  ..., 0., 0., 2.],
          ...,
          [0., 2., 0.,  ..., 0., 2., 0.],
          [0., 2., 2.,  ..., 2., 2., 0.],
          [2., 0., 0.,  ..., 0., 2., 0.]],

         [[0., 0., 2.,  ..., 0., 2., 2.],
          [2., 2., 2.,  ..., 0., 2., 0.],
          [0., 2., 0.,  ..., 2., 2., 0.],
          ...,
          [0., 2., 0.,  ..., 2., 2., 0.],
          [0., 0., 2.,  ..., 2., 0., 2.],
          [0., 2., 2.,  ..., 2., 2., 0.]],

         ...,

         [[2., 2., 2.,  ..., 2., 0., 2.],
          [2., 2., 0.,  ..., 0., 0., 0.],
          [2., 2., 0.,  ..., 2., 2., 0.],
          ...,
          [0., 0., 0.,  ..., 2., 0., 0.],
          [2., 2., 0.,  ..., 0., 0., 0.],
          [0., 2., 2.,  ..., 0., 0., 2.]],

         [[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 2.,  ..., 2., 2., 2.],
          [0., 0., 0.,  ..., 0., 2., 0.],
          ...,
          [2., 0., 2.,  ..., 2., 2., 2.],
          [2., 2., 0.,  ..., 2., 2., 0.],
          [0., 2., 2.,  ..., 2., 0., 2.]],

         [[0., 2., 2.,  ..., 2., 0., 0.],
          [0., 0., 2.,  ..., 2., 0., 0.],
          [2., 0., 0.,  ..., 2., 2., 0.],
          ...,
          [2., 2., 2.,  ..., 0., 0., 2.],
          [2., 2., 0.,  ..., 0., 2., 0.],
          [0., 0., 0.,  ..., 0., 2., 0.]]],


        [[[2., 2., 2.,  ..., 2., 0., 0.],
          [0., 0., 0.,  ..., 2., 2., 0.],
          [2., 2., 0.,  ..., 0., 0., 2.],
          ...,
          [0., 2., 2.,  ..., 0., 0., 2.],
          [0., 2., 0.,  ..., 2., 0., 2.],
          [0., 0., 2.,  ..., 2., 0., 2.]],

         [[0., 0., 2.,  ..., 0., 0., 2.],
          [0., 2., 2.,  ..., 0., 2., 2.],
          [0., 2., 2.,  ..., 2., 0., 0.],
          ...,
          [2., 2., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 2., 2.],
          [0., 0., 2.,  ..., 0., 2., 0.]],

         [[2., 2., 0.,  ..., 0., 2., 2.],
          [0., 2., 0.,  ..., 2., 0., 2.],
          [2., 2., 2.,  ..., 2., 0., 2.],
          ...,
          [0., 0., 0.,  ..., 0., 2., 0.],
          [2., 0., 2.,  ..., 0., 2., 0.],
          [2., 2., 0.,  ..., 2., 2., 0.]],

         ...,

         [[2., 0., 0.,  ..., 0., 2., 2.],
          [0., 2., 0.,  ..., 2., 2., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          ...,
          [2., 0., 2.,  ..., 2., 2., 2.],
          [2., 2., 0.,  ..., 2., 0., 2.],
          [0., 0., 0.,  ..., 2., 0., 2.]],

         [[2., 2., 0.,  ..., 0., 0., 0.],
          [0., 0., 2.,  ..., 0., 0., 2.],
          [0., 2., 0.,  ..., 2., 2., 0.],
          ...,
          [0., 0., 2.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 2., 2.],
          [0., 0., 2.,  ..., 2., 0., 0.]],

         [[0., 0., 2.,  ..., 2., 0., 0.],
          [0., 2., 0.,  ..., 2., 2., 2.],
          [2., 2., 0.,  ..., 0., 2., 0.],
          ...,
          [2., 0., 0.,  ..., 2., 2., 2.],
          [0., 0., 2.,  ..., 2., 2., 2.],
          [0., 2., 2.,  ..., 2., 2., 2.]]]], device='cuda:0',
       dtype=torch.float64, grad_fn=<ClampBackward1>)
Tensor(shape=[24, 17, 256, 256], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [[[[0., 2., 2., ..., 2., 0., 2.],
          [0., 0., 2., ..., 0., 0., 0.],
          [2., 0., 2., ..., 2., 0., 2.],
          ...,
          [0., 0., 2., ..., 2., 0., 2.],
          [2., 0., 0., ..., 0., 0., 0.],
          [0., 2., 0., ..., 0., 2., 0.]],

         [[2., 0., 2., ..., 2., 0., 0.],
          [0., 2., 2., ..., 2., 2., 2.],
          [0., 0., 2., ..., 2., 0., 2.],
          ...,
          [2., 2., 2., ..., 2., 0., 2.],
          [2., 2., 0., ..., 0., 2., 0.],
          [0., 0., 0., ..., 0., 2., 0.]],

         [[2., 2., 0., ..., 0., 2., 0.],
          [0., 0., 0., ..., 2., 0., 0.],
          [0., 2., 0., ..., 0., 2., 0.],
          ...,
          [0., 2., 2., ..., 2., 2., 0.],
          [0., 2., 2., ..., 0., 0., 2.],
          [0., 0., 0., ..., 0., 0., 0.]],

         ...,

         [[0., 2., 0., ..., 0., 2., 2.],
          [2., 0., 0., ..., 0., 2., 0.],
          [0., 2., 2., ..., 0., 2., 0.],
          ...,
          [0., 2., 2., ..., 2., 0., 0.],
          [2., 0., 0., ..., 2., 2., 2.],
          [2., 2., 0., ..., 0., 2., 0.]],

         [[0., 0., 2., ..., 2., 0., 2.],
          [2., 2., 2., ..., 0., 2., 0.],
          [2., 0., 2., ..., 2., 2., 2.],
          ...,
          [2., 0., 2., ..., 0., 2., 0.],
          [0., 0., 0., ..., 2., 2., 0.],
          [2., 2., 2., ..., 2., 0., 2.]],

         [[0., 0., 2., ..., 2., 0., 2.],
          [2., 2., 2., ..., 0., 2., 0.],
          [2., 0., 0., ..., 2., 0., 0.],
          ...,
          [2., 0., 2., ..., 0., 2., 2.],
          [2., 2., 2., ..., 2., 2., 2.],
          [0., 2., 2., ..., 2., 0., 0.]]],


        [[[2., 0., 0., ..., 0., 0., 2.],
          [2., 0., 2., ..., 0., 2., 2.],
          [2., 2., 2., ..., 2., 0., 2.],
          ...,
          [0., 0., 0., ..., 0., 0., 2.],
          [2., 2., 0., ..., 2., 2., 2.],
          [2., 0., 0., ..., 0., 0., 2.]],

         [[0., 0., 0., ..., 0., 0., 2.],
          [0., 0., 2., ..., 0., 0., 2.],
          [0., 2., 2., ..., 2., 0., 0.],
          ...,
          [2., 0., 0., ..., 0., 0., 2.],
          [2., 0., 2., ..., 2., 0., 2.],
          [2., 2., 0., ..., 2., 0., 0.]],

         [[0., 2., 0., ..., 2., 2., 2.],
          [2., 0., 0., ..., 0., 0., 0.],
          [0., 0., 0., ..., 2., 2., 0.],
          ...,
          [2., 2., 2., ..., 0., 2., 2.],
          [2., 0., 2., ..., 2., 0., 0.],
          [0., 2., 2., ..., 0., 0., 2.]],

         ...,

         [[2., 0., 2., ..., 0., 0., 2.],
          [0., 2., 2., ..., 0., 2., 0.],
          [2., 0., 0., ..., 2., 0., 2.],
          ...,
          [0., 0., 2., ..., 2., 0., 2.],
          [2., 2., 2., ..., 0., 0., 0.],
          [0., 2., 0., ..., 2., 0., 0.]],

         [[0., 2., 0., ..., 2., 0., 0.],
          [2., 2., 0., ..., 0., 0., 0.],
          [2., 2., 2., ..., 2., 0., 0.],
          ...,
          [2., 2., 2., ..., 0., 2., 2.],
          [2., 0., 2., ..., 2., 0., 0.],
          [0., 0., 0., ..., 2., 2., 0.]],

         [[0., 2., 2., ..., 0., 0., 0.],
          [2., 2., 2., ..., 0., 0., 2.],
          [2., 2., 2., ..., 2., 0., 2.],
          ...,
          [0., 0., 2., ..., 2., 0., 0.],
          [2., 0., 0., ..., 2., 0., 2.],
          [2., 0., 0., ..., 0., 2., 2.]]],


        [[[2., 2., 2., ..., 0., 0., 2.],
          [0., 0., 2., ..., 2., 0., 2.],
          [0., 0., 0., ..., 2., 0., 0.],
          ...,
          [2., 2., 2., ..., 0., 0., 0.],
          [0., 2., 2., ..., 2., 0., 2.],
          [0., 0., 2., ..., 2., 0., 2.]],

         [[0., 0., 2., ..., 2., 0., 0.],
          [0., 0., 0., ..., 2., 0., 0.],
          [2., 2., 0., ..., 0., 0., 0.],
          ...,
          [2., 0., 2., ..., 0., 2., 0.],
          [0., 0., 2., ..., 2., 2., 2.],
          [0., 2., 0., ..., 0., 0., 2.]],

         [[0., 0., 0., ..., 0., 0., 2.],
          [0., 2., 0., ..., 2., 0., 0.],
          [2., 2., 0., ..., 2., 0., 2.],
          ...,
          [2., 0., 0., ..., 2., 0., 2.],
          [2., 0., 0., ..., 0., 2., 0.],
          [2., 2., 0., ..., 0., 2., 0.]],

         ...,

         [[0., 0., 0., ..., 2., 2., 2.],
          [0., 0., 0., ..., 0., 0., 2.],
          [2., 0., 2., ..., 2., 2., 0.],
          ...,
          [0., 2., 2., ..., 0., 2., 0.],
          [0., 0., 0., ..., 2., 2., 2.],
          [2., 2., 0., ..., 2., 0., 2.]],

         [[2., 2., 0., ..., 0., 2., 2.],
          [2., 2., 2., ..., 2., 0., 2.],
          [2., 0., 2., ..., 2., 2., 0.],
          ...,
          [2., 2., 2., ..., 2., 2., 2.],
          [0., 0., 0., ..., 2., 0., 2.],
          [2., 0., 2., ..., 2., 0., 2.]],

         [[2., 0., 0., ..., 2., 2., 0.],
          [2., 2., 2., ..., 0., 0., 0.],
          [0., 2., 0., ..., 0., 2., 0.],
          ...,
          [0., 0., 0., ..., 0., 0., 2.],
          [0., 0., 2., ..., 2., 2., 2.],
          [2., 0., 0., ..., 0., 0., 2.]]],


        ...,


        [[[0., 0., 0., ..., 0., 0., 0.],
          [2., 0., 2., ..., 2., 2., 0.],
          [0., 2., 0., ..., 0., 2., 0.],
          ...,
          [2., 2., 2., ..., 0., 2., 2.],
          [2., 2., 2., ..., 2., 2., 0.],
          [2., 0., 2., ..., 2., 0., 0.]],

         [[2., 0., 0., ..., 0., 0., 2.],
          [0., 0., 2., ..., 2., 2., 0.],
          [0., 2., 0., ..., 0., 2., 2.],
          ...,
          [0., 0., 0., ..., 2., 0., 2.],
          [2., 0., 0., ..., 2., 2., 0.],
          [2., 0., 2., ..., 0., 0., 2.]],

         [[2., 0., 0., ..., 2., 0., 2.],
          [0., 0., 2., ..., 0., 0., 2.],
          [0., 0., 2., ..., 2., 0., 0.],
          ...,
          [0., 0., 0., ..., 2., 0., 0.],
          [0., 0., 2., ..., 2., 0., 0.],
          [0., 2., 0., ..., 0., 0., 0.]],

         ...,

         [[2., 2., 2., ..., 0., 0., 0.],
          [2., 2., 0., ..., 0., 2., 0.],
          [0., 0., 0., ..., 0., 0., 2.],
          ...,
          [2., 2., 2., ..., 0., 2., 0.],
          [2., 0., 0., ..., 2., 2., 0.],
          [0., 2., 0., ..., 0., 2., 0.]],

         [[2., 2., 0., ..., 0., 2., 2.],
          [2., 2., 2., ..., 2., 2., 2.],
          [0., 2., 2., ..., 0., 0., 0.],
          ...,
          [0., 0., 2., ..., 2., 2., 2.],
          [0., 0., 0., ..., 0., 2., 0.],
          [2., 2., 0., ..., 2., 2., 0.]],

         [[2., 0., 2., ..., 0., 2., 0.],
          [2., 2., 0., ..., 2., 0., 2.],
          [2., 2., 2., ..., 0., 0., 0.],
          ...,
          [0., 0., 0., ..., 2., 2., 0.],
          [0., 2., 2., ..., 0., 0., 0.],
          [2., 0., 0., ..., 0., 2., 2.]]],


        [[[0., 0., 0., ..., 2., 0., 2.],
          [2., 2., 0., ..., 0., 0., 0.],
          [2., 2., 0., ..., 0., 2., 2.],
          ...,
          [2., 0., 2., ..., 0., 2., 0.],
          [2., 2., 0., ..., 0., 0., 0.],
          [2., 2., 0., ..., 2., 2., 0.]],

         [[0., 0., 0., ..., 2., 0., 0.],
          [2., 2., 2., ..., 2., 0., 2.],
          [2., 2., 2., ..., 0., 0., 2.],
          ...,
          [0., 2., 0., ..., 0., 2., 0.],
          [0., 2., 2., ..., 2., 2., 0.],
          [2., 0., 0., ..., 0., 2., 0.]],

         [[0., 0., 2., ..., 0., 2., 2.],
          [2., 2., 2., ..., 0., 2., 0.],
          [0., 2., 0., ..., 2., 2., 0.],
          ...,
          [0., 2., 0., ..., 2., 2., 0.],
          [0., 0., 2., ..., 2., 0., 2.],
          [0., 2., 2., ..., 2., 2., 0.]],

         ...,

         [[2., 2., 2., ..., 2., 0., 2.],
          [2., 2., 0., ..., 0., 0., 0.],
          [2., 2., 0., ..., 2., 2., 0.],
          ...,
          [0., 0., 0., ..., 2., 0., 0.],
          [2., 2., 0., ..., 0., 0., 0.],
          [0., 2., 2., ..., 0., 0., 2.]],

         [[0., 0., 0., ..., 0., 0., 0.],
          [0., 0., 2., ..., 2., 2., 2.],
          [0., 0., 0., ..., 0., 2., 0.],
          ...,
          [2., 0., 2., ..., 2., 2., 2.],
          [2., 2., 0., ..., 2., 2., 0.],
          [0., 2., 2., ..., 2., 0., 2.]],

         [[0., 2., 2., ..., 2., 0., 0.],
          [0., 0., 2., ..., 2., 0., 0.],
          [2., 0., 0., ..., 2., 2., 0.],
          ...,
          [2., 2., 2., ..., 0., 0., 2.],
          [2., 2., 0., ..., 0., 2., 0.],
          [0., 0., 0., ..., 0., 2., 0.]]],


        [[[2., 2., 2., ..., 2., 0., 0.],
          [0., 0., 0., ..., 2., 2., 0.],
          [2., 2., 0., ..., 0., 0., 2.],
          ...,
          [0., 2., 2., ..., 0., 0., 2.],
          [0., 2., 0., ..., 2., 0., 2.],
          [0., 0., 2., ..., 2., 0., 2.]],

         [[0., 0., 2., ..., 0., 0., 2.],
          [0., 2., 2., ..., 0., 2., 2.],
          [0., 2., 2., ..., 2., 0., 0.],
          ...,
          [2., 2., 0., ..., 0., 0., 0.],
          [0., 0., 0., ..., 0., 2., 2.],
          [0., 0., 2., ..., 0., 2., 0.]],

         [[2., 2., 0., ..., 0., 2., 2.],
          [0., 2., 0., ..., 2., 0., 2.],
          [2., 2., 2., ..., 2., 0., 2.],
          ...,
          [0., 0., 0., ..., 0., 2., 0.],
          [2., 0., 2., ..., 0., 2., 0.],
          [2., 2., 0., ..., 2., 2., 0.]],

         ...,

         [[2., 0., 0., ..., 0., 2., 2.],
          [0., 2., 0., ..., 2., 2., 0.],
          [0., 0., 0., ..., 0., 0., 0.],
          ...,
          [2., 0., 2., ..., 2., 2., 2.],
          [2., 2., 0., ..., 2., 0., 2.],
          [0., 0., 0., ..., 2., 0., 2.]],

         [[2., 2., 0., ..., 0., 0., 0.],
          [0., 0., 2., ..., 0., 0., 2.],
          [0., 2., 0., ..., 2., 2., 0.],
          ...,
          [0., 0., 2., ..., 0., 0., 0.],
          [0., 0., 0., ..., 0., 2., 2.],
          [0., 0., 2., ..., 2., 0., 0.]],

         [[0., 0., 2., ..., 2., 0., 0.],
          [0., 2., 0., ..., 2., 2., 2.],
          [2., 2., 0., ..., 0., 2., 0.],
          ...,
          [2., 0., 0., ..., 2., 2., 2.],
          [0., 0., 2., ..., 2., 2., 2.],
          [0., 2., 2., ..., 2., 2., 2.]]]])
[Pass] paddle.clip(Tensor([24, 17, 256, 256],"float64"), min=0, max=2, )
2025-05-16 05:34:46.290233 test begin: paddle.clip(Tensor([25, 1],"float64"), min=-2.0, max=2.0, )
tensor([[-1.6130e+307],
        [ 4.4186e+307],
        [ 3.4556e+307],
        [ 2.8897e+307],
        [-2.9879e+307],
        [-8.3435e+306],
        [-8.5768e+307],
        [-1.5701e+307],
        [-3.4179e+307],
        [-1.7387e+307],
        [ 6.6474e+307],
        [-5.3636e+307],
        [-7.3022e+307],
        [ 2.2458e+307],
        [-8.7455e+307],
        [ 8.6921e+307],
        [-6.1584e+307],
        [-5.0871e+307],
        [ 8.3772e+307],
        [ 6.7324e+307],
        [ 3.6027e+307],
        [-1.5683e+307],
        [ 1.7288e+307],
        [-5.2675e+307],
        [ 4.5922e+306]], device='cuda:0', dtype=torch.float64,
       requires_grad=True)
-----------
tensor([[-2.],
        [ 2.],
        [ 2.],
        [ 2.],
        [-2.],
        [-2.],
        [-2.],
        [-2.],
        [-2.],
        [-2.],
        [ 2.],
        [-2.],
        [-2.],
        [ 2.],
        [-2.],
        [ 2.],
        [-2.],
        [-2.],
        [ 2.],
        [ 2.],
        [ 2.],
        [-2.],
        [ 2.],
        [-2.],
        [ 2.]], device='cuda:0', dtype=torch.float64, grad_fn=<ClampBackward1>)
Tensor(shape=[25, 1], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [[-2.],
        [ 2.],
        [ 2.],
        [ 2.],
        [-2.],
        [-2.],
        [-2.],
        [-2.],
        [-2.],
        [-2.],
        [ 2.],
        [-2.],
        [-2.],
        [ 2.],
        [-2.],
        [ 2.],
        [-2.],
        [-2.],
        [ 2.],
        [ 2.],
        [ 2.],
        [-2.],
        [ 2.],
        [-2.],
        [ 2.]])
[Pass] paddle.clip(Tensor([25, 1],"float64"), min=-2.0, max=2.0, )
2025-05-16 05:34:46.482063 test begin: paddle.clip(Tensor([25, 25],"float64"), min=-2.0, max=2.0, )
tensor([[ 5.4814e+307, -2.0464e+307, -5.5393e+307, -8.6460e+307, -8.0505e+307,
         -7.5570e+307, -1.8818e+307, -1.2865e+307, -4.9268e+307, -5.8416e+307,
          6.2099e+306, -4.4154e+307,  3.4757e+307, -1.8379e+307, -5.5541e+307,
          2.8115e+307,  3.6116e+307,  3.8433e+307,  8.7070e+305,  4.1212e+307,
         -7.4466e+307,  4.1898e+307,  5.0621e+307,  7.9512e+307,  2.2619e+307],
        [ 5.4449e+307,  6.5566e+307, -8.1643e+307,  4.6811e+307, -1.7928e+307,
         -4.4074e+307, -1.1940e+307,  6.0433e+307, -4.8958e+307, -2.5792e+307,
          1.9692e+307, -4.0082e+307,  4.8787e+307, -3.5898e+307,  8.3895e+307,
          6.0010e+307,  5.7035e+307,  3.2967e+307,  2.7065e+307,  7.3033e+307,
          8.4303e+307, -5.8858e+307,  8.2628e+307,  5.8080e+307,  6.6926e+307],
        [ 2.4313e+307,  5.2223e+307, -3.7557e+307, -3.3116e+307,  3.4079e+307,
         -3.9586e+307,  5.4236e+307, -7.2925e+307, -6.5920e+307,  2.6496e+307,
          1.6435e+307,  2.4404e+307,  2.4297e+307, -2.5586e+307, -1.2581e+307,
         -1.7392e+307, -4.5181e+307,  4.1359e+307,  1.8582e+307, -5.8634e+307,
          7.5388e+307, -7.4214e+307, -4.2358e+307,  3.3158e+305, -5.7679e+307],
        [ 4.1917e+307,  1.9431e+306, -8.3062e+307, -6.6975e+307,  5.4613e+306,
         -6.2343e+307, -5.3634e+307,  3.1476e+307, -2.5427e+307,  6.9972e+307,
         -4.7840e+307,  3.1911e+307, -1.7117e+307,  1.6939e+306,  3.4562e+307,
         -7.7182e+307, -8.4809e+307,  5.1677e+307, -6.1169e+307,  3.3224e+306,
         -1.2646e+307,  8.3618e+307,  3.5956e+307,  6.3453e+307,  2.5451e+307],
        [-4.2293e+307, -5.2480e+307, -6.1313e+307, -5.8257e+307, -5.9136e+307,
         -1.2123e+307,  3.0831e+307, -7.5247e+307, -4.0819e+307,  4.8255e+307,
         -5.5811e+307, -4.1750e+307, -1.5102e+307, -4.8749e+307, -6.1772e+306,
         -4.0595e+307, -1.4687e+307,  7.6180e+307,  2.5527e+307,  7.7321e+307,
         -3.5962e+307, -8.1296e+307,  4.8072e+307,  1.6341e+307, -1.4938e+307],
        [ 8.0407e+307, -7.1606e+307, -8.1518e+307,  8.3066e+307,  8.8540e+306,
          6.8601e+307, -7.3017e+307,  5.9288e+307,  8.6050e+307, -1.7606e+307,
         -5.3980e+306,  8.8673e+307, -8.9964e+306,  5.5788e+307, -4.9609e+307,
          4.3556e+307,  2.4341e+307, -3.7680e+306,  7.2290e+307,  7.4939e+306,
          3.8537e+307,  6.0591e+307,  8.2401e+307,  8.7879e+306,  2.9578e+307],
        [-5.9412e+307, -1.1539e+307, -1.2835e+307,  4.0511e+307, -3.9200e+306,
          2.9866e+307,  7.5971e+307, -6.0219e+307,  7.5543e+307,  3.2478e+307,
          5.0932e+307,  5.6240e+307,  3.5558e+307,  1.9793e+307,  2.3002e+306,
          6.2647e+307,  7.5364e+307,  5.7961e+307, -3.7317e+307,  5.6725e+307,
         -6.8125e+307, -4.5358e+307, -2.0705e+307, -1.9395e+307,  3.1698e+307],
        [-4.9154e+307,  2.5949e+307, -3.3184e+307, -3.3091e+307, -2.7346e+307,
         -6.0489e+307, -5.9998e+307, -8.7924e+307, -2.3181e+307,  8.4204e+307,
          3.1609e+307, -4.1015e+307,  6.2870e+307,  4.3045e+307,  8.6402e+307,
          5.7609e+307, -1.9751e+307, -8.9815e+307,  2.6856e+307,  6.3424e+307,
         -4.5771e+307,  8.2475e+307,  5.5731e+306, -2.5817e+307, -6.9997e+307],
        [ 8.1642e+307,  7.7903e+307,  8.8896e+307,  8.2200e+307,  6.1748e+307,
          7.9089e+307, -6.9154e+307,  7.6776e+307, -4.7324e+307,  9.3076e+306,
         -7.7416e+307, -4.3541e+306, -4.9505e+307,  4.6352e+307,  4.4298e+306,
         -2.5211e+307, -3.3202e+307,  8.3491e+307,  6.9963e+307, -8.5047e+307,
         -1.0167e+307, -2.4316e+307,  5.8085e+307,  3.3296e+307,  3.2105e+307],
        [ 6.0770e+307,  5.0712e+307, -1.8208e+307,  6.9052e+307, -2.5408e+307,
         -1.0857e+307, -5.5924e+307, -8.7581e+307, -4.7584e+307,  2.5617e+307,
          5.0502e+307, -2.6237e+307,  3.6159e+307, -8.5360e+307,  8.7504e+307,
         -2.3365e+307,  2.3446e+306, -5.8255e+307,  4.5251e+307, -5.6352e+307,
         -2.1075e+307, -2.4851e+307,  5.4071e+307,  8.4720e+307,  1.9864e+307],
        [ 3.7985e+306,  5.5228e+307, -4.2548e+307, -2.2452e+307, -5.1242e+307,
         -7.2376e+307, -5.9173e+307, -9.0192e+306, -2.2554e+307, -3.7520e+307,
         -4.1504e+307,  3.6337e+307, -8.8722e+307,  2.1114e+307,  2.0182e+307,
          4.3552e+307, -7.9292e+307, -4.9414e+307,  5.9271e+307,  8.4619e+307,
          5.4705e+307,  2.6174e+307, -4.6213e+307,  8.4952e+307,  6.7778e+307],
        [ 8.0378e+307,  4.0502e+307, -7.5627e+307, -2.4892e+306,  2.8131e+307,
         -8.2286e+307,  4.2841e+307, -2.8536e+307,  7.5491e+307, -6.7931e+307,
         -5.2697e+307,  3.0218e+307, -8.5452e+306, -4.0596e+307,  5.8644e+307,
          8.3430e+307, -5.7384e+307,  5.8128e+307,  6.3965e+307, -1.3046e+307,
         -8.7247e+307,  2.0144e+307, -3.4363e+307, -5.7293e+307, -7.7164e+307],
        [-7.0042e+307, -1.7640e+307,  7.4684e+306, -5.5614e+307, -7.2627e+307,
          4.7181e+307, -3.9644e+307,  3.8525e+307,  2.1907e+307, -6.4851e+307,
         -1.0410e+307,  8.3459e+307,  5.9884e+307,  4.5479e+307,  6.1469e+307,
          1.8668e+307, -5.9527e+307, -6.2175e+307, -4.0716e+307,  4.9635e+307,
          9.7573e+306,  3.7549e+307, -2.9813e+307, -4.1684e+307, -7.1808e+307],
        [ 1.1068e+307,  6.6729e+307, -1.3344e+307,  3.2912e+307, -6.2819e+306,
          7.6376e+307,  7.9249e+307, -2.2732e+307,  2.1130e+307,  7.2003e+307,
         -5.2029e+307, -2.4652e+307, -5.3047e+307,  8.2230e+307,  5.1966e+307,
         -8.4363e+306,  1.9903e+307, -4.0220e+307,  4.9684e+307, -2.7885e+307,
          8.7805e+306,  3.3166e+307,  5.7898e+307, -2.3556e+307,  3.1269e+307],
        [ 5.7616e+307, -3.1065e+307, -5.5380e+307, -7.4781e+307,  6.3783e+306,
         -3.1620e+307, -4.6216e+307, -3.2350e+307, -5.1989e+306,  6.9423e+306,
          8.6052e+307, -5.3587e+307, -1.0280e+307, -5.6925e+307, -7.8497e+307,
         -6.1567e+307, -1.7739e+307,  6.8814e+307, -1.0675e+307,  7.1579e+307,
         -2.2739e+306,  7.9719e+307, -6.5967e+306, -3.7126e+306, -3.6701e+307],
        [ 4.3958e+307, -2.8484e+307, -1.8652e+307, -1.4331e+307, -3.2466e+307,
          7.0594e+307, -8.3211e+307, -5.8672e+307,  6.5547e+307, -1.6122e+306,
          5.9543e+307, -7.6909e+307,  4.4223e+307, -4.3626e+307,  5.9807e+307,
          6.3029e+307, -1.1439e+307, -5.8740e+307,  6.9201e+307, -5.0474e+307,
          2.7452e+307,  7.1958e+307, -7.1811e+307,  3.6787e+307, -3.5985e+307],
        [-5.1863e+307,  3.6835e+307,  6.4024e+307, -8.2488e+307, -8.5549e+307,
         -2.6650e+307,  2.5176e+307, -3.5613e+307, -1.9661e+307,  7.4840e+306,
          4.3036e+307, -5.2738e+307,  3.1240e+307, -2.7038e+307, -1.2652e+307,
         -8.3910e+307, -6.6904e+307,  8.5767e+307,  3.7396e+307,  8.4728e+307,
          5.6256e+307,  8.9788e+307, -8.2567e+307,  2.5511e+307,  3.0112e+307],
        [-3.1497e+307, -3.0813e+307, -1.7215e+307, -2.7419e+307,  7.3434e+307,
          7.8278e+307,  6.4649e+307,  2.9117e+307, -5.4140e+307,  7.2138e+307,
          8.1686e+307,  1.4863e+307,  3.7236e+305, -2.9867e+306, -2.0799e+307,
         -6.8350e+307, -7.7104e+307, -4.8465e+307, -8.4939e+307, -6.3855e+307,
          5.2852e+307, -2.8738e+307, -1.6571e+307, -4.4724e+307,  1.7421e+306],
        [-6.0841e+307,  1.0046e+307,  7.3846e+307,  3.2448e+307,  6.7820e+307,
         -2.6719e+307,  7.1670e+307,  6.3172e+307,  9.6164e+306, -8.6307e+307,
          1.3904e+307, -4.7634e+307,  8.4129e+307,  7.8390e+307,  2.9193e+306,
         -7.7918e+307,  3.8549e+307, -3.2271e+307,  3.7556e+307,  5.8274e+307,
          3.4460e+306, -5.3334e+307, -6.3575e+306, -1.2401e+307, -3.0481e+307],
        [ 7.2113e+307,  7.6927e+307, -1.4409e+307, -8.3249e+307,  8.4088e+307,
         -7.7641e+307,  1.6248e+307,  5.6883e+307, -7.9564e+307,  8.8535e+307,
          8.9025e+307,  6.7214e+307,  3.7086e+307,  5.1349e+307,  8.8059e+307,
          6.2648e+307,  8.4841e+307, -8.4376e+307, -1.3847e+307,  8.7557e+307,
         -6.6758e+307,  6.7096e+307,  2.2228e+307, -3.9798e+307, -6.9113e+307],
        [ 7.0723e+307,  4.3468e+306,  4.9379e+307,  1.2928e+307,  7.2988e+307,
          8.7173e+307,  1.3544e+307, -7.0246e+307,  1.7918e+307,  8.3695e+307,
          1.2511e+307, -6.5621e+307,  8.3600e+307, -3.5238e+307, -8.2186e+307,
          4.2349e+306, -7.2930e+307, -2.7512e+306,  7.6182e+306,  3.9625e+307,
          1.5266e+307, -1.2994e+307, -8.4395e+307, -1.4186e+307, -1.4243e+307],
        [ 2.5149e+307, -4.5272e+306, -8.7992e+307, -8.3073e+307,  1.0278e+307,
          5.8925e+307,  1.7767e+307,  1.2139e+306,  8.2027e+307, -3.6718e+307,
         -3.7285e+307, -4.5709e+307, -1.0931e+307, -5.3481e+307, -7.5354e+307,
          4.7897e+307, -5.5071e+307,  2.6513e+307,  3.4356e+307, -8.2046e+306,
          1.4043e+307, -4.2477e+307, -3.1940e+307, -4.1311e+307,  8.6776e+307],
        [-6.4472e+307, -2.5616e+307,  2.4171e+307,  4.9130e+306, -8.0411e+307,
         -2.3547e+307,  5.6803e+307,  4.8257e+307,  7.1906e+307,  2.3079e+307,
         -3.4783e+307, -5.3585e+307,  7.6345e+307,  7.6924e+307,  3.7387e+307,
          6.4198e+307, -8.6351e+307, -6.7412e+307,  2.3303e+307,  6.0895e+307,
          5.9824e+307,  1.6863e+306, -2.5860e+307, -4.2419e+307, -1.5466e+307],
        [-2.4300e+307, -4.2136e+306,  5.9871e+307, -6.4191e+307,  7.1103e+307,
         -1.3483e+307, -2.8606e+307,  3.3985e+307, -3.0035e+307,  6.4096e+307,
         -3.9904e+307,  7.0477e+307, -7.9433e+307,  8.2162e+307,  8.7144e+307,
          5.3523e+307, -3.4793e+307, -4.6036e+307, -7.1463e+307, -7.2969e+307,
          3.1212e+307,  3.5600e+307,  1.5676e+307,  5.6599e+307,  5.6979e+307],
        [-7.8783e+306,  4.1248e+307,  6.8256e+307,  2.9989e+304,  8.5415e+307,
          3.1486e+307, -4.8043e+307, -6.8937e+307, -1.4085e+307,  8.6924e+307,
          4.0388e+307, -3.7086e+307, -3.9235e+307, -6.7583e+307,  7.5782e+307,
         -5.0972e+307,  5.2335e+307,  1.9423e+307, -1.8127e+306, -5.8446e+307,
         -6.4685e+307, -5.7997e+307,  6.1341e+307, -6.0288e+307, -8.3357e+307]],
       device='cuda:0', dtype=torch.float64, requires_grad=True)
-----------
tensor([[ 2., -2., -2., -2., -2., -2., -2., -2., -2., -2.,  2., -2.,  2., -2.,
         -2.,  2.,  2.,  2.,  2.,  2., -2.,  2.,  2.,  2.,  2.],
        [ 2.,  2., -2.,  2., -2., -2., -2.,  2., -2., -2.,  2., -2.,  2., -2.,
          2.,  2.,  2.,  2.,  2.,  2.,  2., -2.,  2.,  2.,  2.],
        [ 2.,  2., -2., -2.,  2., -2.,  2., -2., -2.,  2.,  2.,  2.,  2., -2.,
         -2., -2., -2.,  2.,  2., -2.,  2., -2., -2.,  2., -2.],
        [ 2.,  2., -2., -2.,  2., -2., -2.,  2., -2.,  2., -2.,  2., -2.,  2.,
          2., -2., -2.,  2., -2.,  2., -2.,  2.,  2.,  2.,  2.],
        [-2., -2., -2., -2., -2., -2.,  2., -2., -2.,  2., -2., -2., -2., -2.,
         -2., -2., -2.,  2.,  2.,  2., -2., -2.,  2.,  2., -2.],
        [ 2., -2., -2.,  2.,  2.,  2., -2.,  2.,  2., -2., -2.,  2., -2.,  2.,
         -2.,  2.,  2., -2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.],
        [-2., -2., -2.,  2., -2.,  2.,  2., -2.,  2.,  2.,  2.,  2.,  2.,  2.,
          2.,  2.,  2.,  2., -2.,  2., -2., -2., -2., -2.,  2.],
        [-2.,  2., -2., -2., -2., -2., -2., -2., -2.,  2.,  2., -2.,  2.,  2.,
          2.,  2., -2., -2.,  2.,  2., -2.,  2.,  2., -2., -2.],
        [ 2.,  2.,  2.,  2.,  2.,  2., -2.,  2., -2.,  2., -2., -2., -2.,  2.,
          2., -2., -2.,  2.,  2., -2., -2., -2.,  2.,  2.,  2.],
        [ 2.,  2., -2.,  2., -2., -2., -2., -2., -2.,  2.,  2., -2.,  2., -2.,
          2., -2.,  2., -2.,  2., -2., -2., -2.,  2.,  2.,  2.],
        [ 2.,  2., -2., -2., -2., -2., -2., -2., -2., -2., -2.,  2., -2.,  2.,
          2.,  2., -2., -2.,  2.,  2.,  2.,  2., -2.,  2.,  2.],
        [ 2.,  2., -2., -2.,  2., -2.,  2., -2.,  2., -2., -2.,  2., -2., -2.,
          2.,  2., -2.,  2.,  2., -2., -2.,  2., -2., -2., -2.],
        [-2., -2.,  2., -2., -2.,  2., -2.,  2.,  2., -2., -2.,  2.,  2.,  2.,
          2.,  2., -2., -2., -2.,  2.,  2.,  2., -2., -2., -2.],
        [ 2.,  2., -2.,  2., -2.,  2.,  2., -2.,  2.,  2., -2., -2., -2.,  2.,
          2., -2.,  2., -2.,  2., -2.,  2.,  2.,  2., -2.,  2.],
        [ 2., -2., -2., -2.,  2., -2., -2., -2., -2.,  2.,  2., -2., -2., -2.,
         -2., -2., -2.,  2., -2.,  2., -2.,  2., -2., -2., -2.],
        [ 2., -2., -2., -2., -2.,  2., -2., -2.,  2., -2.,  2., -2.,  2., -2.,
          2.,  2., -2., -2.,  2., -2.,  2.,  2., -2.,  2., -2.],
        [-2.,  2.,  2., -2., -2., -2.,  2., -2., -2.,  2.,  2., -2.,  2., -2.,
         -2., -2., -2.,  2.,  2.,  2.,  2.,  2., -2.,  2.,  2.],
        [-2., -2., -2., -2.,  2.,  2.,  2.,  2., -2.,  2.,  2.,  2.,  2., -2.,
         -2., -2., -2., -2., -2., -2.,  2., -2., -2., -2.,  2.],
        [-2.,  2.,  2.,  2.,  2., -2.,  2.,  2.,  2., -2.,  2., -2.,  2.,  2.,
          2., -2.,  2., -2.,  2.,  2.,  2., -2., -2., -2., -2.],
        [ 2.,  2., -2., -2.,  2., -2.,  2.,  2., -2.,  2.,  2.,  2.,  2.,  2.,
          2.,  2.,  2., -2., -2.,  2., -2.,  2.,  2., -2., -2.],
        [ 2.,  2.,  2.,  2.,  2.,  2.,  2., -2.,  2.,  2.,  2., -2.,  2., -2.,
         -2.,  2., -2., -2.,  2.,  2.,  2., -2., -2., -2., -2.],
        [ 2., -2., -2., -2.,  2.,  2.,  2.,  2.,  2., -2., -2., -2., -2., -2.,
         -2.,  2., -2.,  2.,  2., -2.,  2., -2., -2., -2.,  2.],
        [-2., -2.,  2.,  2., -2., -2.,  2.,  2.,  2.,  2., -2., -2.,  2.,  2.,
          2.,  2., -2., -2.,  2.,  2.,  2.,  2., -2., -2., -2.],
        [-2., -2.,  2., -2.,  2., -2., -2.,  2., -2.,  2., -2.,  2., -2.,  2.,
          2.,  2., -2., -2., -2., -2.,  2.,  2.,  2.,  2.,  2.],
        [-2.,  2.,  2.,  2.,  2.,  2., -2., -2., -2.,  2.,  2., -2., -2., -2.,
          2., -2.,  2.,  2., -2., -2., -2., -2.,  2., -2., -2.]],
       device='cuda:0', dtype=torch.float64, grad_fn=<ClampBackward1>)
Tensor(shape=[25, 25], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [[ 2., -2., -2., -2., -2., -2., -2., -2., -2., -2.,  2., -2.,  2., -2.,
         -2.,  2.,  2.,  2.,  2.,  2., -2.,  2.,  2.,  2.,  2.],
        [ 2.,  2., -2.,  2., -2., -2., -2.,  2., -2., -2.,  2., -2.,  2., -2.,
          2.,  2.,  2.,  2.,  2.,  2.,  2., -2.,  2.,  2.,  2.],
        [ 2.,  2., -2., -2.,  2., -2.,  2., -2., -2.,  2.,  2.,  2.,  2., -2.,
         -2., -2., -2.,  2.,  2., -2.,  2., -2., -2.,  2., -2.],
        [ 2.,  2., -2., -2.,  2., -2., -2.,  2., -2.,  2., -2.,  2., -2.,  2.,
          2., -2., -2.,  2., -2.,  2., -2.,  2.,  2.,  2.,  2.],
        [-2., -2., -2., -2., -2., -2.,  2., -2., -2.,  2., -2., -2., -2., -2.,
         -2., -2., -2.,  2.,  2.,  2., -2., -2.,  2.,  2., -2.],
        [ 2., -2., -2.,  2.,  2.,  2., -2.,  2.,  2., -2., -2.,  2., -2.,  2.,
         -2.,  2.,  2., -2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.],
        [-2., -2., -2.,  2., -2.,  2.,  2., -2.,  2.,  2.,  2.,  2.,  2.,  2.,
          2.,  2.,  2.,  2., -2.,  2., -2., -2., -2., -2.,  2.],
        [-2.,  2., -2., -2., -2., -2., -2., -2., -2.,  2.,  2., -2.,  2.,  2.,
          2.,  2., -2., -2.,  2.,  2., -2.,  2.,  2., -2., -2.],
        [ 2.,  2.,  2.,  2.,  2.,  2., -2.,  2., -2.,  2., -2., -2., -2.,  2.,
          2., -2., -2.,  2.,  2., -2., -2., -2.,  2.,  2.,  2.],
        [ 2.,  2., -2.,  2., -2., -2., -2., -2., -2.,  2.,  2., -2.,  2., -2.,
          2., -2.,  2., -2.,  2., -2., -2., -2.,  2.,  2.,  2.],
        [ 2.,  2., -2., -2., -2., -2., -2., -2., -2., -2., -2.,  2., -2.,  2.,
          2.,  2., -2., -2.,  2.,  2.,  2.,  2., -2.,  2.,  2.],
        [ 2.,  2., -2., -2.,  2., -2.,  2., -2.,  2., -2., -2.,  2., -2., -2.,
          2.,  2., -2.,  2.,  2., -2., -2.,  2., -2., -2., -2.],
        [-2., -2.,  2., -2., -2.,  2., -2.,  2.,  2., -2., -2.,  2.,  2.,  2.,
          2.,  2., -2., -2., -2.,  2.,  2.,  2., -2., -2., -2.],
        [ 2.,  2., -2.,  2., -2.,  2.,  2., -2.,  2.,  2., -2., -2., -2.,  2.,
          2., -2.,  2., -2.,  2., -2.,  2.,  2.,  2., -2.,  2.],
        [ 2., -2., -2., -2.,  2., -2., -2., -2., -2.,  2.,  2., -2., -2., -2.,
         -2., -2., -2.,  2., -2.,  2., -2.,  2., -2., -2., -2.],
        [ 2., -2., -2., -2., -2.,  2., -2., -2.,  2., -2.,  2., -2.,  2., -2.,
          2.,  2., -2., -2.,  2., -2.,  2.,  2., -2.,  2., -2.],
        [-2.,  2.,  2., -2., -2., -2.,  2., -2., -2.,  2.,  2., -2.,  2., -2.,
         -2., -2., -2.,  2.,  2.,  2.,  2.,  2., -2.,  2.,  2.],
        [-2., -2., -2., -2.,  2.,  2.,  2.,  2., -2.,  2.,  2.,  2.,  2., -2.,
         -2., -2., -2., -2., -2., -2.,  2., -2., -2., -2.,  2.],
        [-2.,  2.,  2.,  2.,  2., -2.,  2.,  2.,  2., -2.,  2., -2.,  2.,  2.,
          2., -2.,  2., -2.,  2.,  2.,  2., -2., -2., -2., -2.],
        [ 2.,  2., -2., -2.,  2., -2.,  2.,  2., -2.,  2.,  2.,  2.,  2.,  2.,
          2.,  2.,  2., -2., -2.,  2., -2.,  2.,  2., -2., -2.],
        [ 2.,  2.,  2.,  2.,  2.,  2.,  2., -2.,  2.,  2.,  2., -2.,  2., -2.,
         -2.,  2., -2., -2.,  2.,  2.,  2., -2., -2., -2., -2.],
        [ 2., -2., -2., -2.,  2.,  2.,  2.,  2.,  2., -2., -2., -2., -2., -2.,
         -2.,  2., -2.,  2.,  2., -2.,  2., -2., -2., -2.,  2.],
        [-2., -2.,  2.,  2., -2., -2.,  2.,  2.,  2.,  2., -2., -2.,  2.,  2.,
          2.,  2., -2., -2.,  2.,  2.,  2.,  2., -2., -2., -2.],
        [-2., -2.,  2., -2.,  2., -2., -2.,  2., -2.,  2., -2.,  2., -2.,  2.,
          2.,  2., -2., -2., -2., -2.,  2.,  2.,  2.,  2.,  2.],
        [-2.,  2.,  2.,  2.,  2.,  2., -2., -2., -2.,  2.,  2., -2., -2., -2.,
          2., -2.,  2.,  2., -2., -2., -2., -2.,  2., -2., -2.]])
[Pass] paddle.clip(Tensor([25, 25],"float64"), min=-2.0, max=2.0, )
2025-05-16 05:34:46.726084 test begin: paddle.clip(Tensor([30, 1],"float64"), min=-2.0, max=2.0, )
tensor([[-1.8516e+307],
        [-7.3874e+307],
        [-2.3334e+307],
        [-6.5878e+307],
        [-1.2525e+306],
        [-3.7931e+307],
        [-4.9794e+307],
        [-2.1873e+307],
        [ 3.1513e+307],
        [ 8.7018e+307],
        [-5.7769e+307],
        [-8.4568e+307],
        [-7.8962e+307],
        [-3.2756e+307],
        [-9.9953e+305],
        [ 7.9002e+307],
        [-3.0802e+306],
        [-6.3479e+307],
        [-6.7070e+307],
        [-6.5828e+307],
        [-8.8631e+307],
        [ 5.9936e+307],
        [-1.8750e+306],
        [ 7.7548e+307],
        [-2.0019e+307],
        [ 8.0010e+307],
        [-3.6054e+307],
        [-6.2083e+307],
        [-6.3358e+307],
        [-7.9892e+307]], device='cuda:0', dtype=torch.float64,
       requires_grad=True)
-----------
tensor([[-2.],
        [-2.],
        [-2.],
        [-2.],
        [-2.],
        [-2.],
        [-2.],
        [-2.],
        [ 2.],
        [ 2.],
        [-2.],
        [-2.],
        [-2.],
        [-2.],
        [-2.],
        [ 2.],
        [-2.],
        [-2.],
        [-2.],
        [-2.],
        [-2.],
        [ 2.],
        [-2.],
        [ 2.],
        [-2.],
        [ 2.],
        [-2.],
        [-2.],
        [-2.],
        [-2.]], device='cuda:0', dtype=torch.float64, grad_fn=<ClampBackward1>)
Tensor(shape=[30, 1], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [[-2.],
        [-2.],
        [-2.],
        [-2.],
        [-2.],
        [-2.],
        [-2.],
        [-2.],
        [ 2.],
        [ 2.],
        [-2.],
        [-2.],
        [-2.],
        [-2.],
        [-2.],
        [ 2.],
        [-2.],
        [-2.],
        [-2.],
        [-2.],
        [-2.],
        [ 2.],
        [-2.],
        [ 2.],
        [-2.],
        [ 2.],
        [-2.],
        [-2.],
        [-2.],
        [-2.]])
[Pass] paddle.clip(Tensor([30, 1],"float64"), min=-2.0, max=2.0, )
2025-05-16 05:34:46.900338 test begin: paddle.clip(Tensor([30, 30],"float64"), min=-2.0, max=2.0, )
tensor([[-7.6056e+307,  1.6603e+307,  1.2295e+307, -5.6014e+306,  2.5599e+307,
         -8.7528e+307, -7.5561e+307, -1.8289e+307,  3.4120e+307, -4.4760e+307,
         -3.7411e+307,  5.7341e+307, -7.9758e+307,  2.1650e+307,  8.5331e+306,
         -4.8245e+307,  7.1938e+307,  6.6823e+307, -2.1725e+307,  8.4558e+307,
         -6.1208e+307, -2.7048e+307,  5.4993e+307, -2.5415e+307, -4.5374e+306,
          7.8576e+307,  4.9759e+307,  3.2580e+307, -3.6245e+307,  3.8592e+307],
        [ 3.7273e+307, -9.3711e+306,  6.4150e+307, -4.8844e+307, -5.4874e+307,
          1.7173e+307, -5.0050e+306,  5.6030e+307,  3.4314e+306, -6.6017e+307,
          7.2931e+307, -6.0056e+306,  6.7593e+307, -3.8181e+307,  6.1738e+307,
          8.2396e+307,  4.7476e+307, -1.3486e+306, -4.8486e+307, -8.4408e+307,
         -5.7987e+307,  9.5428e+306, -8.7922e+307,  1.1961e+307,  7.5337e+306,
         -1.2622e+307, -6.7593e+307, -6.7893e+307, -7.6622e+307,  6.9734e+307],
        [ 2.7755e+307,  8.0523e+307,  2.3571e+307, -5.8505e+307, -2.3532e+307,
          4.2017e+307, -4.6167e+307, -4.4121e+307,  3.6823e+307,  7.4752e+307,
         -1.4407e+307, -3.3855e+307,  6.0649e+307,  3.1342e+307,  5.1166e+307,
          4.0910e+307,  5.7922e+307,  5.1194e+307, -6.3076e+307,  1.7432e+306,
         -8.7357e+307, -3.3380e+307,  6.7963e+307,  4.6028e+306, -1.5304e+307,
         -7.0907e+307, -8.3835e+307, -5.3714e+307, -4.1444e+307,  3.4218e+306],
        [ 2.2753e+307, -4.8001e+307, -3.1646e+307,  4.3388e+307,  7.1286e+307,
         -6.5402e+307,  7.4762e+307, -8.6541e+307,  2.9080e+307, -2.4514e+307,
          1.4895e+307, -2.1975e+305, -7.3002e+307, -7.8955e+307,  8.1355e+307,
         -5.5528e+307, -4.4975e+307, -6.7585e+306, -8.1731e+307, -8.3428e+307,
          5.0278e+306, -1.2978e+307,  2.4498e+307,  6.1092e+307, -2.4018e+307,
          7.7926e+307,  1.5970e+307,  6.1576e+307,  4.7903e+307,  2.7710e+307],
        [-7.6245e+307, -8.3274e+307, -7.4655e+307,  5.4715e+307, -7.6270e+306,
         -3.9744e+307, -1.8192e+307,  8.7842e+307, -6.1888e+307,  7.6086e+307,
         -5.2121e+307, -5.4546e+307,  6.8012e+307, -6.2369e+307, -1.3410e+306,
          4.4346e+307, -8.2854e+307, -1.5766e+306,  5.7332e+307,  7.1283e+307,
          2.8322e+307, -5.7461e+306,  8.5306e+307, -5.4873e+306, -8.2577e+307,
         -4.6062e+307,  5.6302e+307, -1.6230e+307, -3.3104e+307,  8.8339e+307],
        [ 5.6546e+307, -8.8294e+307, -7.1655e+306, -3.4952e+307,  1.7036e+307,
         -8.0142e+307, -5.4307e+306, -3.7104e+307,  4.5658e+305, -3.6580e+307,
         -5.0200e+307, -6.1465e+307,  1.0748e+307,  4.2885e+307, -1.8084e+306,
          7.4000e+307,  6.5010e+307,  4.1964e+307,  2.3123e+307,  6.0945e+307,
          3.1463e+307,  8.9707e+306,  2.6725e+307, -5.6345e+307, -2.5615e+307,
          8.6462e+306,  4.9679e+307, -8.1372e+307,  2.3486e+307, -1.3321e+306],
        [ 3.4919e+307, -4.1452e+307, -5.9219e+306,  6.3838e+307,  2.5887e+307,
         -3.6931e+307,  5.8386e+307,  6.1615e+307, -4.0848e+307, -2.8641e+307,
          1.8483e+305,  2.7144e+307, -8.3206e+307, -3.4125e+307,  8.7348e+307,
         -6.7005e+307,  4.2956e+307,  6.9687e+307, -5.9791e+307,  6.1953e+307,
         -2.8324e+307,  5.5161e+307, -7.8137e+307, -6.8336e+307, -6.8112e+307,
          2.9241e+307,  3.7074e+307, -9.8106e+305,  1.6640e+306,  7.4563e+307],
        [-4.9486e+307,  8.9614e+307,  4.1492e+307,  8.4973e+307,  2.2486e+307,
         -8.5627e+307,  5.4688e+307,  4.3355e+307, -7.8577e+307, -7.0902e+307,
         -6.8947e+307,  3.1246e+307,  2.4421e+307, -3.7878e+307,  8.4448e+307,
          6.8134e+305, -3.3662e+307, -5.5083e+307, -3.4035e+307,  2.0540e+307,
         -5.6878e+307,  6.2492e+307,  5.3526e+307, -7.3603e+307,  1.7483e+307,
          4.1738e+307, -8.2494e+307, -4.5251e+307,  6.1155e+307,  6.5525e+307],
        [ 3.9189e+307,  8.6030e+307,  1.9566e+307, -1.6719e+307,  5.6583e+306,
         -5.4191e+307,  7.2280e+307, -2.2806e+307, -6.9856e+307,  7.6100e+307,
         -3.9249e+307, -8.2376e+306,  8.5314e+307, -6.4361e+307,  5.9105e+307,
          4.7963e+307,  7.2736e+307, -5.4040e+307, -2.7460e+307, -1.2428e+306,
          4.8059e+307,  7.5915e+307,  3.1741e+307, -1.0203e+307,  2.1121e+307,
         -1.9621e+307,  5.6285e+307,  6.3464e+307, -7.7439e+307, -7.1590e+306],
        [ 1.7347e+306,  6.5720e+307, -4.7942e+306, -2.1070e+307,  5.1383e+307,
          3.8782e+307,  3.7162e+307,  4.7252e+307, -8.5687e+307, -7.8246e+306,
         -4.9538e+307,  2.9294e+307,  7.9368e+306,  4.9391e+307,  6.4317e+307,
          2.3744e+307,  7.6698e+307, -7.1642e+307,  2.0783e+307,  4.0710e+307,
          3.2020e+307, -4.1793e+307,  3.5987e+307,  8.6205e+307, -8.2037e+307,
          7.1837e+307,  8.2773e+307,  3.8621e+307, -8.2637e+307, -6.4326e+307],
        [ 8.6021e+307,  4.1777e+307,  8.7107e+307, -5.9317e+307,  7.9848e+307,
          3.2554e+307, -5.7380e+307, -4.8533e+307, -7.4091e+307,  2.5991e+307,
         -8.1801e+307,  6.5671e+307, -7.7898e+307, -6.0475e+307,  6.6654e+307,
          6.2223e+307,  4.8114e+307, -4.2341e+306,  2.0856e+307, -6.0510e+307,
          8.8225e+307, -2.2860e+306, -3.1734e+306, -7.8648e+307,  1.8334e+307,
         -3.8709e+307,  6.8030e+307,  2.9827e+307,  7.7758e+307, -5.3118e+307],
        [-7.9289e+306, -6.1445e+307,  7.2150e+307,  4.2860e+306, -3.0041e+307,
          2.7578e+307,  3.6378e+306, -8.7257e+307, -2.5057e+307, -6.7734e+307,
          6.7787e+306,  5.7475e+307,  5.2917e+307, -8.3476e+307, -3.7523e+307,
          6.4007e+307,  7.3304e+307, -2.4709e+307,  8.9901e+306,  8.2448e+307,
         -4.7055e+307,  7.0992e+307, -7.8265e+307,  6.7584e+307,  3.5658e+307,
         -4.1174e+306,  7.1747e+307,  8.9564e+307, -7.0547e+307, -2.9436e+307],
        [-4.6109e+307, -3.4673e+307, -3.7592e+307, -7.1280e+307, -6.1789e+307,
          1.4702e+307, -7.1959e+307,  3.4283e+307,  4.9629e+307, -4.0931e+307,
          1.6500e+307,  1.9121e+307, -1.7377e+305, -5.8930e+307,  6.6411e+307,
          4.1422e+307, -8.2003e+307,  1.9074e+307,  2.0955e+307, -8.1824e+306,
         -1.2471e+307,  3.8323e+307, -7.1665e+307, -1.6036e+307, -6.4466e+307,
         -5.9633e+307,  1.4836e+307, -2.6567e+307, -6.2118e+307,  3.1650e+307],
        [ 7.8275e+307, -6.6383e+307,  3.1334e+307, -7.4310e+307, -5.2885e+307,
          1.9436e+307,  5.3946e+307,  4.6324e+307,  4.8621e+307,  5.6597e+307,
          7.4403e+307, -4.5190e+306, -6.6522e+307, -3.8130e+307, -8.1393e+307,
          8.5735e+307,  5.8475e+306,  6.2490e+307, -5.0865e+306,  3.0014e+307,
          6.8856e+307, -8.3217e+307, -5.6892e+307, -5.1983e+307,  1.5042e+307,
         -4.5703e+306,  2.4161e+307,  5.0837e+307,  5.0023e+307, -7.5093e+306],
        [-8.8848e+307, -5.4552e+307,  6.6022e+307,  6.0855e+307, -1.6282e+307,
          6.6546e+307, -3.4090e+307,  8.8102e+307,  7.9824e+307, -2.6589e+307,
         -1.9027e+307,  8.0980e+307,  1.9634e+307,  1.6893e+306, -1.2349e+307,
         -1.3677e+307,  3.8083e+307, -7.5147e+307, -3.9632e+307, -8.2161e+307,
          1.3983e+307,  1.5887e+306, -7.0182e+307,  4.9044e+307,  3.7841e+307,
          7.4558e+307,  6.6980e+306,  4.3485e+307, -6.4551e+307, -1.8814e+307],
        [-4.1156e+307,  2.4599e+307,  5.3344e+307, -4.4725e+307,  9.3177e+306,
         -6.9079e+307, -8.8904e+305, -3.6528e+307,  4.1614e+307, -6.4178e+307,
          7.2692e+307,  8.3821e+307, -6.5664e+307, -1.2578e+307,  5.6220e+307,
          6.1227e+307, -2.8638e+307, -5.9377e+306, -6.3235e+307,  4.0778e+307,
         -1.1354e+307,  3.8651e+307, -2.2817e+307,  7.9505e+307,  5.7038e+307,
          9.2894e+306, -7.6896e+307,  5.1342e+307, -3.3182e+307,  3.7144e+307],
        [ 6.4195e+307,  8.8495e+307, -4.5585e+307, -7.7640e+307, -6.8974e+307,
         -3.7302e+307, -6.7158e+305, -4.4299e+307,  3.5240e+307, -4.9343e+307,
          4.6072e+307, -5.7724e+307, -2.2946e+307,  5.2565e+307, -7.4674e+307,
          5.6413e+307, -4.4954e+307,  3.6301e+307,  4.4088e+307, -3.0747e+307,
         -6.8432e+307, -3.4366e+307,  1.3859e+307, -2.0240e+307,  6.9785e+307,
         -7.6247e+307, -1.6908e+307,  4.6668e+307,  7.7913e+307, -1.3239e+307],
        [-3.4404e+307, -2.3923e+306,  2.0239e+307, -2.6099e+307,  6.8201e+307,
         -5.7315e+307,  6.4003e+307, -3.6591e+307,  8.8033e+307,  4.8444e+307,
         -8.3634e+307, -1.9116e+307,  4.1591e+307,  3.8447e+307,  5.2343e+307,
          6.8116e+307,  7.0332e+307,  3.7532e+307, -2.1314e+307,  6.3134e+305,
          4.3453e+307, -5.3037e+307, -1.6999e+307,  6.8649e+307, -8.6083e+307,
          1.5926e+307, -6.5211e+307,  5.5544e+307, -5.2765e+306,  7.0994e+307],
        [-5.4054e+307,  1.6993e+307,  8.3912e+307, -3.3864e+306,  4.8018e+307,
          6.8612e+307,  8.0923e+307,  3.1114e+307, -1.7547e+307, -5.9517e+307,
          4.6745e+307,  7.0529e+307, -4.9192e+307,  1.7221e+306, -1.8514e+307,
         -5.0563e+307,  3.5421e+307, -8.5390e+307, -8.5995e+307, -3.2583e+307,
         -6.3871e+307,  8.2637e+307,  6.7441e+307, -5.0291e+307,  8.2478e+307,
         -2.2860e+307, -6.8862e+307,  1.8402e+307, -1.6495e+307, -2.3921e+307],
        [-6.6931e+307, -7.3925e+307,  5.2288e+307,  5.8375e+307,  5.9098e+307,
          4.3911e+307,  7.6881e+307, -6.4181e+306,  5.4393e+307, -8.4309e+307,
         -3.8390e+307,  3.9420e+307, -5.0077e+307, -9.5294e+306, -1.9978e+306,
         -1.3443e+307,  2.9663e+307, -2.2212e+307,  1.1584e+307,  8.9139e+307,
          2.9735e+306,  4.1229e+307,  2.9316e+307, -5.1694e+307, -6.3092e+307,
         -7.3322e+307, -2.4768e+307, -1.3652e+306,  7.3928e+307, -1.6391e+307],
        [ 8.0162e+307, -4.8930e+307, -8.5817e+307,  3.1023e+307, -5.0557e+307,
         -6.5019e+306,  2.4547e+307, -1.5322e+306,  1.8187e+307, -9.5512e+306,
         -7.8126e+307,  4.0024e+307, -3.8421e+307, -2.6758e+307, -1.8149e+307,
         -2.3667e+307,  6.4109e+307,  6.8767e+307, -8.0838e+307,  6.0842e+307,
         -3.0102e+307,  3.5805e+307, -3.5363e+307, -2.5410e+307,  6.8083e+307,
         -5.3471e+307, -2.4327e+307,  8.9381e+307,  1.4583e+307,  8.2642e+307],
        [-3.3186e+307, -8.1187e+307,  2.1381e+307, -6.1386e+307, -6.5997e+307,
          6.5502e+307,  5.5976e+307, -2.4524e+307, -4.1122e+307,  9.8519e+306,
          8.8335e+307, -5.7302e+307,  2.3397e+307, -6.5872e+307,  5.4537e+307,
         -5.8603e+307, -4.0182e+307,  5.8140e+307, -2.8169e+307, -8.0285e+307,
         -5.4089e+307, -5.6301e+307, -4.2461e+307,  3.1374e+306,  2.1429e+307,
          5.3857e+307,  1.1617e+307, -7.7178e+307, -3.2585e+307, -8.1135e+306],
        [ 3.3082e+307,  5.5393e+307,  4.6452e+307, -5.8235e+307,  1.7714e+306,
          8.7869e+307, -8.2545e+307,  7.0681e+307,  6.4458e+307,  4.5344e+307,
         -7.2974e+307,  1.8648e+307,  1.1374e+307,  3.8539e+307, -3.5250e+307,
         -8.6234e+307, -7.4805e+307,  7.8276e+307, -7.1503e+307, -8.6456e+307,
         -9.2787e+306, -8.7356e+307,  8.6184e+307,  7.2474e+307, -2.4221e+307,
          6.7797e+307,  2.6465e+307, -4.3344e+307,  7.6891e+307,  8.4936e+307],
        [-4.9958e+307, -2.3874e+307,  7.8031e+307, -2.2436e+307,  6.3365e+307,
          8.9732e+307,  8.1429e+307,  5.1309e+307, -2.6746e+307, -5.9506e+307,
          4.7807e+307,  1.3649e+307, -5.3421e+307, -2.1374e+307,  4.9967e+307,
         -1.0146e+307,  8.6351e+307, -2.0948e+307, -7.7890e+307,  7.2516e+307,
         -5.7589e+307,  5.7816e+307,  1.9172e+306, -4.1238e+307, -8.4808e+307,
          2.4524e+307,  8.1420e+307, -7.6695e+307,  3.4100e+307,  7.9664e+307],
        [ 8.0355e+307, -1.1829e+307, -3.6748e+307, -3.8386e+307, -1.3373e+307,
          7.6467e+307,  5.9392e+305, -7.9468e+307, -4.4409e+307, -7.5417e+306,
         -2.1965e+307, -4.7355e+307, -3.5559e+307,  4.1947e+307,  3.6546e+307,
         -5.1284e+307, -1.0469e+307,  7.1883e+307,  3.6248e+307, -2.2855e+307,
         -8.9707e+307, -9.4191e+306, -1.0167e+307,  8.2485e+307,  6.5930e+307,
          8.0334e+307,  6.8199e+307, -8.9621e+307, -3.8648e+307, -7.7973e+307],
        [-5.6944e+307, -2.1472e+307,  2.7015e+307, -5.6677e+307, -4.8136e+307,
          2.1099e+307,  5.8089e+307, -1.2472e+307, -7.0875e+307,  1.9588e+306,
         -4.5968e+307,  3.1306e+307, -8.2560e+307, -6.1772e+307,  3.1631e+307,
         -2.1538e+306,  2.7429e+307, -2.1822e+307, -2.3945e+307, -5.7126e+307,
          3.0885e+307, -2.5823e+307, -4.3165e+306, -8.7786e+307, -2.5873e+307,
          8.1502e+307, -3.6703e+307,  6.5321e+307, -8.1500e+307, -2.2524e+307],
        [ 2.8113e+307, -7.0769e+307, -4.4862e+307, -5.9777e+307,  3.9519e+307,
         -4.0696e+307, -5.1510e+307,  2.7972e+307, -8.5419e+307, -6.7054e+307,
         -8.4990e+306, -8.7765e+307, -3.9404e+307,  4.4283e+307,  4.1406e+307,
         -6.7375e+307,  8.2709e+307,  4.4941e+307,  5.2866e+307,  8.0868e+307,
         -5.2713e+307, -2.3749e+307,  7.4042e+307, -5.7194e+307, -6.0636e+307,
          1.8295e+307,  5.1456e+307, -2.8979e+307, -3.4982e+307,  4.7420e+307],
        [ 8.2620e+307, -7.0366e+307,  7.2985e+307, -6.1383e+307,  2.9566e+307,
         -2.7715e+307,  4.4126e+307,  8.0318e+307,  6.4553e+307,  4.1706e+307,
         -2.2642e+307,  8.4142e+307,  8.1369e+307,  4.8027e+307, -4.5712e+307,
         -4.8691e+307, -5.1532e+307, -1.9372e+306,  1.4125e+307,  3.3839e+307,
          2.5461e+307, -7.9953e+307,  5.6271e+307,  8.4422e+307, -4.6509e+307,
          6.2023e+307,  2.5949e+307, -7.6786e+307,  6.6472e+306,  8.9401e+307],
        [ 6.0402e+307,  5.6976e+307, -2.4111e+307, -5.8720e+307, -6.9414e+307,
          1.5290e+307,  3.4955e+307,  3.8093e+307,  6.4018e+307, -5.1656e+307,
         -8.0814e+307,  8.8582e+307,  7.9170e+304, -4.2089e+307,  4.0303e+307,
          3.8651e+307, -3.1654e+307,  6.4272e+307, -6.5546e+307, -7.4221e+307,
         -2.2813e+307, -2.6991e+307,  4.9293e+307,  3.6738e+307, -5.1878e+307,
         -4.4996e+307,  1.9395e+307,  4.7730e+307, -1.9892e+307,  2.0376e+306],
        [ 1.1743e+307,  8.7646e+307,  8.1389e+307,  7.5218e+307, -5.3120e+307,
          7.5851e+307,  5.6305e+307, -1.8884e+307, -7.4851e+307, -7.3328e+307,
          3.3779e+306, -6.1268e+307, -8.8312e+307,  6.9086e+307,  1.7964e+307,
          7.9818e+307,  1.4419e+307,  5.0066e+307,  5.1671e+307, -2.3886e+307,
         -3.3513e+307, -8.4844e+307, -6.5831e+307,  3.1035e+307,  4.6025e+307,
         -1.8859e+307, -4.4917e+306, -2.4877e+307, -5.2147e+306,  4.2551e+307]],
       device='cuda:0', dtype=torch.float64, requires_grad=True)
-----------
tensor([[-2.,  2.,  2., -2.,  2., -2., -2., -2.,  2., -2., -2.,  2., -2.,  2.,
          2., -2.,  2.,  2., -2.,  2., -2., -2.,  2., -2., -2.,  2.,  2.,  2.,
         -2.,  2.],
        [ 2., -2.,  2., -2., -2.,  2., -2.,  2.,  2., -2.,  2., -2.,  2., -2.,
          2.,  2.,  2., -2., -2., -2., -2.,  2., -2.,  2.,  2., -2., -2., -2.,
         -2.,  2.],
        [ 2.,  2.,  2., -2., -2.,  2., -2., -2.,  2.,  2., -2., -2.,  2.,  2.,
          2.,  2.,  2.,  2., -2.,  2., -2., -2.,  2.,  2., -2., -2., -2., -2.,
         -2.,  2.],
        [ 2., -2., -2.,  2.,  2., -2.,  2., -2.,  2., -2.,  2., -2., -2., -2.,
          2., -2., -2., -2., -2., -2.,  2., -2.,  2.,  2., -2.,  2.,  2.,  2.,
          2.,  2.],
        [-2., -2., -2.,  2., -2., -2., -2.,  2., -2.,  2., -2., -2.,  2., -2.,
         -2.,  2., -2., -2.,  2.,  2.,  2., -2.,  2., -2., -2., -2.,  2., -2.,
         -2.,  2.],
        [ 2., -2., -2., -2.,  2., -2., -2., -2.,  2., -2., -2., -2.,  2.,  2.,
         -2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2., -2., -2.,  2.,  2., -2.,
          2., -2.],
        [ 2., -2., -2.,  2.,  2., -2.,  2.,  2., -2., -2.,  2.,  2., -2., -2.,
          2., -2.,  2.,  2., -2.,  2., -2.,  2., -2., -2., -2.,  2.,  2., -2.,
          2.,  2.],
        [-2.,  2.,  2.,  2.,  2., -2.,  2.,  2., -2., -2., -2.,  2.,  2., -2.,
          2.,  2., -2., -2., -2.,  2., -2.,  2.,  2., -2.,  2.,  2., -2., -2.,
          2.,  2.],
        [ 2.,  2.,  2., -2.,  2., -2.,  2., -2., -2.,  2., -2., -2.,  2., -2.,
          2.,  2.,  2., -2., -2., -2.,  2.,  2.,  2., -2.,  2., -2.,  2.,  2.,
         -2., -2.],
        [ 2.,  2., -2., -2.,  2.,  2.,  2.,  2., -2., -2., -2.,  2.,  2.,  2.,
          2.,  2.,  2., -2.,  2.,  2.,  2., -2.,  2.,  2., -2.,  2.,  2.,  2.,
         -2., -2.],
        [ 2.,  2.,  2., -2.,  2.,  2., -2., -2., -2.,  2., -2.,  2., -2., -2.,
          2.,  2.,  2., -2.,  2., -2.,  2., -2., -2., -2.,  2., -2.,  2.,  2.,
          2., -2.],
        [-2., -2.,  2.,  2., -2.,  2.,  2., -2., -2., -2.,  2.,  2.,  2., -2.,
         -2.,  2.,  2., -2.,  2.,  2., -2.,  2., -2.,  2.,  2., -2.,  2.,  2.,
         -2., -2.],
        [-2., -2., -2., -2., -2.,  2., -2.,  2.,  2., -2.,  2.,  2., -2., -2.,
          2.,  2., -2.,  2.,  2., -2., -2.,  2., -2., -2., -2., -2.,  2., -2.,
         -2.,  2.],
        [ 2., -2.,  2., -2., -2.,  2.,  2.,  2.,  2.,  2.,  2., -2., -2., -2.,
         -2.,  2.,  2.,  2., -2.,  2.,  2., -2., -2., -2.,  2., -2.,  2.,  2.,
          2., -2.],
        [-2., -2.,  2.,  2., -2.,  2., -2.,  2.,  2., -2., -2.,  2.,  2.,  2.,
         -2., -2.,  2., -2., -2., -2.,  2.,  2., -2.,  2.,  2.,  2.,  2.,  2.,
         -2., -2.],
        [-2.,  2.,  2., -2.,  2., -2., -2., -2.,  2., -2.,  2.,  2., -2., -2.,
          2.,  2., -2., -2., -2.,  2., -2.,  2., -2.,  2.,  2.,  2., -2.,  2.,
         -2.,  2.],
        [ 2.,  2., -2., -2., -2., -2., -2., -2.,  2., -2.,  2., -2., -2.,  2.,
         -2.,  2., -2.,  2.,  2., -2., -2., -2.,  2., -2.,  2., -2., -2.,  2.,
          2., -2.],
        [-2., -2.,  2., -2.,  2., -2.,  2., -2.,  2.,  2., -2., -2.,  2.,  2.,
          2.,  2.,  2.,  2., -2.,  2.,  2., -2., -2.,  2., -2.,  2., -2.,  2.,
         -2.,  2.],
        [-2.,  2.,  2., -2.,  2.,  2.,  2.,  2., -2., -2.,  2.,  2., -2.,  2.,
         -2., -2.,  2., -2., -2., -2., -2.,  2.,  2., -2.,  2., -2., -2.,  2.,
         -2., -2.],
        [-2., -2.,  2.,  2.,  2.,  2.,  2., -2.,  2., -2., -2.,  2., -2., -2.,
         -2., -2.,  2., -2.,  2.,  2.,  2.,  2.,  2., -2., -2., -2., -2., -2.,
          2., -2.],
        [ 2., -2., -2.,  2., -2., -2.,  2., -2.,  2., -2., -2.,  2., -2., -2.,
         -2., -2.,  2.,  2., -2.,  2., -2.,  2., -2., -2.,  2., -2., -2.,  2.,
          2.,  2.],
        [-2., -2.,  2., -2., -2.,  2.,  2., -2., -2.,  2.,  2., -2.,  2., -2.,
          2., -2., -2.,  2., -2., -2., -2., -2., -2.,  2.,  2.,  2.,  2., -2.,
         -2., -2.],
        [ 2.,  2.,  2., -2.,  2.,  2., -2.,  2.,  2.,  2., -2.,  2.,  2.,  2.,
         -2., -2., -2.,  2., -2., -2., -2., -2.,  2.,  2., -2.,  2.,  2., -2.,
          2.,  2.],
        [-2., -2.,  2., -2.,  2.,  2.,  2.,  2., -2., -2.,  2.,  2., -2., -2.,
          2., -2.,  2., -2., -2.,  2., -2.,  2.,  2., -2., -2.,  2.,  2., -2.,
          2.,  2.],
        [ 2., -2., -2., -2., -2.,  2.,  2., -2., -2., -2., -2., -2., -2.,  2.,
          2., -2., -2.,  2.,  2., -2., -2., -2., -2.,  2.,  2.,  2.,  2., -2.,
         -2., -2.],
        [-2., -2.,  2., -2., -2.,  2.,  2., -2., -2.,  2., -2.,  2., -2., -2.,
          2., -2.,  2., -2., -2., -2.,  2., -2., -2., -2., -2.,  2., -2.,  2.,
         -2., -2.],
        [ 2., -2., -2., -2.,  2., -2., -2.,  2., -2., -2., -2., -2., -2.,  2.,
          2., -2.,  2.,  2.,  2.,  2., -2., -2.,  2., -2., -2.,  2.,  2., -2.,
         -2.,  2.],
        [ 2., -2.,  2., -2.,  2., -2.,  2.,  2.,  2.,  2., -2.,  2.,  2.,  2.,
         -2., -2., -2., -2.,  2.,  2.,  2., -2.,  2.,  2., -2.,  2.,  2., -2.,
          2.,  2.],
        [ 2.,  2., -2., -2., -2.,  2.,  2.,  2.,  2., -2., -2.,  2.,  2., -2.,
          2.,  2., -2.,  2., -2., -2., -2., -2.,  2.,  2., -2., -2.,  2.,  2.,
         -2.,  2.],
        [ 2.,  2.,  2.,  2., -2.,  2.,  2., -2., -2., -2.,  2., -2., -2.,  2.,
          2.,  2.,  2.,  2.,  2., -2., -2., -2., -2.,  2.,  2., -2., -2., -2.,
         -2.,  2.]], device='cuda:0', dtype=torch.float64,
       grad_fn=<ClampBackward1>)
Tensor(shape=[30, 30], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [[-2.,  2.,  2., -2.,  2., -2., -2., -2.,  2., -2., -2.,  2., -2.,  2.,
          2., -2.,  2.,  2., -2.,  2., -2., -2.,  2., -2., -2.,  2.,  2.,  2.,
         -2.,  2.],
        [ 2., -2.,  2., -2., -2.,  2., -2.,  2.,  2., -2.,  2., -2.,  2., -2.,
          2.,  2.,  2., -2., -2., -2., -2.,  2., -2.,  2.,  2., -2., -2., -2.,
         -2.,  2.],
        [ 2.,  2.,  2., -2., -2.,  2., -2., -2.,  2.,  2., -2., -2.,  2.,  2.,
          2.,  2.,  2.,  2., -2.,  2., -2., -2.,  2.,  2., -2., -2., -2., -2.,
         -2.,  2.],
        [ 2., -2., -2.,  2.,  2., -2.,  2., -2.,  2., -2.,  2., -2., -2., -2.,
          2., -2., -2., -2., -2., -2.,  2., -2.,  2.,  2., -2.,  2.,  2.,  2.,
          2.,  2.],
        [-2., -2., -2.,  2., -2., -2., -2.,  2., -2.,  2., -2., -2.,  2., -2.,
         -2.,  2., -2., -2.,  2.,  2.,  2., -2.,  2., -2., -2., -2.,  2., -2.,
         -2.,  2.],
        [ 2., -2., -2., -2.,  2., -2., -2., -2.,  2., -2., -2., -2.,  2.,  2.,
         -2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2., -2., -2.,  2.,  2., -2.,
          2., -2.],
        [ 2., -2., -2.,  2.,  2., -2.,  2.,  2., -2., -2.,  2.,  2., -2., -2.,
          2., -2.,  2.,  2., -2.,  2., -2.,  2., -2., -2., -2.,  2.,  2., -2.,
          2.,  2.],
        [-2.,  2.,  2.,  2.,  2., -2.,  2.,  2., -2., -2., -2.,  2.,  2., -2.,
          2.,  2., -2., -2., -2.,  2., -2.,  2.,  2., -2.,  2.,  2., -2., -2.,
          2.,  2.],
        [ 2.,  2.,  2., -2.,  2., -2.,  2., -2., -2.,  2., -2., -2.,  2., -2.,
          2.,  2.,  2., -2., -2., -2.,  2.,  2.,  2., -2.,  2., -2.,  2.,  2.,
         -2., -2.],
        [ 2.,  2., -2., -2.,  2.,  2.,  2.,  2., -2., -2., -2.,  2.,  2.,  2.,
          2.,  2.,  2., -2.,  2.,  2.,  2., -2.,  2.,  2., -2.,  2.,  2.,  2.,
         -2., -2.],
        [ 2.,  2.,  2., -2.,  2.,  2., -2., -2., -2.,  2., -2.,  2., -2., -2.,
          2.,  2.,  2., -2.,  2., -2.,  2., -2., -2., -2.,  2., -2.,  2.,  2.,
          2., -2.],
        [-2., -2.,  2.,  2., -2.,  2.,  2., -2., -2., -2.,  2.,  2.,  2., -2.,
         -2.,  2.,  2., -2.,  2.,  2., -2.,  2., -2.,  2.,  2., -2.,  2.,  2.,
         -2., -2.],
        [-2., -2., -2., -2., -2.,  2., -2.,  2.,  2., -2.,  2.,  2., -2., -2.,
          2.,  2., -2.,  2.,  2., -2., -2.,  2., -2., -2., -2., -2.,  2., -2.,
         -2.,  2.],
        [ 2., -2.,  2., -2., -2.,  2.,  2.,  2.,  2.,  2.,  2., -2., -2., -2.,
         -2.,  2.,  2.,  2., -2.,  2.,  2., -2., -2., -2.,  2., -2.,  2.,  2.,
          2., -2.],
        [-2., -2.,  2.,  2., -2.,  2., -2.,  2.,  2., -2., -2.,  2.,  2.,  2.,
         -2., -2.,  2., -2., -2., -2.,  2.,  2., -2.,  2.,  2.,  2.,  2.,  2.,
         -2., -2.],
        [-2.,  2.,  2., -2.,  2., -2., -2., -2.,  2., -2.,  2.,  2., -2., -2.,
          2.,  2., -2., -2., -2.,  2., -2.,  2., -2.,  2.,  2.,  2., -2.,  2.,
         -2.,  2.],
        [ 2.,  2., -2., -2., -2., -2., -2., -2.,  2., -2.,  2., -2., -2.,  2.,
         -2.,  2., -2.,  2.,  2., -2., -2., -2.,  2., -2.,  2., -2., -2.,  2.,
          2., -2.],
        [-2., -2.,  2., -2.,  2., -2.,  2., -2.,  2.,  2., -2., -2.,  2.,  2.,
          2.,  2.,  2.,  2., -2.,  2.,  2., -2., -2.,  2., -2.,  2., -2.,  2.,
         -2.,  2.],
        [-2.,  2.,  2., -2.,  2.,  2.,  2.,  2., -2., -2.,  2.,  2., -2.,  2.,
         -2., -2.,  2., -2., -2., -2., -2.,  2.,  2., -2.,  2., -2., -2.,  2.,
         -2., -2.],
        [-2., -2.,  2.,  2.,  2.,  2.,  2., -2.,  2., -2., -2.,  2., -2., -2.,
         -2., -2.,  2., -2.,  2.,  2.,  2.,  2.,  2., -2., -2., -2., -2., -2.,
          2., -2.],
        [ 2., -2., -2.,  2., -2., -2.,  2., -2.,  2., -2., -2.,  2., -2., -2.,
         -2., -2.,  2.,  2., -2.,  2., -2.,  2., -2., -2.,  2., -2., -2.,  2.,
          2.,  2.],
        [-2., -2.,  2., -2., -2.,  2.,  2., -2., -2.,  2.,  2., -2.,  2., -2.,
          2., -2., -2.,  2., -2., -2., -2., -2., -2.,  2.,  2.,  2.,  2., -2.,
         -2., -2.],
        [ 2.,  2.,  2., -2.,  2.,  2., -2.,  2.,  2.,  2., -2.,  2.,  2.,  2.,
         -2., -2., -2.,  2., -2., -2., -2., -2.,  2.,  2., -2.,  2.,  2., -2.,
          2.,  2.],
        [-2., -2.,  2., -2.,  2.,  2.,  2.,  2., -2., -2.,  2.,  2., -2., -2.,
          2., -2.,  2., -2., -2.,  2., -2.,  2.,  2., -2., -2.,  2.,  2., -2.,
          2.,  2.],
        [ 2., -2., -2., -2., -2.,  2.,  2., -2., -2., -2., -2., -2., -2.,  2.,
          2., -2., -2.,  2.,  2., -2., -2., -2., -2.,  2.,  2.,  2.,  2., -2.,
         -2., -2.],
        [-2., -2.,  2., -2., -2.,  2.,  2., -2., -2.,  2., -2.,  2., -2., -2.,
          2., -2.,  2., -2., -2., -2.,  2., -2., -2., -2., -2.,  2., -2.,  2.,
         -2., -2.],
        [ 2., -2., -2., -2.,  2., -2., -2.,  2., -2., -2., -2., -2., -2.,  2.,
          2., -2.,  2.,  2.,  2.,  2., -2., -2.,  2., -2., -2.,  2.,  2., -2.,
         -2.,  2.],
        [ 2., -2.,  2., -2.,  2., -2.,  2.,  2.,  2.,  2., -2.,  2.,  2.,  2.,
         -2., -2., -2., -2.,  2.,  2.,  2., -2.,  2.,  2., -2.,  2.,  2., -2.,
          2.,  2.],
        [ 2.,  2., -2., -2., -2.,  2.,  2.,  2.,  2., -2., -2.,  2.,  2., -2.,
          2.,  2., -2.,  2., -2., -2., -2., -2.,  2.,  2., -2., -2.,  2.,  2.,
         -2.,  2.],
        [ 2.,  2.,  2.,  2., -2.,  2.,  2., -2., -2., -2.,  2., -2., -2.,  2.,
          2.,  2.,  2.,  2.,  2., -2., -2., -2., -2.,  2.,  2., -2., -2., -2.,
         -2.,  2.]])
[Pass] paddle.clip(Tensor([30, 30],"float64"), min=-2.0, max=2.0, )
2025-05-16 05:34:47.190024 test begin: paddle.clip(Tensor([4, 4],"float64"), -1, 1, )
tensor([[ 5.8423e+307, -3.0776e+307,  8.2332e+307, -1.7189e+307],
        [ 1.2695e+307, -4.6598e+307, -8.6751e+307, -8.4440e+307],
        [ 1.7553e+307, -6.1731e+306, -2.1444e+307, -2.3828e+307],
        [-8.7100e+307, -6.7470e+307, -7.2022e+307,  4.8402e+307]],
       device='cuda:0', dtype=torch.float64, requires_grad=True)
-----------
tensor([[ 1., -1.,  1., -1.],
        [ 1., -1., -1., -1.],
        [ 1., -1., -1., -1.],
        [-1., -1., -1.,  1.]], device='cuda:0', dtype=torch.float64,
       grad_fn=<ClampBackward1>)
Tensor(shape=[4, 4], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [[ 1., -1.,  1., -1.],
        [ 1., -1., -1., -1.],
        [ 1., -1., -1., -1.],
        [-1., -1., -1.,  1.]])
[Pass] paddle.clip(Tensor([4, 4],"float64"), -1, 1, )
2025-05-16 05:34:47.344325 test begin: paddle.clip(Tensor([5, 2],"float64"), min=0.0, )
tensor([[-5.3822e+307, -4.9593e+307],
        [ 8.5164e+307, -2.6101e+307],
        [ 3.0325e+307,  2.6155e+307],
        [ 4.2815e+307, -6.2327e+307],
        [-1.6269e+307, -4.4507e+307]], device='cuda:0', dtype=torch.float64,
       requires_grad=True)
-----------
tensor([[ 0.0000e+00,  0.0000e+00],
        [8.5164e+307,  0.0000e+00],
        [3.0325e+307, 2.6155e+307],
        [4.2815e+307,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00]], device='cuda:0', dtype=torch.float64,
       grad_fn=<ClampBackward1>)
Tensor(shape=[5, 2], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [[0.                                      ,
         0.                                      ],
        [340282346638528859811704183484516925440.,
         0.                                      ],
        [340282346638528859811704183484516925440.,
         340282346638528859811704183484516925440.],
        [340282346638528859811704183484516925440.,
         0.                                      ],
        [0.                                      ,
         0.                                      ]])
[accuracy error] paddle.clip(Tensor([5, 2],"float64"), min=0.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4 / 10 (40%)
Max absolute difference: 8.51636968e+307
Max relative difference: 1.
 x: array([[0.000000e+00, 0.000000e+00],
       [3.402823e+38, 0.000000e+00],
       [3.402823e+38, 3.402823e+38],...
 y: array([[0.000000e+000, 0.000000e+000],
       [8.516370e+307, 0.000000e+000],
       [3.032531e+307, 2.615495e+307],...
2025-05-16 05:34:47.498374 test begin: paddle.clip(Tensor([5, 5],"float64"), -1, 1, )
tensor([[ 9.2627e+304, -7.4631e+307, -3.3704e+307,  1.4555e+307, -7.7982e+307],
        [ 1.1473e+307, -7.9338e+307, -4.0688e+307,  6.0282e+307,  7.9207e+307],
        [-2.8915e+307, -8.2492e+307,  1.1626e+307,  1.9739e+307, -5.1785e+307],
        [-2.8252e+307, -8.2504e+307, -5.9453e+307,  7.9219e+307, -4.0319e+307],
        [-1.3798e+306,  7.4138e+307,  2.5725e+306, -1.1649e+307, -2.9600e+307]],
       device='cuda:0', dtype=torch.float64, requires_grad=True)
-----------
tensor([[ 1., -1., -1.,  1., -1.],
        [ 1., -1., -1.,  1.,  1.],
        [-1., -1.,  1.,  1., -1.],
        [-1., -1., -1.,  1., -1.],
        [-1.,  1.,  1., -1., -1.]], device='cuda:0', dtype=torch.float64,
       grad_fn=<ClampBackward1>)
Tensor(shape=[5, 5], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [[ 1., -1., -1.,  1., -1.],
        [ 1., -1., -1.,  1.,  1.],
        [-1., -1.,  1.,  1., -1.],
        [-1., -1., -1.,  1., -1.],
        [-1.,  1.,  1., -1., -1.]])
[Pass] paddle.clip(Tensor([5, 5],"float64"), -1, 1, )
2025-05-16 05:34:47.656412 test begin: paddle.clip(Tensor([5],"float64"), min=0, )
tensor([-1.6757e+307, -1.2148e+307,  2.3211e+307,  7.4977e+307, -6.8031e+307],
       device='cuda:0', dtype=torch.float64, requires_grad=True)
-----------
tensor([ 0.0000e+00,  0.0000e+00, 2.3211e+307, 7.4977e+307,  0.0000e+00],
       device='cuda:0', dtype=torch.float64, grad_fn=<ClampBackward1>)
Tensor(shape=[5], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [0.                                      ,
        0.                                      ,
        340282346638528859811704183484516925440.,
        340282346638528859811704183484516925440.,
        0.                                      ])
[accuracy error] paddle.clip(Tensor([5],"float64"), min=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2 / 5 (40%)
Max absolute difference: 7.49770052e+307
Max relative difference: 1.
 x: array([0.000000e+00, 0.000000e+00, 3.402823e+38, 3.402823e+38,
       0.000000e+00])
 y: array([0.000000e+000, 0.000000e+000, 2.321135e+307, 7.497701e+307,
       0.000000e+000])
2025-05-16 05:34:47.822295 test begin: paddle.clip(Tensor([5],"float64"), min=0.0, )
tensor([-6.5458e+307,  3.4321e+307, -1.3485e+307,  3.3426e+307,  6.1814e+307],
       device='cuda:0', dtype=torch.float64, requires_grad=True)
-----------
tensor([ 0.0000e+00, 3.4321e+307,  0.0000e+00, 3.3426e+307, 6.1814e+307],
       device='cuda:0', dtype=torch.float64, grad_fn=<ClampBackward1>)
Tensor(shape=[5], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [0.                                      ,
        340282346638528859811704183484516925440.,
        0.                                      ,
        340282346638528859811704183484516925440.,
        340282346638528859811704183484516925440.])
[accuracy error] paddle.clip(Tensor([5],"float64"), min=0.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 3 / 5 (60%)
Max absolute difference: 6.18139523e+307
Max relative difference: 1.
 x: array([0.000000e+00, 3.402823e+38, 0.000000e+00, 3.402823e+38,
       3.402823e+38])
 y: array([0.000000e+000, 3.432117e+307, 0.000000e+000, 3.342589e+307,
       6.181395e+307])
2025-05-16 05:34:47.977100 test begin: paddle.clip(Tensor([],"float64"), min=0, )
tensor(-5.7222e+307, device='cuda:0', dtype=torch.float64, requires_grad=True)
-----------
tensor(0., device='cuda:0', dtype=torch.float64, grad_fn=<ClampBackward1>)
Tensor(shape=[], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       0.)
[Pass] paddle.clip(Tensor([],"float64"), min=0, )
2025-05-16 05:34:48.135925 test begin: paddle.clip(x=Tensor([1, 2, 3],"float64"), min=None, max=1, )
tensor([[[-8.6628e+307,  3.9705e+307,  1.1739e+307],
         [ 4.4336e+307,  1.8481e+307,  7.4075e+307]]], device='cuda:0',
       dtype=torch.float64, requires_grad=True)
-----------
tensor([[[-8.6628e+307,   1.0000e+00,   1.0000e+00],
         [  1.0000e+00,   1.0000e+00,   1.0000e+00]]], device='cuda:0',
       dtype=torch.float64, grad_fn=<ClampBackward1>)
Tensor(shape=[1, 2, 3], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [[[-340282346638528859811704183484516925440.,
           1.                                      ,
           1.                                      ],
         [ 1.                                      ,
           1.                                      ,
           1.                                      ]]])
[accuracy error] paddle.clip(x=Tensor([1, 2, 3],"float64"), min=None, max=1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 6 (16.7%)
Max absolute difference: 8.66275274e+307
Max relative difference: 1.
 x: array([[[-3.402823e+38,  1.000000e+00,  1.000000e+00],
        [ 1.000000e+00,  1.000000e+00,  1.000000e+00]]])
 y: array([[[-8.662753e+307,  1.000000e+000,  1.000000e+000],
        [ 1.000000e+000,  1.000000e+000,  1.000000e+000]]])
2025-05-16 05:34:48.300362 test begin: paddle.clip(x=Tensor([1, 2],"float64"), min=Tensor([1],"float64"), max=Tensor([1],"float64"), )
tensor([[-1.8933e+307,  5.9386e+307]], device='cuda:0', dtype=torch.float64,
       requires_grad=True)
-----------
tensor([[4.3406e+307, 5.9386e+307]], device='cuda:0', dtype=torch.float64,
       grad_fn=<ClampBackward1>)
One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
Tensor(shape=[1, 2], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [[43405546009598909979726461337757805711040152868238736568492369685522753583449809180339481433446085100668737139176191181230408987435397576219332146608703400509651313983139806511622688117918346610829088972210455228265270976436524135186598184339004310088028333440519615258830946061431320031340776347802829586432.,
         59385544182542192561044114536873357710113070521049491856901027569790659742588425312727642473937181969421267638371559951520051904120199012477927878164941357431318746973892892705624325291471440076885813903302981333374296645995801943724102404468762266245199052898415449596970274001855227197540938202722416459776.]])
[Pass] paddle.clip(x=Tensor([1, 2],"float64"), min=Tensor([1],"float64"), max=Tensor([1],"float64"), )
2025-05-16 05:34:48.453727 test begin: paddle.clip(x=Tensor([10, 10],"float64"), min=-1.0, max=1.0, )
tensor([[-1.0525e+307, -5.5048e+307,  8.2764e+307,  1.3185e+306,  1.9775e+307,
          2.4814e+307, -5.9386e+307, -4.4558e+307,  6.2888e+307,  2.0004e+307],
        [ 5.0971e+307,  1.7872e+307, -2.4374e+307,  4.7429e+307,  3.6389e+307,
         -6.5072e+307, -6.8587e+307,  2.0679e+305,  1.5708e+307, -1.4350e+307],
        [ 4.4302e+306, -7.2978e+307,  5.3492e+307,  1.5365e+307,  7.2254e+306,
          4.1725e+306, -4.9386e+306, -5.6811e+307,  5.5430e+307, -3.4599e+307],
        [ 8.4435e+307, -6.6510e+306, -6.6465e+307,  9.6666e+305,  3.0008e+307,
          5.2558e+307,  6.8849e+307, -4.8109e+307,  8.1850e+307,  8.1530e+307],
        [ 3.4821e+307,  5.8903e+306,  2.2736e+307, -2.4620e+307,  5.9195e+306,
          4.6153e+307,  1.6375e+307,  3.3710e+307, -4.9232e+305,  5.7432e+306],
        [-4.9374e+307,  5.2308e+307,  2.5807e+307,  3.8233e+307,  6.4692e+307,
         -5.3190e+307,  6.4005e+307,  7.0377e+307,  3.2594e+307,  1.3555e+307],
        [-6.2162e+307, -3.8524e+307,  4.8753e+307,  2.3321e+307, -5.0175e+307,
         -4.0913e+307, -4.9302e+306, -8.5647e+307,  4.6734e+307, -3.8315e+307],
        [ 2.1970e+306, -2.0756e+306,  8.6176e+307,  4.5665e+307,  3.6277e+306,
         -7.5984e+307,  6.6710e+307, -8.6889e+307,  2.1028e+307, -7.7281e+307],
        [-6.1381e+307, -5.8192e+307,  5.3376e+307,  2.3918e+307, -5.5901e+307,
          3.3092e+307, -7.9284e+307,  8.5814e+307,  1.5300e+307, -2.2722e+307],
        [ 5.9756e+307,  2.9031e+307, -6.3708e+307, -5.0314e+307,  7.7606e+307,
          5.7804e+307,  3.3836e+307, -9.2126e+306,  7.5035e+307, -1.2896e+307]],
       device='cuda:0', dtype=torch.float64, requires_grad=True)
-----------
tensor([[-1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1.],
        [ 1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1., -1.],
        [ 1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1., -1.],
        [ 1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1.],
        [ 1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1.],
        [-1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.],
        [-1., -1.,  1.,  1., -1., -1., -1., -1.,  1., -1.],
        [ 1., -1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1.],
        [-1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1., -1.],
        [ 1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1., -1.]], device='cuda:0',
       dtype=torch.float64, grad_fn=<ClampBackward1>)
Tensor(shape=[10, 10], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [[-1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1.,  1.],
        [ 1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1., -1.],
        [ 1., -1.,  1.,  1.,  1.,  1., -1., -1.,  1., -1.],
        [ 1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1.],
        [ 1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1.],
        [-1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.],
        [-1., -1.,  1.,  1., -1., -1., -1., -1.,  1., -1.],
        [ 1., -1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1.],
        [-1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1., -1.],
        [ 1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1., -1.]])
[Pass] paddle.clip(x=Tensor([10, 10],"float64"), min=-1.0, max=1.0, )
2025-05-16 05:34:48.636157 test begin: paddle.clip(x=Tensor([10, 10],"float64"), min=-2.0, max=-1.0, )
tensor([[-3.8576e+307, -2.5847e+306, -6.1725e+307, -4.6849e+307,  2.7216e+307,
         -4.7171e+307, -5.1368e+307,  1.5634e+307,  8.5831e+307,  5.6301e+307],
        [-4.5184e+307,  3.2638e+307,  7.5071e+307,  2.8115e+307, -4.3524e+307,
         -6.2189e+307, -4.0422e+306, -2.3247e+307, -4.2596e+307,  5.3804e+307],
        [ 4.7427e+307,  4.5797e+307,  7.2652e+306, -8.6016e+307, -2.7523e+307,
         -5.2534e+307, -6.0705e+307,  1.0593e+307,  1.9671e+307,  3.8339e+307],
        [ 7.0804e+307,  4.3595e+307, -4.4714e+307,  8.4853e+307, -6.9735e+307,
         -7.7566e+307,  3.6506e+307, -4.6357e+307, -8.8465e+307,  1.2109e+306],
        [-8.0549e+307,  6.4786e+307, -7.6557e+307, -6.8468e+307, -7.8233e+307,
          4.6879e+307, -7.3184e+307,  4.4987e+307,  2.5143e+306,  2.9936e+307],
        [ 6.5156e+307, -6.5603e+306, -4.7806e+307,  6.1685e+307, -7.6885e+307,
         -3.1475e+307,  6.1919e+307,  4.6657e+307, -6.4342e+307,  4.7912e+307],
        [-5.9766e+307, -2.1747e+307, -5.1984e+307,  7.1447e+306,  1.2403e+307,
         -1.8372e+307, -4.8368e+307, -8.8282e+307, -4.2195e+307,  5.2771e+307],
        [-2.5357e+307, -3.2856e+307,  1.6659e+307,  7.7556e+306,  4.1341e+307,
         -8.8303e+307,  8.0760e+307,  1.2915e+307,  4.9037e+307,  4.9027e+307],
        [-6.7608e+306, -8.1719e+307,  6.3953e+307,  7.0706e+307,  6.6649e+307,
          7.7515e+307,  8.6435e+307, -6.8043e+307,  8.7985e+307,  8.2197e+307],
        [-4.3095e+307,  8.7791e+307, -3.9254e+307,  1.5925e+307,  6.5442e+306,
          5.2125e+307, -6.6447e+307, -6.4423e+307,  1.3088e+307, -6.2070e+307]],
       device='cuda:0', dtype=torch.float64, requires_grad=True)
-----------
tensor([[-2., -2., -2., -2., -1., -2., -2., -1., -1., -1.],
        [-2., -1., -1., -1., -2., -2., -2., -2., -2., -1.],
        [-1., -1., -1., -2., -2., -2., -2., -1., -1., -1.],
        [-1., -1., -2., -1., -2., -2., -1., -2., -2., -1.],
        [-2., -1., -2., -2., -2., -1., -2., -1., -1., -1.],
        [-1., -2., -2., -1., -2., -2., -1., -1., -2., -1.],
        [-2., -2., -2., -1., -1., -2., -2., -2., -2., -1.],
        [-2., -2., -1., -1., -1., -2., -1., -1., -1., -1.],
        [-2., -2., -1., -1., -1., -1., -1., -2., -1., -1.],
        [-2., -1., -2., -1., -1., -1., -2., -2., -1., -2.]], device='cuda:0',
       dtype=torch.float64, grad_fn=<ClampBackward1>)
Tensor(shape=[10, 10], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [[-2., -2., -2., -2., -1., -2., -2., -1., -1., -1.],
        [-2., -1., -1., -1., -2., -2., -2., -2., -2., -1.],
        [-1., -1., -1., -2., -2., -2., -2., -1., -1., -1.],
        [-1., -1., -2., -1., -2., -2., -1., -2., -2., -1.],
        [-2., -1., -2., -2., -2., -1., -2., -1., -1., -1.],
        [-1., -2., -2., -1., -2., -2., -1., -1., -2., -1.],
        [-2., -2., -2., -1., -1., -2., -2., -2., -2., -1.],
        [-2., -2., -1., -1., -1., -2., -1., -1., -1., -1.],
        [-2., -2., -1., -1., -1., -1., -1., -2., -1., -1.],
        [-2., -1., -2., -1., -1., -1., -2., -2., -1., -2.]])
[Pass] paddle.clip(x=Tensor([10, 10],"float64"), min=-2.0, max=-1.0, )
2025-05-16 05:34:48.816029 test begin: paddle.clip(x=Tensor([10, 10],"float64"), min=2.0, max=2.0, )
tensor([[ 6.1156e+307, -1.8664e+307,  2.9118e+307,  8.5210e+307,  5.8522e+307,
         -6.3006e+307, -1.4526e+307,  8.2457e+307,  3.8731e+307,  7.7311e+307],
        [ 2.0327e+306,  1.5112e+307,  4.2208e+307,  6.6894e+307, -7.3116e+307,
          2.7281e+307,  6.0615e+307,  8.6236e+307,  2.1555e+307,  2.5694e+307],
        [ 6.5772e+307,  6.6573e+307, -2.1794e+307, -2.2711e+307,  3.9747e+305,
          6.7153e+307,  2.4483e+307,  2.0579e+307, -6.1213e+307, -1.4751e+307],
        [-6.7609e+307, -6.0179e+307,  1.1468e+307, -8.5000e+307,  1.2139e+307,
          6.5934e+307,  4.2564e+307,  5.5875e+307,  1.6805e+307,  5.1149e+307],
        [ 3.1403e+307,  7.5806e+307, -6.2589e+307, -6.3517e+307, -4.5063e+307,
          5.2831e+307,  2.6790e+307,  3.2160e+307, -3.7026e+307, -5.3430e+307],
        [ 4.1940e+307, -5.6283e+307,  9.1960e+306, -5.3740e+307, -2.4728e+307,
         -6.5111e+307, -3.0580e+307, -6.2037e+307, -6.6656e+307, -4.9044e+307],
        [-1.0264e+307, -1.2716e+307, -7.5639e+307,  5.5826e+307, -8.6486e+307,
         -6.5753e+307,  2.8635e+307,  7.4850e+307, -4.5479e+307, -2.6420e+307],
        [-6.1583e+307,  3.8153e+307,  8.8432e+307, -9.7536e+306, -1.5430e+307,
          1.1309e+307, -5.4242e+307, -8.4681e+307,  8.1873e+307,  6.6394e+307],
        [-1.3029e+305, -1.3873e+307, -2.3807e+306,  1.2848e+307,  6.2112e+306,
          8.9728e+307, -5.6626e+307,  4.9201e+307, -2.1379e+307, -8.6208e+307],
        [-3.6465e+307, -8.4158e+307, -3.5533e+307, -6.7407e+306, -4.4782e+306,
          4.3519e+307, -4.1791e+307, -4.9541e+307, -8.1606e+307,  5.2364e+307]],
       device='cuda:0', dtype=torch.float64, requires_grad=True)
-----------
tensor([[2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]], device='cuda:0',
       dtype=torch.float64, grad_fn=<ClampBackward1>)
Tensor(shape=[10, 10], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [[2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],
        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]])
[Pass] paddle.clip(x=Tensor([10, 10],"float64"), min=2.0, max=2.0, )
2025-05-16 05:34:49.000043 test begin: paddle.clip(x=Tensor([2, 2],"float64"), min=Tensor([1],"float64"), max=Tensor([1],"float64"), )
tensor([[-4.4815e+306,  2.3478e+307],
        [ 5.6299e+307,  8.7826e+305]], device='cuda:0', dtype=torch.float64,
       requires_grad=True)
-----------
tensor([[7.0478e+307, 7.0478e+307],
        [7.0478e+307, 7.0478e+307]], device='cuda:0', dtype=torch.float64,
       grad_fn=<ClampBackward1>)
One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
Tensor(shape=[2, 2], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [[70478199918291572449815468155655293258437427493195299813140725139654071600414206794469729360337832018840580030762791795297590422584319165723926628576432447848490495476682006074757055251208661438748356912211670980491517464767730797431008665480419480701191239916323328231731706673025385497061558618031682224128.,
         70478199918291572449815468155655293258437427493195299813140725139654071600414206794469729360337832018840580030762791795297590422584319165723926628576432447848490495476682006074757055251208661438748356912211670980491517464767730797431008665480419480701191239916323328231731706673025385497061558618031682224128.],
        [70478199918291572449815468155655293258437427493195299813140725139654071600414206794469729360337832018840580030762791795297590422584319165723926628576432447848490495476682006074757055251208661438748356912211670980491517464767730797431008665480419480701191239916323328231731706673025385497061558618031682224128.,
         70478199918291572449815468155655293258437427493195299813140725139654071600414206794469729360337832018840580030762791795297590422584319165723926628576432447848490495476682006074757055251208661438748356912211670980491517464767730797431008665480419480701191239916323328231731706673025385497061558618031682224128.]])
[Pass] paddle.clip(x=Tensor([2, 2],"float64"), min=Tensor([1],"float64"), max=Tensor([1],"float64"), )
2025-05-16 05:34:49.156016 test begin: paddle.clip(x=Tensor([2],"float64"), min=Tensor([1],"float64"), max=Tensor([1],"float64"), name="test name", )
tensor([ 7.2397e+306, -1.5141e+307], device='cuda:0', dtype=torch.float64,
       requires_grad=True)
-----------
tensor([ 7.2397e+306, -1.5141e+307], device='cuda:0', dtype=torch.float64,
       grad_fn=<ClampBackward1>)
One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
Tensor(shape=[2], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [ 7239744638183979403114148326355693954835158697776895539552322388310435832309298837053466376657343899015574793847434468862661293784775970283027915130990158347426273353773121587779603740811868200683002151546312861190700720279378381389963274608412185340434485673684690943867712046515450142100396801917691363328. ,
        -15141319490290656171044172169572566450145999865849616363460315274707629119137164257776319925574659842329925254373750883463359303013033949088846108739276830789008616371332564303555790300699882128180495551802177932334891921181218081345646954105726134382150772608111686275454111795054544163930035592167805681664.])
[Pass] paddle.clip(x=Tensor([2],"float64"), min=Tensor([1],"float64"), max=Tensor([1],"float64"), name="test name", )
2025-05-16 05:34:49.308881 test begin: paddle.clip(x=Tensor([3, 3, 3],"float64"), min=None, max=5, )
tensor([[[-3.9483e+307, -3.8294e+307,  4.5883e+307],
         [ 7.6441e+307,  8.8795e+307, -8.2338e+307],
         [-5.2489e+307, -1.5650e+307,  4.6215e+307]],

        [[ 2.0117e+307,  2.4122e+307, -4.7825e+307],
         [-1.8904e+307, -4.2579e+307,  4.4327e+307],
         [-2.1859e+307,  5.4123e+307, -2.6689e+307]],

        [[ 2.1831e+307, -8.9470e+306,  4.7776e+307],
         [ 2.2518e+307,  2.5432e+307,  2.9895e+307],
         [-4.2398e+307, -3.4592e+307, -2.9080e+306]]], device='cuda:0',
       dtype=torch.float64, requires_grad=True)
-----------
tensor([[[-3.9483e+307, -3.8294e+307,   5.0000e+00],
         [  5.0000e+00,   5.0000e+00, -8.2338e+307],
         [-5.2489e+307, -1.5650e+307,   5.0000e+00]],

        [[  5.0000e+00,   5.0000e+00, -4.7825e+307],
         [-1.8904e+307, -4.2579e+307,   5.0000e+00],
         [-2.1859e+307,   5.0000e+00, -2.6689e+307]],

        [[  5.0000e+00, -8.9470e+306,   5.0000e+00],
         [  5.0000e+00,   5.0000e+00,   5.0000e+00],
         [-4.2398e+307, -3.4592e+307, -2.9080e+306]]], device='cuda:0',
       dtype=torch.float64, grad_fn=<ClampBackward1>)
Tensor(shape=[3, 3, 3], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [[[-340282346638528859811704183484516925440.,
          -340282346638528859811704183484516925440.,
           5.                                      ],
         [ 5.                                      ,
           5.                                      ,
          -340282346638528859811704183484516925440.],
         [-340282346638528859811704183484516925440.,
          -340282346638528859811704183484516925440.,
           5.                                      ]],

        [[ 5.                                      ,
           5.                                      ,
          -340282346638528859811704183484516925440.],
         [-340282346638528859811704183484516925440.,
          -340282346638528859811704183484516925440.,
           5.                                      ],
         [-340282346638528859811704183484516925440.,
           5.                                      ,
          -340282346638528859811704183484516925440.]],

        [[ 5.                                      ,
          -340282346638528859811704183484516925440.,
           5.                                      ],
         [ 5.                                      ,
           5.                                      ,
           5.                                      ],
         [-340282346638528859811704183484516925440.,
          -340282346638528859811704183484516925440.,
          -340282346638528859811704183484516925440.]]])
[accuracy error] paddle.clip(x=Tensor([3, 3, 3],"float64"), min=None, max=5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 14 / 27 (51.9%)
Max absolute difference: 8.23378043e+307
Max relative difference: 1.
 x: array([[[-3.402823e+38, -3.402823e+38,  5.000000e+00],
        [ 5.000000e+00,  5.000000e+00, -3.402823e+38],
        [-3.402823e+38, -3.402823e+38,  5.000000e+00]],...
 y: array([[[-3.948332e+307, -3.829428e+307,  5.000000e+000],
        [ 5.000000e+000,  5.000000e+000, -8.233780e+307],
        [-5.248875e+307, -1.564963e+307,  5.000000e+000]],...
2025-05-16 05:34:49.467324 test begin: paddle.clip(x=Tensor([3, 3],"float64"), min=-5.0, max=5.0, )
tensor([[ 1.4282e+307, -2.7120e+307,  8.8118e+307],
        [ 1.1813e+307,  8.3018e+307, -6.0040e+307],
        [-8.6138e+307,  1.3116e+307, -4.3461e+307]], device='cuda:0',
       dtype=torch.float64, requires_grad=True)
-----------
tensor([[ 5., -5.,  5.],
        [ 5.,  5., -5.],
        [-5.,  5., -5.]], device='cuda:0', dtype=torch.float64,
       grad_fn=<ClampBackward1>)
Tensor(shape=[3, 3], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [[ 5., -5.,  5.],
        [ 5.,  5., -5.],
        [-5.,  5., -5.]])
[Pass] paddle.clip(x=Tensor([3, 3],"float64"), min=-5.0, max=5.0, )
2025-05-16 05:34:49.624684 test begin: paddle.clip(x=Tensor([3, 3],"float64"), min=0.0, max=5, )
tensor([[ 4.7276e+307, -6.6194e+307, -3.8555e+307],
        [-6.4080e+307,  4.3746e+307, -6.0619e+307],
        [-1.9965e+307,  4.5974e+307, -6.1906e+307]], device='cuda:0',
       dtype=torch.float64, requires_grad=True)
-----------
tensor([[5., 0., 0.],
        [0., 5., 0.],
        [0., 5., 0.]], device='cuda:0', dtype=torch.float64,
       grad_fn=<ClampBackward1>)
Tensor(shape=[3, 3], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [[5., 0., 0.],
        [0., 5., 0.],
        [0., 5., 0.]])
[Pass] paddle.clip(x=Tensor([3, 3],"float64"), min=0.0, max=5, )
2025-05-16 05:34:49.786396 test begin: paddle.clip(x=Tensor([3, 3],"float64"), min=1, max=Tensor([1],"float64"), )
tensor([[ 5.8476e+307, -1.8197e+307, -7.7879e+307],
        [-8.0782e+307, -3.9894e+307, -1.3080e+307],
        [-5.8433e+307,  4.3602e+307,  8.0672e+307]], device='cuda:0',
       dtype=torch.float64, requires_grad=True)
-----------
tensor([[3.9453e+307,  1.0000e+00,  1.0000e+00],
        [ 1.0000e+00,  1.0000e+00,  1.0000e+00],
        [ 1.0000e+00, 3.9453e+307, 3.9453e+307]], device='cuda:0',
       dtype=torch.float64, grad_fn=<ClampBackward1>)
One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
Tensor(shape=[3, 3], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [[39452610874435953315447434691218989862381145336270046361013785248323764468565734352173190744516567012484073077800668155775456666676262998867105312257952711809381113909983944124226112095594502705750014981738822874124774010887528347570262929077248267441992492632442988927522508434798324643732368244705238974464.,
         1.                                                                                                                                                                                                                                                                                                                   ,
         1.                                                                                                                                                                                                                                                                                                                   ],
        [1.                                                                                                                                                                                                                                                                                                                   ,
         1.                                                                                                                                                                                                                                                                                                                   ,
         1.                                                                                                                                                                                                                                                                                                                   ],
        [1.                                                                                                                                                                                                                                                                                                                   ,
         39452610874435953315447434691218989862381145336270046361013785248323764468565734352173190744516567012484073077800668155775456666676262998867105312257952711809381113909983944124226112095594502705750014981738822874124774010887528347570262929077248267441992492632442988927522508434798324643732368244705238974464.,
         39452610874435953315447434691218989862381145336270046361013785248323764468565734352173190744516567012484073077800668155775456666676262998867105312257952711809381113909983944124226112095594502705750014981738822874124774010887528347570262929077248267441992492632442988927522508434798324643732368244705238974464.]])
[Pass] paddle.clip(x=Tensor([3, 3],"float64"), min=1, max=Tensor([1],"float64"), )
2025-05-16 05:34:49.943005 test begin: paddle.clip(x=Tensor([3, 3],"float64"), min=1.0, max=None, )
tensor([[ 7.3570e+307, -6.5738e+307, -7.5213e+307],
        [-6.7061e+307, -2.8680e+307,  6.2171e+307],
        [ 4.8049e+307, -6.0267e+307,  7.1912e+307]], device='cuda:0',
       dtype=torch.float64, requires_grad=True)
-----------
tensor([[7.3570e+307,  1.0000e+00,  1.0000e+00],
        [ 1.0000e+00,  1.0000e+00, 6.2171e+307],
        [4.8049e+307,  1.0000e+00, 7.1912e+307]], device='cuda:0',
       dtype=torch.float64, grad_fn=<ClampBackward1>)
Tensor(shape=[3, 3], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [[340282346638528859811704183484516925440.,
         1.                                      ,
         1.                                      ],
        [1.                                      ,
         1.                                      ,
         340282346638528859811704183484516925440.],
        [340282346638528859811704183484516925440.,
         1.                                      ,
         340282346638528859811704183484516925440.]])
[accuracy error] paddle.clip(x=Tensor([3, 3],"float64"), min=1.0, max=None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4 / 9 (44.4%)
Max absolute difference: 7.35696134e+307
Max relative difference: 1.
 x: array([[3.402823e+38, 1.000000e+00, 1.000000e+00],
       [1.000000e+00, 1.000000e+00, 3.402823e+38],
       [3.402823e+38, 1.000000e+00, 3.402823e+38]])
 y: array([[7.356961e+307, 1.000000e+000, 1.000000e+000],
       [1.000000e+000, 1.000000e+000, 6.217103e+307],
       [4.804883e+307, 1.000000e+000, 7.191218e+307]])
2025-05-16 05:34:50.105435 test begin: paddle.clip(x=Tensor([3, 3],"float64"), min=5, max=None, )
tensor([[-5.0331e+307,  3.0350e+306, -3.3617e+307],
        [ 6.5773e+307, -6.6189e+307,  7.4571e+307],
        [ 2.0675e+307, -8.0405e+307, -3.7775e+307]], device='cuda:0',
       dtype=torch.float64, requires_grad=True)
-----------
tensor([[ 5.0000e+00, 3.0350e+306,  5.0000e+00],
        [6.5773e+307,  5.0000e+00, 7.4571e+307],
        [2.0675e+307,  5.0000e+00,  5.0000e+00]], device='cuda:0',
       dtype=torch.float64, grad_fn=<ClampBackward1>)
Tensor(shape=[3, 3], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [[5.                                      ,
         340282346638528859811704183484516925440.,
         5.                                      ],
        [340282346638528859811704183484516925440.,
         5.                                      ,
         340282346638528859811704183484516925440.],
        [340282346638528859811704183484516925440.,
         5.                                      ,
         5.                                      ]])
[accuracy error] paddle.clip(x=Tensor([3, 3],"float64"), min=5, max=None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4 / 9 (44.4%)
Max absolute difference: 7.4570813e+307
Max relative difference: 1.
 x: array([[5.000000e+00, 3.402823e+38, 5.000000e+00],
       [3.402823e+38, 5.000000e+00, 3.402823e+38],
       [3.402823e+38, 5.000000e+00, 5.000000e+00]])
 y: array([[5.000000e+000, 3.034995e+306, 5.000000e+000],
       [6.577341e+307, 5.000000e+000, 7.457081e+307],
       [2.067537e+307, 5.000000e+000, 5.000000e+000]])
2025-05-16 05:34:50.259614 test begin: paddle.clip(x=Tensor([3, 3],"float64"), min=Tensor([1],"float64"), max=None, )
tensor([[-6.8395e+307, -7.7072e+307,  2.1616e+306],
        [ 2.0370e+307,  8.9294e+307, -5.9332e+307],
        [ 4.2844e+306, -1.5328e+307,  4.4410e+307]], device='cuda:0',
       dtype=torch.float64, requires_grad=True)
-----------
tensor([[ 3.7005e-01,  3.7005e-01, 2.1616e+306],
        [2.0370e+307, 8.9294e+307,  3.7005e-01],
        [4.2844e+306,  3.7005e-01, 4.4410e+307]], device='cuda:0',
       dtype=torch.float64, grad_fn=<ClampBackward1>)
One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
Tensor(shape=[3, 3], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [[0.37005007                              ,
         0.37005007                              ,
         340282346638528859811704183484516925440.],
        [340282346638528859811704183484516925440.,
         340282346638528859811704183484516925440.,
         0.37005007                              ],
        [340282346638528859811704183484516925440.,
         0.37005007                              ,
         340282346638528859811704183484516925440.]])
[accuracy error] paddle.clip(x=Tensor([3, 3],"float64"), min=Tensor([1],"float64"), max=None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 9 (55.6%)
Max absolute difference: 8.92937203e+307
Max relative difference: 1.
 x: array([[3.700501e-01, 3.700501e-01, 3.402823e+38],
       [3.402823e+38, 3.402823e+38, 3.700501e-01],
       [3.402823e+38, 3.700501e-01, 3.402823e+38]])
 y: array([[3.700501e-001, 3.700501e-001, 2.161637e+306],
       [2.037049e+307, 8.929372e+307, 3.700501e-001],
       [4.284413e+306, 3.700501e-001, 4.441009e+307]])
2025-05-16 05:34:50.419436 test begin: paddle.clip(x=Tensor([3, 3],"float64"), min=Tensor([1],"float64"), max=Tensor([1],"float64"), )
tensor([[ 7.3336e+307,  1.2347e+307,  5.0779e+307],
        [ 7.8168e+304, -2.2209e+306,  1.7776e+307],
        [-3.3875e+307,  4.6279e+307,  2.6713e+307]], device='cuda:0',
       dtype=torch.float64, requires_grad=True)
-----------
tensor([[6.6586e+307, 4.2659e+307, 5.0779e+307],
        [4.2659e+307, 4.2659e+307, 4.2659e+307],
        [4.2659e+307, 4.6279e+307, 4.2659e+307]], device='cuda:0',
       dtype=torch.float64, grad_fn=<ClampBackward1>)
One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
Tensor(shape=[3, 3], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [[66585895099943888718808066882465856770603487312765036595647879497914153769245724333826627420524071131245449298850291652397870281214872272300804758704065695664472171501109413485052281089025781520633545717090342171844529265195540424823600370062369538670876793766074587211074609380532410702147629046508340379648.,
         42658629726330323658960199555045031670918397768018241255910717133977142592595729309525945421162720464369001045413004135959273747341387149841743867651552012310237892745162280352599861795785350270462529370400924559797073771706062590830187515683489162096976544149852756077990162794991382396805016324876168855552.,
         50779120936832786952587295166521440407172988115934494363191622882006460335860711931239651424150716038854450245072170438240838227953376257108382420143087388049921325896988710627527785037920287512082837645333909023257706823018545859704404789502014078391717919412740037679774674257584987314666786041344716439552.],
        [42658629726330323658960199555045031670918397768018241255910717133977142592595729309525945421162720464369001045413004135959273747341387149841743867651552012310237892745162280352599861795785350270462529370400924559797073771706062590830187515683489162096976544149852756077990162794991382396805016324876168855552.,
         42658629726330323658960199555045031670918397768018241255910717133977142592595729309525945421162720464369001045413004135959273747341387149841743867651552012310237892745162280352599861795785350270462529370400924559797073771706062590830187515683489162096976544149852756077990162794991382396805016324876168855552.,
         42658629726330323658960199555045031670918397768018241255910717133977142592595729309525945421162720464369001045413004135959273747341387149841743867651552012310237892745162280352599861795785350270462529370400924559797073771706062590830187515683489162096976544149852756077990162794991382396805016324876168855552.],
        [42658629726330323658960199555045031670918397768018241255910717133977142592595729309525945421162720464369001045413004135959273747341387149841743867651552012310237892745162280352599861795785350270462529370400924559797073771706062590830187515683489162096976544149852756077990162794991382396805016324876168855552.,
         46278501740189438614774249824864641979881111289989158056223339326704072965659853999810485983830587203478897692963909870004627456975805547929612978620843159163653008586569077956452574626820572653660973906536305506910228358369031066687303435882940813159759179939649562842717836141965103437776502208498742853632.,
         42658629726330323658960199555045031670918397768018241255910717133977142592595729309525945421162720464369001045413004135959273747341387149841743867651552012310237892745162280352599861795785350270462529370400924559797073771706062590830187515683489162096976544149852756077990162794991382396805016324876168855552.]])
[Pass] paddle.clip(x=Tensor([3, 3],"float64"), min=Tensor([1],"float64"), max=Tensor([1],"float64"), )
2025-05-16 05:34:50.605387 test begin: paddle.clip(x=Tensor([3],"float64"), )
tensor([3.3242e+307, 2.6758e+306, 7.8233e+307], device='cuda:0',
       dtype=torch.float64, requires_grad=True)
-----------
tensor([3.3242e+307, 2.6758e+306, 7.8233e+307], device='cuda:0',
       dtype=torch.float64, grad_fn=<ClampBackward1>)
Tensor(shape=[3], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [340282346638528859811704183484516925440.,
        340282346638528859811704183484516925440.,
        340282346638528859811704183484516925440.])
[accuracy error] paddle.clip(x=Tensor([3],"float64"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 3 / 3 (100%)
Max absolute difference: 7.82328567e+307
Max relative difference: 1.
 x: array([3.402823e+38, 3.402823e+38, 3.402823e+38])
 y: array([3.324240e+307, 2.675765e+306, 7.823286e+307])
2025-05-16 05:34:50.768361 test begin: paddle.clip(x=Tensor([3],"float64"), min=2.0, max=2.0, )
tensor([-1.7838e+307,  3.3548e+307, -5.1399e+307], device='cuda:0',
       dtype=torch.float64, requires_grad=True)
-----------
tensor([2., 2., 2.], device='cuda:0', dtype=torch.float64,
       grad_fn=<ClampBackward1>)
Tensor(shape=[3], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [2., 2., 2.])
[Pass] paddle.clip(x=Tensor([3],"float64"), min=2.0, max=2.0, )
2025-05-16 05:34:50.924852 test begin: paddle.clip(x=Tensor([3],"float64"), min=None, max=-1, )
tensor([-7.2062e+307,  6.7689e+307, -1.8595e+307], device='cuda:0',
       dtype=torch.float64, requires_grad=True)
-----------
tensor([-7.2062e+307,  -1.0000e+00, -1.8595e+307], device='cuda:0',
       dtype=torch.float64, grad_fn=<ClampBackward1>)
Tensor(shape=[3], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [-340282346638528859811704183484516925440.,
        -1.                                      ,
        -340282346638528859811704183484516925440.])
[accuracy error] paddle.clip(x=Tensor([3],"float64"), min=None, max=-1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2 / 3 (66.7%)
Max absolute difference: 7.20621716e+307
Max relative difference: 1.
 x: array([-3.402823e+38, -1.000000e+00, -3.402823e+38])
 y: array([-7.206217e+307, -1.000000e+000, -1.859501e+307])
2025-05-16 05:34:51.078325 test begin: paddle.clip(x=Tensor([3],"float64"), min=Tensor([1],"float64"), max=Tensor([1],"float64"), )
tensor([-7.6431e+307, -3.9687e+307, -5.6358e+307], device='cuda:0',
       dtype=torch.float64, requires_grad=True)
-----------
tensor([7.0045e+307, 7.0045e+307, 7.0045e+307], device='cuda:0',
       dtype=torch.float64, grad_fn=<ClampBackward1>)
One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
Tensor(shape=[3], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [70044563753623256321442227373911842738067438157768414779632883453882774300561912718922544425462859914071746026620281935531441360246594128709060466309262242387118390469263982082976881582143557582510401468914067378303348730395431420127594815073354440470150827060678033393372522978474672591863039669250264399872.,
        70044563753623256321442227373911842738067438157768414779632883453882774300561912718922544425462859914071746026620281935531441360246594128709060466309262242387118390469263982082976881582143557582510401468914067378303348730395431420127594815073354440470150827060678033393372522978474672591863039669250264399872.,
        70044563753623256321442227373911842738067438157768414779632883453882774300561912718922544425462859914071746026620281935531441360246594128709060466309262242387118390469263982082976881582143557582510401468914067378303348730395431420127594815073354440470150827060678033393372522978474672591863039669250264399872.])
[Pass] paddle.clip(x=Tensor([3],"float64"), min=Tensor([1],"float64"), max=Tensor([1],"float64"), )
2025-05-16 05:34:51.240004 test begin: paddle.clip(x=Tensor([4, 10, 10],"float64"), min=-1.0, max=1.0, )
tensor([[[-3.2102e+307, -2.6230e+307, -3.5431e+307,  7.0370e+307, -7.5462e+307,
           5.0510e+307,  1.0268e+307, -1.8462e+307,  7.2421e+307,  2.7086e+307],
         [-8.8589e+307, -7.2140e+307, -4.8610e+307, -4.2488e+307, -1.1497e+307,
          -2.6747e+307,  5.8662e+307, -7.0081e+307, -7.9643e+307, -2.9613e+307],
         [ 8.1215e+307, -2.4952e+307, -7.6750e+307,  4.5001e+307,  7.6418e+307,
          -7.3132e+307, -7.5449e+307,  2.5922e+307, -1.0708e+307,  4.2351e+307],
         [ 4.6997e+307,  2.8871e+307,  1.7160e+307, -5.7979e+307, -3.1870e+307,
          -2.9451e+307,  2.0777e+307, -8.9429e+307, -7.3751e+307,  2.5680e+307],
         [ 8.5671e+306, -8.7611e+307, -8.5460e+307, -6.8855e+307, -6.5841e+307,
           4.5811e+307,  7.3053e+307, -7.6139e+307, -1.1202e+307,  4.8922e+307],
         [ 2.2499e+307,  1.8665e+306,  1.8359e+306, -7.8063e+307,  6.2037e+307,
           6.1249e+307, -7.7442e+306,  2.5692e+307,  5.2813e+307, -2.7915e+307],
         [ 7.6682e+307, -7.6505e+307,  2.8234e+307, -3.5628e+306,  3.7879e+307,
          -4.0814e+307, -6.6889e+307,  4.7692e+307, -7.2512e+306, -2.7335e+307],
         [ 1.8362e+307, -7.4674e+307,  4.6678e+307,  6.4800e+307, -5.1528e+307,
          -6.1674e+307, -8.7769e+307, -2.7931e+307, -4.4950e+307,  8.1784e+307],
         [ 5.7925e+307,  6.4662e+307, -5.2860e+307, -3.7209e+307,  2.7320e+307,
          -8.0568e+307, -5.4768e+307,  4.5482e+307,  1.0883e+307, -3.9717e+307],
         [-8.4053e+307,  6.5793e+307,  6.6659e+307, -5.2896e+307, -8.4956e+307,
           7.7716e+307,  3.1908e+307,  4.8541e+306,  7.2313e+307,  6.8616e+307]],

        [[-4.8724e+307,  2.4198e+307, -1.4433e+307,  2.5241e+307,  6.7368e+306,
           4.1044e+307,  8.7325e+307,  5.2533e+307, -2.8956e+306, -7.2110e+307],
         [ 2.7276e+307, -2.6983e+307,  4.8895e+307,  4.6522e+307,  6.6558e+307,
          -7.1765e+307,  1.6262e+307, -7.9773e+307,  7.3743e+307,  5.5935e+307],
         [-2.1631e+307, -6.6112e+307,  5.3780e+307,  1.8476e+307, -6.0189e+307,
           1.5648e+307,  1.4540e+307, -5.6105e+306, -4.2319e+307,  7.2231e+307],
         [ 7.1314e+307, -4.3675e+307,  5.9148e+307, -2.8132e+307, -4.1158e+307,
           8.9595e+307,  4.4889e+307, -8.7894e+307, -8.8840e+307,  6.7688e+307],
         [-7.2453e+307, -8.5579e+307, -6.9415e+307,  6.3528e+307, -3.9686e+307,
          -5.0573e+307, -3.8256e+307,  6.7889e+307, -7.1312e+307, -4.4876e+307],
         [ 2.3480e+307, -5.9323e+307,  5.5355e+307,  5.0496e+307,  1.5988e+307,
           5.7074e+307,  8.0107e+306,  5.3013e+307, -7.0680e+307,  3.7610e+307],
         [-5.2544e+307,  2.8423e+307,  6.5578e+307, -7.5609e+307, -8.6436e+307,
           8.4242e+306,  5.0989e+307, -1.6101e+307,  8.0572e+307, -8.3215e+307],
         [-1.2718e+307,  6.0276e+307, -8.4307e+307,  7.1707e+307,  3.0640e+307,
          -7.0678e+307,  5.8511e+307,  7.0083e+306,  6.5739e+307,  3.4203e+307],
         [ 5.8044e+307, -1.7962e+307,  5.6253e+307,  3.8429e+307,  1.2171e+307,
          -2.0232e+307, -3.5843e+307,  8.6628e+307,  3.0620e+307, -2.7336e+307],
         [ 1.9624e+307, -8.5506e+307,  3.1709e+305, -2.9862e+306,  4.4810e+307,
           2.3880e+307,  4.7981e+307, -5.3694e+307,  4.9189e+307,  7.1001e+307]],

        [[-2.2200e+306,  5.4837e+307, -4.2675e+307,  2.2267e+307,  7.4062e+307,
          -9.0875e+306,  7.9336e+307,  2.5777e+307,  8.2885e+306, -8.8346e+307],
         [-7.8696e+307,  7.0413e+307,  2.0099e+307, -3.7266e+307, -6.7091e+307,
          -2.2432e+307, -2.4115e+307, -2.2332e+306,  8.1010e+307,  4.2229e+307],
         [-5.7477e+306, -4.1218e+307,  3.6502e+306, -8.9115e+307,  8.1952e+307,
           2.9277e+307,  6.4277e+307,  7.5635e+307,  1.5274e+307,  5.6098e+307],
         [ 6.3478e+307,  7.7115e+307, -7.0877e+307, -6.6577e+307,  8.9043e+307,
          -3.2211e+307,  1.0196e+307,  4.8601e+306, -2.8611e+307, -6.7234e+307],
         [ 7.5892e+307,  3.7341e+307, -8.2145e+307,  1.8740e+307, -9.5689e+306,
           6.5218e+307,  6.3551e+307,  2.1547e+307, -2.0905e+307,  7.2936e+307],
         [ 8.6776e+307,  4.6369e+307,  8.0861e+307,  8.1559e+307, -1.7157e+307,
          -2.9760e+307,  9.0240e+306,  3.2107e+306,  2.6226e+307, -5.7735e+307],
         [ 4.8774e+307, -8.3121e+307,  4.6303e+306, -1.6564e+307,  3.2247e+307,
           6.6687e+307, -8.0107e+307,  6.8975e+307,  2.6178e+307, -1.2722e+307],
         [ 7.5145e+307,  2.2024e+306,  3.1871e+307,  6.1940e+307, -3.5812e+307,
          -3.2940e+307,  6.4739e+307, -2.8393e+307, -5.7485e+307, -2.7576e+307],
         [-8.3190e+307,  4.2002e+307,  8.4872e+307, -3.5301e+307, -1.0480e+307,
           5.6592e+306, -5.7449e+307,  1.7087e+306, -8.4487e+307, -2.9826e+307],
         [-1.0581e+307,  4.0630e+307,  5.7124e+306, -7.4374e+307,  1.3071e+307,
          -2.7298e+306, -1.8318e+307,  7.1288e+306, -6.7755e+307,  5.7173e+307]],

        [[-3.5780e+307, -6.7976e+307,  6.5229e+307,  2.9639e+307,  5.9038e+307,
           8.5367e+307, -6.7297e+307,  4.0286e+307,  3.6583e+307, -8.1290e+307],
         [ 1.5677e+307, -8.3290e+307, -1.8938e+306, -1.2328e+306,  8.5691e+307,
          -5.3184e+307,  4.5765e+307,  1.7335e+306, -7.6700e+307,  1.2523e+307],
         [-4.6458e+307, -3.0063e+307, -1.0826e+307, -1.7917e+306, -5.7413e+307,
           7.8583e+307, -8.3373e+306,  2.5343e+307,  4.2333e+307,  5.4743e+307],
         [-1.9556e+307, -7.8912e+307, -3.8149e+307, -5.3514e+307, -2.9207e+307,
           9.4808e+306,  2.8088e+307,  7.6383e+307,  7.3657e+307, -7.1687e+307],
         [ 4.1814e+307, -2.4808e+307,  7.7555e+307, -7.8370e+307, -6.2941e+307,
          -3.8806e+307, -1.0901e+307, -5.1119e+307, -2.0250e+307, -6.2864e+307],
         [ 8.8072e+307, -8.1967e+307,  2.5093e+306, -1.2484e+307, -4.4977e+307,
          -7.7813e+307,  5.2396e+307, -3.4143e+307, -7.8184e+307, -3.6319e+307],
         [-4.8539e+307, -5.3091e+307,  7.3040e+307,  2.8693e+307,  7.6312e+307,
           1.5948e+306, -8.2176e+307,  2.0975e+307, -5.3023e+307, -2.1650e+307],
         [ 1.8754e+306,  7.7543e+307, -8.7309e+307,  4.5623e+307, -5.8315e+307,
          -5.9100e+307, -3.6342e+307, -4.2034e+307,  3.8842e+307,  5.3240e+307],
         [ 4.1722e+307, -1.1553e+307, -1.8581e+307, -5.5838e+307, -6.5052e+307,
          -3.9491e+307,  4.8001e+307, -1.4186e+307,  6.3690e+307, -8.7339e+307],
         [-3.5679e+307, -4.2484e+307, -1.0028e+307,  5.7539e+307,  2.0207e+307,
          -3.3278e+307, -6.1154e+307,  2.5818e+307,  6.2813e+307,  3.3789e+307]]],
       device='cuda:0', dtype=torch.float64, requires_grad=True)
-----------
tensor([[[-1., -1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1.],
         [-1., -1., -1., -1., -1., -1.,  1., -1., -1., -1.],
         [ 1., -1., -1.,  1.,  1., -1., -1.,  1., -1.,  1.],
         [ 1.,  1.,  1., -1., -1., -1.,  1., -1., -1.,  1.],
         [ 1., -1., -1., -1., -1.,  1.,  1., -1., -1.,  1.],
         [ 1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1.],
         [ 1., -1.,  1., -1.,  1., -1., -1.,  1., -1., -1.],
         [ 1., -1.,  1.,  1., -1., -1., -1., -1., -1.,  1.],
         [ 1.,  1., -1., -1.,  1., -1., -1.,  1.,  1., -1.],
         [-1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1.]],

        [[-1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1.],
         [ 1., -1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.],
         [-1., -1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1.],
         [ 1., -1.,  1., -1., -1.,  1.,  1., -1., -1.,  1.],
         [-1., -1., -1.,  1., -1., -1., -1.,  1., -1., -1.],
         [ 1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.],
         [-1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1., -1.],
         [-1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1.],
         [ 1., -1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1.],
         [ 1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1.]],

        [[-1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1., -1.],
         [-1.,  1.,  1., -1., -1., -1., -1., -1.,  1.,  1.],
         [-1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.],
         [ 1.,  1., -1., -1.,  1., -1.,  1.,  1., -1., -1.],
         [ 1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1.],
         [ 1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1.],
         [ 1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1.],
         [ 1.,  1.,  1.,  1., -1., -1.,  1., -1., -1., -1.],
         [-1.,  1.,  1., -1., -1.,  1., -1.,  1., -1., -1.],
         [-1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.]],

        [[-1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1.],
         [ 1., -1., -1., -1.,  1., -1.,  1.,  1., -1.,  1.],
         [-1., -1., -1., -1., -1.,  1., -1.,  1.,  1.,  1.],
         [-1., -1., -1., -1., -1.,  1.,  1.,  1.,  1., -1.],
         [ 1., -1.,  1., -1., -1., -1., -1., -1., -1., -1.],
         [ 1., -1.,  1., -1., -1., -1.,  1., -1., -1., -1.],
         [-1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1., -1.],
         [ 1.,  1., -1.,  1., -1., -1., -1., -1.,  1.,  1.],
         [ 1., -1., -1., -1., -1., -1.,  1., -1.,  1., -1.],
         [-1., -1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1.]]], device='cuda:0',
       dtype=torch.float64, grad_fn=<ClampBackward1>)
Tensor(shape=[4, 10, 10], dtype=float64, place=Place(gpu:0), stop_gradient=False,
       [[[-1., -1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1.],
         [-1., -1., -1., -1., -1., -1.,  1., -1., -1., -1.],
         [ 1., -1., -1.,  1.,  1., -1., -1.,  1., -1.,  1.],
         [ 1.,  1.,  1., -1., -1., -1.,  1., -1., -1.,  1.],
         [ 1., -1., -1., -1., -1.,  1.,  1., -1., -1.,  1.],
         [ 1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1.],
         [ 1., -1.,  1., -1.,  1., -1., -1.,  1., -1., -1.],
         [ 1., -1.,  1.,  1., -1., -1., -1., -1., -1.,  1.],
         [ 1.,  1., -1., -1.,  1., -1., -1.,  1.,  1., -1.],
         [-1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1.]],

        [[-1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1.],
         [ 1., -1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.],
         [-1., -1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1.],
         [ 1., -1.,  1., -1., -1.,  1.,  1., -1., -1.,  1.],
         [-1., -1., -1.,  1., -1., -1., -1.,  1., -1., -1.],
         [ 1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.],
         [-1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1., -1.],
         [-1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1.],
         [ 1., -1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1.],
         [ 1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1.]],

        [[-1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1., -1.],
         [-1.,  1.,  1., -1., -1., -1., -1., -1.,  1.,  1.],
         [-1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.],
         [ 1.,  1., -1., -1.,  1., -1.,  1.,  1., -1., -1.],
         [ 1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1.],
         [ 1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1.],
         [ 1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1.],
         [ 1.,  1.,  1.,  1., -1., -1.,  1., -1., -1., -1.],
         [-1.,  1.,  1., -1., -1.,  1., -1.,  1., -1., -1.],
         [-1.,  1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.]],

        [[-1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1.],
         [ 1., -1., -1., -1.,  1., -1.,  1.,  1., -1.,  1.],
         [-1., -1., -1., -1., -1.,  1., -1.,  1.,  1.,  1.],
         [-1., -1., -1., -1., -1.,  1.,  1.,  1.,  1., -1.],
         [ 1., -1.,  1., -1., -1., -1., -1., -1., -1., -1.],
         [ 1., -1.,  1., -1., -1., -1.,  1., -1., -1., -1.],
         [-1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1., -1.],
         [ 1.,  1., -1.,  1., -1., -1., -1., -1.,  1.,  1.],
         [ 1., -1., -1., -1., -1., -1.,  1., -1.,  1., -1.],
         [-1., -1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1.]]])
[Pass] paddle.clip(x=Tensor([4, 10, 10],"float64"), min=-1.0, max=1.0, )
