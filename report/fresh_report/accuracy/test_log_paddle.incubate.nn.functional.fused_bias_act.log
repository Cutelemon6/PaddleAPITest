paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009313154732808471, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009654839523136616, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009928022045642138, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0010831302497535944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0010981468949466944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0014022786635905504, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.001479289960116148, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0016999575309455395, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009313154732808471, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009654839523136616, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009928022045642138, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0010831302497535944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0010981468949466944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0014022786635905504, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.001479289960116148, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0016999575309455395, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 10],"int32"), bias=Tensor([10],"float16"), dequant_scales=Tensor([10],"float32"), act_method="gelu", compute_dtype="fp16", )
paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"int32"), bias=Tensor([512],"float16"), dequant_scales=Tensor([512],"float32"), shift=Tensor([256],"float16"), smooth=Tensor([256],"float16"), act_method="geglu", compute_dtype="fp16", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"int32"), bias=Tensor([512],"float16"), dequant_scales=Tensor([512],"float32"), shift=Tensor([512],"float16"), smooth=Tensor([512],"float16"), act_method="gelu", compute_dtype="fp16", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )
paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"int32"), bias=Tensor([512],"float32"), dequant_scales=Tensor([512],"float32"), shift=Tensor([512],"float32"), smooth=Tensor([512],"float32"), act_method="gelu", compute_dtype="fp32", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )

2025-05-14 19:52:27.802952 GPU 0 52777 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009313154732808471, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009313154732808471, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 40 / 22016 (0.182%)
Max absolute difference among violations: 119
Max relative difference among violations: 1.
 ACTUAL: array([[   0,    0,    0, ...,    0,    0, -127],
       [ 127,  127,    0, ...,  127,  127,  127]],
      shape=(2, 11008), dtype=int8)
 DESIRED: array([[   0,    0,    0, ...,    0,    0, -127],
       [ 127,  127,    0, ...,  127,  127,  127]],
      shape=(2, 11008), dtype=int32)
2025-05-14 19:52:27.916416 GPU 0 52777 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009654839523136616, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009654839523136616, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 35 / 22016 (0.159%)
Max absolute difference among violations: 119
Max relative difference among violations: 1.
 ACTUAL: array([[   0, -127,  127, ...,  127,  127, -127],
       [-127, -127,    0, ...,    0, -127,    0]],
      shape=(2, 11008), dtype=int8)
 DESIRED: array([[   0, -127,  127, ...,  127,  127, -127],
       [-127, -127,    0, ...,    0, -127,    0]],
      shape=(2, 11008), dtype=int32)
2025-05-14 19:52:28.025232 GPU 0 52777 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009928022045642138, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009928022045642138, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 24 / 22016 (0.109%)
Max absolute difference among violations: 117
Max relative difference among violations: 1.
 ACTUAL: array([[   0,    0,    0, ...,  127, -127,  127],
       [ 127,    0,    0, ..., -127,  127,    0]],
      shape=(2, 11008), dtype=int8)
 DESIRED: array([[   0,    0,    0, ...,  127, -127,  127],
       [ 127,    0,    0, ..., -127,  127,    0]],
      shape=(2, 11008), dtype=int32)
2025-05-14 19:52:28.120231 GPU 0 52777 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0010831302497535944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0010831302497535944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 25 / 22016 (0.114%)
Max absolute difference among violations: 116
Max relative difference among violations: 1.
 ACTUAL: array([[ 127,  127,    0, ...,  127,    0,  127],
       [   0,    0,    0, ...,  127, -127,    0]],
      shape=(2, 11008), dtype=int8)
 DESIRED: array([[ 127,  127,    0, ...,  127,    0,  127],
       [   0,    0,    0, ...,  127, -127,    0]],
      shape=(2, 11008), dtype=int32)
2025-05-14 19:52:28.216126 GPU 0 52777 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0010981468949466944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0010981468949466944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 31 / 22016 (0.141%)
Max absolute difference among violations: 113
Max relative difference among violations: 1.
 ACTUAL: array([[   0,    0, -127, ...,    0,    0, -127],
       [   0,  127,  127, ...,    0,  127,    0]],
      shape=(2, 11008), dtype=int8)
 DESIRED: array([[   0,    0, -127, ...,    0,    0, -127],
       [   0,  127,  127, ...,    0,  127,    0]],
      shape=(2, 11008), dtype=int32)
2025-05-14 19:52:28.307245 GPU 0 52777 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0014022786635905504, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0014022786635905504, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 36 / 22016 (0.164%)
Max absolute difference among violations: 110
Max relative difference among violations: 1.
 ACTUAL: array([[-127,    0,    0, ...,    0,    0,    0],
       [   0,  127,    0, ...,    0,    0,  127]],
      shape=(2, 11008), dtype=int8)
 DESIRED: array([[-127,    0,    0, ...,    0,    0,    0],
       [   0,  127,    0, ...,    0,    0,  127]],
      shape=(2, 11008), dtype=int32)
2025-05-14 19:52:28.377459 GPU 0 52777 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.001479289960116148, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.001479289960116148, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 28 / 22016 (0.127%)
Max absolute difference among violations: 105
Max relative difference among violations: 1.
 ACTUAL: array([[-127,    0, -127, ...,    0,    0,    0],
       [ 127,    0,  127, ..., -127,  127,    0]],
      shape=(2, 11008), dtype=int8)
 DESIRED: array([[-127,    0, -127, ...,    0,    0,    0],
       [ 127,    0,  127, ..., -127,  127,    0]],
      shape=(2, 11008), dtype=int32)
2025-05-14 19:52:28.446424 GPU 0 52777 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0016999575309455395, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(Tensor([2, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0016999575309455395, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 39 / 22016 (0.177%)
Max absolute difference among violations: 99
Max relative difference among violations: 1.
 ACTUAL: array([[   0,    0,  127, ...,    0,    0,  127],
       [   0, -127,    0, ...,  127,    0,  127]],
      shape=(2, 11008), dtype=int8)
 DESIRED: array([[   0,    0,  127, ...,    0,    0,  127],
       [   0, -127,    0, ...,  127,    0,  127]],
      shape=(2, 11008), dtype=int32)
2025-05-14 19:52:29.514033 GPU 0 52777 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009313154732808471, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009313154732808471, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 8604 / 5107712 (0.168%)
Max absolute difference among violations: 125
Max relative difference among violations: 1.
 ACTUAL: array([[   0, -127,    0, ...,    0, -127,  127],
       [   0, -127, -127, ...,  127, -127,    0],
       [-127,  127,    0, ..., -127,    0, -127],...
 DESIRED: array([[   0, -127,    0, ...,    0, -127,  127],
       [   0, -127, -127, ...,  127, -127,    0],
       [-127,  127,    0, ..., -127,    0, -127],...
2025-05-14 19:52:29.936553 GPU 0 52777 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009654839523136616, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009654839523136616, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 7722 / 5107712 (0.151%)
Max absolute difference among violations: 127
Max relative difference among violations: 1.
 ACTUAL: array([[   0,  127,    0, ..., -127,    0, -127],
       [   0,    0,    0, ...,    0,  127,  127],
       [-127,    0,    0, ...,  127,    0,    0],...
 DESIRED: array([[   0,  127,    0, ..., -127,    0, -127],
       [   0,    0,    0, ...,    0,  127,  127],
       [-127,    0,    0, ...,  127,    0,    0],...
2025-05-14 19:52:30.331127 GPU 0 52777 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009928022045642138, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0009928022045642138, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 6754 / 5107712 (0.132%)
Max absolute difference among violations: 126
Max relative difference among violations: 1.
 ACTUAL: array([[-127,    0,    0, ...,    0, -127,    0],
       [   0,  127, -127, ...,    0,    0, -127],
       [   0,    0,  127, ...,    0, -127,    0],...
 DESIRED: array([[-127,    0,    0, ...,    0, -127,    0],
       [   0,  127, -127, ...,    0,    0, -127],
       [   0,    0,  127, ...,    0, -127,    0],...
2025-05-14 19:52:30.757425 GPU 0 52777 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0010831302497535944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0010831302497535944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 6115 / 5107712 (0.12%)
Max absolute difference among violations: 125
Max relative difference among violations: 1.
 ACTUAL: array([[-127,  127,  127, ...,    0,    0,    0],
       [   0,    0,    0, ...,  127,  127,    0],
       [ 127,    0,  127, ...,    0,  127,    0],...
 DESIRED: array([[-127,  127,  127, ...,    0,    0,    0],
       [   0,    0,    0, ...,  127,  127,    0],
       [ 127,    0,  127, ...,    0,  127,    0],...
2025-05-14 19:52:31.156304 GPU 0 52777 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0010981468949466944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0010981468949466944, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 6927 / 5107712 (0.136%)
Max absolute difference among violations: 127
Max relative difference among violations: 1.
 ACTUAL: array([[   0,  127,    0, ...,    0, -127, -127],
       [ 127,  127,  127, ...,    0,    0, -127],
       [   0,  127,    0, ...,    0,  127,    0],...
 DESIRED: array([[   0,  127,    0, ...,    0, -127, -127],
       [ 127,  127,  127, ...,    0,    0, -127],
       [   0,  127,    0, ...,    0,  127,    0],...
2025-05-14 19:52:31.586047 GPU 0 52777 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0014022786635905504, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0014022786635905504, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 6313 / 5107712 (0.124%)
Max absolute difference among violations: 127
Max relative difference among violations: 1.
 ACTUAL: array([[   0, -127,    0, ...,    0, -127,  127],
       [   0,  127,    0, ...,  127,    0,    0],
       [-127, -127,  127, ...,    0, -127,    0],...
 DESIRED: array([[   0, -127,    0, ...,    0, -127,  127],
       [   0,  127,    0, ...,  127,    0,    0],
       [-127, -127,  127, ...,    0, -127,    0],...
2025-05-14 19:52:32.004266 GPU 0 52777 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.001479289960116148, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.001479289960116148, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 6198 / 5107712 (0.121%)
Max absolute difference among violations: 112
Max relative difference among violations: 1.
 ACTUAL: array([[   0,  127, -127, ...,    0,    0, -127],
       [ 127,    0,    0, ...,    0, -127,    0],
       [   0, -127,    0, ...,    0,    0, -127],...
 DESIRED: array([[   0,  127, -127, ...,    0,    0, -127],
       [ 127,    0,    0, ...,    0, -127,    0],
       [   0, -127,    0, ...,    0,    0, -127],...
2025-05-14 19:52:32.411479 GPU 0 52777 test begin: paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0016999575309455395, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(Tensor([464, 22016],"int32"), None, act_method="swiglu", compute_dtype="fp16", dequant_scales=Tensor([22016],"float32"), shift=None, smooth=None, quant_scale=0.0016999575309455395, quant_round_type=0, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5920 / 5107712 (0.116%)
Max absolute difference among violations: 127
Max relative difference among violations: 1.
 ACTUAL: array([[-127,    0,    0, ...,    0,    0,  127],
       [-127,    0,  127, ..., -127,    0,    0],
       [ 127,  127,  127, ...,  127,  127,  127],...
 DESIRED: array([[-127,    0,    0, ...,    0,    0,  127],
       [-127,    0,  127, ..., -127,    0,    0],
       [ 127,  127,  127, ...,  127,  127,  127],...
2025-05-14 19:52:35.120256 GPU 0 52777 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 10],"int32"), bias=Tensor([10],"float16"), dequant_scales=Tensor([10],"float32"), act_method="gelu", compute_dtype="fp16", )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 10],"int32"), bias=Tensor([10],"float16"), dequant_scales=Tensor([10],"float32"), act_method="gelu", compute_dtype="fp16", ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 3 / 400 (0.75%)
Max absolute difference among violations: 0.921875
Max relative difference among violations: 0.184375
 ACTUAL: array([[[ 2.4208e+04, -0.0000e+00,  5.0720e+03,  7.0640e+03,
          1.0400e+04, -0.0000e+00,  2.8100e+03, -0.0000e+00,
          1.9008e+04,  8.3100e+02],...
 DESIRED: array([[[24208,     0,  5068,  7064, 10400,     0,  2810,     0, 19008,
           831],
        [    0,     0,     0, 17648,  9744,     0,     0,  8768,     0,...
2025-05-14 19:52:36.160060 GPU 0 52777 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"int32"), bias=Tensor([512],"float16"), dequant_scales=Tensor([512],"float32"), shift=Tensor([256],"float16"), smooth=Tensor([256],"float16"), act_method="geglu", compute_dtype="fp16", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"int32"), bias=Tensor([512],"float16"), dequant_scales=Tensor([512],"float32"), shift=Tensor([256],"float16"), smooth=Tensor([256],"float16"), act_method="geglu", compute_dtype="fp16", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 7426 / 10240 (72.5%)
Max absolute difference among violations: 254
Max relative difference among violations: 126.
 ACTUAL: array([[[ 127,  127,    3, ...,  127,   -1,  127],
        [  -1,   -3, -127, ...,  127,   -1,    2],
        [ 127,   -3,    3, ...,    7,   -1,  127],...
 DESIRED: array([[[   0,  127,    0, ...,  127, -127,    0],
        [ 127,    0, -127, ...,  127, -127, -127],
        [   0,    0,    0, ...,    0, -127,    0],...
2025-05-14 19:52:36.254643 GPU 0 52777 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"int32"), bias=Tensor([512],"float16"), dequant_scales=Tensor([512],"float32"), shift=Tensor([512],"float16"), smooth=Tensor([512],"float16"), act_method="gelu", compute_dtype="fp16", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"int32"), bias=Tensor([512],"float16"), dequant_scales=Tensor([512],"float32"), shift=Tensor([512],"float16"), smooth=Tensor([512],"float16"), act_method="gelu", compute_dtype="fp16", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 15282 / 20480 (74.6%)
Max absolute difference among violations: 142
Max relative difference among violations: 111.
 ACTUAL: array([[[   2, -127, -127, ..., -127,    2,    7],
        [   2,    4,  -10, ...,   -4,    2,    7],
        [   2, -127, -127, ...,   -4, -127, -127],...
 DESIRED: array([[[  0,   0,   0, ...,   0,  53, 127],
        [  0, 127, 127, ..., 127,  19, 127],
        [  0,   0,   0, ..., 127,   0,   0],...
2025-05-14 19:52:36.344787 GPU 0 52777 test begin: paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"int32"), bias=Tensor([512],"float32"), dequant_scales=Tensor([512],"float32"), shift=Tensor([512],"float32"), smooth=Tensor([512],"float32"), act_method="gelu", compute_dtype="fp32", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, )
[accuracy error] paddle.incubate.nn.functional.fused_bias_act(x=Tensor([2, 20, 512],"int32"), bias=Tensor([512],"float32"), dequant_scales=Tensor([512],"float32"), shift=Tensor([512],"float32"), smooth=Tensor([512],"float32"), act_method="gelu", compute_dtype="fp32", quant_scale=0.5, quant_round_type=1, quant_max_bound=127.0, quant_min_bound=-127.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 15398 / 20480 (75.2%)
Max absolute difference among violations: 141
Max relative difference among violations: 110.
 ACTUAL: array([[[  -1,    4,   -6, ...,    5,  127, -127],
        [-127,  127,  127, ..., -127,  127,    2],
        [  -1,    4,   -6, ...,    5,    7, -127],...
 DESIRED: array([[[127,   0,   0, ..., 127, 127,   0],
        [  0, 127, 127, ...,   0, 127, 127],
        [127,   0,   0, ..., 127,   0,   0],...
