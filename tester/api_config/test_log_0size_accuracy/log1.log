2025-03-05 15:21:05.574672 test begin: paddle.abs(Tensor([0, 1, 32, 32],"float32"), )

W0305 15:21:11.142771 135485 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0305 15:21:11.143623 135485 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.abs(Tensor([0, 1, 32, 32],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.018302 test begin: paddle.abs(Tensor([0, 14, 5, 14],"complex128"), )

[cuda error] paddle.abs(Tensor([0, 14, 5, 14],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.024110 test begin: paddle.abs(Tensor([0, 1],"float64"), )

[cuda error] paddle.abs(Tensor([0, 1],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.027620 test begin: paddle.abs(Tensor([0, 1],"int64"), )

[cuda error] paddle.abs(Tensor([0, 1],"int64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.031948 test begin: paddle.abs(Tensor([0, 2, 5],"float32"), )

[cuda error] paddle.abs(Tensor([0, 2, 5],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.035103 test begin: paddle.abs(Tensor([0, 20, 1],"float32"), )

[cuda error] paddle.abs(Tensor([0, 20, 1],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.038235 test begin: paddle.abs(Tensor([0, 3, 10, 10, 1],"float32"), )

[cuda error] paddle.abs(Tensor([0, 3, 10, 10, 1],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.041029 test begin: paddle.abs(Tensor([0, 3, 20, 20, 1],"float32"), )

[cuda error] paddle.abs(Tensor([0, 3, 20, 20, 1],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.044328 test begin: paddle.abs(Tensor([0],"float32"), )

[cuda error] paddle.abs(Tensor([0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.050747 test begin: paddle.abs(Tensor([0],"float64"), )

[cuda error] paddle.abs(Tensor([0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.056606 test begin: paddle.abs(Tensor([1, 0, 32, 32],"float32"), )

[cuda error] paddle.abs(Tensor([1, 0, 32, 32],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.062510 test begin: paddle.abs(Tensor([1, 0, 5, 14],"complex128"), )

[cuda error] paddle.abs(Tensor([1, 0, 5, 14],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.068239 test begin: paddle.abs(Tensor([1, 0],"float64"), )

[cuda error] paddle.abs(Tensor([1, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.074768 test begin: paddle.abs(Tensor([1, 0],"int64"), )

[cuda error] paddle.abs(Tensor([1, 0],"int64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.080931 test begin: paddle.abs(Tensor([1, 1, 0, 32],"float32"), )

[cuda error] paddle.abs(Tensor([1, 1, 0, 32],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.087084 test begin: paddle.abs(Tensor([1, 1, 32, 0],"float32"), )

[cuda error] paddle.abs(Tensor([1, 1, 32, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.089306 test begin: paddle.abs(Tensor([1, 14, 0, 14],"complex128"), )

[cuda error] paddle.abs(Tensor([1, 14, 0, 14],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.091267 test begin: paddle.abs(Tensor([1, 14, 5, 0],"complex128"), )

[cuda error] paddle.abs(Tensor([1, 14, 5, 0],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.093978 test begin: paddle.abs(Tensor([10, 0, 1],"float32"), )

[cuda error] paddle.abs(Tensor([10, 0, 1],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.095979 test begin: paddle.abs(Tensor([10, 0, 5],"float32"), )

[cuda error] paddle.abs(Tensor([10, 0, 5],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.099289 test begin: paddle.abs(Tensor([10, 2, 0],"float32"), )

[cuda error] paddle.abs(Tensor([10, 2, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.101661 test begin: paddle.abs(Tensor([10, 20, 0],"float32"), )

[cuda error] paddle.abs(Tensor([10, 20, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.103825 test begin: paddle.abs(Tensor([12, 0, 10, 10, 1],"float32"), )

[cuda error] paddle.abs(Tensor([12, 0, 10, 10, 1],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.106318 test begin: paddle.abs(Tensor([12, 0, 20, 20, 1],"float32"), )

[cuda error] paddle.abs(Tensor([12, 0, 20, 20, 1],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.108667 test begin: paddle.abs(Tensor([12, 3, 0, 10, 1],"float32"), )

[cuda error] paddle.abs(Tensor([12, 3, 0, 10, 1],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.110784 test begin: paddle.abs(Tensor([12, 3, 0, 20, 1],"float32"), )

[cuda error] paddle.abs(Tensor([12, 3, 0, 20, 1],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.112631 test begin: paddle.abs(Tensor([12, 3, 10, 0, 1],"float32"), )

[cuda error] paddle.abs(Tensor([12, 3, 10, 0, 1],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.114946 test begin: paddle.abs(Tensor([12, 3, 10, 10, 0],"float32"), )

[cuda error] paddle.abs(Tensor([12, 3, 10, 10, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.117172 test begin: paddle.abs(Tensor([12, 3, 20, 0, 1],"float32"), )

[cuda error] paddle.abs(Tensor([12, 3, 20, 0, 1],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.119003 test begin: paddle.abs(Tensor([12, 3, 20, 20, 0],"float32"), )

[cuda error] paddle.abs(Tensor([12, 3, 20, 20, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.120685 test begin: paddle.abs(x=Tensor([0, 3, 3],"float32"), )

[cuda error] paddle.abs(x=Tensor([0, 3, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.122442 test begin: paddle.abs(x=Tensor([0, 3, 3],"float64"), )

[cuda error] paddle.abs(x=Tensor([0, 3, 3],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.125086 test begin: paddle.abs(x=Tensor([0, 3],"float64"), )

[cuda error] paddle.abs(x=Tensor([0, 3],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.127238 test begin: paddle.abs(x=Tensor([0],"float64"), )

[cuda error] paddle.abs(x=Tensor([0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.130063 test begin: paddle.abs(x=Tensor([3, 0, 3],"float32"), )

[cuda error] paddle.abs(x=Tensor([3, 0, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.132176 test begin: paddle.abs(x=Tensor([3, 0, 3],"float64"), )

[cuda error] paddle.abs(x=Tensor([3, 0, 3],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.133951 test begin: paddle.abs(x=Tensor([3, 3, 0],"float32"), )

[cuda error] paddle.abs(x=Tensor([3, 3, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.136356 test begin: paddle.abs(x=Tensor([3, 3, 0],"float64"), )

[cuda error] paddle.abs(x=Tensor([3, 3, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.138537 test begin: paddle.abs(x=Tensor([5, 0],"float64"), )

[cuda error] paddle.abs(x=Tensor([5, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.140671 test begin: paddle.acos(Tensor([0, 10],"float32"), )

[cuda error] paddle.acos(Tensor([0, 10],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.143422 test begin: paddle.acos(Tensor([0, 20, 1],"float32"), )

[cuda error] paddle.acos(Tensor([0, 20, 1],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.145332 test begin: paddle.acos(Tensor([0, 4],"float64"), )

[cuda error] paddle.acos(Tensor([0, 4],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.148132 test begin: paddle.acos(Tensor([10, 0, 1],"float32"), )

[cuda error] paddle.acos(Tensor([10, 0, 1],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.150311 test begin: paddle.acos(Tensor([10, 0],"float32"), )

[cuda error] paddle.acos(Tensor([10, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.152298 test begin: paddle.acos(Tensor([10, 20, 0],"float32"), )

[cuda error] paddle.acos(Tensor([10, 20, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.154394 test begin: paddle.acos(Tensor([2, 0],"float64"), )

[cuda error] paddle.acos(Tensor([2, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.156409 test begin: paddle.acos(x=Tensor([0, 3, 3],"float32"), )

[cuda error] paddle.acos(x=Tensor([0, 3, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.158664 test begin: paddle.acos(x=Tensor([0, 3, 3],"float64"), )

[cuda error] paddle.acos(x=Tensor([0, 3, 3],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.160708 test begin: paddle.acos(x=Tensor([3, 0, 3],"float32"), )

[cuda error] paddle.acos(x=Tensor([3, 0, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.162696 test begin: paddle.acos(x=Tensor([3, 0, 3],"float64"), )

[cuda error] paddle.acos(x=Tensor([3, 0, 3],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.165597 test begin: paddle.acos(x=Tensor([3, 3, 0],"float32"), )

[cuda error] paddle.acos(x=Tensor([3, 3, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.167378 test begin: paddle.acos(x=Tensor([3, 3, 0],"float64"), )

[cuda error] paddle.acos(x=Tensor([3, 3, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.169438 test begin: paddle.acosh(Tensor([0, 20, 1],"float32"), )

[cuda error] paddle.acosh(Tensor([0, 20, 1],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.171669 test begin: paddle.acosh(Tensor([10, 0, 1],"float32"), )

[cuda error] paddle.acosh(Tensor([10, 0, 1],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.173462 test begin: paddle.acosh(Tensor([10, 20, 0],"float32"), )

[cuda error] paddle.acosh(Tensor([10, 20, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:34.175473 test begin: paddle.addmm(Tensor([0, 10],"float32"), x=Tensor([0, 10],"float32"), y=Tensor([0, 10],"float32"), )

[torch error] paddle.addmm(Tensor([0, 10],"float32"), x=Tensor([0, 10],"float32"), y=Tensor([0, 10],"float32"), ) 
 mat1 and mat2 shapes cannot be multiplied (0x10 and 0x10)
2025-03-05 15:21:34.178053 test begin: paddle.addmm(Tensor([0, 10],"float32"), x=Tensor([10, 10],"float32"), y=Tensor([10, 10],"float32"), )

[torch error] paddle.addmm(Tensor([0, 10],"float32"), x=Tensor([10, 10],"float32"), y=Tensor([10, 10],"float32"), ) 
 The expanded size of the tensor (10) must match the existing size (0) at non-singleton dimension 0.  Target sizes: [10, 10].  Tensor sizes: [0, 10]
2025-03-05 15:21:34.180756 test begin: paddle.addmm(Tensor([0, 50],"float32"), Tensor([0, 80],"float32"), Tensor([0, 50],"float32"), alpha=1.0, beta=2.0, )

[torch error] paddle.addmm(Tensor([0, 50],"float32"), Tensor([0, 80],"float32"), Tensor([0, 50],"float32"), alpha=1.0, beta=2.0, ) 
 mat1 and mat2 shapes cannot be multiplied (0x80 and 0x50)
2025-03-05 15:21:34.182248 test begin: paddle.addmm(Tensor([0, 50],"float32"), Tensor([30, 80],"float32"), Tensor([80, 50],"float32"), alpha=1.0, beta=2.0, )

[torch error] paddle.addmm(Tensor([0, 50],"float32"), Tensor([30, 80],"float32"), Tensor([80, 50],"float32"), alpha=1.0, beta=2.0, ) 
 The expanded size of the tensor (30) must match the existing size (0) at non-singleton dimension 0.  Target sizes: [30, 50].  Tensor sizes: [0, 50]
2025-03-05 15:21:34.184972 test begin: paddle.addmm(Tensor([0, 64],"float16"), Tensor([0, 8],"float16"), Tensor([0, 64],"float16"), beta=1.0, alpha=2.0, )

[torch error] paddle.addmm(Tensor([0, 64],"float16"), Tensor([0, 8],"float16"), Tensor([0, 64],"float16"), beta=1.0, alpha=2.0, ) 
 mat1 and mat2 shapes cannot be multiplied (0x8 and 0x64)
2025-03-05 15:21:34.186421 test begin: paddle.addmm(Tensor([0, 64],"float16"), Tensor([32, 8],"float16"), Tensor([8, 64],"float16"), beta=1.0, alpha=2.0, )

[torch error] paddle.addmm(Tensor([0, 64],"float16"), Tensor([32, 8],"float16"), Tensor([8, 64],"float16"), beta=1.0, alpha=2.0, ) 
 The expanded size of the tensor (32) must match the existing size (0) at non-singleton dimension 0.  Target sizes: [32, 64].  Tensor sizes: [0, 64]
2025-03-05 15:21:34.188065 test begin: paddle.addmm(Tensor([0, 768],"float16"), Tensor([0, 8],"float16"), Tensor([0, 768],"float16"), beta=1.0, alpha=2.0, )

[torch error] paddle.addmm(Tensor([0, 768],"float16"), Tensor([0, 8],"float16"), Tensor([0, 768],"float16"), beta=1.0, alpha=2.0, ) 
 mat1 and mat2 shapes cannot be multiplied (0x8 and 0x768)
2025-03-05 15:21:34.189274 test begin: paddle.addmm(Tensor([0, 768],"float16"), Tensor([11008, 8],"float16"), Tensor([8, 768],"float16"), beta=1.0, alpha=2.0, )

[torch error] paddle.addmm(Tensor([0, 768],"float16"), Tensor([11008, 8],"float16"), Tensor([8, 768],"float16"), beta=1.0, alpha=2.0, ) 
 The expanded size of the tensor (11008) must match the existing size (0) at non-singleton dimension 0.  Target sizes: [11008, 768].  Tensor sizes: [0, 768]
2025-03-05 15:21:34.192616 test begin: paddle.addmm(Tensor([10, 0],"float32"), x=Tensor([10, 0],"float32"), y=Tensor([10, 0],"float32"), )

[torch error] paddle.addmm(Tensor([10, 0],"float32"), x=Tensor([10, 0],"float32"), y=Tensor([10, 0],"float32"), ) 
 mat1 and mat2 shapes cannot be multiplied (10x0 and 10x0)
2025-03-05 15:21:34.193886 test begin: paddle.addmm(Tensor([10, 0],"float32"), x=Tensor([10, 10],"float32"), y=Tensor([10, 10],"float32"), )

[torch error] paddle.addmm(Tensor([10, 0],"float32"), x=Tensor([10, 10],"float32"), y=Tensor([10, 10],"float32"), ) 
 The expanded size of the tensor (10) must match the existing size (0) at non-singleton dimension 1.  Target sizes: [10, 10].  Tensor sizes: [10, 0]
2025-03-05 15:21:34.195631 test begin: paddle.addmm(Tensor([10, 10],"float32"), x=Tensor([0, 10],"float32"), y=Tensor([10, 10],"float32"), )

[torch error] paddle.addmm(Tensor([10, 10],"float32"), x=Tensor([0, 10],"float32"), y=Tensor([10, 10],"float32"), ) 
 The expanded size of the tensor (0) must match the existing size (10) at non-singleton dimension 0.  Target sizes: [0, 10].  Tensor sizes: [10, 10]
2025-03-05 15:21:34.197225 test begin: paddle.addmm(Tensor([10, 10],"float32"), x=Tensor([10, 0],"float32"), y=Tensor([10, 10],"float32"), )

[torch error] paddle.addmm(Tensor([10, 10],"float32"), x=Tensor([10, 0],"float32"), y=Tensor([10, 10],"float32"), ) 
 mat1 and mat2 shapes cannot be multiplied (10x0 and 10x10)
2025-03-05 15:21:34.198352 test begin: paddle.addmm(Tensor([10, 10],"float32"), x=Tensor([10, 10],"float32"), y=Tensor([0, 10],"float32"), )

[torch error] paddle.addmm(Tensor([10, 10],"float32"), x=Tensor([10, 10],"float32"), y=Tensor([0, 10],"float32"), ) 
 mat1 and mat2 shapes cannot be multiplied (10x10 and 0x10)
2025-03-05 15:21:34.199276 test begin: paddle.addmm(Tensor([10, 10],"float32"), x=Tensor([10, 10],"float32"), y=Tensor([10, 0],"float32"), )

[torch error] paddle.addmm(Tensor([10, 10],"float32"), x=Tensor([10, 10],"float32"), y=Tensor([10, 0],"float32"), ) 
 The expanded size of the tensor (0) must match the existing size (10) at non-singleton dimension 1.  Target sizes: [10, 0].  Tensor sizes: [10, 10]
2025-03-05 15:21:34.200340 test begin: paddle.addmm(Tensor([11008, 0],"float16"), Tensor([11008, 0],"float16"), Tensor([8, 0],"float16"), beta=1.0, alpha=2.0, )

[torch error] paddle.addmm(Tensor([11008, 0],"float16"), Tensor([11008, 0],"float16"), Tensor([8, 0],"float16"), beta=1.0, alpha=2.0, ) 
 mat1 and mat2 shapes cannot be multiplied (11008x0 and 8x0)
2025-03-05 15:21:34.201477 test begin: paddle.addmm(Tensor([11008, 0],"float16"), Tensor([11008, 8],"float16"), Tensor([8, 768],"float16"), beta=1.0, alpha=2.0, )

[torch error] paddle.addmm(Tensor([11008, 0],"float16"), Tensor([11008, 8],"float16"), Tensor([8, 768],"float16"), beta=1.0, alpha=2.0, ) 
 The expanded size of the tensor (768) must match the existing size (0) at non-singleton dimension 1.  Target sizes: [11008, 768].  Tensor sizes: [11008, 0]
2025-03-05 15:21:34.205643 test begin: paddle.addmm(Tensor([11008, 768],"float16"), Tensor([0, 8],"float16"), Tensor([8, 768],"float16"), beta=1.0, alpha=2.0, )

[torch error] paddle.addmm(Tensor([11008, 768],"float16"), Tensor([0, 8],"float16"), Tensor([8, 768],"float16"), beta=1.0, alpha=2.0, ) 
 The expanded size of the tensor (0) must match the existing size (11008) at non-singleton dimension 0.  Target sizes: [0, 768].  Tensor sizes: [11008, 768]
2025-03-05 15:21:34.351217 test begin: paddle.addmm(Tensor([11008, 768],"float16"), Tensor([11008, 0],"float16"), Tensor([8, 768],"float16"), beta=1.0, alpha=2.0, )

[torch error] paddle.addmm(Tensor([11008, 768],"float16"), Tensor([11008, 0],"float16"), Tensor([8, 768],"float16"), beta=1.0, alpha=2.0, ) 
 mat1 and mat2 shapes cannot be multiplied (11008x0 and 8x768)
2025-03-05 15:21:34.499517 test begin: paddle.addmm(Tensor([11008, 768],"float16"), Tensor([11008, 8],"float16"), Tensor([0, 768],"float16"), beta=1.0, alpha=2.0, )

[torch error] paddle.addmm(Tensor([11008, 768],"float16"), Tensor([11008, 8],"float16"), Tensor([0, 768],"float16"), beta=1.0, alpha=2.0, ) 
 mat1 and mat2 shapes cannot be multiplied (11008x8 and 0x768)
2025-03-05 15:21:34.648063 test begin: paddle.addmm(Tensor([11008, 768],"float16"), Tensor([11008, 8],"float16"), Tensor([8, 0],"float16"), beta=1.0, alpha=2.0, )

[torch error] paddle.addmm(Tensor([11008, 768],"float16"), Tensor([11008, 8],"float16"), Tensor([8, 0],"float16"), beta=1.0, alpha=2.0, ) 
 The expanded size of the tensor (0) must match the existing size (768) at non-singleton dimension 1.  Target sizes: [11008, 0].  Tensor sizes: [11008, 768]
2025-03-05 15:21:34.799803 test begin: paddle.addmm(Tensor([30, 0],"float32"), Tensor([30, 0],"float32"), Tensor([80, 0],"float32"), alpha=1.0, beta=2.0, )

[torch error] paddle.addmm(Tensor([30, 0],"float32"), Tensor([30, 0],"float32"), Tensor([80, 0],"float32"), alpha=1.0, beta=2.0, ) 
 mat1 and mat2 shapes cannot be multiplied (30x0 and 80x0)
2025-03-05 15:21:34.801913 test begin: paddle.addmm(Tensor([30, 0],"float32"), Tensor([30, 80],"float32"), Tensor([80, 50],"float32"), alpha=1.0, beta=2.0, )

[torch error] paddle.addmm(Tensor([30, 0],"float32"), Tensor([30, 80],"float32"), Tensor([80, 50],"float32"), alpha=1.0, beta=2.0, ) 
 The expanded size of the tensor (50) must match the existing size (0) at non-singleton dimension 1.  Target sizes: [30, 50].  Tensor sizes: [30, 0]
2025-03-05 15:21:34.803990 test begin: paddle.addmm(Tensor([30, 50],"float32"), Tensor([0, 80],"float32"), Tensor([80, 50],"float32"), alpha=1.0, beta=2.0, )

[torch error] paddle.addmm(Tensor([30, 50],"float32"), Tensor([0, 80],"float32"), Tensor([80, 50],"float32"), alpha=1.0, beta=2.0, ) 
 The expanded size of the tensor (0) must match the existing size (30) at non-singleton dimension 0.  Target sizes: [0, 50].  Tensor sizes: [30, 50]
2025-03-05 15:21:34.806212 test begin: paddle.addmm(Tensor([30, 50],"float32"), Tensor([30, 0],"float32"), Tensor([80, 50],"float32"), alpha=1.0, beta=2.0, )

[torch error] paddle.addmm(Tensor([30, 50],"float32"), Tensor([30, 0],"float32"), Tensor([80, 50],"float32"), alpha=1.0, beta=2.0, ) 
 mat1 and mat2 shapes cannot be multiplied (30x0 and 80x50)
2025-03-05 15:21:34.807423 test begin: paddle.addmm(Tensor([30, 50],"float32"), Tensor([30, 80],"float32"), Tensor([0, 50],"float32"), alpha=1.0, beta=2.0, )

[torch error] paddle.addmm(Tensor([30, 50],"float32"), Tensor([30, 80],"float32"), Tensor([0, 50],"float32"), alpha=1.0, beta=2.0, ) 
 mat1 and mat2 shapes cannot be multiplied (30x80 and 0x50)
2025-03-05 15:21:34.809329 test begin: paddle.addmm(Tensor([30, 50],"float32"), Tensor([30, 80],"float32"), Tensor([80, 0],"float32"), alpha=1.0, beta=2.0, )

[torch error] paddle.addmm(Tensor([30, 50],"float32"), Tensor([30, 80],"float32"), Tensor([80, 0],"float32"), alpha=1.0, beta=2.0, ) 
 The expanded size of the tensor (0) must match the existing size (50) at non-singleton dimension 1.  Target sizes: [30, 0].  Tensor sizes: [30, 50]
2025-03-05 15:21:34.810572 test begin: paddle.addmm(Tensor([32, 0],"float16"), Tensor([32, 0],"float16"), Tensor([8, 0],"float16"), beta=1.0, alpha=2.0, )

[torch error] paddle.addmm(Tensor([32, 0],"float16"), Tensor([32, 0],"float16"), Tensor([8, 0],"float16"), beta=1.0, alpha=2.0, ) 
 mat1 and mat2 shapes cannot be multiplied (32x0 and 8x0)
2025-03-05 15:21:34.811799 test begin: paddle.addmm(Tensor([32, 0],"float16"), Tensor([32, 8],"float16"), Tensor([8, 64],"float16"), beta=1.0, alpha=2.0, )

[torch error] paddle.addmm(Tensor([32, 0],"float16"), Tensor([32, 8],"float16"), Tensor([8, 64],"float16"), beta=1.0, alpha=2.0, ) 
 The expanded size of the tensor (64) must match the existing size (0) at non-singleton dimension 1.  Target sizes: [32, 64].  Tensor sizes: [32, 0]
2025-03-05 15:21:34.813358 test begin: paddle.addmm(Tensor([32, 64],"float16"), Tensor([0, 8],"float16"), Tensor([8, 64],"float16"), beta=1.0, alpha=2.0, )

[torch error] paddle.addmm(Tensor([32, 64],"float16"), Tensor([0, 8],"float16"), Tensor([8, 64],"float16"), beta=1.0, alpha=2.0, ) 
 The expanded size of the tensor (0) must match the existing size (32) at non-singleton dimension 0.  Target sizes: [0, 64].  Tensor sizes: [32, 64]
2025-03-05 15:21:34.814866 test begin: paddle.addmm(Tensor([32, 64],"float16"), Tensor([32, 0],"float16"), Tensor([8, 64],"float16"), beta=1.0, alpha=2.0, )

[torch error] paddle.addmm(Tensor([32, 64],"float16"), Tensor([32, 0],"float16"), Tensor([8, 64],"float16"), beta=1.0, alpha=2.0, ) 
 mat1 and mat2 shapes cannot be multiplied (32x0 and 8x64)
2025-03-05 15:21:34.816534 test begin: paddle.addmm(Tensor([32, 64],"float16"), Tensor([32, 8],"float16"), Tensor([0, 64],"float16"), beta=1.0, alpha=2.0, )

[torch error] paddle.addmm(Tensor([32, 64],"float16"), Tensor([32, 8],"float16"), Tensor([0, 64],"float16"), beta=1.0, alpha=2.0, ) 
 mat1 and mat2 shapes cannot be multiplied (32x8 and 0x64)
2025-03-05 15:21:34.817703 test begin: paddle.addmm(Tensor([32, 64],"float16"), Tensor([32, 8],"float16"), Tensor([8, 0],"float16"), beta=1.0, alpha=2.0, )

[torch error] paddle.addmm(Tensor([32, 64],"float16"), Tensor([32, 8],"float16"), Tensor([8, 0],"float16"), beta=1.0, alpha=2.0, ) 
 The expanded size of the tensor (0) must match the existing size (64) at non-singleton dimension 1.  Target sizes: [32, 0].  Tensor sizes: [32, 64]
2025-03-05 15:21:34.819016 test begin: paddle.addmm(input=Tensor([0, 1],"float64"), x=Tensor([0, 4],"float64"), y=Tensor([0, 5],"float64"), beta=-3.3, alpha=3.3, )

[torch error] paddle.addmm(input=Tensor([0, 1],"float64"), x=Tensor([0, 4],"float64"), y=Tensor([0, 5],"float64"), beta=-3.3, alpha=3.3, ) 
 mat1 and mat2 shapes cannot be multiplied (0x4 and 0x5)
2025-03-05 15:21:34.820554 test begin: paddle.addmm(input=Tensor([0, 1],"float64"), x=Tensor([5, 4],"float64"), y=Tensor([4, 5],"float64"), beta=-3.3, alpha=3.3, )

[torch error] paddle.addmm(input=Tensor([0, 1],"float64"), x=Tensor([5, 4],"float64"), y=Tensor([4, 5],"float64"), beta=-3.3, alpha=3.3, ) 
 The expanded size of the tensor (5) must match the existing size (0) at non-singleton dimension 0.  Target sizes: [5, 5].  Tensor sizes: [0, 1]
2025-03-05 15:21:34.822681 test begin: paddle.addmm(input=Tensor([0, 2],"float32"), x=Tensor([0, 2],"float32"), y=Tensor([0, 2],"float32"), beta=0.5, alpha=5.0, )

[torch error] paddle.addmm(input=Tensor([0, 2],"float32"), x=Tensor([0, 2],"float32"), y=Tensor([0, 2],"float32"), beta=0.5, alpha=5.0, ) 
 mat1 and mat2 shapes cannot be multiplied (0x2 and 0x2)
2025-03-05 15:21:34.823851 test begin: paddle.addmm(input=Tensor([0, 2],"float32"), x=Tensor([2, 2],"float32"), y=Tensor([2, 2],"float32"), beta=0.5, alpha=5.0, )

[torch error] paddle.addmm(input=Tensor([0, 2],"float32"), x=Tensor([2, 2],"float32"), y=Tensor([2, 2],"float32"), beta=0.5, alpha=5.0, ) 
 The expanded size of the tensor (2) must match the existing size (0) at non-singleton dimension 0.  Target sizes: [2, 2].  Tensor sizes: [0, 2]
2025-03-05 15:21:34.825356 test begin: paddle.addmm(input=Tensor([0, 2],"float64"), x=Tensor([0, 2],"float64"), y=Tensor([0, 2],"float64"), beta=0.5, alpha=5.0, )

[torch error] paddle.addmm(input=Tensor([0, 2],"float64"), x=Tensor([0, 2],"float64"), y=Tensor([0, 2],"float64"), beta=0.5, alpha=5.0, ) 
 mat1 and mat2 shapes cannot be multiplied (0x2 and 0x2)
2025-03-05 15:21:34.826454 test begin: paddle.addmm(input=Tensor([0, 2],"float64"), x=Tensor([2, 2],"float64"), y=Tensor([2, 2],"float64"), beta=0.5, alpha=5.0, )

[torch error] paddle.addmm(input=Tensor([0, 2],"float64"), x=Tensor([2, 2],"float64"), y=Tensor([2, 2],"float64"), beta=0.5, alpha=5.0, ) 
 The expanded size of the tensor (2) must match the existing size (0) at non-singleton dimension 0.  Target sizes: [2, 2].  Tensor sizes: [0, 2]
2025-03-05 15:21:34.828005 test begin: paddle.addmm(input=Tensor([0, 5],"float64"), x=Tensor([0, 3],"float64"), y=Tensor([0, 5],"float64"), )

[torch error] paddle.addmm(input=Tensor([0, 5],"float64"), x=Tensor([0, 3],"float64"), y=Tensor([0, 5],"float64"), ) 
 mat1 and mat2 shapes cannot be multiplied (0x3 and 0x5)
2025-03-05 15:21:34.829268 test begin: paddle.addmm(input=Tensor([0, 5],"float64"), x=Tensor([0, 3],"float64"), y=Tensor([0, 5],"float64"), beta=-0.7, alpha=-3.3, )

[torch error] paddle.addmm(input=Tensor([0, 5],"float64"), x=Tensor([0, 3],"float64"), y=Tensor([0, 5],"float64"), beta=-0.7, alpha=-3.3, ) 
 mat1 and mat2 shapes cannot be multiplied (0x3 and 0x5)
2025-03-05 15:21:34.830098 test begin: paddle.addmm(input=Tensor([0, 5],"float64"), x=Tensor([0, 3],"float64"), y=Tensor([0, 5],"float64"), beta=0, alpha=-3.3, )

[torch error] paddle.addmm(input=Tensor([0, 5],"float64"), x=Tensor([0, 3],"float64"), y=Tensor([0, 5],"float64"), beta=0, alpha=-3.3, ) 
 mat1 and mat2 shapes cannot be multiplied (0x3 and 0x5)
2025-03-05 15:21:34.831048 test begin: paddle.addmm(input=Tensor([0, 5],"float64"), x=Tensor([0, 3],"float64"), y=Tensor([0, 5],"float64"), beta=0, alpha=0, )

[torch error] paddle.addmm(input=Tensor([0, 5],"float64"), x=Tensor([0, 3],"float64"), y=Tensor([0, 5],"float64"), beta=0, alpha=0, ) 
 mat1 and mat2 shapes cannot be multiplied (0x3 and 0x5)
2025-03-05 15:21:34.831863 test begin: paddle.addmm(input=Tensor([0, 5],"float64"), x=Tensor([0, 3],"float64"), y=Tensor([0, 5],"float64"), beta=0.5, alpha=-3.3, )

[torch error] paddle.addmm(input=Tensor([0, 5],"float64"), x=Tensor([0, 3],"float64"), y=Tensor([0, 5],"float64"), beta=0.5, alpha=-3.3, ) 
 mat1 and mat2 shapes cannot be multiplied (0x3 and 0x5)
2025-03-05 15:21:34.832774 test begin: paddle.addmm(input=Tensor([0, 5],"float64"), x=Tensor([0, 3],"float64"), y=Tensor([0, 5],"float64"), beta=0.5, alpha=0, )

[torch error] paddle.addmm(input=Tensor([0, 5],"float64"), x=Tensor([0, 3],"float64"), y=Tensor([0, 5],"float64"), beta=0.5, alpha=0, ) 
 mat1 and mat2 shapes cannot be multiplied (0x3 and 0x5)
2025-03-05 15:21:34.833539 test begin: paddle.addmm(input=Tensor([0, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 5],"float64"), )

[torch error] paddle.addmm(input=Tensor([0, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 5],"float64"), ) 
 The expanded size of the tensor (5) must match the existing size (0) at non-singleton dimension 0.  Target sizes: [5, 5].  Tensor sizes: [0, 5]
2025-03-05 15:21:34.835101 test begin: paddle.addmm(input=Tensor([0, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 5],"float64"), beta=-0.7, alpha=-3.3, )

[torch error] paddle.addmm(input=Tensor([0, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 5],"float64"), beta=-0.7, alpha=-3.3, ) 
 The expanded size of the tensor (5) must match the existing size (0) at non-singleton dimension 0.  Target sizes: [5, 5].  Tensor sizes: [0, 5]
2025-03-05 15:21:34.836244 test begin: paddle.addmm(input=Tensor([0, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 5],"float64"), beta=0, alpha=-3.3, )

[torch error] paddle.addmm(input=Tensor([0, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 5],"float64"), beta=0, alpha=-3.3, ) 
 The expanded size of the tensor (5) must match the existing size (0) at non-singleton dimension 0.  Target sizes: [5, 5].  Tensor sizes: [0, 5]
2025-03-05 15:21:34.837437 test begin: paddle.addmm(input=Tensor([0, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 5],"float64"), beta=0, alpha=0, )

[torch error] paddle.addmm(input=Tensor([0, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 5],"float64"), beta=0, alpha=0, ) 
 The expanded size of the tensor (5) must match the existing size (0) at non-singleton dimension 0.  Target sizes: [5, 5].  Tensor sizes: [0, 5]
2025-03-05 15:21:34.839569 test begin: paddle.addmm(input=Tensor([0, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 5],"float64"), beta=0.5, alpha=-3.3, )

[torch error] paddle.addmm(input=Tensor([0, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 5],"float64"), beta=0.5, alpha=-3.3, ) 
 The expanded size of the tensor (5) must match the existing size (0) at non-singleton dimension 0.  Target sizes: [5, 5].  Tensor sizes: [0, 5]
2025-03-05 15:21:34.840803 test begin: paddle.addmm(input=Tensor([0, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 5],"float64"), beta=0.5, alpha=0, )

[torch error] paddle.addmm(input=Tensor([0, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 5],"float64"), beta=0.5, alpha=0, ) 
 The expanded size of the tensor (5) must match the existing size (0) at non-singleton dimension 0.  Target sizes: [5, 5].  Tensor sizes: [0, 5]
2025-03-05 15:21:34.841855 test begin: paddle.addmm(input=Tensor([1, 0],"float64"), x=Tensor([5, 0],"float64"), y=Tensor([4, 0],"float64"), beta=-3.3, alpha=3.3, )

[torch error] paddle.addmm(input=Tensor([1, 0],"float64"), x=Tensor([5, 0],"float64"), y=Tensor([4, 0],"float64"), beta=-3.3, alpha=3.3, ) 
 mat1 and mat2 shapes cannot be multiplied (5x0 and 4x0)
2025-03-05 15:21:34.843063 test begin: paddle.addmm(input=Tensor([1, 0],"float64"), x=Tensor([5, 4],"float64"), y=Tensor([4, 5],"float64"), beta=-3.3, alpha=3.3, )

[torch error] paddle.addmm(input=Tensor([1, 0],"float64"), x=Tensor([5, 4],"float64"), y=Tensor([4, 5],"float64"), beta=-3.3, alpha=3.3, ) 
 The expanded size of the tensor (5) must match the existing size (0) at non-singleton dimension 1.  Target sizes: [5, 5].  Tensor sizes: [1, 0]
2025-03-05 15:21:34.844750 test begin: paddle.addmm(input=Tensor([1, 1],"float64"), x=Tensor([0, 4],"float64"), y=Tensor([4, 5],"float64"), beta=-3.3, alpha=3.3, )

[paddle error] paddle.addmm(input=Tensor([1, 1],"float64"), x=Tensor([0, 4],"float64"), y=Tensor([4, 5],"float64"), beta=-3.3, alpha=3.3, ) 
 (PreconditionNotMet) The Input variable 'x' has not been initialized. You may need to confirm if you put exe.run(startup_program) after optimizer.minimize function.
  [Hint: Expected product(x_dims) != 0, but received product(x_dims):0 == 0:0.] (at ../paddle/phi/infermeta/ternary.cc:115)

2025-03-05 15:21:34.852659 test begin: paddle.addmm(input=Tensor([1, 1],"float64"), x=Tensor([5, 0],"float64"), y=Tensor([4, 5],"float64"), beta=-3.3, alpha=3.3, )

[torch error] paddle.addmm(input=Tensor([1, 1],"float64"), x=Tensor([5, 0],"float64"), y=Tensor([4, 5],"float64"), beta=-3.3, alpha=3.3, ) 
 mat1 and mat2 shapes cannot be multiplied (5x0 and 4x5)
2025-03-05 15:21:34.854147 test begin: paddle.addmm(input=Tensor([1, 1],"float64"), x=Tensor([5, 4],"float64"), y=Tensor([0, 5],"float64"), beta=-3.3, alpha=3.3, )

[torch error] paddle.addmm(input=Tensor([1, 1],"float64"), x=Tensor([5, 4],"float64"), y=Tensor([0, 5],"float64"), beta=-3.3, alpha=3.3, ) 
 mat1 and mat2 shapes cannot be multiplied (5x4 and 0x5)
2025-03-05 15:21:34.855282 test begin: paddle.addmm(input=Tensor([1, 1],"float64"), x=Tensor([5, 4],"float64"), y=Tensor([4, 0],"float64"), beta=-3.3, alpha=3.3, )

[paddle error] paddle.addmm(input=Tensor([1, 1],"float64"), x=Tensor([5, 4],"float64"), y=Tensor([4, 0],"float64"), beta=-3.3, alpha=3.3, ) 
 (PreconditionNotMet) The Input variable 'y' has not been initialized. You may need to confirm if you put exe.run(startup_program) after optimizer.minimize function.
  [Hint: Expected product(y_dims) != 0, but received product(y_dims):0 == 0:0.] (at ../paddle/phi/infermeta/ternary.cc:123)

2025-03-05 15:21:34.858103 test begin: paddle.addmm(input=Tensor([2, 0],"float32"), x=Tensor([2, 0],"float32"), y=Tensor([2, 0],"float32"), beta=0.5, alpha=5.0, )

[torch error] paddle.addmm(input=Tensor([2, 0],"float32"), x=Tensor([2, 0],"float32"), y=Tensor([2, 0],"float32"), beta=0.5, alpha=5.0, ) 
 mat1 and mat2 shapes cannot be multiplied (2x0 and 2x0)
2025-03-05 15:21:34.859545 test begin: paddle.addmm(input=Tensor([2, 0],"float32"), x=Tensor([2, 2],"float32"), y=Tensor([2, 2],"float32"), beta=0.5, alpha=5.0, )

[torch error] paddle.addmm(input=Tensor([2, 0],"float32"), x=Tensor([2, 2],"float32"), y=Tensor([2, 2],"float32"), beta=0.5, alpha=5.0, ) 
 The expanded size of the tensor (2) must match the existing size (0) at non-singleton dimension 1.  Target sizes: [2, 2].  Tensor sizes: [2, 0]
2025-03-05 15:21:34.861490 test begin: paddle.addmm(input=Tensor([2, 0],"float64"), x=Tensor([2, 0],"float64"), y=Tensor([2, 0],"float64"), beta=0.5, alpha=5.0, )

[torch error] paddle.addmm(input=Tensor([2, 0],"float64"), x=Tensor([2, 0],"float64"), y=Tensor([2, 0],"float64"), beta=0.5, alpha=5.0, ) 
 mat1 and mat2 shapes cannot be multiplied (2x0 and 2x0)
2025-03-05 15:21:34.862476 test begin: paddle.addmm(input=Tensor([2, 0],"float64"), x=Tensor([2, 2],"float64"), y=Tensor([2, 2],"float64"), beta=0.5, alpha=5.0, )

[torch error] paddle.addmm(input=Tensor([2, 0],"float64"), x=Tensor([2, 2],"float64"), y=Tensor([2, 2],"float64"), beta=0.5, alpha=5.0, ) 
 The expanded size of the tensor (2) must match the existing size (0) at non-singleton dimension 1.  Target sizes: [2, 2].  Tensor sizes: [2, 0]
2025-03-05 15:21:34.863698 test begin: paddle.addmm(input=Tensor([2, 2],"float32"), x=Tensor([0, 2],"float32"), y=Tensor([2, 2],"float32"), beta=0.5, alpha=5.0, )

[torch error] paddle.addmm(input=Tensor([2, 2],"float32"), x=Tensor([0, 2],"float32"), y=Tensor([2, 2],"float32"), beta=0.5, alpha=5.0, ) 
 The expanded size of the tensor (0) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [0, 2].  Tensor sizes: [2, 2]
2025-03-05 15:21:34.865153 test begin: paddle.addmm(input=Tensor([2, 2],"float32"), x=Tensor([2, 0],"float32"), y=Tensor([2, 2],"float32"), beta=0.5, alpha=5.0, )

[torch error] paddle.addmm(input=Tensor([2, 2],"float32"), x=Tensor([2, 0],"float32"), y=Tensor([2, 2],"float32"), beta=0.5, alpha=5.0, ) 
 mat1 and mat2 shapes cannot be multiplied (2x0 and 2x2)
2025-03-05 15:21:34.866367 test begin: paddle.addmm(input=Tensor([2, 2],"float32"), x=Tensor([2, 2],"float32"), y=Tensor([0, 2],"float32"), beta=0.5, alpha=5.0, )

[torch error] paddle.addmm(input=Tensor([2, 2],"float32"), x=Tensor([2, 2],"float32"), y=Tensor([0, 2],"float32"), beta=0.5, alpha=5.0, ) 
 mat1 and mat2 shapes cannot be multiplied (2x2 and 0x2)
2025-03-05 15:21:34.867470 test begin: paddle.addmm(input=Tensor([2, 2],"float32"), x=Tensor([2, 2],"float32"), y=Tensor([2, 0],"float32"), beta=0.5, alpha=5.0, )

[torch error] paddle.addmm(input=Tensor([2, 2],"float32"), x=Tensor([2, 2],"float32"), y=Tensor([2, 0],"float32"), beta=0.5, alpha=5.0, ) 
 The expanded size of the tensor (0) must match the existing size (2) at non-singleton dimension 1.  Target sizes: [2, 0].  Tensor sizes: [2, 2]
2025-03-05 15:21:34.868935 test begin: paddle.addmm(input=Tensor([2, 2],"float64"), x=Tensor([0, 2],"float64"), y=Tensor([2, 2],"float64"), beta=0.5, alpha=5.0, )

[torch error] paddle.addmm(input=Tensor([2, 2],"float64"), x=Tensor([0, 2],"float64"), y=Tensor([2, 2],"float64"), beta=0.5, alpha=5.0, ) 
 The expanded size of the tensor (0) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [0, 2].  Tensor sizes: [2, 2]
2025-03-05 15:21:34.869941 test begin: paddle.addmm(input=Tensor([2, 2],"float64"), x=Tensor([2, 0],"float64"), y=Tensor([2, 2],"float64"), beta=0.5, alpha=5.0, )

[torch error] paddle.addmm(input=Tensor([2, 2],"float64"), x=Tensor([2, 0],"float64"), y=Tensor([2, 2],"float64"), beta=0.5, alpha=5.0, ) 
 mat1 and mat2 shapes cannot be multiplied (2x0 and 2x2)
2025-03-05 15:21:34.870901 test begin: paddle.addmm(input=Tensor([2, 2],"float64"), x=Tensor([2, 2],"float64"), y=Tensor([0, 2],"float64"), beta=0.5, alpha=5.0, )

[torch error] paddle.addmm(input=Tensor([2, 2],"float64"), x=Tensor([2, 2],"float64"), y=Tensor([0, 2],"float64"), beta=0.5, alpha=5.0, ) 
 mat1 and mat2 shapes cannot be multiplied (2x2 and 0x2)
2025-03-05 15:21:34.872103 test begin: paddle.addmm(input=Tensor([2, 2],"float64"), x=Tensor([2, 2],"float64"), y=Tensor([2, 0],"float64"), beta=0.5, alpha=5.0, )

[torch error] paddle.addmm(input=Tensor([2, 2],"float64"), x=Tensor([2, 2],"float64"), y=Tensor([2, 0],"float64"), beta=0.5, alpha=5.0, ) 
 The expanded size of the tensor (0) must match the existing size (2) at non-singleton dimension 1.  Target sizes: [2, 0].  Tensor sizes: [2, 2]
2025-03-05 15:21:34.873214 test begin: paddle.addmm(input=Tensor([5, 0],"float64"), x=Tensor([5, 0],"float64"), y=Tensor([3, 0],"float64"), )

[torch error] paddle.addmm(input=Tensor([5, 0],"float64"), x=Tensor([5, 0],"float64"), y=Tensor([3, 0],"float64"), ) 
 mat1 and mat2 shapes cannot be multiplied (5x0 and 3x0)
2025-03-05 15:21:34.874236 test begin: paddle.addmm(input=Tensor([5, 0],"float64"), x=Tensor([5, 0],"float64"), y=Tensor([3, 0],"float64"), beta=-0.7, alpha=-3.3, )

[torch error] paddle.addmm(input=Tensor([5, 0],"float64"), x=Tensor([5, 0],"float64"), y=Tensor([3, 0],"float64"), beta=-0.7, alpha=-3.3, ) 
 mat1 and mat2 shapes cannot be multiplied (5x0 and 3x0)
2025-03-05 15:21:34.875226 test begin: paddle.addmm(input=Tensor([5, 0],"float64"), x=Tensor([5, 0],"float64"), y=Tensor([3, 0],"float64"), beta=0, alpha=-3.3, )

[torch error] paddle.addmm(input=Tensor([5, 0],"float64"), x=Tensor([5, 0],"float64"), y=Tensor([3, 0],"float64"), beta=0, alpha=-3.3, ) 
 mat1 and mat2 shapes cannot be multiplied (5x0 and 3x0)
2025-03-05 15:21:34.876073 test begin: paddle.addmm(input=Tensor([5, 0],"float64"), x=Tensor([5, 0],"float64"), y=Tensor([3, 0],"float64"), beta=0, alpha=0, )

[torch error] paddle.addmm(input=Tensor([5, 0],"float64"), x=Tensor([5, 0],"float64"), y=Tensor([3, 0],"float64"), beta=0, alpha=0, ) 
 mat1 and mat2 shapes cannot be multiplied (5x0 and 3x0)
2025-03-05 15:21:34.876998 test begin: paddle.addmm(input=Tensor([5, 0],"float64"), x=Tensor([5, 0],"float64"), y=Tensor([3, 0],"float64"), beta=0.5, alpha=-3.3, )

[torch error] paddle.addmm(input=Tensor([5, 0],"float64"), x=Tensor([5, 0],"float64"), y=Tensor([3, 0],"float64"), beta=0.5, alpha=-3.3, ) 
 mat1 and mat2 shapes cannot be multiplied (5x0 and 3x0)
2025-03-05 15:21:34.878138 test begin: paddle.addmm(input=Tensor([5, 0],"float64"), x=Tensor([5, 0],"float64"), y=Tensor([3, 0],"float64"), beta=0.5, alpha=0, )

[torch error] paddle.addmm(input=Tensor([5, 0],"float64"), x=Tensor([5, 0],"float64"), y=Tensor([3, 0],"float64"), beta=0.5, alpha=0, ) 
 mat1 and mat2 shapes cannot be multiplied (5x0 and 3x0)
2025-03-05 15:21:34.878975 test begin: paddle.addmm(input=Tensor([5, 0],"float64"), x=Tensor([5, 0],"float64"), y=Tensor([4, 0],"float64"), beta=-3.3, alpha=3.3, )

[torch error] paddle.addmm(input=Tensor([5, 0],"float64"), x=Tensor([5, 0],"float64"), y=Tensor([4, 0],"float64"), beta=-3.3, alpha=3.3, ) 
 mat1 and mat2 shapes cannot be multiplied (5x0 and 4x0)
2025-03-05 15:21:34.879870 test begin: paddle.addmm(input=Tensor([5, 0],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 5],"float64"), )

[torch error] paddle.addmm(input=Tensor([5, 0],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 5],"float64"), ) 
 The expanded size of the tensor (5) must match the existing size (0) at non-singleton dimension 1.  Target sizes: [5, 5].  Tensor sizes: [5, 0]
2025-03-05 15:21:34.881292 test begin: paddle.addmm(input=Tensor([5, 0],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 5],"float64"), beta=-0.7, alpha=-3.3, )

[torch error] paddle.addmm(input=Tensor([5, 0],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 5],"float64"), beta=-0.7, alpha=-3.3, ) 
 The expanded size of the tensor (5) must match the existing size (0) at non-singleton dimension 1.  Target sizes: [5, 5].  Tensor sizes: [5, 0]
2025-03-05 15:21:34.882446 test begin: paddle.addmm(input=Tensor([5, 0],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 5],"float64"), beta=0, alpha=-3.3, )

[torch error] paddle.addmm(input=Tensor([5, 0],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 5],"float64"), beta=0, alpha=-3.3, ) 
 The expanded size of the tensor (5) must match the existing size (0) at non-singleton dimension 1.  Target sizes: [5, 5].  Tensor sizes: [5, 0]
2025-03-05 15:21:34.883621 test begin: paddle.addmm(input=Tensor([5, 0],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 5],"float64"), beta=0, alpha=0, )

[torch error] paddle.addmm(input=Tensor([5, 0],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 5],"float64"), beta=0, alpha=0, ) 
 The expanded size of the tensor (5) must match the existing size (0) at non-singleton dimension 1.  Target sizes: [5, 5].  Tensor sizes: [5, 0]
2025-03-05 15:21:34.884846 test begin: paddle.addmm(input=Tensor([5, 0],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 5],"float64"), beta=0.5, alpha=-3.3, )

[torch error] paddle.addmm(input=Tensor([5, 0],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 5],"float64"), beta=0.5, alpha=-3.3, ) 
 The expanded size of the tensor (5) must match the existing size (0) at non-singleton dimension 1.  Target sizes: [5, 5].  Tensor sizes: [5, 0]
2025-03-05 15:21:34.885954 test begin: paddle.addmm(input=Tensor([5, 0],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 5],"float64"), beta=0.5, alpha=0, )

[torch error] paddle.addmm(input=Tensor([5, 0],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 5],"float64"), beta=0.5, alpha=0, ) 
 The expanded size of the tensor (5) must match the existing size (0) at non-singleton dimension 1.  Target sizes: [5, 5].  Tensor sizes: [5, 0]
2025-03-05 15:21:34.887073 test begin: paddle.addmm(input=Tensor([5, 0],"float64"), x=Tensor([5, 4],"float64"), y=Tensor([4, 5],"float64"), beta=-3.3, alpha=3.3, )

[torch error] paddle.addmm(input=Tensor([5, 0],"float64"), x=Tensor([5, 4],"float64"), y=Tensor([4, 5],"float64"), beta=-3.3, alpha=3.3, ) 
 The expanded size of the tensor (5) must match the existing size (0) at non-singleton dimension 1.  Target sizes: [5, 5].  Tensor sizes: [5, 0]
2025-03-05 15:21:34.888518 test begin: paddle.addmm(input=Tensor([5, 1],"float64"), x=Tensor([0, 4],"float64"), y=Tensor([4, 5],"float64"), beta=-3.3, alpha=3.3, )

[torch error] paddle.addmm(input=Tensor([5, 1],"float64"), x=Tensor([0, 4],"float64"), y=Tensor([4, 5],"float64"), beta=-3.3, alpha=3.3, ) 
 The expanded size of the tensor (0) must match the existing size (5) at non-singleton dimension 0.  Target sizes: [0, 5].  Tensor sizes: [5, 1]
2025-03-05 15:21:34.889670 test begin: paddle.addmm(input=Tensor([5, 1],"float64"), x=Tensor([5, 0],"float64"), y=Tensor([4, 5],"float64"), beta=-3.3, alpha=3.3, )

[torch error] paddle.addmm(input=Tensor([5, 1],"float64"), x=Tensor([5, 0],"float64"), y=Tensor([4, 5],"float64"), beta=-3.3, alpha=3.3, ) 
 mat1 and mat2 shapes cannot be multiplied (5x0 and 4x5)
2025-03-05 15:21:34.890691 test begin: paddle.addmm(input=Tensor([5, 1],"float64"), x=Tensor([5, 4],"float64"), y=Tensor([0, 5],"float64"), beta=-3.3, alpha=3.3, )

[torch error] paddle.addmm(input=Tensor([5, 1],"float64"), x=Tensor([5, 4],"float64"), y=Tensor([0, 5],"float64"), beta=-3.3, alpha=3.3, ) 
 mat1 and mat2 shapes cannot be multiplied (5x4 and 0x5)
2025-03-05 15:21:34.891766 test begin: paddle.addmm(input=Tensor([5, 1],"float64"), x=Tensor([5, 4],"float64"), y=Tensor([4, 0],"float64"), beta=-3.3, alpha=3.3, )

[paddle error] paddle.addmm(input=Tensor([5, 1],"float64"), x=Tensor([5, 4],"float64"), y=Tensor([4, 0],"float64"), beta=-3.3, alpha=3.3, ) 
 (PreconditionNotMet) The Input variable 'y' has not been initialized. You may need to confirm if you put exe.run(startup_program) after optimizer.minimize function.
  [Hint: Expected product(y_dims) != 0, but received product(y_dims):0 == 0:0.] (at ../paddle/phi/infermeta/ternary.cc:123)

2025-03-05 15:21:34.894793 test begin: paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([0, 3],"float64"), y=Tensor([3, 5],"float64"), )

[torch error] paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([0, 3],"float64"), y=Tensor([3, 5],"float64"), ) 
 The expanded size of the tensor (0) must match the existing size (5) at non-singleton dimension 0.  Target sizes: [0, 5].  Tensor sizes: [5, 5]
2025-03-05 15:21:34.896306 test begin: paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([0, 3],"float64"), y=Tensor([3, 5],"float64"), beta=-0.7, alpha=-3.3, )

[torch error] paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([0, 3],"float64"), y=Tensor([3, 5],"float64"), beta=-0.7, alpha=-3.3, ) 
 The expanded size of the tensor (0) must match the existing size (5) at non-singleton dimension 0.  Target sizes: [0, 5].  Tensor sizes: [5, 5]
2025-03-05 15:21:34.897464 test begin: paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([0, 3],"float64"), y=Tensor([3, 5],"float64"), beta=0, alpha=-3.3, )

[torch error] paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([0, 3],"float64"), y=Tensor([3, 5],"float64"), beta=0, alpha=-3.3, ) 
 The expanded size of the tensor (0) must match the existing size (5) at non-singleton dimension 0.  Target sizes: [0, 5].  Tensor sizes: [5, 5]
2025-03-05 15:21:34.898626 test begin: paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([0, 3],"float64"), y=Tensor([3, 5],"float64"), beta=0, alpha=0, )

[torch error] paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([0, 3],"float64"), y=Tensor([3, 5],"float64"), beta=0, alpha=0, ) 
 The expanded size of the tensor (0) must match the existing size (5) at non-singleton dimension 0.  Target sizes: [0, 5].  Tensor sizes: [5, 5]
2025-03-05 15:21:34.899712 test begin: paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([0, 3],"float64"), y=Tensor([3, 5],"float64"), beta=0.5, alpha=-3.3, )

[torch error] paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([0, 3],"float64"), y=Tensor([3, 5],"float64"), beta=0.5, alpha=-3.3, ) 
 The expanded size of the tensor (0) must match the existing size (5) at non-singleton dimension 0.  Target sizes: [0, 5].  Tensor sizes: [5, 5]
2025-03-05 15:21:34.900863 test begin: paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([0, 3],"float64"), y=Tensor([3, 5],"float64"), beta=0.5, alpha=0, )

[torch error] paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([0, 3],"float64"), y=Tensor([3, 5],"float64"), beta=0.5, alpha=0, ) 
 The expanded size of the tensor (0) must match the existing size (5) at non-singleton dimension 0.  Target sizes: [0, 5].  Tensor sizes: [5, 5]
2025-03-05 15:21:34.902304 test begin: paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 0],"float64"), y=Tensor([3, 5],"float64"), )

[torch error] paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 0],"float64"), y=Tensor([3, 5],"float64"), ) 
 mat1 and mat2 shapes cannot be multiplied (5x0 and 3x5)
2025-03-05 15:21:34.903358 test begin: paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 0],"float64"), y=Tensor([3, 5],"float64"), beta=-0.7, alpha=-3.3, )

[torch error] paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 0],"float64"), y=Tensor([3, 5],"float64"), beta=-0.7, alpha=-3.3, ) 
 mat1 and mat2 shapes cannot be multiplied (5x0 and 3x5)
2025-03-05 15:21:34.904477 test begin: paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 0],"float64"), y=Tensor([3, 5],"float64"), beta=0, alpha=-3.3, )

[torch error] paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 0],"float64"), y=Tensor([3, 5],"float64"), beta=0, alpha=-3.3, ) 
 mat1 and mat2 shapes cannot be multiplied (5x0 and 3x5)
2025-03-05 15:21:34.906049 test begin: paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 0],"float64"), y=Tensor([3, 5],"float64"), beta=0, alpha=0, )

[torch error] paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 0],"float64"), y=Tensor([3, 5],"float64"), beta=0, alpha=0, ) 
 mat1 and mat2 shapes cannot be multiplied (5x0 and 3x5)
2025-03-05 15:21:34.907053 test begin: paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 0],"float64"), y=Tensor([3, 5],"float64"), beta=0.5, alpha=-3.3, )

[torch error] paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 0],"float64"), y=Tensor([3, 5],"float64"), beta=0.5, alpha=-3.3, ) 
 mat1 and mat2 shapes cannot be multiplied (5x0 and 3x5)
2025-03-05 15:21:34.908067 test begin: paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 0],"float64"), y=Tensor([3, 5],"float64"), beta=0.5, alpha=0, )

[torch error] paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 0],"float64"), y=Tensor([3, 5],"float64"), beta=0.5, alpha=0, ) 
 mat1 and mat2 shapes cannot be multiplied (5x0 and 3x5)
2025-03-05 15:21:34.909098 test begin: paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([0, 5],"float64"), )

[torch error] paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([0, 5],"float64"), ) 
 mat1 and mat2 shapes cannot be multiplied (5x3 and 0x5)
2025-03-05 15:21:34.910143 test begin: paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([0, 5],"float64"), beta=-0.7, alpha=-3.3, )

[torch error] paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([0, 5],"float64"), beta=-0.7, alpha=-3.3, ) 
 mat1 and mat2 shapes cannot be multiplied (5x3 and 0x5)
2025-03-05 15:21:34.911384 test begin: paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([0, 5],"float64"), beta=0, alpha=-3.3, )

[torch error] paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([0, 5],"float64"), beta=0, alpha=-3.3, ) 
 mat1 and mat2 shapes cannot be multiplied (5x3 and 0x5)
2025-03-05 15:21:34.912621 test begin: paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([0, 5],"float64"), beta=0, alpha=0, )

[torch error] paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([0, 5],"float64"), beta=0, alpha=0, ) 
 mat1 and mat2 shapes cannot be multiplied (5x3 and 0x5)
2025-03-05 15:21:34.913683 test begin: paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([0, 5],"float64"), beta=0.5, alpha=-3.3, )

[torch error] paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([0, 5],"float64"), beta=0.5, alpha=-3.3, ) 
 mat1 and mat2 shapes cannot be multiplied (5x3 and 0x5)
2025-03-05 15:21:34.914758 test begin: paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([0, 5],"float64"), beta=0.5, alpha=0, )

[torch error] paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([0, 5],"float64"), beta=0.5, alpha=0, ) 
 mat1 and mat2 shapes cannot be multiplied (5x3 and 0x5)
2025-03-05 15:21:34.916302 test begin: paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 0],"float64"), )

[torch error] paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 0],"float64"), ) 
 The expanded size of the tensor (0) must match the existing size (5) at non-singleton dimension 1.  Target sizes: [5, 0].  Tensor sizes: [5, 5]
2025-03-05 15:21:34.917320 test begin: paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 0],"float64"), beta=-0.7, alpha=-3.3, )

[torch error] paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 0],"float64"), beta=-0.7, alpha=-3.3, ) 
 The expanded size of the tensor (0) must match the existing size (5) at non-singleton dimension 1.  Target sizes: [5, 0].  Tensor sizes: [5, 5]
2025-03-05 15:21:34.918486 test begin: paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 0],"float64"), beta=0, alpha=-3.3, )

[torch error] paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 0],"float64"), beta=0, alpha=-3.3, ) 
 The expanded size of the tensor (0) must match the existing size (5) at non-singleton dimension 1.  Target sizes: [5, 0].  Tensor sizes: [5, 5]
2025-03-05 15:21:34.919534 test begin: paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 0],"float64"), beta=0, alpha=0, )

[torch error] paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 0],"float64"), beta=0, alpha=0, ) 
 The expanded size of the tensor (0) must match the existing size (5) at non-singleton dimension 1.  Target sizes: [5, 0].  Tensor sizes: [5, 5]
2025-03-05 15:21:34.920441 test begin: paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 0],"float64"), beta=0.5, alpha=-3.3, )

[torch error] paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 0],"float64"), beta=0.5, alpha=-3.3, ) 
 The expanded size of the tensor (0) must match the existing size (5) at non-singleton dimension 1.  Target sizes: [5, 0].  Tensor sizes: [5, 5]
2025-03-05 15:21:34.921561 test begin: paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 0],"float64"), beta=0.5, alpha=0, )

[torch error] paddle.addmm(input=Tensor([5, 5],"float64"), x=Tensor([5, 3],"float64"), y=Tensor([3, 0],"float64"), beta=0.5, alpha=0, ) 
 The expanded size of the tensor (0) must match the existing size (5) at non-singleton dimension 1.  Target sizes: [5, 0].  Tensor sizes: [5, 5]
2025-03-05 15:21:34.922609 test begin: paddle.all(Tensor([0, 10],"bool"), )

[Pass] paddle.all(Tensor([0, 10],"bool"), )
2025-03-05 15:21:34.930706 test begin: paddle.all(Tensor([0, 10],"bool"), axis=-1, )

[Pass] paddle.all(Tensor([0, 10],"bool"), axis=-1, )
2025-03-05 15:21:34.932720 test begin: paddle.all(Tensor([0, 10],"bool"), axis=0, )

[Pass] paddle.all(Tensor([0, 10],"bool"), axis=0, )
2025-03-05 15:21:34.935887 test begin: paddle.all(Tensor([0, 10],"bool"), axis=1, keepdim=True, )

[Pass] paddle.all(Tensor([0, 10],"bool"), axis=1, keepdim=True, )
2025-03-05 15:21:34.938349 test begin: paddle.all(Tensor([0, 3, 4, 5],"bool"), )

[Pass] paddle.all(Tensor([0, 3, 4, 5],"bool"), )
2025-03-05 15:21:34.941090 test begin: paddle.all(Tensor([0, 5],"bool"), )

[Pass] paddle.all(Tensor([0, 5],"bool"), )
2025-03-05 15:21:34.943499 test begin: paddle.all(Tensor([0, 5],"bool"), axis=None, )

[Pass] paddle.all(Tensor([0, 5],"bool"), axis=None, )
2025-03-05 15:21:34.946244 test begin: paddle.all(Tensor([0, 5],"bool"), keepdim=True, )

[Pass] paddle.all(Tensor([0, 5],"bool"), keepdim=True, )
2025-03-05 15:21:34.948659 test begin: paddle.all(Tensor([0, 6, 10],"bool"), None, False, None, )

[Pass] paddle.all(Tensor([0, 6, 10],"bool"), None, False, None, )
2025-03-05 15:21:34.950985 test begin: paddle.all(Tensor([0, 6, 10],"float64"), None, False, None, )

[Pass] paddle.all(Tensor([0, 6, 10],"float64"), None, False, None, )
2025-03-05 15:21:34.954106 test begin: paddle.all(Tensor([0],"bool"), )

[Pass] paddle.all(Tensor([0],"bool"), )
2025-03-05 15:21:34.956511 test begin: paddle.all(Tensor([0],"bool"), axis=-1, )

[Pass] paddle.all(Tensor([0],"bool"), axis=-1, )
2025-03-05 15:21:34.959395 test begin: paddle.all(Tensor([0],"bool"), axis=0, )

[Pass] paddle.all(Tensor([0],"bool"), axis=0, )
2025-03-05 15:21:34.961989 test begin: paddle.all(Tensor([1, 0],"bool"), )

[Pass] paddle.all(Tensor([1, 0],"bool"), )
2025-03-05 15:21:34.964225 test begin: paddle.all(Tensor([12, 0],"bool"), )

[Pass] paddle.all(Tensor([12, 0],"bool"), )
2025-03-05 15:21:34.966259 test begin: paddle.all(Tensor([12, 0],"bool"), axis=-1, )

[Pass] paddle.all(Tensor([12, 0],"bool"), axis=-1, )
2025-03-05 15:21:34.969312 test begin: paddle.all(Tensor([12, 0],"bool"), axis=0, )

[Pass] paddle.all(Tensor([12, 0],"bool"), axis=0, )
2025-03-05 15:21:34.971103 test begin: paddle.all(Tensor([12, 0],"bool"), axis=1, keepdim=True, )

[Pass] paddle.all(Tensor([12, 0],"bool"), axis=1, keepdim=True, )
2025-03-05 15:21:34.974391 test begin: paddle.all(Tensor([2, 0, 4, 5],"bool"), )

[Pass] paddle.all(Tensor([2, 0, 4, 5],"bool"), )
2025-03-05 15:21:34.976577 test begin: paddle.all(Tensor([2, 3, 0, 5],"bool"), )

[Pass] paddle.all(Tensor([2, 3, 0, 5],"bool"), )
2025-03-05 15:21:34.979144 test begin: paddle.all(Tensor([2, 3, 4, 0],"bool"), )

[Pass] paddle.all(Tensor([2, 3, 4, 0],"bool"), )
2025-03-05 15:21:34.981261 test begin: paddle.all(Tensor([3, 0],"bool"), axis=None, )

[Pass] paddle.all(Tensor([3, 0],"bool"), axis=None, )
2025-03-05 15:21:34.983274 test begin: paddle.all(Tensor([3, 0],"bool"), keepdim=True, )

[Pass] paddle.all(Tensor([3, 0],"bool"), keepdim=True, )
2025-03-05 15:21:34.985878 test begin: paddle.all(Tensor([5, 0, 10],"bool"), None, False, None, )

[Pass] paddle.all(Tensor([5, 0, 10],"bool"), None, False, None, )
2025-03-05 15:21:34.988207 test begin: paddle.all(Tensor([5, 0, 10],"float64"), None, False, None, )

[Pass] paddle.all(Tensor([5, 0, 10],"float64"), None, False, None, )
2025-03-05 15:21:34.990946 test begin: paddle.all(Tensor([5, 6, 0],"bool"), None, False, None, )

[Pass] paddle.all(Tensor([5, 6, 0],"bool"), None, False, None, )
2025-03-05 15:21:34.994124 test begin: paddle.all(Tensor([5, 6, 0],"float64"), None, False, None, )

[Pass] paddle.all(Tensor([5, 6, 0],"float64"), None, False, None, )
2025-03-05 15:21:34.996764 test begin: paddle.all(x=Tensor([0, 10],"bool"), )

[Pass] paddle.all(x=Tensor([0, 10],"bool"), )
2025-03-05 15:21:34.999082 test begin: paddle.all(x=Tensor([0, 4, 2],"bool"), )

[Pass] paddle.all(x=Tensor([0, 4, 2],"bool"), )
2025-03-05 15:21:35.005400 test begin: paddle.all(x=Tensor([0, 4, 4, 2],"bool"), )

[Pass] paddle.all(x=Tensor([0, 4, 4, 2],"bool"), )
2025-03-05 15:21:35.014798 test begin: paddle.all(x=Tensor([0, 4, 4, 2],"bool"), axis=1, )

[Pass] paddle.all(x=Tensor([0, 4, 4, 2],"bool"), axis=1, )
2025-03-05 15:21:35.019841 test begin: paddle.all(x=Tensor([0, 4, 4, 2],"bool"), axis=tuple(0,1,), )

[Pass] paddle.all(x=Tensor([0, 4, 4, 2],"bool"), axis=tuple(0,1,), )
2025-03-05 15:21:35.022408 test begin: paddle.all(x=Tensor([0, 4],"bool"), keepdim=True, )

[Pass] paddle.all(x=Tensor([0, 4],"bool"), keepdim=True, )
2025-03-05 15:21:35.024933 test begin: paddle.all(x=Tensor([0],"bool"), )

[Pass] paddle.all(x=Tensor([0],"bool"), )
2025-03-05 15:21:35.027394 test begin: paddle.all(x=Tensor([10, 0],"bool"), )

[Pass] paddle.all(x=Tensor([10, 0],"bool"), )
2025-03-05 15:21:35.030909 test begin: paddle.all(x=Tensor([2, 0, 4, 2],"bool"), )

[Pass] paddle.all(x=Tensor([2, 0, 4, 2],"bool"), )
2025-03-05 15:21:35.033347 test begin: paddle.all(x=Tensor([2, 0, 4, 2],"bool"), axis=1, )

[Pass] paddle.all(x=Tensor([2, 0, 4, 2],"bool"), axis=1, )
2025-03-05 15:21:35.035947 test begin: paddle.all(x=Tensor([2, 0, 4, 2],"bool"), axis=tuple(0,1,), )

[Pass] paddle.all(x=Tensor([2, 0, 4, 2],"bool"), axis=tuple(0,1,), )
2025-03-05 15:21:35.038658 test begin: paddle.all(x=Tensor([2, 0],"bool"), keepdim=True, )

[Pass] paddle.all(x=Tensor([2, 0],"bool"), keepdim=True, )
2025-03-05 15:21:35.040884 test begin: paddle.all(x=Tensor([2, 4, 0, 2],"bool"), )

[Pass] paddle.all(x=Tensor([2, 4, 0, 2],"bool"), )
2025-03-05 15:21:35.043526 test begin: paddle.all(x=Tensor([2, 4, 0, 2],"bool"), axis=1, )

[Pass] paddle.all(x=Tensor([2, 4, 0, 2],"bool"), axis=1, )
2025-03-05 15:21:35.045497 test begin: paddle.all(x=Tensor([2, 4, 0, 2],"bool"), axis=tuple(0,1,), )

[Pass] paddle.all(x=Tensor([2, 4, 0, 2],"bool"), axis=tuple(0,1,), )
2025-03-05 15:21:35.047345 test begin: paddle.all(x=Tensor([2, 4, 4, 0],"bool"), )

[Pass] paddle.all(x=Tensor([2, 4, 4, 0],"bool"), )
2025-03-05 15:21:35.049503 test begin: paddle.all(x=Tensor([2, 4, 4, 0],"bool"), axis=1, )

[Pass] paddle.all(x=Tensor([2, 4, 4, 0],"bool"), axis=1, )
2025-03-05 15:21:35.051306 test begin: paddle.all(x=Tensor([2, 4, 4, 0],"bool"), axis=tuple(0,1,), )

[Pass] paddle.all(x=Tensor([2, 4, 4, 0],"bool"), axis=tuple(0,1,), )
2025-03-05 15:21:35.053792 test begin: paddle.all(x=Tensor([3, 0, 2],"bool"), )

[Pass] paddle.all(x=Tensor([3, 0, 2],"bool"), )
2025-03-05 15:21:35.056861 test begin: paddle.all(x=Tensor([3, 4, 0],"bool"), )

[Pass] paddle.all(x=Tensor([3, 4, 0],"bool"), )
2025-03-05 15:21:35.059657 test begin: paddle.amax(Tensor([0, 10, 10],"float32"), axis=list[-1,-2,], keepdim=False, )

[paddle error] paddle.amax(Tensor([0, 10, 10],"float32"), axis=list[-1,-2,], keepdim=False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-05 15:21:35.062035 test begin: paddle.amax(Tensor([0, 10, 10],"float32"), axis=list[-1,0,], keepdim=False, )

[torch error] paddle.amax(Tensor([0, 10, 10],"float32"), axis=list[-1,0,], keepdim=False, ) 
 amax(): Expected reduction dim 0 to have non-zero size.
2025-03-05 15:21:35.062983 test begin: paddle.amax(Tensor([0, 10, 10],"float32"), axis=list[0,1,], keepdim=False, )

[torch error] paddle.amax(Tensor([0, 10, 10],"float32"), axis=list[0,1,], keepdim=False, ) 
 amax(): Expected reduction dim 0 to have non-zero size.
2025-03-05 15:21:35.063964 test begin: paddle.amax(Tensor([0, 10, 10],"float32"), keepdim=False, )

[torch error] paddle.amax(Tensor([0, 10, 10],"float32"), keepdim=False, ) 
 amax(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.064760 test begin: paddle.amax(Tensor([0, 10, 10],"float32"), keepdim=True, )

[torch error] paddle.amax(Tensor([0, 10, 10],"float32"), keepdim=True, ) 
 amax(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.065414 test begin: paddle.amax(Tensor([0, 2, 2],"int32"), tuple(0,1,), False, )

[torch error] paddle.amax(Tensor([0, 2, 2],"int32"), tuple(0,1,), False, ) 
 amax(): Expected reduction dim 0 to have non-zero size.
2025-03-05 15:21:35.066371 test begin: paddle.amax(Tensor([0, 2, 4, 5],"float32"), axis=-1, keepdim=True, )

[paddle error] paddle.amax(Tensor([0, 2, 4, 5],"float32"), axis=-1, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-05 15:21:35.068811 test begin: paddle.amax(Tensor([0, 2, 5, 4],"float32"), axis=2, keepdim=True, )

[paddle error] paddle.amax(Tensor([0, 2, 5, 4],"float32"), axis=2, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-05 15:21:35.071140 test begin: paddle.amax(Tensor([0, 2, 5, 4],"float32"), axis=None, keepdim=False, )

[torch error] paddle.amax(Tensor([0, 2, 5, 4],"float32"), axis=None, keepdim=False, ) 
 amax(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.071930 test begin: paddle.amax(Tensor([0, 2, 5, 4],"float32"), axis=tuple(1,2,), keepdim=True, )

[paddle error] paddle.amax(Tensor([0, 2, 5, 4],"float32"), axis=tuple(1,2,), keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-05 15:21:35.074362 test begin: paddle.amax(Tensor([0, 4],"float32"), axis=None, keepdim=False, )

[torch error] paddle.amax(Tensor([0, 4],"float32"), axis=None, keepdim=False, ) 
 amax(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.075206 test begin: paddle.amax(Tensor([0, 4],"float64"), 0, False, )

[torch error] paddle.amax(Tensor([0, 4],"float64"), 0, False, ) 
 amax(): Expected reduction dim 0 to have non-zero size.
2025-03-05 15:21:35.075919 test begin: paddle.amax(Tensor([0, 4],"float64"), 1, True, )

[paddle error] paddle.amax(Tensor([0, 4],"float64"), 1, True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-05 15:21:35.077925 test begin: paddle.amax(Tensor([0, 4],"float64"), None, False, )

[torch error] paddle.amax(Tensor([0, 4],"float64"), None, False, ) 
 amax(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.079096 test begin: paddle.amax(Tensor([0, 5, 4],"float32"), axis=None, keepdim=False, )

[torch error] paddle.amax(Tensor([0, 5, 4],"float32"), axis=None, keepdim=False, ) 
 amax(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.079914 test begin: paddle.amax(Tensor([0, 5],"float32"), axis=None, )

[torch error] paddle.amax(Tensor([0, 5],"float32"), axis=None, ) 
 amax(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.080621 test begin: paddle.amax(Tensor([0, 5],"float32"), keepdim=True, )

[torch error] paddle.amax(Tensor([0, 5],"float32"), keepdim=True, ) 
 amax(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.081260 test begin: paddle.amax(Tensor([0],"float32"), axis=0, )

[torch error] paddle.amax(Tensor([0],"float32"), axis=0, ) 
 amax(): Expected reduction dim 0 to have non-zero size.
2025-03-05 15:21:35.081909 test begin: paddle.amax(Tensor([0],"float32"), axis=None, keepdim=False, )

[torch error] paddle.amax(Tensor([0],"float32"), axis=None, keepdim=False, ) 
 amax(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.082547 test begin: paddle.amax(Tensor([0],"float32"), keepdim=True, )

[torch error] paddle.amax(Tensor([0],"float32"), keepdim=True, ) 
 amax(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.083631 test begin: paddle.amax(Tensor([0],"float64"), axis=None, keepdim=False, )

[torch error] paddle.amax(Tensor([0],"float64"), axis=None, keepdim=False, ) 
 amax(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.084645 test begin: paddle.amax(Tensor([10, 0, 10],"float32"), axis=list[-1,-2,], keepdim=False, )

[torch error] paddle.amax(Tensor([10, 0, 10],"float32"), axis=list[-1,-2,], keepdim=False, ) 
 amax(): Expected reduction dim -2 to have non-zero size.
2025-03-05 15:21:35.085375 test begin: paddle.amax(Tensor([10, 0, 10],"float32"), axis=list[-1,0,], keepdim=False, )

[paddle error] paddle.amax(Tensor([10, 0, 10],"float32"), axis=list[-1,0,], keepdim=False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-05 15:21:35.087652 test begin: paddle.amax(Tensor([10, 0, 10],"float32"), axis=list[0,1,], keepdim=False, )

[torch error] paddle.amax(Tensor([10, 0, 10],"float32"), axis=list[0,1,], keepdim=False, ) 
 amax(): Expected reduction dim 1 to have non-zero size.
2025-03-05 15:21:35.088525 test begin: paddle.amax(Tensor([10, 0, 10],"float32"), keepdim=False, )

[torch error] paddle.amax(Tensor([10, 0, 10],"float32"), keepdim=False, ) 
 amax(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.089237 test begin: paddle.amax(Tensor([10, 0, 10],"float32"), keepdim=True, )

[torch error] paddle.amax(Tensor([10, 0, 10],"float32"), keepdim=True, ) 
 amax(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.089897 test begin: paddle.amax(Tensor([10, 10, 0],"float32"), axis=list[-1,-2,], keepdim=False, )

[torch error] paddle.amax(Tensor([10, 10, 0],"float32"), axis=list[-1,-2,], keepdim=False, ) 
 amax(): Expected reduction dim -1 to have non-zero size.
2025-03-05 15:21:35.090835 test begin: paddle.amax(Tensor([10, 10, 0],"float32"), axis=list[-1,0,], keepdim=False, )

[torch error] paddle.amax(Tensor([10, 10, 0],"float32"), axis=list[-1,0,], keepdim=False, ) 
 amax(): Expected reduction dim -1 to have non-zero size.
2025-03-05 15:21:35.091573 test begin: paddle.amax(Tensor([10, 10, 0],"float32"), axis=list[0,1,], keepdim=False, )

[paddle error] paddle.amax(Tensor([10, 10, 0],"float32"), axis=list[0,1,], keepdim=False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-05 15:21:35.093594 test begin: paddle.amax(Tensor([10, 10, 0],"float32"), keepdim=False, )

[torch error] paddle.amax(Tensor([10, 10, 0],"float32"), keepdim=False, ) 
 amax(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.094389 test begin: paddle.amax(Tensor([10, 10, 0],"float32"), keepdim=True, )

[torch error] paddle.amax(Tensor([10, 10, 0],"float32"), keepdim=True, ) 
 amax(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.095078 test begin: paddle.amax(Tensor([2, 0, 2],"int32"), tuple(0,1,), False, )

[torch error] paddle.amax(Tensor([2, 0, 2],"int32"), tuple(0,1,), False, ) 
 amax(): Expected reduction dim 1 to have non-zero size.
2025-03-05 15:21:35.095865 test begin: paddle.amax(Tensor([2, 0, 4],"float32"), axis=None, keepdim=False, )

[torch error] paddle.amax(Tensor([2, 0, 4],"float32"), axis=None, keepdim=False, ) 
 amax(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.097190 test begin: paddle.amax(Tensor([2, 0],"float32"), axis=None, keepdim=False, )

[torch error] paddle.amax(Tensor([2, 0],"float32"), axis=None, keepdim=False, ) 
 amax(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.097888 test begin: paddle.amax(Tensor([2, 0],"float64"), 0, False, )

[paddle error] paddle.amax(Tensor([2, 0],"float64"), 0, False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-05 15:21:35.102088 test begin: paddle.amax(Tensor([2, 0],"float64"), 1, True, )

[torch error] paddle.amax(Tensor([2, 0],"float64"), 1, True, ) 
 amax(): Expected reduction dim 1 to have non-zero size.
2025-03-05 15:21:35.102875 test begin: paddle.amax(Tensor([2, 0],"float64"), None, False, )

[torch error] paddle.amax(Tensor([2, 0],"float64"), None, False, ) 
 amax(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.103523 test begin: paddle.amax(Tensor([2, 2, 0],"int32"), tuple(0,1,), False, )

[paddle error] paddle.amax(Tensor([2, 2, 0],"int32"), tuple(0,1,), False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-05 15:21:35.106710 test begin: paddle.amax(Tensor([2, 5, 0],"float32"), axis=None, keepdim=False, )

[torch error] paddle.amax(Tensor([2, 5, 0],"float32"), axis=None, keepdim=False, ) 
 amax(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.107452 test begin: paddle.amax(Tensor([3, 0, 4, 5],"float32"), axis=-1, keepdim=True, )

[paddle error] paddle.amax(Tensor([3, 0, 4, 5],"float32"), axis=-1, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-05 15:21:35.109469 test begin: paddle.amax(Tensor([3, 0, 5, 4],"float32"), axis=2, keepdim=True, )

[paddle error] paddle.amax(Tensor([3, 0, 5, 4],"float32"), axis=2, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-05 15:21:35.111779 test begin: paddle.amax(Tensor([3, 0, 5, 4],"float32"), axis=None, keepdim=False, )

[torch error] paddle.amax(Tensor([3, 0, 5, 4],"float32"), axis=None, keepdim=False, ) 
 amax(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.112691 test begin: paddle.amax(Tensor([3, 0, 5, 4],"float32"), axis=tuple(1,2,), keepdim=True, )

[torch error] paddle.amax(Tensor([3, 0, 5, 4],"float32"), axis=tuple(1,2,), keepdim=True, ) 
 amax(): Expected reduction dim 1 to have non-zero size.
2025-03-05 15:21:35.113673 test begin: paddle.amax(Tensor([3, 0],"float32"), axis=None, )

[torch error] paddle.amax(Tensor([3, 0],"float32"), axis=None, ) 
 amax(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.114505 test begin: paddle.amax(Tensor([3, 0],"float32"), keepdim=True, )

[torch error] paddle.amax(Tensor([3, 0],"float32"), keepdim=True, ) 
 amax(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.115171 test begin: paddle.amax(Tensor([3, 2, 0, 4],"float32"), axis=2, keepdim=True, )

[torch error] paddle.amax(Tensor([3, 2, 0, 4],"float32"), axis=2, keepdim=True, ) 
 amax(): Expected reduction dim 2 to have non-zero size.
2025-03-05 15:21:35.115981 test begin: paddle.amax(Tensor([3, 2, 0, 4],"float32"), axis=None, keepdim=False, )

[torch error] paddle.amax(Tensor([3, 2, 0, 4],"float32"), axis=None, keepdim=False, ) 
 amax(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.116774 test begin: paddle.amax(Tensor([3, 2, 0, 4],"float32"), axis=tuple(1,2,), keepdim=True, )

[torch error] paddle.amax(Tensor([3, 2, 0, 4],"float32"), axis=tuple(1,2,), keepdim=True, ) 
 amax(): Expected reduction dim 2 to have non-zero size.
2025-03-05 15:21:35.117574 test begin: paddle.amax(Tensor([3, 2, 0, 5],"float32"), axis=-1, keepdim=True, )

[paddle error] paddle.amax(Tensor([3, 2, 0, 5],"float32"), axis=-1, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-05 15:21:35.119473 test begin: paddle.amax(Tensor([3, 2, 4, 0],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.amax(Tensor([3, 2, 4, 0],"float32"), axis=-1, keepdim=True, ) 
 amax(): Expected reduction dim -1 to have non-zero size.
2025-03-05 15:21:35.120280 test begin: paddle.amax(Tensor([3, 2, 5, 0],"float32"), axis=2, keepdim=True, )

[paddle error] paddle.amax(Tensor([3, 2, 5, 0],"float32"), axis=2, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-05 15:21:35.122380 test begin: paddle.amax(Tensor([3, 2, 5, 0],"float32"), axis=None, keepdim=False, )

[torch error] paddle.amax(Tensor([3, 2, 5, 0],"float32"), axis=None, keepdim=False, ) 
 amax(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.123073 test begin: paddle.amax(Tensor([3, 2, 5, 0],"float32"), axis=tuple(1,2,), keepdim=True, )

[paddle error] paddle.amax(Tensor([3, 2, 5, 0],"float32"), axis=tuple(1,2,), keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-05 15:21:35.125129 test begin: paddle.amin(Tensor([0, 10, 10],"float32"), axis=list[-1,-2,], keepdim=False, )

[paddle error] paddle.amin(Tensor([0, 10, 10],"float32"), axis=list[-1,-2,], keepdim=False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-05 15:21:35.127363 test begin: paddle.amin(Tensor([0, 10, 10],"float32"), axis=list[-1,0,], keepdim=False, )

[torch error] paddle.amin(Tensor([0, 10, 10],"float32"), axis=list[-1,0,], keepdim=False, ) 
 amin(): Expected reduction dim 0 to have non-zero size.
2025-03-05 15:21:35.128112 test begin: paddle.amin(Tensor([0, 10, 10],"float32"), axis=list[0,1,], keepdim=False, )

[torch error] paddle.amin(Tensor([0, 10, 10],"float32"), axis=list[0,1,], keepdim=False, ) 
 amin(): Expected reduction dim 0 to have non-zero size.
2025-03-05 15:21:35.128988 test begin: paddle.amin(Tensor([0, 10, 10],"float32"), keepdim=False, )

[torch error] paddle.amin(Tensor([0, 10, 10],"float32"), keepdim=False, ) 
 amin(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.129856 test begin: paddle.amin(Tensor([0, 10, 10],"float32"), keepdim=True, )

[torch error] paddle.amin(Tensor([0, 10, 10],"float32"), keepdim=True, ) 
 amin(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.130616 test begin: paddle.amin(Tensor([0, 2, 2],"int32"), tuple(0,1,), False, )

[torch error] paddle.amin(Tensor([0, 2, 2],"int32"), tuple(0,1,), False, ) 
 amin(): Expected reduction dim 0 to have non-zero size.
2025-03-05 15:21:35.131427 test begin: paddle.amin(Tensor([0, 2, 4, 5],"float32"), axis=-1, keepdim=True, )

[paddle error] paddle.amin(Tensor([0, 2, 4, 5],"float32"), axis=-1, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-05 15:21:35.134145 test begin: paddle.amin(Tensor([0, 2, 5, 4],"float32"), axis=2, keepdim=True, )

[paddle error] paddle.amin(Tensor([0, 2, 5, 4],"float32"), axis=2, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-05 15:21:35.136913 test begin: paddle.amin(Tensor([0, 2, 5, 4],"float32"), axis=None, keepdim=False, )

[torch error] paddle.amin(Tensor([0, 2, 5, 4],"float32"), axis=None, keepdim=False, ) 
 amin(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.137607 test begin: paddle.amin(Tensor([0, 2, 5, 4],"float32"), axis=tuple(1,2,), keepdim=True, )

[paddle error] paddle.amin(Tensor([0, 2, 5, 4],"float32"), axis=tuple(1,2,), keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-05 15:21:35.140011 test begin: paddle.amin(Tensor([0, 4],"float32"), axis=None, keepdim=False, )

[torch error] paddle.amin(Tensor([0, 4],"float32"), axis=None, keepdim=False, ) 
 amin(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.141127 test begin: paddle.amin(Tensor([0, 4],"float64"), 0, False, )

[torch error] paddle.amin(Tensor([0, 4],"float64"), 0, False, ) 
 amin(): Expected reduction dim 0 to have non-zero size.
2025-03-05 15:21:35.141901 test begin: paddle.amin(Tensor([0, 4],"float64"), 1, True, )

[paddle error] paddle.amin(Tensor([0, 4],"float64"), 1, True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-05 15:21:35.145210 test begin: paddle.amin(Tensor([0, 4],"float64"), None, False, )

[torch error] paddle.amin(Tensor([0, 4],"float64"), None, False, ) 
 amin(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.146016 test begin: paddle.amin(Tensor([0, 5, 4],"float32"), axis=None, keepdim=False, )

[torch error] paddle.amin(Tensor([0, 5, 4],"float32"), axis=None, keepdim=False, ) 
 amin(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.146709 test begin: paddle.amin(Tensor([0, 5],"float32"), axis=None, )

[torch error] paddle.amin(Tensor([0, 5],"float32"), axis=None, ) 
 amin(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.147395 test begin: paddle.amin(Tensor([0, 5],"float32"), keepdim=True, )

[torch error] paddle.amin(Tensor([0, 5],"float32"), keepdim=True, ) 
 amin(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.148164 test begin: paddle.amin(Tensor([0],"float32"), axis=0, )

[torch error] paddle.amin(Tensor([0],"float32"), axis=0, ) 
 amin(): Expected reduction dim 0 to have non-zero size.
2025-03-05 15:21:35.148750 test begin: paddle.amin(Tensor([0],"float32"), axis=None, keepdim=False, )

[torch error] paddle.amin(Tensor([0],"float32"), axis=None, keepdim=False, ) 
 amin(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.149497 test begin: paddle.amin(Tensor([0],"float32"), keepdim=True, )

[torch error] paddle.amin(Tensor([0],"float32"), keepdim=True, ) 
 amin(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.150382 test begin: paddle.amin(Tensor([0],"float64"), axis=None, keepdim=False, )

[torch error] paddle.amin(Tensor([0],"float64"), axis=None, keepdim=False, ) 
 amin(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.151023 test begin: paddle.amin(Tensor([10, 0, 10],"float32"), axis=list[-1,-2,], keepdim=False, )

[torch error] paddle.amin(Tensor([10, 0, 10],"float32"), axis=list[-1,-2,], keepdim=False, ) 
 amin(): Expected reduction dim -2 to have non-zero size.
2025-03-05 15:21:35.152208 test begin: paddle.amin(Tensor([10, 0, 10],"float32"), axis=list[-1,0,], keepdim=False, )

[paddle error] paddle.amin(Tensor([10, 0, 10],"float32"), axis=list[-1,0,], keepdim=False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-05 15:21:35.155200 test begin: paddle.amin(Tensor([10, 0, 10],"float32"), axis=list[0,1,], keepdim=False, )

[torch error] paddle.amin(Tensor([10, 0, 10],"float32"), axis=list[0,1,], keepdim=False, ) 
 amin(): Expected reduction dim 1 to have non-zero size.
2025-03-05 15:21:35.156197 test begin: paddle.amin(Tensor([10, 0, 10],"float32"), keepdim=False, )

[torch error] paddle.amin(Tensor([10, 0, 10],"float32"), keepdim=False, ) 
 amin(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.156970 test begin: paddle.amin(Tensor([10, 0, 10],"float32"), keepdim=True, )

[torch error] paddle.amin(Tensor([10, 0, 10],"float32"), keepdim=True, ) 
 amin(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.157647 test begin: paddle.amin(Tensor([10, 10, 0],"float32"), axis=list[-1,-2,], keepdim=False, )

[torch error] paddle.amin(Tensor([10, 10, 0],"float32"), axis=list[-1,-2,], keepdim=False, ) 
 amin(): Expected reduction dim -1 to have non-zero size.
2025-03-05 15:21:35.158337 test begin: paddle.amin(Tensor([10, 10, 0],"float32"), axis=list[-1,0,], keepdim=False, )

[torch error] paddle.amin(Tensor([10, 10, 0],"float32"), axis=list[-1,0,], keepdim=False, ) 
 amin(): Expected reduction dim -1 to have non-zero size.
2025-03-05 15:21:35.159024 test begin: paddle.amin(Tensor([10, 10, 0],"float32"), axis=list[0,1,], keepdim=False, )

[paddle error] paddle.amin(Tensor([10, 10, 0],"float32"), axis=list[0,1,], keepdim=False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-05 15:21:35.163034 test begin: paddle.amin(Tensor([10, 10, 0],"float32"), keepdim=False, )

[torch error] paddle.amin(Tensor([10, 10, 0],"float32"), keepdim=False, ) 
 amin(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.163748 test begin: paddle.amin(Tensor([10, 10, 0],"float32"), keepdim=True, )

[torch error] paddle.amin(Tensor([10, 10, 0],"float32"), keepdim=True, ) 
 amin(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.164354 test begin: paddle.amin(Tensor([2, 0, 2],"int32"), tuple(0,1,), False, )

[torch error] paddle.amin(Tensor([2, 0, 2],"int32"), tuple(0,1,), False, ) 
 amin(): Expected reduction dim 1 to have non-zero size.
2025-03-05 15:21:35.165197 test begin: paddle.amin(Tensor([2, 0, 4],"float32"), axis=None, keepdim=False, )

[torch error] paddle.amin(Tensor([2, 0, 4],"float32"), axis=None, keepdim=False, ) 
 amin(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.166545 test begin: paddle.amin(Tensor([2, 0],"float32"), axis=None, keepdim=False, )

[torch error] paddle.amin(Tensor([2, 0],"float32"), axis=None, keepdim=False, ) 
 amin(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.167220 test begin: paddle.amin(Tensor([2, 0],"float64"), 0, False, )

[paddle error] paddle.amin(Tensor([2, 0],"float64"), 0, False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-05 15:21:35.170252 test begin: paddle.amin(Tensor([2, 0],"float64"), 1, True, )

[torch error] paddle.amin(Tensor([2, 0],"float64"), 1, True, ) 
 amin(): Expected reduction dim 1 to have non-zero size.
2025-03-05 15:21:35.171316 test begin: paddle.amin(Tensor([2, 0],"float64"), None, False, )

[torch error] paddle.amin(Tensor([2, 0],"float64"), None, False, ) 
 amin(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.172180 test begin: paddle.amin(Tensor([2, 2, 0],"int32"), tuple(0,1,), False, )

[paddle error] paddle.amin(Tensor([2, 2, 0],"int32"), tuple(0,1,), False, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-05 15:21:35.176271 test begin: paddle.amin(Tensor([2, 5, 0],"float32"), axis=None, keepdim=False, )

[torch error] paddle.amin(Tensor([2, 5, 0],"float32"), axis=None, keepdim=False, ) 
 amin(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.177275 test begin: paddle.amin(Tensor([3, 0, 4, 5],"float32"), axis=-1, keepdim=True, )

[paddle error] paddle.amin(Tensor([3, 0, 4, 5],"float32"), axis=-1, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-05 15:21:35.181657 test begin: paddle.amin(Tensor([3, 0, 5, 4],"float32"), axis=2, keepdim=True, )

[paddle error] paddle.amin(Tensor([3, 0, 5, 4],"float32"), axis=2, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-05 15:21:35.187423 test begin: paddle.amin(Tensor([3, 0, 5, 4],"float32"), axis=None, keepdim=False, )

[torch error] paddle.amin(Tensor([3, 0, 5, 4],"float32"), axis=None, keepdim=False, ) 
 amin(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.188307 test begin: paddle.amin(Tensor([3, 0, 5, 4],"float32"), axis=tuple(1,2,), keepdim=True, )

[torch error] paddle.amin(Tensor([3, 0, 5, 4],"float32"), axis=tuple(1,2,), keepdim=True, ) 
 amin(): Expected reduction dim 1 to have non-zero size.
2025-03-05 15:21:35.189001 test begin: paddle.amin(Tensor([3, 0],"float32"), axis=None, )

[torch error] paddle.amin(Tensor([3, 0],"float32"), axis=None, ) 
 amin(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.189801 test begin: paddle.amin(Tensor([3, 0],"float32"), keepdim=True, )

[torch error] paddle.amin(Tensor([3, 0],"float32"), keepdim=True, ) 
 amin(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.191187 test begin: paddle.amin(Tensor([3, 2, 0, 4],"float32"), axis=2, keepdim=True, )

[torch error] paddle.amin(Tensor([3, 2, 0, 4],"float32"), axis=2, keepdim=True, ) 
 amin(): Expected reduction dim 2 to have non-zero size.
2025-03-05 15:21:35.191955 test begin: paddle.amin(Tensor([3, 2, 0, 4],"float32"), axis=None, keepdim=False, )

[torch error] paddle.amin(Tensor([3, 2, 0, 4],"float32"), axis=None, keepdim=False, ) 
 amin(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.192834 test begin: paddle.amin(Tensor([3, 2, 0, 4],"float32"), axis=tuple(1,2,), keepdim=True, )

[torch error] paddle.amin(Tensor([3, 2, 0, 4],"float32"), axis=tuple(1,2,), keepdim=True, ) 
 amin(): Expected reduction dim 2 to have non-zero size.
2025-03-05 15:21:35.193578 test begin: paddle.amin(Tensor([3, 2, 0, 5],"float32"), axis=-1, keepdim=True, )

[paddle error] paddle.amin(Tensor([3, 2, 0, 5],"float32"), axis=-1, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-05 15:21:35.198943 test begin: paddle.amin(Tensor([3, 2, 4, 0],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.amin(Tensor([3, 2, 4, 0],"float32"), axis=-1, keepdim=True, ) 
 amin(): Expected reduction dim -1 to have non-zero size.
2025-03-05 15:21:35.199642 test begin: paddle.amin(Tensor([3, 2, 5, 0],"float32"), axis=2, keepdim=True, )

[paddle error] paddle.amin(Tensor([3, 2, 5, 0],"float32"), axis=2, keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-05 15:21:35.201780 test begin: paddle.amin(Tensor([3, 2, 5, 0],"float32"), axis=None, keepdim=False, )

[torch error] paddle.amin(Tensor([3, 2, 5, 0],"float32"), axis=None, keepdim=False, ) 
 amin(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
2025-03-05 15:21:35.202885 test begin: paddle.amin(Tensor([3, 2, 5, 0],"float32"), axis=tuple(1,2,), keepdim=True, )

[paddle error] paddle.amin(Tensor([3, 2, 5, 0],"float32"), axis=tuple(1,2,), keepdim=True, ) 
 (InvalidArgument) Tensor need be reduced must not empty.
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/funcs/reduce_function.h:1055)

2025-03-05 15:21:35.205622 test begin: paddle.angle(Tensor([0, 3],"complex128"), )

[cuda error] paddle.angle(Tensor([0, 3],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:35.208022 test begin: paddle.angle(Tensor([2, 0],"complex128"), )

[cuda error] paddle.angle(Tensor([2, 0],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:21:35.209877 test begin: paddle.any(Tensor([0, 10],"bool"), axis=-1, )

[Pass] paddle.any(Tensor([0, 10],"bool"), axis=-1, )
2025-03-05 15:21:35.212268 test begin: paddle.any(Tensor([0, 10],"bool"), axis=0, )

[Pass] paddle.any(Tensor([0, 10],"bool"), axis=0, )
2025-03-05 15:21:35.215012 test begin: paddle.any(Tensor([0, 10],"bool"), axis=1, keepdim=True, )

[Pass] paddle.any(Tensor([0, 10],"bool"), axis=1, keepdim=True, )
2025-03-05 15:21:35.216776 test begin: paddle.any(Tensor([0, 11],"bool"), )

[Pass] paddle.any(Tensor([0, 11],"bool"), )
2025-03-05 15:21:35.219527 test begin: paddle.any(Tensor([0, 1],"bool"), )

[Pass] paddle.any(Tensor([0, 1],"bool"), )
2025-03-05 15:21:35.221788 test begin: paddle.any(Tensor([0, 2, 1],"bool"), )

[Pass] paddle.any(Tensor([0, 2, 1],"bool"), )
2025-03-05 15:21:35.224789 test begin: paddle.any(Tensor([0, 2, 4, 5],"bool"), )

[Pass] paddle.any(Tensor([0, 2, 4, 5],"bool"), )
2025-03-05 15:21:35.226921 test begin: paddle.any(Tensor([0, 2],"bool"), list[0,], )

[Pass] paddle.any(Tensor([0, 2],"bool"), list[0,], )
2025-03-05 15:21:35.229699 test begin: paddle.any(Tensor([0, 300, 4096],"bool"), )

[Pass] paddle.any(Tensor([0, 300, 4096],"bool"), )
2025-03-05 15:21:35.232147 test begin: paddle.any(Tensor([0, 5],"bool"), axis=None, )

[Pass] paddle.any(Tensor([0, 5],"bool"), axis=None, )
2025-03-05 15:21:35.235173 test begin: paddle.any(Tensor([0, 5],"bool"), keepdim=True, )

[Pass] paddle.any(Tensor([0, 5],"bool"), keepdim=True, )
2025-03-05 15:21:35.238111 test begin: paddle.any(Tensor([0, 6, 10],"bool"), None, False, None, )

[Pass] paddle.any(Tensor([0, 6, 10],"bool"), None, False, None, )
2025-03-05 15:21:35.241153 test begin: paddle.any(Tensor([0, 6, 10],"float64"), None, False, None, )

[Pass] paddle.any(Tensor([0, 6, 10],"float64"), None, False, None, )
2025-03-05 15:21:35.243534 test begin: paddle.any(Tensor([0],"bool"), )

[Pass] paddle.any(Tensor([0],"bool"), )
2025-03-05 15:21:35.246115 test begin: paddle.any(Tensor([0],"bool"), axis=0, )

[Pass] paddle.any(Tensor([0],"bool"), axis=0, )
2025-03-05 15:21:35.248482 test begin: paddle.any(Tensor([1, 0, 4096],"bool"), )

[Pass] paddle.any(Tensor([1, 0, 4096],"bool"), )
2025-03-05 15:21:35.251296 test begin: paddle.any(Tensor([1, 0],"bool"), )

[Pass] paddle.any(Tensor([1, 0],"bool"), )
2025-03-05 15:21:35.253641 test begin: paddle.any(Tensor([1, 300, 0],"bool"), )

[Pass] paddle.any(Tensor([1, 300, 0],"bool"), )
2025-03-05 15:21:35.256156 test begin: paddle.any(Tensor([10, 0, 1],"bool"), )

[Pass] paddle.any(Tensor([10, 0, 1],"bool"), )
2025-03-05 15:21:35.258473 test begin: paddle.any(Tensor([10, 2, 0],"bool"), )

[Pass] paddle.any(Tensor([10, 2, 0],"bool"), )
2025-03-05 15:21:35.260649 test begin: paddle.any(Tensor([12, 0],"bool"), axis=-1, )

[Pass] paddle.any(Tensor([12, 0],"bool"), axis=-1, )
2025-03-05 15:21:35.262665 test begin: paddle.any(Tensor([12, 0],"bool"), axis=0, )

[Pass] paddle.any(Tensor([12, 0],"bool"), axis=0, )
2025-03-05 15:21:35.265333 test begin: paddle.any(Tensor([12, 0],"bool"), axis=1, keepdim=True, )

[Pass] paddle.any(Tensor([12, 0],"bool"), axis=1, keepdim=True, )
2025-03-05 15:21:35.267907 test begin: paddle.any(Tensor([2, 0],"bool"), list[0,], )

[Pass] paddle.any(Tensor([2, 0],"bool"), list[0,], )
2025-03-05 15:21:35.270660 test begin: paddle.any(Tensor([3, 0],"bool"), axis=None, )

[Pass] paddle.any(Tensor([3, 0],"bool"), axis=None, )
2025-03-05 15:21:35.272979 test begin: paddle.any(Tensor([3, 0],"bool"), keepdim=True, )

[Pass] paddle.any(Tensor([3, 0],"bool"), keepdim=True, )
2025-03-05 15:21:35.274992 test begin: paddle.any(Tensor([5, 0, 10],"bool"), None, False, None, )

[Pass] paddle.any(Tensor([5, 0, 10],"bool"), None, False, None, )
2025-03-05 15:21:35.278544 test begin: paddle.any(Tensor([5, 0, 10],"float64"), None, False, None, )

[Pass] paddle.any(Tensor([5, 0, 10],"float64"), None, False, None, )
2025-03-05 15:21:35.281071 test begin: paddle.any(Tensor([5, 6, 0],"bool"), None, False, None, )

[Pass] paddle.any(Tensor([5, 6, 0],"bool"), None, False, None, )
2025-03-05 15:21:35.283389 test begin: paddle.any(Tensor([5, 6, 0],"float64"), None, False, None, )

[Pass] paddle.any(Tensor([5, 6, 0],"float64"), None, False, None, )
2025-03-05 15:21:35.285562 test begin: paddle.any(Tensor([6, 0, 4, 5],"bool"), )

[Pass] paddle.any(Tensor([6, 0, 4, 5],"bool"), )
2025-03-05 15:21:35.288466 test begin: paddle.any(Tensor([6, 2, 0, 5],"bool"), )

[Pass] paddle.any(Tensor([6, 2, 0, 5],"bool"), )
2025-03-05 15:21:35.290505 test begin: paddle.any(Tensor([6, 2, 4, 0],"bool"), )

[Pass] paddle.any(Tensor([6, 2, 4, 0],"bool"), )
2025-03-05 15:21:35.293259 test begin: paddle.any(x=Tensor([0, 10],"bool"), )

[Pass] paddle.any(x=Tensor([0, 10],"bool"), )
2025-03-05 15:21:35.295599 test begin: paddle.any(x=Tensor([0, 4, 2],"bool"), )

[Pass] paddle.any(x=Tensor([0, 4, 2],"bool"), )
2025-03-05 15:21:35.297663 test begin: paddle.any(x=Tensor([0, 4, 4, 2],"bool"), )

[Pass] paddle.any(x=Tensor([0, 4, 4, 2],"bool"), )
2025-03-05 15:21:35.300026 test begin: paddle.any(x=Tensor([0, 4, 4, 2],"bool"), axis=1, )

[Pass] paddle.any(x=Tensor([0, 4, 4, 2],"bool"), axis=1, )
2025-03-05 15:21:35.301784 test begin: paddle.any(x=Tensor([0, 4, 4, 2],"bool"), axis=tuple(0,1,), )

[Pass] paddle.any(x=Tensor([0, 4, 4, 2],"bool"), axis=tuple(0,1,), )
2025-03-05 15:21:35.304076 test begin: paddle.any(x=Tensor([0, 4],"bool"), keepdim=True, )

[Pass] paddle.any(x=Tensor([0, 4],"bool"), keepdim=True, )
2025-03-05 15:21:35.306312 test begin: paddle.any(x=Tensor([0],"bool"), )

[Pass] paddle.any(x=Tensor([0],"bool"), )
2025-03-05 15:21:35.308356 test begin: paddle.any(x=Tensor([10, 0],"bool"), )

[Pass] paddle.any(x=Tensor([10, 0],"bool"), )
2025-03-05 15:21:35.310452 test begin: paddle.any(x=Tensor([2, 0, 4, 2],"bool"), )

[Pass] paddle.any(x=Tensor([2, 0, 4, 2],"bool"), )
2025-03-05 15:21:35.312755 test begin: paddle.any(x=Tensor([2, 0, 4, 2],"bool"), axis=1, )

[Pass] paddle.any(x=Tensor([2, 0, 4, 2],"bool"), axis=1, )
2025-03-05 15:21:35.315213 test begin: paddle.any(x=Tensor([2, 0, 4, 2],"bool"), axis=tuple(0,1,), )

[Pass] paddle.any(x=Tensor([2, 0, 4, 2],"bool"), axis=tuple(0,1,), )
2025-03-05 15:21:35.317529 test begin: paddle.any(x=Tensor([2, 0],"bool"), keepdim=True, )

[Pass] paddle.any(x=Tensor([2, 0],"bool"), keepdim=True, )
2025-03-05 15:21:35.319792 test begin: paddle.any(x=Tensor([2, 4, 0, 2],"bool"), )

[Pass] paddle.any(x=Tensor([2, 4, 0, 2],"bool"), )
2025-03-05 15:21:35.321893 test begin: paddle.any(x=Tensor([2, 4, 0, 2],"bool"), axis=1, )

[Pass] paddle.any(x=Tensor([2, 4, 0, 2],"bool"), axis=1, )
2025-03-05 15:21:35.323688 test begin: paddle.any(x=Tensor([2, 4, 0, 2],"bool"), axis=tuple(0,1,), )

[Pass] paddle.any(x=Tensor([2, 4, 0, 2],"bool"), axis=tuple(0,1,), )
2025-03-05 15:21:35.325415 test begin: paddle.any(x=Tensor([2, 4, 4, 0],"bool"), )

[Pass] paddle.any(x=Tensor([2, 4, 4, 0],"bool"), )
2025-03-05 15:21:35.328671 test begin: paddle.any(x=Tensor([2, 4, 4, 0],"bool"), axis=1, )

[Pass] paddle.any(x=Tensor([2, 4, 4, 0],"bool"), axis=1, )
2025-03-05 15:21:35.330394 test begin: paddle.any(x=Tensor([2, 4, 4, 0],"bool"), axis=tuple(0,1,), )

[Pass] paddle.any(x=Tensor([2, 4, 4, 0],"bool"), axis=tuple(0,1,), )
2025-03-05 15:21:35.332081 test begin: paddle.any(x=Tensor([3, 0, 2],"bool"), )

[Pass] paddle.any(x=Tensor([3, 0, 2],"bool"), )
2025-03-05 15:21:35.334547 test begin: paddle.any(x=Tensor([3, 4, 0],"bool"), )

[Pass] paddle.any(x=Tensor([3, 4, 0],"bool"), )
2025-03-05 15:21:35.337697 test begin: paddle.argmax(Tensor([0, 1000],"float32"), axis=1, )

[paddle error] paddle.argmax(Tensor([0, 1000],"float32"), axis=1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.339682 test begin: paddle.argmax(Tensor([0, 100],"float32"), axis=-1, )

[paddle error] paddle.argmax(Tensor([0, 100],"float32"), axis=-1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.341181 test begin: paddle.argmax(Tensor([0, 1024, 50304],"float16"), -1, )

[paddle error] paddle.argmax(Tensor([0, 1024, 50304],"float16"), -1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.343003 test begin: paddle.argmax(Tensor([0, 10],"float32"), )

[torch error] paddle.argmax(Tensor([0, 10],"float32"), ) 
 argmax(): Expected reduction dim to be specified for input.numel() == 0.
2025-03-05 15:21:35.344509 test begin: paddle.argmax(Tensor([0, 10],"float32"), axis=-1, keepdim=True, )

[paddle error] paddle.argmax(Tensor([0, 10],"float32"), axis=-1, keepdim=True, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.345990 test begin: paddle.argmax(Tensor([0, 10],"float32"), axis=1, )

[paddle error] paddle.argmax(Tensor([0, 10],"float32"), axis=1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.347400 test begin: paddle.argmax(Tensor([0, 2, 4, 16, 2],"float32"), axis=-1, )

[paddle error] paddle.argmax(Tensor([0, 2, 4, 16, 2],"float32"), axis=-1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.348624 test begin: paddle.argmax(Tensor([0, 256],"float32"), axis=-1, )

[paddle error] paddle.argmax(Tensor([0, 256],"float32"), axis=-1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.350023 test begin: paddle.argmax(Tensor([0, 3, 3, 3, 3, 3],"float64"), axis=0, )

[torch error] paddle.argmax(Tensor([0, 3, 3, 3, 3, 3],"float64"), axis=0, ) 
 argmax(): Expected reduction dim 0 to have non-zero size.
2025-03-05 15:21:35.350704 test begin: paddle.argmax(Tensor([0, 3, 4],"float64"), axis=-1, keepdim=True, )

[paddle error] paddle.argmax(Tensor([0, 3, 4],"float64"), axis=-1, keepdim=True, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.351970 test begin: paddle.argmax(Tensor([0, 32, 64],"float16"), axis=1, )

[paddle error] paddle.argmax(Tensor([0, 32, 64],"float16"), axis=1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.353735 test begin: paddle.argmax(Tensor([0, 32, 64],"float32"), axis=1, )

[paddle error] paddle.argmax(Tensor([0, 32, 64],"float32"), axis=1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.355552 test begin: paddle.argmax(Tensor([0, 4, 4, 4, 4],"float64"), axis=0, )

[torch error] paddle.argmax(Tensor([0, 4, 4, 4, 4],"float64"), axis=0, ) 
 argmax(): Expected reduction dim 0 to have non-zero size.
2025-03-05 15:21:35.356178 test begin: paddle.argmax(Tensor([0, 5, 5, 5],"float64"), axis=0, )

[torch error] paddle.argmax(Tensor([0, 5, 5, 5],"float64"), axis=0, ) 
 argmax(): Expected reduction dim 0 to have non-zero size.
2025-03-05 15:21:35.356848 test begin: paddle.argmax(Tensor([0, 5],"float32"), )

[torch error] paddle.argmax(Tensor([0, 5],"float32"), ) 
 argmax(): Expected reduction dim to be specified for input.numel() == 0.
2025-03-05 15:21:35.357708 test begin: paddle.argmax(Tensor([0, 5],"float32"), keepdim=True, )

[torch error] paddle.argmax(Tensor([0, 5],"float32"), keepdim=True, ) 
 argmax(): Expected reduction dim to be specified for input.numel() == 0.
2025-03-05 15:21:35.359051 test begin: paddle.argmax(Tensor([0, 7, 99],"float32"), axis=-1, )

[paddle error] paddle.argmax(Tensor([0, 7, 99],"float32"), axis=-1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.360635 test begin: paddle.argmax(Tensor([0, 8, 14, 12],"float32"), axis=1, keepdim=True, )

[paddle error] paddle.argmax(Tensor([0, 8, 14, 12],"float32"), axis=1, keepdim=True, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.362666 test begin: paddle.argmax(Tensor([0],"float32"), 0, )

[torch error] paddle.argmax(Tensor([0],"float32"), 0, ) 
 argmax(): Expected reduction dim 0 to have non-zero size.
2025-03-05 15:21:35.363522 test begin: paddle.argmax(Tensor([1, 0, 14, 12],"float32"), axis=1, keepdim=True, )

[torch error] paddle.argmax(Tensor([1, 0, 14, 12],"float32"), axis=1, keepdim=True, ) 
 argmax(): Expected reduction dim 1 to have non-zero size.
2025-03-05 15:21:35.364679 test begin: paddle.argmax(Tensor([1, 0],"float32"), axis=-1, )

[torch error] paddle.argmax(Tensor([1, 0],"float32"), axis=-1, ) 
 argmax(): Expected reduction dim 1 to have non-zero size.
2025-03-05 15:21:35.365546 test begin: paddle.argmax(Tensor([1, 0],"float32"), axis=1, )

[torch error] paddle.argmax(Tensor([1, 0],"float32"), axis=1, ) 
 argmax(): Expected reduction dim 1 to have non-zero size.
2025-03-05 15:21:35.366331 test begin: paddle.argmax(Tensor([1, 8, 0, 12],"float32"), axis=1, keepdim=True, )

[paddle error] paddle.argmax(Tensor([1, 8, 0, 12],"float32"), axis=1, keepdim=True, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.368133 test begin: paddle.argmax(Tensor([1, 8, 14, 0],"float32"), axis=1, keepdim=True, )

[paddle error] paddle.argmax(Tensor([1, 8, 14, 0],"float32"), axis=1, keepdim=True, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.369546 test begin: paddle.argmax(Tensor([10, 0],"float32"), )

[torch error] paddle.argmax(Tensor([10, 0],"float32"), ) 
 argmax(): Expected reduction dim to be specified for input.numel() == 0.
2025-03-05 15:21:35.370239 test begin: paddle.argmax(Tensor([10, 0],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.argmax(Tensor([10, 0],"float32"), axis=-1, keepdim=True, ) 
 argmax(): Expected reduction dim 1 to have non-zero size.
2025-03-05 15:21:35.370903 test begin: paddle.argmax(Tensor([10, 0],"float32"), axis=1, )

[torch error] paddle.argmax(Tensor([10, 0],"float32"), axis=1, ) 
 argmax(): Expected reduction dim 1 to have non-zero size.
2025-03-05 15:21:35.371534 test begin: paddle.argmax(Tensor([12988, 0, 64],"float16"), axis=1, )

[torch error] paddle.argmax(Tensor([12988, 0, 64],"float16"), axis=1, ) 
 argmax(): Expected reduction dim 1 to have non-zero size.
2025-03-05 15:21:35.372174 test begin: paddle.argmax(Tensor([12988, 0, 64],"float32"), axis=1, )

[torch error] paddle.argmax(Tensor([12988, 0, 64],"float32"), axis=1, ) 
 argmax(): Expected reduction dim 1 to have non-zero size.
2025-03-05 15:21:35.373041 test begin: paddle.argmax(Tensor([12988, 32, 0],"float16"), axis=1, )

[paddle error] paddle.argmax(Tensor([12988, 32, 0],"float16"), axis=1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.374971 test begin: paddle.argmax(Tensor([12988, 32, 0],"float32"), axis=1, )

[paddle error] paddle.argmax(Tensor([12988, 32, 0],"float32"), axis=1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.376554 test begin: paddle.argmax(Tensor([13, 0, 4, 16, 2],"float32"), axis=-1, )

[paddle error] paddle.argmax(Tensor([13, 0, 4, 16, 2],"float32"), axis=-1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.378677 test begin: paddle.argmax(Tensor([13, 0, 99],"float32"), axis=-1, )

[paddle error] paddle.argmax(Tensor([13, 0, 99],"float32"), axis=-1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.381212 test begin: paddle.argmax(Tensor([13, 2, 0, 16, 2],"float32"), axis=-1, )

[paddle error] paddle.argmax(Tensor([13, 2, 0, 16, 2],"float32"), axis=-1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.383276 test begin: paddle.argmax(Tensor([13, 2, 4, 0, 2],"float32"), axis=-1, )

[paddle error] paddle.argmax(Tensor([13, 2, 4, 0, 2],"float32"), axis=-1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.385342 test begin: paddle.argmax(Tensor([13, 2, 4, 16, 0],"float32"), axis=-1, )

[torch error] paddle.argmax(Tensor([13, 2, 4, 16, 0],"float32"), axis=-1, ) 
 argmax(): Expected reduction dim 4 to have non-zero size.
2025-03-05 15:21:35.386220 test begin: paddle.argmax(Tensor([13, 7, 0],"float32"), axis=-1, )

[torch error] paddle.argmax(Tensor([13, 7, 0],"float32"), axis=-1, ) 
 argmax(): Expected reduction dim 2 to have non-zero size.
2025-03-05 15:21:35.386861 test begin: paddle.argmax(Tensor([16, 0, 50304],"float16"), -1, )

[paddle error] paddle.argmax(Tensor([16, 0, 50304],"float16"), -1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.388253 test begin: paddle.argmax(Tensor([16, 1024, 0],"float16"), -1, )

[torch error] paddle.argmax(Tensor([16, 1024, 0],"float16"), -1, ) 
 argmax(): Expected reduction dim 2 to have non-zero size.
2025-03-05 15:21:35.389323 test begin: paddle.argmax(Tensor([2, 0, 4],"float64"), axis=-1, keepdim=True, )

[paddle error] paddle.argmax(Tensor([2, 0, 4],"float64"), axis=-1, keepdim=True, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.390706 test begin: paddle.argmax(Tensor([2, 3, 0],"float64"), axis=-1, keepdim=True, )

[torch error] paddle.argmax(Tensor([2, 3, 0],"float64"), axis=-1, keepdim=True, ) 
 argmax(): Expected reduction dim 2 to have non-zero size.
2025-03-05 15:21:35.391334 test begin: paddle.argmax(Tensor([3, 0, 3, 3, 3, 3],"float64"), axis=0, )

[paddle error] paddle.argmax(Tensor([3, 0, 3, 3, 3, 3],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.392501 test begin: paddle.argmax(Tensor([3, 0],"float32"), )

[torch error] paddle.argmax(Tensor([3, 0],"float32"), ) 
 argmax(): Expected reduction dim to be specified for input.numel() == 0.
2025-03-05 15:21:35.393198 test begin: paddle.argmax(Tensor([3, 0],"float32"), keepdim=True, )

[torch error] paddle.argmax(Tensor([3, 0],"float32"), keepdim=True, ) 
 argmax(): Expected reduction dim to be specified for input.numel() == 0.
2025-03-05 15:21:35.393868 test begin: paddle.argmax(Tensor([3, 3, 0, 3, 3, 3],"float64"), axis=0, )

[paddle error] paddle.argmax(Tensor([3, 3, 0, 3, 3, 3],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.395019 test begin: paddle.argmax(Tensor([3, 3, 3, 0, 3, 3],"float64"), axis=0, )

[paddle error] paddle.argmax(Tensor([3, 3, 3, 0, 3, 3],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.396709 test begin: paddle.argmax(Tensor([3, 3, 3, 3, 0, 3],"float64"), axis=0, )

[paddle error] paddle.argmax(Tensor([3, 3, 3, 3, 0, 3],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.398763 test begin: paddle.argmax(Tensor([3, 3, 3, 3, 3, 0],"float64"), axis=0, )

[paddle error] paddle.argmax(Tensor([3, 3, 3, 3, 3, 0],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.400138 test begin: paddle.argmax(Tensor([4, 0, 4, 4, 4],"float64"), axis=0, )

[paddle error] paddle.argmax(Tensor([4, 0, 4, 4, 4],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.402221 test begin: paddle.argmax(Tensor([4, 4, 0, 4, 4],"float64"), axis=0, )

[paddle error] paddle.argmax(Tensor([4, 4, 0, 4, 4],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.403361 test begin: paddle.argmax(Tensor([4, 4, 4, 0, 4],"float64"), axis=0, )

[paddle error] paddle.argmax(Tensor([4, 4, 4, 0, 4],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.404520 test begin: paddle.argmax(Tensor([4, 4, 4, 4, 0],"float64"), axis=0, )

[paddle error] paddle.argmax(Tensor([4, 4, 4, 4, 0],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.406495 test begin: paddle.argmax(Tensor([5, 0, 5, 5],"float64"), axis=0, )

[paddle error] paddle.argmax(Tensor([5, 0, 5, 5],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.408022 test begin: paddle.argmax(Tensor([5, 5, 0, 5],"float64"), axis=0, )

[paddle error] paddle.argmax(Tensor([5, 5, 0, 5],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.409412 test begin: paddle.argmax(Tensor([5, 5, 5, 0],"float64"), axis=0, )

[paddle error] paddle.argmax(Tensor([5, 5, 5, 0],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.410782 test begin: paddle.argmax(x=Tensor([0, 3, 3],"float64"), )

[torch error] paddle.argmax(x=Tensor([0, 3, 3],"float64"), ) 
 argmax(): Expected reduction dim to be specified for input.numel() == 0.
2025-03-05 15:21:35.411431 test begin: paddle.argmax(x=Tensor([0, 3, 4],"float64"), axis=1, keepdim=False, )

[paddle error] paddle.argmax(x=Tensor([0, 3, 4],"float64"), axis=1, keepdim=False, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.413602 test begin: paddle.argmax(x=Tensor([0, 3],"int64"), axis=-1, )

[paddle error] paddle.argmax(x=Tensor([0, 3],"int64"), axis=-1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.415495 test begin: paddle.argmax(x=Tensor([0, 3],"int64"), axis=-2, )

[torch error] paddle.argmax(x=Tensor([0, 3],"int64"), axis=-2, ) 
 argmax(): Expected reduction dim 0 to have non-zero size.
2025-03-05 15:21:35.416295 test begin: paddle.argmax(x=Tensor([0],"int64"), axis=-1, keepdim=True, )

[torch error] paddle.argmax(x=Tensor([0],"int64"), axis=-1, keepdim=True, ) 
 argmax(): Expected reduction dim 0 to have non-zero size.
2025-03-05 15:21:35.417475 test begin: paddle.argmax(x=Tensor([2, 0],"int64"), axis=-1, )

[torch error] paddle.argmax(x=Tensor([2, 0],"int64"), axis=-1, ) 
 argmax(): Expected reduction dim 1 to have non-zero size.
2025-03-05 15:21:35.418330 test begin: paddle.argmax(x=Tensor([3, 0, 3],"float64"), )

[torch error] paddle.argmax(x=Tensor([3, 0, 3],"float64"), ) 
 argmax(): Expected reduction dim to be specified for input.numel() == 0.
2025-03-05 15:21:35.418993 test begin: paddle.argmax(x=Tensor([3, 0, 4],"float64"), axis=1, keepdim=False, )

[torch error] paddle.argmax(x=Tensor([3, 0, 4],"float64"), axis=1, keepdim=False, ) 
 argmax(): Expected reduction dim 1 to have non-zero size.
2025-03-05 15:21:35.419693 test begin: paddle.argmax(x=Tensor([3, 0],"int64"), axis=-2, )

[paddle error] paddle.argmax(x=Tensor([3, 0],"int64"), axis=-2, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.421208 test begin: paddle.argmax(x=Tensor([3, 3, 0],"float64"), )

[torch error] paddle.argmax(x=Tensor([3, 3, 0],"float64"), ) 
 argmax(): Expected reduction dim to be specified for input.numel() == 0.
2025-03-05 15:21:35.422172 test begin: paddle.argmax(x=Tensor([3, 3, 0],"float64"), axis=1, keepdim=False, )

[paddle error] paddle.argmax(x=Tensor([3, 3, 0],"float64"), axis=1, keepdim=False, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.423850 test begin: paddle.argmin(Tensor([0, 10],"float32"), )

[torch error] paddle.argmin(Tensor([0, 10],"float32"), ) 
 argmin(): Expected reduction dim to be specified for input.numel() == 0.
2025-03-05 15:21:35.425048 test begin: paddle.argmin(Tensor([0, 10],"float32"), axis=-1, )

[paddle error] paddle.argmin(Tensor([0, 10],"float32"), axis=-1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.426483 test begin: paddle.argmin(Tensor([0, 10],"float32"), axis=-1, keepdim=True, )

[paddle error] paddle.argmin(Tensor([0, 10],"float32"), axis=-1, keepdim=True, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.428176 test begin: paddle.argmin(Tensor([0, 10],"float32"), axis=1, )

[paddle error] paddle.argmin(Tensor([0, 10],"float32"), axis=1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.429723 test begin: paddle.argmin(Tensor([0, 3, 3, 3, 3, 3],"float64"), axis=0, )

[torch error] paddle.argmin(Tensor([0, 3, 3, 3, 3, 3],"float64"), axis=0, ) 
 argmin(): Expected reduction dim 0 to have non-zero size.
2025-03-05 15:21:35.430477 test begin: paddle.argmin(Tensor([0, 4, 4, 4, 4],"float64"), axis=0, )

[torch error] paddle.argmin(Tensor([0, 4, 4, 4, 4],"float64"), axis=0, ) 
 argmin(): Expected reduction dim 0 to have non-zero size.
2025-03-05 15:21:35.431246 test begin: paddle.argmin(Tensor([0, 5, 5, 5],"float64"), axis=0, )

[torch error] paddle.argmin(Tensor([0, 5, 5, 5],"float64"), axis=0, ) 
 argmin(): Expected reduction dim 0 to have non-zero size.
2025-03-05 15:21:35.432129 test begin: paddle.argmin(Tensor([0, 5],"float32"), )

[torch error] paddle.argmin(Tensor([0, 5],"float32"), ) 
 argmin(): Expected reduction dim to be specified for input.numel() == 0.
2025-03-05 15:21:35.432962 test begin: paddle.argmin(Tensor([0, 5],"float32"), keepdim=True, )

[torch error] paddle.argmin(Tensor([0, 5],"float32"), keepdim=True, ) 
 argmin(): Expected reduction dim to be specified for input.numel() == 0.
2025-03-05 15:21:35.433681 test begin: paddle.argmin(Tensor([0],"float32"), 0, )

[torch error] paddle.argmin(Tensor([0],"float32"), 0, ) 
 argmin(): Expected reduction dim 0 to have non-zero size.
2025-03-05 15:21:35.434457 test begin: paddle.argmin(Tensor([10, 0],"float32"), )

[torch error] paddle.argmin(Tensor([10, 0],"float32"), ) 
 argmin(): Expected reduction dim to be specified for input.numel() == 0.
2025-03-05 15:21:35.435194 test begin: paddle.argmin(Tensor([10, 0],"float32"), axis=-1, )

[torch error] paddle.argmin(Tensor([10, 0],"float32"), axis=-1, ) 
 argmin(): Expected reduction dim 1 to have non-zero size.
2025-03-05 15:21:35.436244 test begin: paddle.argmin(Tensor([10, 0],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.argmin(Tensor([10, 0],"float32"), axis=-1, keepdim=True, ) 
 argmin(): Expected reduction dim 1 to have non-zero size.
2025-03-05 15:21:35.436968 test begin: paddle.argmin(Tensor([10, 0],"float32"), axis=1, )

[torch error] paddle.argmin(Tensor([10, 0],"float32"), axis=1, ) 
 argmin(): Expected reduction dim 1 to have non-zero size.
2025-03-05 15:21:35.437696 test begin: paddle.argmin(Tensor([3, 0, 3, 3, 3, 3],"float64"), axis=0, )

[paddle error] paddle.argmin(Tensor([3, 0, 3, 3, 3, 3],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.439072 test begin: paddle.argmin(Tensor([3, 0],"float32"), )

[torch error] paddle.argmin(Tensor([3, 0],"float32"), ) 
 argmin(): Expected reduction dim to be specified for input.numel() == 0.
2025-03-05 15:21:35.439775 test begin: paddle.argmin(Tensor([3, 0],"float32"), keepdim=True, )

[torch error] paddle.argmin(Tensor([3, 0],"float32"), keepdim=True, ) 
 argmin(): Expected reduction dim to be specified for input.numel() == 0.
2025-03-05 15:21:35.441047 test begin: paddle.argmin(Tensor([3, 3, 0, 3, 3, 3],"float64"), axis=0, )

[paddle error] paddle.argmin(Tensor([3, 3, 0, 3, 3, 3],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.443335 test begin: paddle.argmin(Tensor([3, 3, 3, 0, 3, 3],"float64"), axis=0, )

[paddle error] paddle.argmin(Tensor([3, 3, 3, 0, 3, 3],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.445053 test begin: paddle.argmin(Tensor([3, 3, 3, 3, 0, 3],"float64"), axis=0, )

[paddle error] paddle.argmin(Tensor([3, 3, 3, 3, 0, 3],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.446653 test begin: paddle.argmin(Tensor([3, 3, 3, 3, 3, 0],"float64"), axis=0, )

[paddle error] paddle.argmin(Tensor([3, 3, 3, 3, 3, 0],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.447885 test begin: paddle.argmin(Tensor([4, 0, 4, 4, 4],"float64"), axis=0, )

[paddle error] paddle.argmin(Tensor([4, 0, 4, 4, 4],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.449028 test begin: paddle.argmin(Tensor([4, 4, 0, 4, 4],"float64"), axis=0, )

[paddle error] paddle.argmin(Tensor([4, 4, 0, 4, 4],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.450107 test begin: paddle.argmin(Tensor([4, 4, 4, 0, 4],"float64"), axis=0, )

[paddle error] paddle.argmin(Tensor([4, 4, 4, 0, 4],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.451659 test begin: paddle.argmin(Tensor([4, 4, 4, 4, 0],"float64"), axis=0, )

[paddle error] paddle.argmin(Tensor([4, 4, 4, 4, 0],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.453485 test begin: paddle.argmin(Tensor([5, 0, 5, 5],"float64"), axis=0, )

[paddle error] paddle.argmin(Tensor([5, 0, 5, 5],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.454825 test begin: paddle.argmin(Tensor([5, 5, 0, 5],"float64"), axis=0, )

[paddle error] paddle.argmin(Tensor([5, 5, 0, 5],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.456884 test begin: paddle.argmin(Tensor([5, 5, 5, 0],"float64"), axis=0, )

[paddle error] paddle.argmin(Tensor([5, 5, 5, 0],"float64"), axis=0, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.458175 test begin: paddle.argmin(x=Tensor([0, 3, 3],"float64"), )

[torch error] paddle.argmin(x=Tensor([0, 3, 3],"float64"), ) 
 argmin(): Expected reduction dim to be specified for input.numel() == 0.
2025-03-05 15:21:35.458910 test begin: paddle.argmin(x=Tensor([0, 3, 4],"float64"), axis=1, keepdim=False, )

[paddle error] paddle.argmin(x=Tensor([0, 3, 4],"float64"), axis=1, keepdim=False, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.460414 test begin: paddle.argmin(x=Tensor([0, 3],"int64"), axis=-1, )

[paddle error] paddle.argmin(x=Tensor([0, 3],"int64"), axis=-1, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.462144 test begin: paddle.argmin(x=Tensor([0, 3],"int64"), axis=-2, )

[torch error] paddle.argmin(x=Tensor([0, 3],"int64"), axis=-2, ) 
 argmin(): Expected reduction dim 0 to have non-zero size.
2025-03-05 15:21:35.462844 test begin: paddle.argmin(x=Tensor([0],"int64"), axis=-1, keepdim=True, )

[torch error] paddle.argmin(x=Tensor([0],"int64"), axis=-1, keepdim=True, ) 
 argmin(): Expected reduction dim 0 to have non-zero size.
2025-03-05 15:21:35.463711 test begin: paddle.argmin(x=Tensor([2, 0],"int64"), axis=-1, )

[torch error] paddle.argmin(x=Tensor([2, 0],"int64"), axis=-1, ) 
 argmin(): Expected reduction dim 1 to have non-zero size.
2025-03-05 15:21:35.464516 test begin: paddle.argmin(x=Tensor([3, 0, 3],"float64"), )

[torch error] paddle.argmin(x=Tensor([3, 0, 3],"float64"), ) 
 argmin(): Expected reduction dim to be specified for input.numel() == 0.
2025-03-05 15:21:35.465285 test begin: paddle.argmin(x=Tensor([3, 0, 4],"float64"), axis=1, keepdim=False, )

[torch error] paddle.argmin(x=Tensor([3, 0, 4],"float64"), axis=1, keepdim=False, ) 
 argmin(): Expected reduction dim 1 to have non-zero size.
2025-03-05 15:21:35.466129 test begin: paddle.argmin(x=Tensor([3, 0],"int64"), axis=-2, )

[paddle error] paddle.argmin(x=Tensor([3, 0],"int64"), axis=-2, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.467555 test begin: paddle.argmin(x=Tensor([3, 3, 0],"float64"), )

[torch error] paddle.argmin(x=Tensor([3, 3, 0],"float64"), ) 
 argmin(): Expected reduction dim to be specified for input.numel() == 0.
2025-03-05 15:21:35.468315 test begin: paddle.argmin(x=Tensor([3, 3, 0],"float64"), axis=1, keepdim=False, )

[paddle error] paddle.argmin(x=Tensor([3, 3, 0],"float64"), axis=1, keepdim=False, ) 
 (InvalidArgument) argmin/argmax input numel must > 0, bug got 0
  [Hint: Expected x.numel() > 0, but received x.numel():0 <= 0:0.] (at ../paddle/phi/kernels/gpu/arg_min_max_kernel.cu:218)

2025-03-05 15:21:35.469909 test begin: paddle.argsort(Tensor([0, 10000],"float64"), axis=1, )

CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1759]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1864]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1953]: invalid configuration argument
[paddle error] paddle.argsort(Tensor([0, 10000],"float64"), axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/phi/kernels/gpu/argsort_kernel.cu:225)

2025-03-05 15:21:46.087991 test begin: paddle.argsort(Tensor([0, 10000],"float64"), axis=1, descending=True, )

W0305 15:21:50.880162 136483 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0305 15:21:50.880952 136483 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1759]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1864]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1953]: invalid configuration argument
[paddle error] paddle.argsort(Tensor([0, 10000],"float64"), axis=1, descending=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/phi/kernels/gpu/argsort_kernel.cu:225)

2025-03-05 15:22:23.933510 test begin: paddle.argsort(Tensor([0, 100],"int64"), axis=1, stable=True, )

W0305 15:22:30.989888 138059 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0305 15:22:30.990742 138059 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1759]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1864]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1953]: invalid configuration argument
[paddle error] paddle.argsort(Tensor([0, 100],"int64"), axis=1, stable=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/phi/kernels/gpu/argsort_kernel.cu:225)

2025-03-05 15:23:25.897486 test begin: paddle.argsort(Tensor([0, 10],"float32"), descending=True, )

W0305 15:23:31.228147 24953 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0305 15:23:31.229288 24953 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1759]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1864]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1953]: invalid configuration argument
[paddle error] paddle.argsort(Tensor([0, 10],"float32"), descending=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/phi/kernels/gpu/argsort_kernel.cu:225)

2025-03-05 15:24:16.782351 test begin: paddle.argsort(Tensor([0, 1],"float64"), axis=0, )

W0305 15:24:22.175184 32140 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0305 15:24:22.176103 32140 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.argsort(Tensor([0, 1],"float64"), axis=0, )
2025-03-05 15:25:01.077874 test begin: paddle.argsort(Tensor([0, 1],"float64"), axis=0, descending=True, )

[Pass] paddle.argsort(Tensor([0, 1],"float64"), axis=0, descending=True, )
2025-03-05 15:25:01.085196 test begin: paddle.argsort(Tensor([0, 1],"float64"), axis=0, descending=True, stable=True, )

[Pass] paddle.argsort(Tensor([0, 1],"float64"), axis=0, descending=True, stable=True, )
2025-03-05 15:25:01.090528 test begin: paddle.argsort(Tensor([0, 1],"float64"), axis=0, stable=True, )

[Pass] paddle.argsort(Tensor([0, 1],"float64"), axis=0, stable=True, )
2025-03-05 15:25:01.095503 test begin: paddle.argsort(Tensor([0, 3, 4],"float64"), axis=0, descending=True, stable=True, )

[Pass] paddle.argsort(Tensor([0, 3, 4],"float64"), axis=0, descending=True, stable=True, )
2025-03-05 15:25:01.099946 test begin: paddle.argsort(Tensor([0, 3, 4],"float64"), axis=0, stable=True, )

[Pass] paddle.argsort(Tensor([0, 3, 4],"float64"), axis=0, stable=True, )
2025-03-05 15:25:01.104121 test begin: paddle.argsort(Tensor([0, 3, 4],"float64"), axis=1, )

CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1759]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1864]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1953]: invalid configuration argument
[paddle error] paddle.argsort(Tensor([0, 3, 4],"float64"), axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/phi/kernels/gpu/argsort_kernel.cu:225)

2025-03-05 15:25:14.680488 test begin: paddle.argsort(Tensor([0, 3, 4],"float64"), axis=1, descending=True, )

W0305 15:25:20.915038 39047 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0305 15:25:20.915851 39047 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1759]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1864]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1953]: invalid configuration argument
[paddle error] paddle.argsort(Tensor([0, 3, 4],"float64"), axis=1, descending=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/phi/kernels/gpu/argsort_kernel.cu:225)

2025-03-05 15:26:34.340758 test begin: paddle.argsort(Tensor([0, 3, 4],"float64"), descending=True, )

W0305 15:26:43.144240 47058 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0305 15:26:43.145226 47058 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1759]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1864]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1953]: invalid configuration argument
[paddle error] paddle.argsort(Tensor([0, 3, 4],"float64"), descending=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/phi/kernels/gpu/argsort_kernel.cu:225)

2025-03-05 15:28:11.152806 test begin: paddle.argsort(Tensor([0, 30],"float64"), axis=1, descending=True, stable=True, )

W0305 15:28:22.288259 59974 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0305 15:28:22.289145 59974 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1759]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1864]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1953]: invalid configuration argument
[paddle error] paddle.argsort(Tensor([0, 30],"float64"), axis=1, descending=True, stable=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/phi/kernels/gpu/argsort_kernel.cu:225)

2025-03-05 15:29:48.816363 test begin: paddle.argsort(Tensor([0, 30],"float64"), axis=1, stable=True, )

W0305 15:30:05.935592 68922 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0305 15:30:05.936702 68922 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1759]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1864]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1953]: invalid configuration argument
[paddle error] paddle.argsort(Tensor([0, 30],"float64"), axis=1, stable=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/phi/kernels/gpu/argsort_kernel.cu:225)

2025-03-05 15:31:50.631806 test begin: paddle.argsort(Tensor([0, 4, 2],"int64"), axis=0, stable=True, )

W0305 15:32:03.477959 81584 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0305 15:32:03.478878 81584 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.argsort(Tensor([0, 4, 2],"int64"), axis=0, stable=True, )
2025-03-05 15:32:46.868857 test begin: paddle.argsort(Tensor([0, 4, 2],"int64"), axis=1, stable=True, )

CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1759]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1864]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1953]: invalid configuration argument
[paddle error] paddle.argsort(Tensor([0, 4, 2],"int64"), axis=1, stable=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/phi/kernels/gpu/argsort_kernel.cu:225)

2025-03-05 15:33:00.123568 test begin: paddle.argsort(Tensor([0, 4, 2],"int64"), axis=2, stable=True, )

W0305 15:33:06.114998 88725 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0305 15:33:06.115862 88725 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1759]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1864]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1953]: invalid configuration argument
[paddle error] paddle.argsort(Tensor([0, 4, 2],"int64"), axis=2, stable=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/phi/kernels/gpu/argsort_kernel.cu:225)

2025-03-05 15:33:45.484501 test begin: paddle.argsort(Tensor([0, 4],"int64"), axis=0, stable=True, )

W0305 15:33:51.327788 93288 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0305 15:33:51.328747 93288 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.argsort(Tensor([0, 4],"int64"), axis=0, stable=True, )
2025-03-05 15:34:23.530072 test begin: paddle.argsort(Tensor([0, 5],"float32"), descending=True, )

CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1759]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1864]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1953]: invalid configuration argument
[paddle error] paddle.argsort(Tensor([0, 5],"float32"), descending=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/phi/kernels/gpu/argsort_kernel.cu:225)

2025-03-05 15:34:35.817776 test begin: paddle.argsort(Tensor([0, 64],"int64"), axis=-1, )

W0305 15:34:42.401885 98575 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0305 15:34:42.402837 98575 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1759]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1864]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1953]: invalid configuration argument
[paddle error] paddle.argsort(Tensor([0, 64],"int64"), axis=-1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/phi/kernels/gpu/argsort_kernel.cu:225)

2025-03-05 15:35:18.503577 test begin: paddle.argsort(Tensor([0],"float16"), stable=True, )

W0305 15:35:24.091313 103184 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0305 15:35:24.092185 103184 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.argsort(Tensor([0],"float16"), stable=True, )
2025-03-05 15:35:52.538721 test begin: paddle.argsort(Tensor([0],"float32"), )

[Pass] paddle.argsort(Tensor([0],"float32"), )
2025-03-05 15:35:52.544190 test begin: paddle.argsort(Tensor([0],"float32"), descending=True, )

[Pass] paddle.argsort(Tensor([0],"float32"), descending=True, )
2025-03-05 15:35:52.547931 test begin: paddle.argsort(Tensor([0],"float32"), stable=True, )

[Pass] paddle.argsort(Tensor([0],"float32"), stable=True, )
2025-03-05 15:35:52.551100 test begin: paddle.argsort(Tensor([0],"float64"), axis=0, )

[Pass] paddle.argsort(Tensor([0],"float64"), axis=0, )
2025-03-05 15:35:52.557983 test begin: paddle.argsort(Tensor([0],"float64"), axis=0, descending=True, )

[Pass] paddle.argsort(Tensor([0],"float64"), axis=0, descending=True, )
2025-03-05 15:35:52.561765 test begin: paddle.argsort(Tensor([0],"float64"), axis=0, descending=True, stable=True, )

[Pass] paddle.argsort(Tensor([0],"float64"), axis=0, descending=True, stable=True, )
2025-03-05 15:35:52.566998 test begin: paddle.argsort(Tensor([0],"float64"), axis=0, stable=True, )

[Pass] paddle.argsort(Tensor([0],"float64"), axis=0, stable=True, )
2025-03-05 15:35:52.570455 test begin: paddle.argsort(Tensor([0],"int64"), )

[Pass] paddle.argsort(Tensor([0],"int64"), )
2025-03-05 15:35:52.573569 test begin: paddle.argsort(Tensor([1, 0],"float64"), axis=1, )

[Pass] paddle.argsort(Tensor([1, 0],"float64"), axis=1, )
2025-03-05 15:35:52.576464 test begin: paddle.argsort(Tensor([1, 0],"float64"), axis=1, descending=True, )

[Pass] paddle.argsort(Tensor([1, 0],"float64"), axis=1, descending=True, )
2025-03-05 15:35:52.579533 test begin: paddle.argsort(Tensor([1, 0],"float64"), axis=1, descending=True, stable=True, )

[Pass] paddle.argsort(Tensor([1, 0],"float64"), axis=1, descending=True, stable=True, )
2025-03-05 15:35:52.582520 test begin: paddle.argsort(Tensor([1, 0],"float64"), axis=1, stable=True, )

[Pass] paddle.argsort(Tensor([1, 0],"float64"), axis=1, stable=True, )
2025-03-05 15:35:52.585594 test begin: paddle.argsort(Tensor([1000, 0],"float32"), descending=True, )

[Pass] paddle.argsort(Tensor([1000, 0],"float32"), descending=True, )
2025-03-05 15:35:52.588520 test begin: paddle.argsort(Tensor([10000, 0],"float64"), axis=0, )

CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1759]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1864]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1953]: invalid configuration argument
[paddle error] paddle.argsort(Tensor([10000, 0],"float64"), axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/phi/kernels/gpu/argsort_kernel.cu:225)

2025-03-05 15:36:05.159068 test begin: paddle.argsort(Tensor([10000, 0],"float64"), axis=0, descending=True, )

W0305 15:36:10.613204 108348 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0305 15:36:10.614287 108348 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1759]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1864]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1953]: invalid configuration argument
[paddle error] paddle.argsort(Tensor([10000, 0],"float64"), axis=0, descending=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/phi/kernels/gpu/argsort_kernel.cu:225)

2025-03-05 15:36:51.772388 test begin: paddle.argsort(Tensor([2, 0, 4],"float64"), axis=1, )

W0305 15:36:57.270596 113553 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0305 15:36:57.271508 113553 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.argsort(Tensor([2, 0, 4],"float64"), axis=1, )
2025-03-05 15:37:30.587231 test begin: paddle.argsort(Tensor([2, 0, 4],"float64"), axis=1, descending=True, )

[Pass] paddle.argsort(Tensor([2, 0, 4],"float64"), axis=1, descending=True, )
2025-03-05 15:37:30.597312 test begin: paddle.argsort(Tensor([2, 0, 4],"float64"), descending=True, )

CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1759]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1864]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1953]: invalid configuration argument
[paddle error] paddle.argsort(Tensor([2, 0, 4],"float64"), descending=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/phi/kernels/gpu/argsort_kernel.cu:225)

2025-03-05 15:37:43.960785 test begin: paddle.argsort(Tensor([2, 0],"int64"), axis=1, stable=True, )

W0305 15:37:51.082484 118826 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0305 15:37:51.083442 118826 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.argsort(Tensor([2, 0],"int64"), axis=1, stable=True, )
2025-03-05 15:38:26.398172 test begin: paddle.argsort(Tensor([2, 3, 0],"float64"), axis=1, )

CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1759]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1864]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1953]: invalid configuration argument
[paddle error] paddle.argsort(Tensor([2, 3, 0],"float64"), axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/phi/kernels/gpu/argsort_kernel.cu:225)

2025-03-05 15:39:11.430837 test begin: paddle.argsort(Tensor([2, 3, 0],"float64"), axis=1, descending=True, )

W0305 15:39:16.695104 124839 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0305 15:39:16.695987 124839 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1759]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1864]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1953]: invalid configuration argument
[paddle error] paddle.argsort(Tensor([2, 3, 0],"float64"), axis=1, descending=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/phi/kernels/gpu/argsort_kernel.cu:225)

2025-03-05 15:39:49.205923 test begin: paddle.argsort(Tensor([2, 3, 0],"float64"), descending=True, )

W0305 15:39:55.209434 125869 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0305 15:39:55.210239 125869 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.argsort(Tensor([2, 3, 0],"float64"), descending=True, )
2025-03-05 15:40:15.933248 test begin: paddle.argsort(Tensor([26, 0],"int64"), axis=-1, )

[Pass] paddle.argsort(Tensor([26, 0],"int64"), axis=-1, )
2025-03-05 15:40:15.937255 test begin: paddle.argsort(Tensor([3, 0, 2],"int64"), axis=0, stable=True, )

CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1759]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1864]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1953]: invalid configuration argument
[paddle error] paddle.argsort(Tensor([3, 0, 2],"int64"), axis=0, stable=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/phi/kernels/gpu/argsort_kernel.cu:225)

2025-03-05 15:40:26.852907 test begin: paddle.argsort(Tensor([3, 0, 2],"int64"), axis=1, stable=True, )

W0305 15:40:31.611351 129073 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0305 15:40:31.612253 129073 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.argsort(Tensor([3, 0, 2],"int64"), axis=1, stable=True, )
2025-03-05 15:40:54.612847 test begin: paddle.argsort(Tensor([3, 0, 2],"int64"), axis=2, stable=True, )

CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1759]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1864]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1953]: invalid configuration argument
[paddle error] paddle.argsort(Tensor([3, 0, 2],"int64"), axis=2, stable=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/phi/kernels/gpu/argsort_kernel.cu:225)

2025-03-05 15:41:05.055448 test begin: paddle.argsort(Tensor([3, 0],"int64"), axis=0, stable=True, )

W0305 15:41:10.337831 138574 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0305 15:41:10.338699 138574 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1759]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1864]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1953]: invalid configuration argument
[paddle error] paddle.argsort(Tensor([3, 0],"int64"), axis=0, stable=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/phi/kernels/gpu/argsort_kernel.cu:225)

2025-03-05 15:41:46.443145 test begin: paddle.argsort(Tensor([3, 4, 0],"int64"), axis=0, stable=True, )

W0305 15:41:51.763803 143068 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0305 15:41:51.764812 143068 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1759]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1864]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1953]: invalid configuration argument
[paddle error] paddle.argsort(Tensor([3, 4, 0],"int64"), axis=0, stable=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/phi/kernels/gpu/argsort_kernel.cu:225)

2025-03-05 15:42:29.090830 test begin: paddle.argsort(Tensor([3, 4, 0],"int64"), axis=1, stable=True, )

W0305 15:42:34.473687 148115 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0305 15:42:34.474565 148115 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1759]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1864]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1953]: invalid configuration argument
[paddle error] paddle.argsort(Tensor([3, 4, 0],"int64"), axis=1, stable=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/phi/kernels/gpu/argsort_kernel.cu:225)

2025-03-05 15:43:14.315454 test begin: paddle.argsort(Tensor([3, 4, 0],"int64"), axis=2, stable=True, )

W0305 15:43:19.920044 155921 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0305 15:43:19.920974 155921 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.argsort(Tensor([3, 4, 0],"int64"), axis=2, stable=True, )
2025-03-05 15:43:47.347167 test begin: paddle.argsort(Tensor([30, 0],"float64"), axis=0, descending=True, stable=True, )

CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1759]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1864]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1953]: invalid configuration argument
[paddle error] paddle.argsort(Tensor([30, 0],"float64"), axis=0, descending=True, stable=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/phi/kernels/gpu/argsort_kernel.cu:225)

2025-03-05 15:43:57.386330 test begin: paddle.argsort(Tensor([30, 0],"float64"), axis=0, stable=True, )

W0305 15:44:02.972852 160302 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0305 15:44:02.973968 160302 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1759]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1864]: invalid configuration argument
CUDA error 9 [/usr/local/cuda/targets/x86_64-linux/include/cub/device/dispatch/dispatch_radix_sort.cuh, 1953]: invalid configuration argument
[paddle error] paddle.argsort(Tensor([30, 0],"float64"), axis=0, stable=True, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/phi/kernels/gpu/argsort_kernel.cu:225)

2025-03-05 15:44:48.296249 test begin: paddle.argsort(Tensor([40, 0, 4],"float64"), axis=0, descending=True, stable=True, )

[Skip]
2025-03-05 15:44:48.297081 test begin: paddle.argsort(Tensor([40, 0, 4],"float64"), axis=0, stable=True, )

[Skip]
2025-03-05 15:44:48.297403 test begin: paddle.argsort(Tensor([40, 3, 0],"float64"), axis=0, descending=True, stable=True, )

[Skip]
2025-03-05 15:44:48.297740 test begin: paddle.argsort(Tensor([40, 3, 0],"float64"), axis=0, stable=True, )

[Skip]
2025-03-05 15:44:48.298351 test begin: paddle.argsort(x=Tensor([0, 2, 2],"float32"), axis=0, )

[Skip]
2025-03-05 15:44:48.298686 test begin: paddle.argsort(x=Tensor([0, 2, 2],"float64"), axis=-1, )

[Skip]
2025-03-05 15:44:48.299072 test begin: paddle.argsort(x=Tensor([0, 2, 2],"float64"), axis=0, )

[Skip]
2025-03-05 15:44:48.299346 test begin: paddle.argsort(x=Tensor([0, 2, 2],"float64"), axis=0, descending=True, )

[Skip]
2025-03-05 15:44:48.299672 test begin: paddle.argsort(x=Tensor([0, 2, 2],"float64"), axis=1, )

[Skip]
2025-03-05 15:44:48.299939 test begin: paddle.argsort(x=Tensor([3, 0, 2],"float32"), axis=0, )

[Skip]
2025-03-05 15:44:48.300194 test begin: paddle.argsort(x=Tensor([3, 0, 2],"float64"), axis=-1, )

[Skip]
2025-03-05 15:44:48.300374 test begin: paddle.argsort(x=Tensor([3, 0, 2],"float64"), axis=0, )

[Skip]
2025-03-05 15:44:48.300537 test begin: paddle.argsort(x=Tensor([3, 0, 2],"float64"), axis=0, descending=True, )

[Skip]
2025-03-05 15:44:48.300750 test begin: paddle.argsort(x=Tensor([3, 0, 2],"float64"), axis=1, )

[Skip]
2025-03-05 15:44:48.301365 test begin: paddle.argsort(x=Tensor([3, 2, 0],"float32"), axis=0, )

[Skip]
2025-03-05 15:44:48.301589 test begin: paddle.argsort(x=Tensor([3, 2, 0],"float64"), axis=-1, )

[Skip]
2025-03-05 15:44:48.302054 test begin: paddle.argsort(x=Tensor([3, 2, 0],"float64"), axis=0, )

[Skip]
2025-03-05 15:44:48.302244 test begin: paddle.argsort(x=Tensor([3, 2, 0],"float64"), axis=0, descending=True, )

[Skip]
2025-03-05 15:44:48.302517 test begin: paddle.argsort(x=Tensor([3, 2, 0],"float64"), axis=1, )

[Skip]
2025-03-05 15:44:48.302813 test begin: paddle.as_complex(Tensor([0, 10, 10, 2],"float32"), )

W0305 15:44:51.461426  1542 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0305 15:44:51.462944  1542 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.as_complex(Tensor([0, 10, 10, 2],"float32"), ) 
 Tensor must have a last dimension with stride 1
2025-03-05 15:45:21.527066 test begin: paddle.as_complex(Tensor([0, 10, 2],"float64"), )

[torch error] paddle.as_complex(Tensor([0, 10, 2],"float64"), ) 
 Tensor must have a last dimension with stride 1
2025-03-05 15:45:21.529627 test begin: paddle.as_complex(Tensor([0, 4, 2],"float32"), )

[torch error] paddle.as_complex(Tensor([0, 4, 2],"float32"), ) 
 Tensor must have a last dimension with stride 1
2025-03-05 15:45:21.530693 test begin: paddle.as_complex(Tensor([0],"float32"), )

[torch error] paddle.as_complex(Tensor([0],"float32"), ) 
 Tensor must have a last dimension of size 2
2025-03-05 15:45:21.531549 test begin: paddle.as_complex(Tensor([10, 0, 10, 2],"float32"), )

[torch error] paddle.as_complex(Tensor([10, 0, 10, 2],"float32"), ) 
 Tensor must have a last dimension with stride 1
2025-03-05 15:45:21.532723 test begin: paddle.as_complex(Tensor([10, 0, 2],"float64"), )

[torch error] paddle.as_complex(Tensor([10, 0, 2],"float64"), ) 
 Tensor must have a last dimension with stride 1
2025-03-05 15:45:21.533317 test begin: paddle.as_complex(Tensor([10, 10, 0, 2],"float32"), )

[torch error] paddle.as_complex(Tensor([10, 10, 0, 2],"float32"), ) 
 Tensor must have a last dimension with stride 1
2025-03-05 15:45:21.533873 test begin: paddle.as_complex(Tensor([10, 10, 0],"float64"), )

[torch error] paddle.as_complex(Tensor([10, 10, 0],"float64"), ) 
 Tensor must have a last dimension of size 2
2025-03-05 15:45:21.534539 test begin: paddle.as_complex(Tensor([10, 10, 10, 0],"float32"), )

[torch error] paddle.as_complex(Tensor([10, 10, 10, 0],"float32"), ) 
 Tensor must have a last dimension of size 2
2025-03-05 15:45:21.535451 test begin: paddle.as_complex(Tensor([2, 0, 2],"float32"), )

[torch error] paddle.as_complex(Tensor([2, 0, 2],"float32"), ) 
 Tensor must have a last dimension with stride 1
2025-03-05 15:45:21.536155 test begin: paddle.as_complex(Tensor([2, 4, 0],"float32"), )

[torch error] paddle.as_complex(Tensor([2, 4, 0],"float32"), ) 
 Tensor must have a last dimension of size 2
2025-03-05 15:45:21.537027 test begin: paddle.as_complex(x=Tensor([0, 2, 3, 2],"float64"), )

[torch error] paddle.as_complex(x=Tensor([0, 2, 3, 2],"float64"), ) 
 Tensor must have a last dimension with stride 1
2025-03-05 15:45:21.537739 test begin: paddle.as_complex(x=Tensor([0, 2],"float32"), )

[torch error] paddle.as_complex(x=Tensor([0, 2],"float32"), ) 
 Tensor must have a last dimension with stride 1
2025-03-05 15:45:21.538554 test begin: paddle.as_complex(x=Tensor([0, 2],"float64"), )

[torch error] paddle.as_complex(x=Tensor([0, 2],"float64"), ) 
 Tensor must have a last dimension with stride 1
2025-03-05 15:45:21.539267 test begin: paddle.as_complex(x=Tensor([0, 3, 2],"float64"), )

[torch error] paddle.as_complex(x=Tensor([0, 3, 2],"float64"), ) 
 Tensor must have a last dimension with stride 1
2025-03-05 15:45:21.540354 test begin: paddle.as_complex(x=Tensor([3, 0],"float32"), )

[torch error] paddle.as_complex(x=Tensor([3, 0],"float32"), ) 
 Tensor must have a last dimension of size 2
2025-03-05 15:45:21.541280 test begin: paddle.as_complex(x=Tensor([3, 0],"float64"), )

[torch error] paddle.as_complex(x=Tensor([3, 0],"float64"), ) 
 Tensor must have a last dimension of size 2
2025-03-05 15:45:21.542868 test begin: paddle.as_complex(x=Tensor([9, 0, 2],"float64"), )

[torch error] paddle.as_complex(x=Tensor([9, 0, 2],"float64"), ) 
 Tensor must have a last dimension with stride 1
2025-03-05 15:45:21.552703 test begin: paddle.as_complex(x=Tensor([9, 0, 3, 2],"float64"), )

[torch error] paddle.as_complex(x=Tensor([9, 0, 3, 2],"float64"), ) 
 Tensor must have a last dimension with stride 1
2025-03-05 15:45:21.562213 test begin: paddle.as_complex(x=Tensor([9, 2, 0, 2],"float64"), )

[torch error] paddle.as_complex(x=Tensor([9, 2, 0, 2],"float64"), ) 
 Tensor must have a last dimension with stride 1
2025-03-05 15:45:21.570054 test begin: paddle.as_complex(x=Tensor([9, 2, 3, 0],"float64"), )

[torch error] paddle.as_complex(x=Tensor([9, 2, 3, 0],"float64"), ) 
 Tensor must have a last dimension of size 2
2025-03-05 15:45:21.571332 test begin: paddle.as_complex(x=Tensor([9, 3, 0],"float64"), )

[torch error] paddle.as_complex(x=Tensor([9, 3, 0],"float64"), ) 
 Tensor must have a last dimension of size 2
2025-03-05 15:45:21.572182 test begin: paddle.as_real(Tensor([0, 10, 10, 20],"complex128"), )

[Pass] paddle.as_real(Tensor([0, 10, 10, 20],"complex128"), )
2025-03-05 15:45:21.582036 test begin: paddle.as_real(Tensor([0, 10],"complex128"), )

[Pass] paddle.as_real(Tensor([0, 10],"complex128"), )
2025-03-05 15:45:21.585077 test begin: paddle.as_real(Tensor([0, 4],"complex128"), )

[Pass] paddle.as_real(Tensor([0, 4],"complex128"), )
2025-03-05 15:45:21.587486 test begin: paddle.as_real(Tensor([10, 0, 10, 20],"complex128"), )

[Pass] paddle.as_real(Tensor([10, 0, 10, 20],"complex128"), )
2025-03-05 15:45:21.589741 test begin: paddle.as_real(Tensor([10, 0],"complex128"), )

[Pass] paddle.as_real(Tensor([10, 0],"complex128"), )
2025-03-05 15:45:21.591855 test begin: paddle.as_real(Tensor([10, 10, 0, 20],"complex128"), )

[Pass] paddle.as_real(Tensor([10, 10, 0, 20],"complex128"), )
2025-03-05 15:45:21.594052 test begin: paddle.as_real(Tensor([10, 10, 10, 0],"complex128"), )

[Pass] paddle.as_real(Tensor([10, 10, 10, 0],"complex128"), )
2025-03-05 15:45:21.596407 test begin: paddle.as_real(Tensor([2, 0],"complex128"), )

[Pass] paddle.as_real(Tensor([2, 0],"complex128"), )
2025-03-05 15:45:21.598772 test begin: paddle.as_real(x=Tensor([0, 2, 3],"complex128"), )

[Pass] paddle.as_real(x=Tensor([0, 2, 3],"complex128"), )
2025-03-05 15:45:21.600994 test begin: paddle.as_real(x=Tensor([0, 2, 3],"complex64"), )

[Pass] paddle.as_real(x=Tensor([0, 2, 3],"complex64"), )
2025-03-05 15:45:21.606079 test begin: paddle.as_real(x=Tensor([0, 3],"complex128"), )

[Pass] paddle.as_real(x=Tensor([0, 3],"complex128"), )
2025-03-05 15:45:21.609715 test begin: paddle.as_real(x=Tensor([0, 3],"complex64"), )

[Pass] paddle.as_real(x=Tensor([0, 3],"complex64"), )
2025-03-05 15:45:21.612543 test begin: paddle.as_real(x=Tensor([0],"complex128"), )

[Pass] paddle.as_real(x=Tensor([0],"complex128"), )
2025-03-05 15:45:21.618690 test begin: paddle.as_real(x=Tensor([0],"complex64"), )

[Pass] paddle.as_real(x=Tensor([0],"complex64"), )
2025-03-05 15:45:21.621982 test begin: paddle.as_real(x=Tensor([9, 0, 3],"complex128"), )

[Pass] paddle.as_real(x=Tensor([9, 0, 3],"complex128"), )
2025-03-05 15:45:21.624291 test begin: paddle.as_real(x=Tensor([9, 0, 3],"complex64"), )

[Pass] paddle.as_real(x=Tensor([9, 0, 3],"complex64"), )
2025-03-05 15:45:21.627226 test begin: paddle.as_real(x=Tensor([9, 0],"complex128"), )

[Pass] paddle.as_real(x=Tensor([9, 0],"complex128"), )
2025-03-05 15:45:21.640426 test begin: paddle.as_real(x=Tensor([9, 0],"complex64"), )

[Pass] paddle.as_real(x=Tensor([9, 0],"complex64"), )
2025-03-05 15:45:21.643016 test begin: paddle.as_real(x=Tensor([9, 2, 0],"complex128"), )

[Pass] paddle.as_real(x=Tensor([9, 2, 0],"complex128"), )
2025-03-05 15:45:21.645294 test begin: paddle.as_real(x=Tensor([9, 2, 0],"complex64"), )

[Pass] paddle.as_real(x=Tensor([9, 2, 0],"complex64"), )
2025-03-05 15:45:21.658320 test begin: paddle.asin(Tensor([0, 16, 32],"complex128"), )

[cuda error] paddle.asin(Tensor([0, 16, 32],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:21.671012 test begin: paddle.asin(Tensor([0, 20, 1],"float32"), )

[cuda error] paddle.asin(Tensor([0, 20, 1],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:21.683086 test begin: paddle.asin(Tensor([10, 0, 1],"float32"), )

[cuda error] paddle.asin(Tensor([10, 0, 1],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:21.701173 test begin: paddle.asin(Tensor([10, 20, 0],"float32"), )

[cuda error] paddle.asin(Tensor([10, 20, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:21.719859 test begin: paddle.asin(Tensor([8, 0, 32],"complex128"), )

[cuda error] paddle.asin(Tensor([8, 0, 32],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:21.737008 test begin: paddle.asin(Tensor([8, 16, 0],"complex128"), )

[cuda error] paddle.asin(Tensor([8, 16, 0],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:21.742322 test begin: paddle.asin(x=Tensor([0, 3, 3],"float32"), )

[cuda error] paddle.asin(x=Tensor([0, 3, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:21.749632 test begin: paddle.asin(x=Tensor([0, 3, 3],"float64"), )

[cuda error] paddle.asin(x=Tensor([0, 3, 3],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:21.752345 test begin: paddle.asin(x=Tensor([3, 0, 3],"float32"), )

[cuda error] paddle.asin(x=Tensor([3, 0, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:21.758743 test begin: paddle.asin(x=Tensor([3, 0, 3],"float64"), )

[cuda error] paddle.asin(x=Tensor([3, 0, 3],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:21.766844 test begin: paddle.asin(x=Tensor([3, 3, 0],"float32"), )

[cuda error] paddle.asin(x=Tensor([3, 3, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:21.810677 test begin: paddle.asin(x=Tensor([3, 3, 0],"float64"), )

[cuda error] paddle.asin(x=Tensor([3, 3, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:21.826691 test begin: paddle.asinh(Tensor([0, 16, 32],"complex128"), )

[cuda error] paddle.asinh(Tensor([0, 16, 32],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:21.840138 test begin: paddle.asinh(Tensor([0, 20, 1],"float32"), )

[cuda error] paddle.asinh(Tensor([0, 20, 1],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:21.848768 test begin: paddle.asinh(Tensor([10, 0, 1],"float32"), )

[cuda error] paddle.asinh(Tensor([10, 0, 1],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:21.873700 test begin: paddle.asinh(Tensor([10, 20, 0],"float32"), )

[cuda error] paddle.asinh(Tensor([10, 20, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:21.877032 test begin: paddle.asinh(Tensor([8, 0, 32],"complex128"), )

[cuda error] paddle.asinh(Tensor([8, 0, 32],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:21.879163 test begin: paddle.asinh(Tensor([8, 16, 0],"complex128"), )

[cuda error] paddle.asinh(Tensor([8, 16, 0],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:21.881034 test begin: paddle.atan(Tensor([0, 16, 32],"complex128"), )

[cuda error] paddle.atan(Tensor([0, 16, 32],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:21.882923 test begin: paddle.atan(Tensor([0, 20, 1],"float32"), )

[cuda error] paddle.atan(Tensor([0, 20, 1],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:21.884694 test begin: paddle.atan(Tensor([0],"float32"), name="Cauchy_cdf", )

[cuda error] paddle.atan(Tensor([0],"float32"), name="Cauchy_cdf", ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:21.886464 test begin: paddle.atan(Tensor([0],"float64"), name="Cauchy_cdf", )

[cuda error] paddle.atan(Tensor([0],"float64"), name="Cauchy_cdf", ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:21.888296 test begin: paddle.atan(Tensor([10, 0, 1],"float32"), )

[cuda error] paddle.atan(Tensor([10, 0, 1],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:21.889961 test begin: paddle.atan(Tensor([10, 20, 0],"float32"), )

[cuda error] paddle.atan(Tensor([10, 20, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:21.891834 test begin: paddle.atan(Tensor([8, 0, 32],"complex128"), )

[cuda error] paddle.atan(Tensor([8, 0, 32],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:21.893526 test begin: paddle.atan(Tensor([8, 16, 0],"complex128"), )

[cuda error] paddle.atan(Tensor([8, 16, 0],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:21.895215 test begin: paddle.atan(x=Tensor([0, 3, 3],"float32"), )

[cuda error] paddle.atan(x=Tensor([0, 3, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:21.896893 test begin: paddle.atan(x=Tensor([0, 3, 3],"float64"), )

[cuda error] paddle.atan(x=Tensor([0, 3, 3],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:21.898572 test begin: paddle.atan(x=Tensor([3, 0, 3],"float32"), )

[cuda error] paddle.atan(x=Tensor([3, 0, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:21.900281 test begin: paddle.atan(x=Tensor([3, 0, 3],"float64"), )

[cuda error] paddle.atan(x=Tensor([3, 0, 3],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:21.901984 test begin: paddle.atan(x=Tensor([3, 3, 0],"float32"), )

[cuda error] paddle.atan(x=Tensor([3, 3, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:21.904957 test begin: paddle.atan(x=Tensor([3, 3, 0],"float64"), )

[cuda error] paddle.atan(x=Tensor([3, 3, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:21.907171 test begin: paddle.atan2(Tensor([0, 17],"float64"), Tensor([0, 17],"float64"), )

[Pass] paddle.atan2(Tensor([0, 17],"float64"), Tensor([0, 17],"float64"), )
2025-03-05 15:45:21.913711 test begin: paddle.atan2(Tensor([0, 17],"float64"), Tensor([11, 17],"float64"), )

[torch error] paddle.atan2(Tensor([0, 17],"float64"), Tensor([11, 17],"float64"), ) 
 The size of tensor a (0) must match the size of tensor b (11) at non-singleton dimension 0
2025-03-05 15:45:21.915594 test begin: paddle.atan2(Tensor([0, 222, 333],"float64"), Tensor([222, 333],"float64"), )

[Pass] paddle.atan2(Tensor([0, 222, 333],"float64"), Tensor([222, 333],"float64"), )
2025-03-05 15:45:21.920036 test begin: paddle.atan2(Tensor([0, 3, 2],"float16"), Tensor([0, 3, 2],"float32"), )

W0305 15:45:21.921424  6574 dygraph_functions.cc:9455] got different data type, run type promotion automatically, this may cause data type been changed.
[Pass] paddle.atan2(Tensor([0, 3, 2],"float16"), Tensor([0, 3, 2],"float32"), )
2025-03-05 15:45:21.922224 test begin: paddle.atan2(Tensor([0, 3, 2],"float16"), Tensor([0, 3, 2],"float64"), )

[Pass] paddle.atan2(Tensor([0, 3, 2],"float16"), Tensor([0, 3, 2],"float64"), )
2025-03-05 15:45:21.924532 test begin: paddle.atan2(Tensor([0, 3, 2],"float16"), Tensor([4, 3, 2],"float32"), )

[torch error] paddle.atan2(Tensor([0, 3, 2],"float16"), Tensor([4, 3, 2],"float32"), ) 
 The size of tensor a (0) must match the size of tensor b (4) at non-singleton dimension 0
2025-03-05 15:45:21.925699 test begin: paddle.atan2(Tensor([0, 3, 2],"float16"), Tensor([4, 3, 2],"float64"), )

[torch error] paddle.atan2(Tensor([0, 3, 2],"float16"), Tensor([4, 3, 2],"float64"), ) 
 The size of tensor a (0) must match the size of tensor b (4) at non-singleton dimension 0
2025-03-05 15:45:21.926441 test begin: paddle.atan2(Tensor([0],"float64"), Tensor([100, 100],"float64"), )

[torch error] paddle.atan2(Tensor([0],"float64"), Tensor([100, 100],"float64"), ) 
 The size of tensor a (0) must match the size of tensor b (100) at non-singleton dimension 1
2025-03-05 15:45:21.927278 test begin: paddle.atan2(Tensor([100],"float64"), Tensor([0, 100],"float64"), )

[Pass] paddle.atan2(Tensor([100],"float64"), Tensor([0, 100],"float64"), )
2025-03-05 15:45:21.929638 test begin: paddle.atan2(Tensor([100],"float64"), Tensor([100, 0],"float64"), )

[torch error] paddle.atan2(Tensor([100],"float64"), Tensor([100, 0],"float64"), ) 
 The size of tensor a (100) must match the size of tensor b (0) at non-singleton dimension 1
2025-03-05 15:45:21.930532 test begin: paddle.atan2(Tensor([11, 0],"float64"), Tensor([11, 0],"float64"), )

[Pass] paddle.atan2(Tensor([11, 0],"float64"), Tensor([11, 0],"float64"), )
2025-03-05 15:45:21.932950 test begin: paddle.atan2(Tensor([11, 0],"float64"), Tensor([11, 17],"float64"), )

[torch error] paddle.atan2(Tensor([11, 0],"float64"), Tensor([11, 17],"float64"), ) 
 The size of tensor a (0) must match the size of tensor b (17) at non-singleton dimension 1
2025-03-05 15:45:21.933954 test begin: paddle.atan2(Tensor([11, 17],"float64"), Tensor([0, 17],"float64"), )

[torch error] paddle.atan2(Tensor([11, 17],"float64"), Tensor([0, 17],"float64"), ) 
 The size of tensor a (11) must match the size of tensor b (0) at non-singleton dimension 0
2025-03-05 15:45:21.934715 test begin: paddle.atan2(Tensor([11, 17],"float64"), Tensor([11, 0],"float64"), )

[torch error] paddle.atan2(Tensor([11, 17],"float64"), Tensor([11, 0],"float64"), ) 
 The size of tensor a (17) must match the size of tensor b (0) at non-singleton dimension 1
2025-03-05 15:45:21.935446 test begin: paddle.atan2(Tensor([111, 0, 333],"float64"), Tensor([222, 333],"float64"), )

[torch error] paddle.atan2(Tensor([111, 0, 333],"float64"), Tensor([222, 333],"float64"), ) 
 The size of tensor a (0) must match the size of tensor b (222) at non-singleton dimension 1
2025-03-05 15:45:21.936976 test begin: paddle.atan2(Tensor([111, 222, 0],"float64"), Tensor([222, 333],"float64"), )

[torch error] paddle.atan2(Tensor([111, 222, 0],"float64"), Tensor([222, 333],"float64"), ) 
 The size of tensor a (0) must match the size of tensor b (333) at non-singleton dimension 2
2025-03-05 15:45:21.938445 test begin: paddle.atan2(Tensor([111, 222, 333],"float64"), Tensor([0, 333],"float64"), )

[torch error] paddle.atan2(Tensor([111, 222, 333],"float64"), Tensor([0, 333],"float64"), ) 
 The size of tensor a (222) must match the size of tensor b (0) at non-singleton dimension 1
2025-03-05 15:45:22.087863 test begin: paddle.atan2(Tensor([111, 222, 333],"float64"), Tensor([222, 0],"float64"), )

[torch error] paddle.atan2(Tensor([111, 222, 333],"float64"), Tensor([222, 0],"float64"), ) 
 The size of tensor a (333) must match the size of tensor b (0) at non-singleton dimension 2
2025-03-05 15:45:22.237195 test begin: paddle.atan2(Tensor([4, 0, 2],"float16"), Tensor([4, 0, 2],"float32"), )

[Pass] paddle.atan2(Tensor([4, 0, 2],"float16"), Tensor([4, 0, 2],"float32"), )
2025-03-05 15:45:22.245349 test begin: paddle.atan2(Tensor([4, 0, 2],"float16"), Tensor([4, 0, 2],"float64"), )

[Pass] paddle.atan2(Tensor([4, 0, 2],"float16"), Tensor([4, 0, 2],"float64"), )
2025-03-05 15:45:22.252765 test begin: paddle.atan2(Tensor([4, 0, 2],"float16"), Tensor([4, 3, 2],"float32"), )

[torch error] paddle.atan2(Tensor([4, 0, 2],"float16"), Tensor([4, 3, 2],"float32"), ) 
 The size of tensor a (0) must match the size of tensor b (3) at non-singleton dimension 1
2025-03-05 15:45:22.255759 test begin: paddle.atan2(Tensor([4, 0, 2],"float16"), Tensor([4, 3, 2],"float64"), )

[torch error] paddle.atan2(Tensor([4, 0, 2],"float16"), Tensor([4, 3, 2],"float64"), ) 
 The size of tensor a (0) must match the size of tensor b (3) at non-singleton dimension 1
2025-03-05 15:45:22.256471 test begin: paddle.atan2(Tensor([4, 3, 0],"float16"), Tensor([4, 3, 0],"float32"), )

[Pass] paddle.atan2(Tensor([4, 3, 0],"float16"), Tensor([4, 3, 0],"float32"), )
2025-03-05 15:45:22.267326 test begin: paddle.atan2(Tensor([4, 3, 0],"float16"), Tensor([4, 3, 0],"float64"), )

[Pass] paddle.atan2(Tensor([4, 3, 0],"float16"), Tensor([4, 3, 0],"float64"), )
2025-03-05 15:45:22.272486 test begin: paddle.atan2(Tensor([4, 3, 0],"float16"), Tensor([4, 3, 2],"float32"), )

[torch error] paddle.atan2(Tensor([4, 3, 0],"float16"), Tensor([4, 3, 2],"float32"), ) 
 The size of tensor a (0) must match the size of tensor b (2) at non-singleton dimension 2
2025-03-05 15:45:22.275429 test begin: paddle.atan2(Tensor([4, 3, 0],"float16"), Tensor([4, 3, 2],"float64"), )

[torch error] paddle.atan2(Tensor([4, 3, 0],"float16"), Tensor([4, 3, 2],"float64"), ) 
 The size of tensor a (0) must match the size of tensor b (2) at non-singleton dimension 2
2025-03-05 15:45:22.276173 test begin: paddle.atan2(Tensor([4, 3, 2],"float16"), Tensor([0, 3, 2],"float32"), )

[torch error] paddle.atan2(Tensor([4, 3, 2],"float16"), Tensor([0, 3, 2],"float32"), ) 
 The size of tensor a (4) must match the size of tensor b (0) at non-singleton dimension 0
2025-03-05 15:45:22.276890 test begin: paddle.atan2(Tensor([4, 3, 2],"float16"), Tensor([0, 3, 2],"float64"), )

[torch error] paddle.atan2(Tensor([4, 3, 2],"float16"), Tensor([0, 3, 2],"float64"), ) 
 The size of tensor a (4) must match the size of tensor b (0) at non-singleton dimension 0
2025-03-05 15:45:22.277617 test begin: paddle.atan2(Tensor([4, 3, 2],"float16"), Tensor([4, 0, 2],"float32"), )

[torch error] paddle.atan2(Tensor([4, 3, 2],"float16"), Tensor([4, 0, 2],"float32"), ) 
 The size of tensor a (3) must match the size of tensor b (0) at non-singleton dimension 1
2025-03-05 15:45:22.278311 test begin: paddle.atan2(Tensor([4, 3, 2],"float16"), Tensor([4, 0, 2],"float64"), )

[torch error] paddle.atan2(Tensor([4, 3, 2],"float16"), Tensor([4, 0, 2],"float64"), ) 
 The size of tensor a (3) must match the size of tensor b (0) at non-singleton dimension 1
2025-03-05 15:45:22.279019 test begin: paddle.atan2(Tensor([4, 3, 2],"float16"), Tensor([4, 3, 0],"float32"), )

[torch error] paddle.atan2(Tensor([4, 3, 2],"float16"), Tensor([4, 3, 0],"float32"), ) 
 The size of tensor a (2) must match the size of tensor b (0) at non-singleton dimension 2
2025-03-05 15:45:22.279720 test begin: paddle.atan2(Tensor([4, 3, 2],"float16"), Tensor([4, 3, 0],"float64"), )

[torch error] paddle.atan2(Tensor([4, 3, 2],"float16"), Tensor([4, 3, 0],"float64"), ) 
 The size of tensor a (2) must match the size of tensor b (0) at non-singleton dimension 2
2025-03-05 15:45:22.280406 test begin: paddle.atan2(x=Tensor([0, 6, 6, 6, 6],"float64"), y=Tensor([0, 6, 6, 6, 6],"float64"), )

[Pass] paddle.atan2(x=Tensor([0, 6, 6, 6, 6],"float64"), y=Tensor([0, 6, 6, 6, 6],"float64"), )
2025-03-05 15:45:22.294726 test begin: paddle.atan2(x=Tensor([0, 6, 6, 6, 6],"float64"), y=Tensor([3, 6, 6, 6, 6],"float64"), )

[torch error] paddle.atan2(x=Tensor([0, 6, 6, 6, 6],"float64"), y=Tensor([3, 6, 6, 6, 6],"float64"), ) 
 The size of tensor a (0) must match the size of tensor b (3) at non-singleton dimension 0
2025-03-05 15:45:22.296226 test begin: paddle.atan2(x=Tensor([0, 6, 6, 6],"float64"), y=Tensor([0, 6, 6, 6],"float64"), )

[Pass] paddle.atan2(x=Tensor([0, 6, 6, 6],"float64"), y=Tensor([0, 6, 6, 6],"float64"), )
2025-03-05 15:45:22.305327 test begin: paddle.atan2(x=Tensor([0, 6, 6, 6],"float64"), y=Tensor([6, 6, 6, 6],"float64"), )

[torch error] paddle.atan2(x=Tensor([0, 6, 6, 6],"float64"), y=Tensor([6, 6, 6, 6],"float64"), ) 
 The size of tensor a (0) must match the size of tensor b (6) at non-singleton dimension 0
2025-03-05 15:45:22.306708 test begin: paddle.atan2(x=Tensor([0, 6, 6],"float64"), y=Tensor([0, 6, 6],"float64"), )

[Pass] paddle.atan2(x=Tensor([0, 6, 6],"float64"), y=Tensor([0, 6, 6],"float64"), )
2025-03-05 15:45:22.310754 test begin: paddle.atan2(x=Tensor([0, 6, 6],"float64"), y=Tensor([6, 6, 6],"float64"), )

[torch error] paddle.atan2(x=Tensor([0, 6, 6],"float64"), y=Tensor([6, 6, 6],"float64"), ) 
 The size of tensor a (0) must match the size of tensor b (6) at non-singleton dimension 0
2025-03-05 15:45:22.311937 test begin: paddle.atan2(x=Tensor([0, 6],"float16"), y=Tensor([0, 6],"float16"), )

[Pass] paddle.atan2(x=Tensor([0, 6],"float16"), y=Tensor([0, 6],"float16"), )
2025-03-05 15:45:22.315597 test begin: paddle.atan2(x=Tensor([0, 6],"float16"), y=Tensor([6, 6],"float16"), )

[torch error] paddle.atan2(x=Tensor([0, 6],"float16"), y=Tensor([6, 6],"float16"), ) 
 The size of tensor a (0) must match the size of tensor b (6) at non-singleton dimension 0
2025-03-05 15:45:22.316593 test begin: paddle.atan2(x=Tensor([0, 6],"float32"), y=Tensor([0, 6],"float32"), )

[Pass] paddle.atan2(x=Tensor([0, 6],"float32"), y=Tensor([0, 6],"float32"), )
2025-03-05 15:45:22.319034 test begin: paddle.atan2(x=Tensor([0, 6],"float32"), y=Tensor([6, 6],"float32"), )

[torch error] paddle.atan2(x=Tensor([0, 6],"float32"), y=Tensor([6, 6],"float32"), ) 
 The size of tensor a (0) must match the size of tensor b (6) at non-singleton dimension 0
2025-03-05 15:45:22.320023 test begin: paddle.atan2(x=Tensor([3, 0, 6, 6, 6],"float64"), y=Tensor([3, 0, 6, 6, 6],"float64"), )

[Pass] paddle.atan2(x=Tensor([3, 0, 6, 6, 6],"float64"), y=Tensor([3, 0, 6, 6, 6],"float64"), )
2025-03-05 15:45:22.322502 test begin: paddle.atan2(x=Tensor([3, 0, 6, 6, 6],"float64"), y=Tensor([3, 6, 6, 6, 6],"float64"), )

[torch error] paddle.atan2(x=Tensor([3, 0, 6, 6, 6],"float64"), y=Tensor([3, 6, 6, 6, 6],"float64"), ) 
 The size of tensor a (0) must match the size of tensor b (6) at non-singleton dimension 1
2025-03-05 15:45:22.323551 test begin: paddle.atan2(x=Tensor([3, 6, 0, 6, 6],"float64"), y=Tensor([3, 6, 0, 6, 6],"float64"), )

[Pass] paddle.atan2(x=Tensor([3, 6, 0, 6, 6],"float64"), y=Tensor([3, 6, 0, 6, 6],"float64"), )
2025-03-05 15:45:22.329136 test begin: paddle.atan2(x=Tensor([3, 6, 0, 6, 6],"float64"), y=Tensor([3, 6, 6, 6, 6],"float64"), )

[torch error] paddle.atan2(x=Tensor([3, 6, 0, 6, 6],"float64"), y=Tensor([3, 6, 6, 6, 6],"float64"), ) 
 The size of tensor a (0) must match the size of tensor b (6) at non-singleton dimension 2
2025-03-05 15:45:22.332540 test begin: paddle.atan2(x=Tensor([3, 6, 6, 0, 6],"float64"), y=Tensor([3, 6, 6, 0, 6],"float64"), )

[Pass] paddle.atan2(x=Tensor([3, 6, 6, 0, 6],"float64"), y=Tensor([3, 6, 6, 0, 6],"float64"), )
2025-03-05 15:45:22.340831 test begin: paddle.atan2(x=Tensor([3, 6, 6, 0, 6],"float64"), y=Tensor([3, 6, 6, 6, 6],"float64"), )

[torch error] paddle.atan2(x=Tensor([3, 6, 6, 0, 6],"float64"), y=Tensor([3, 6, 6, 6, 6],"float64"), ) 
 The size of tensor a (0) must match the size of tensor b (6) at non-singleton dimension 3
2025-03-05 15:45:22.342172 test begin: paddle.atan2(x=Tensor([3, 6, 6, 6, 0],"float64"), y=Tensor([3, 6, 6, 6, 0],"float64"), )

[Pass] paddle.atan2(x=Tensor([3, 6, 6, 6, 0],"float64"), y=Tensor([3, 6, 6, 6, 0],"float64"), )
2025-03-05 15:45:22.350051 test begin: paddle.atan2(x=Tensor([3, 6, 6, 6, 0],"float64"), y=Tensor([3, 6, 6, 6, 6],"float64"), )

[torch error] paddle.atan2(x=Tensor([3, 6, 6, 6, 0],"float64"), y=Tensor([3, 6, 6, 6, 6],"float64"), ) 
 The size of tensor a (0) must match the size of tensor b (6) at non-singleton dimension 4
2025-03-05 15:45:22.353058 test begin: paddle.atan2(x=Tensor([3, 6, 6, 6, 6],"float64"), y=Tensor([0, 6, 6, 6, 6],"float64"), )

[torch error] paddle.atan2(x=Tensor([3, 6, 6, 6, 6],"float64"), y=Tensor([0, 6, 6, 6, 6],"float64"), ) 
 The size of tensor a (3) must match the size of tensor b (0) at non-singleton dimension 0
2025-03-05 15:45:22.353815 test begin: paddle.atan2(x=Tensor([3, 6, 6, 6, 6],"float64"), y=Tensor([3, 0, 6, 6, 6],"float64"), )

[torch error] paddle.atan2(x=Tensor([3, 6, 6, 6, 6],"float64"), y=Tensor([3, 0, 6, 6, 6],"float64"), ) 
 The size of tensor a (6) must match the size of tensor b (0) at non-singleton dimension 1
2025-03-05 15:45:22.354546 test begin: paddle.atan2(x=Tensor([3, 6, 6, 6, 6],"float64"), y=Tensor([3, 6, 0, 6, 6],"float64"), )

[torch error] paddle.atan2(x=Tensor([3, 6, 6, 6, 6],"float64"), y=Tensor([3, 6, 0, 6, 6],"float64"), ) 
 The size of tensor a (6) must match the size of tensor b (0) at non-singleton dimension 2
2025-03-05 15:45:22.355338 test begin: paddle.atan2(x=Tensor([3, 6, 6, 6, 6],"float64"), y=Tensor([3, 6, 6, 0, 6],"float64"), )

[torch error] paddle.atan2(x=Tensor([3, 6, 6, 6, 6],"float64"), y=Tensor([3, 6, 6, 0, 6],"float64"), ) 
 The size of tensor a (6) must match the size of tensor b (0) at non-singleton dimension 3
2025-03-05 15:45:22.356093 test begin: paddle.atan2(x=Tensor([3, 6, 6, 6, 6],"float64"), y=Tensor([3, 6, 6, 6, 0],"float64"), )

[torch error] paddle.atan2(x=Tensor([3, 6, 6, 6, 6],"float64"), y=Tensor([3, 6, 6, 6, 0],"float64"), ) 
 The size of tensor a (6) must match the size of tensor b (0) at non-singleton dimension 4
2025-03-05 15:45:22.356841 test begin: paddle.atan2(x=Tensor([6, 0, 6, 6],"float64"), y=Tensor([6, 0, 6, 6],"float64"), )

[Pass] paddle.atan2(x=Tensor([6, 0, 6, 6],"float64"), y=Tensor([6, 0, 6, 6],"float64"), )
2025-03-05 15:45:22.365148 test begin: paddle.atan2(x=Tensor([6, 0, 6, 6],"float64"), y=Tensor([6, 6, 6, 6],"float64"), )

[torch error] paddle.atan2(x=Tensor([6, 0, 6, 6],"float64"), y=Tensor([6, 6, 6, 6],"float64"), ) 
 The size of tensor a (0) must match the size of tensor b (6) at non-singleton dimension 1
2025-03-05 15:45:22.368067 test begin: paddle.atan2(x=Tensor([6, 0, 6],"float64"), y=Tensor([6, 0, 6],"float64"), )

[Pass] paddle.atan2(x=Tensor([6, 0, 6],"float64"), y=Tensor([6, 0, 6],"float64"), )
2025-03-05 15:45:22.374960 test begin: paddle.atan2(x=Tensor([6, 0, 6],"float64"), y=Tensor([6, 6, 6],"float64"), )

[torch error] paddle.atan2(x=Tensor([6, 0, 6],"float64"), y=Tensor([6, 6, 6],"float64"), ) 
 The size of tensor a (0) must match the size of tensor b (6) at non-singleton dimension 1
2025-03-05 15:45:22.377657 test begin: paddle.atan2(x=Tensor([6, 0],"float16"), y=Tensor([6, 0],"float16"), )

[Pass] paddle.atan2(x=Tensor([6, 0],"float16"), y=Tensor([6, 0],"float16"), )
2025-03-05 15:45:22.385700 test begin: paddle.atan2(x=Tensor([6, 0],"float16"), y=Tensor([6, 6],"float16"), )

[torch error] paddle.atan2(x=Tensor([6, 0],"float16"), y=Tensor([6, 6],"float16"), ) 
 The size of tensor a (0) must match the size of tensor b (6) at non-singleton dimension 1
2025-03-05 15:45:22.388203 test begin: paddle.atan2(x=Tensor([6, 0],"float32"), y=Tensor([6, 0],"float32"), )

[Pass] paddle.atan2(x=Tensor([6, 0],"float32"), y=Tensor([6, 0],"float32"), )
2025-03-05 15:45:22.400008 test begin: paddle.atan2(x=Tensor([6, 0],"float32"), y=Tensor([6, 6],"float32"), )

[torch error] paddle.atan2(x=Tensor([6, 0],"float32"), y=Tensor([6, 6],"float32"), ) 
 The size of tensor a (0) must match the size of tensor b (6) at non-singleton dimension 1
2025-03-05 15:45:22.417464 test begin: paddle.atan2(x=Tensor([6, 6, 0, 6],"float64"), y=Tensor([6, 6, 0, 6],"float64"), )

[Pass] paddle.atan2(x=Tensor([6, 6, 0, 6],"float64"), y=Tensor([6, 6, 0, 6],"float64"), )
2025-03-05 15:45:22.433961 test begin: paddle.atan2(x=Tensor([6, 6, 0, 6],"float64"), y=Tensor([6, 6, 6, 6],"float64"), )

[torch error] paddle.atan2(x=Tensor([6, 6, 0, 6],"float64"), y=Tensor([6, 6, 6, 6],"float64"), ) 
 The size of tensor a (0) must match the size of tensor b (6) at non-singleton dimension 2
2025-03-05 15:45:22.440231 test begin: paddle.atan2(x=Tensor([6, 6, 0],"float64"), y=Tensor([6, 6, 0],"float64"), )

[Pass] paddle.atan2(x=Tensor([6, 6, 0],"float64"), y=Tensor([6, 6, 0],"float64"), )
2025-03-05 15:45:22.457503 test begin: paddle.atan2(x=Tensor([6, 6, 0],"float64"), y=Tensor([6, 6, 6],"float64"), )

[torch error] paddle.atan2(x=Tensor([6, 6, 0],"float64"), y=Tensor([6, 6, 6],"float64"), ) 
 The size of tensor a (0) must match the size of tensor b (6) at non-singleton dimension 2
2025-03-05 15:45:22.462243 test begin: paddle.atan2(x=Tensor([6, 6, 6, 0],"float64"), y=Tensor([6, 6, 6, 0],"float64"), )

[Pass] paddle.atan2(x=Tensor([6, 6, 6, 0],"float64"), y=Tensor([6, 6, 6, 0],"float64"), )
2025-03-05 15:45:22.476204 test begin: paddle.atan2(x=Tensor([6, 6, 6, 0],"float64"), y=Tensor([6, 6, 6, 6],"float64"), )

[torch error] paddle.atan2(x=Tensor([6, 6, 6, 0],"float64"), y=Tensor([6, 6, 6, 6],"float64"), ) 
 The size of tensor a (0) must match the size of tensor b (6) at non-singleton dimension 3
2025-03-05 15:45:22.479396 test begin: paddle.atan2(x=Tensor([6, 6, 6, 6],"float64"), y=Tensor([0, 6, 6, 6],"float64"), )

[torch error] paddle.atan2(x=Tensor([6, 6, 6, 6],"float64"), y=Tensor([0, 6, 6, 6],"float64"), ) 
 The size of tensor a (6) must match the size of tensor b (0) at non-singleton dimension 0
2025-03-05 15:45:22.480180 test begin: paddle.atan2(x=Tensor([6, 6, 6, 6],"float64"), y=Tensor([6, 0, 6, 6],"float64"), )

[torch error] paddle.atan2(x=Tensor([6, 6, 6, 6],"float64"), y=Tensor([6, 0, 6, 6],"float64"), ) 
 The size of tensor a (6) must match the size of tensor b (0) at non-singleton dimension 1
2025-03-05 15:45:22.480916 test begin: paddle.atan2(x=Tensor([6, 6, 6, 6],"float64"), y=Tensor([6, 6, 0, 6],"float64"), )

[torch error] paddle.atan2(x=Tensor([6, 6, 6, 6],"float64"), y=Tensor([6, 6, 0, 6],"float64"), ) 
 The size of tensor a (6) must match the size of tensor b (0) at non-singleton dimension 2
2025-03-05 15:45:22.481704 test begin: paddle.atan2(x=Tensor([6, 6, 6, 6],"float64"), y=Tensor([6, 6, 6, 0],"float64"), )

[torch error] paddle.atan2(x=Tensor([6, 6, 6, 6],"float64"), y=Tensor([6, 6, 6, 0],"float64"), ) 
 The size of tensor a (6) must match the size of tensor b (0) at non-singleton dimension 3
2025-03-05 15:45:22.482428 test begin: paddle.atan2(x=Tensor([6, 6, 6],"float64"), y=Tensor([0, 6, 6],"float64"), )

[torch error] paddle.atan2(x=Tensor([6, 6, 6],"float64"), y=Tensor([0, 6, 6],"float64"), ) 
 The size of tensor a (6) must match the size of tensor b (0) at non-singleton dimension 0
2025-03-05 15:45:22.483144 test begin: paddle.atan2(x=Tensor([6, 6, 6],"float64"), y=Tensor([6, 0, 6],"float64"), )

[torch error] paddle.atan2(x=Tensor([6, 6, 6],"float64"), y=Tensor([6, 0, 6],"float64"), ) 
 The size of tensor a (6) must match the size of tensor b (0) at non-singleton dimension 1
2025-03-05 15:45:22.483864 test begin: paddle.atan2(x=Tensor([6, 6, 6],"float64"), y=Tensor([6, 6, 0],"float64"), )

[torch error] paddle.atan2(x=Tensor([6, 6, 6],"float64"), y=Tensor([6, 6, 0],"float64"), ) 
 The size of tensor a (6) must match the size of tensor b (0) at non-singleton dimension 2
2025-03-05 15:45:22.484590 test begin: paddle.atan2(x=Tensor([6, 6],"float16"), y=Tensor([0, 6],"float16"), )

[torch error] paddle.atan2(x=Tensor([6, 6],"float16"), y=Tensor([0, 6],"float16"), ) 
 The size of tensor a (6) must match the size of tensor b (0) at non-singleton dimension 0
2025-03-05 15:45:22.485294 test begin: paddle.atan2(x=Tensor([6, 6],"float16"), y=Tensor([6, 0],"float16"), )

[torch error] paddle.atan2(x=Tensor([6, 6],"float16"), y=Tensor([6, 0],"float16"), ) 
 The size of tensor a (6) must match the size of tensor b (0) at non-singleton dimension 1
2025-03-05 15:45:22.485991 test begin: paddle.atan2(x=Tensor([6, 6],"float32"), y=Tensor([0, 6],"float32"), )

[torch error] paddle.atan2(x=Tensor([6, 6],"float32"), y=Tensor([0, 6],"float32"), ) 
 The size of tensor a (6) must match the size of tensor b (0) at non-singleton dimension 0
2025-03-05 15:45:22.486724 test begin: paddle.atan2(x=Tensor([6, 6],"float32"), y=Tensor([6, 0],"float32"), )

[torch error] paddle.atan2(x=Tensor([6, 6],"float32"), y=Tensor([6, 0],"float32"), ) 
 The size of tensor a (6) must match the size of tensor b (0) at non-singleton dimension 1
2025-03-05 15:45:22.487442 test begin: paddle.atanh(Tensor([0, 16, 32],"complex128"), )

[cuda error] paddle.atanh(Tensor([0, 16, 32],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:22.492120 test begin: paddle.atanh(Tensor([0, 20, 1],"float32"), )

[cuda error] paddle.atanh(Tensor([0, 20, 1],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:22.501480 test begin: paddle.atanh(Tensor([10, 0, 1],"float32"), )

[cuda error] paddle.atanh(Tensor([10, 0, 1],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:22.508140 test begin: paddle.atanh(Tensor([10, 20, 0],"float32"), )

[cuda error] paddle.atanh(Tensor([10, 20, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:22.518318 test begin: paddle.atanh(Tensor([8, 0, 32],"complex128"), )

[cuda error] paddle.atanh(Tensor([8, 0, 32],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:22.525001 test begin: paddle.atanh(Tensor([8, 16, 0],"complex128"), )

[cuda error] paddle.atanh(Tensor([8, 16, 0],"complex128"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:45:22.534865 test begin: paddle.bitwise_and(Tensor([0, 3, 1, 5],"int64"), Tensor([3, 4, 1],"int64"), )

[Pass] paddle.bitwise_and(Tensor([0, 3, 1, 5],"int64"), Tensor([3, 4, 1],"int64"), )
2025-03-05 15:45:22.563661 test begin: paddle.bitwise_and(Tensor([0, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([0, 3, 3, 3, 4, 1, 5, 2],"bool"), )

[Pass] paddle.bitwise_and(Tensor([0, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([0, 3, 3, 3, 4, 1, 5, 2],"bool"), )
2025-03-05 15:45:22.572607 test begin: paddle.bitwise_and(Tensor([0, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), )

[torch error] paddle.bitwise_and(Tensor([0, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 
 The size of tensor a (0) must match the size of tensor b (2) at non-singleton dimension 0
2025-03-05 15:45:22.575993 test begin: paddle.bitwise_and(Tensor([0, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([0, 3, 3, 3, 4, 1, 5, 2],"int16"), )

[Pass] paddle.bitwise_and(Tensor([0, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([0, 3, 3, 3, 4, 1, 5, 2],"int16"), )
2025-03-05 15:45:22.584181 test begin: paddle.bitwise_and(Tensor([0, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), )

[torch error] paddle.bitwise_and(Tensor([0, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 
 The size of tensor a (0) must match the size of tensor b (2) at non-singleton dimension 0
2025-03-05 15:45:22.586116 test begin: paddle.bitwise_and(Tensor([0, 3, 5],"int32"), Tensor([0, 3, 5],"int32"), )

[Pass] paddle.bitwise_and(Tensor([0, 3, 5],"int32"), Tensor([0, 3, 5],"int32"), )
2025-03-05 15:45:22.611107 test begin: paddle.bitwise_and(Tensor([0, 3, 5],"int32"), Tensor([2, 3, 5],"int32"), )

[torch error] paddle.bitwise_and(Tensor([0, 3, 5],"int32"), Tensor([2, 3, 5],"int32"), ) 
 The size of tensor a (0) must match the size of tensor b (2) at non-singleton dimension 0
2025-03-05 15:45:22.614520 test begin: paddle.bitwise_and(Tensor([0, 4, 1],"int32"), Tensor([0, 4, 1],"int32"), )

[Pass] paddle.bitwise_and(Tensor([0, 4, 1],"int32"), Tensor([0, 4, 1],"int32"), )
2025-03-05 15:45:22.619995 test begin: paddle.bitwise_and(Tensor([0, 4, 1],"int32"), Tensor([3, 4, 1],"int32"), )

[torch error] paddle.bitwise_and(Tensor([0, 4, 1],"int32"), Tensor([3, 4, 1],"int32"), ) 
 The size of tensor a (0) must match the size of tensor b (3) at non-singleton dimension 0
2025-03-05 15:45:22.621379 test begin: paddle.bitwise_and(Tensor([0, 4, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), )

[torch error] paddle.bitwise_and(Tensor([0, 4, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), ) 
 The size of tensor a (0) must match the size of tensor b (3) at non-singleton dimension 1
2025-03-05 15:45:22.622221 test begin: paddle.bitwise_and(Tensor([0],"int32"), Tensor([0],"int32"), )

[Pass] paddle.bitwise_and(Tensor([0],"int32"), Tensor([0],"int32"), )
2025-03-05 15:45:22.625378 test begin: paddle.bitwise_and(Tensor([0],"int32"), Tensor([3],"int32"), )

[torch error] paddle.bitwise_and(Tensor([0],"int32"), Tensor([3],"int32"), ) 
 The size of tensor a (0) must match the size of tensor b (3) at non-singleton dimension 0
2025-03-05 15:45:22.627739 test begin: paddle.bitwise_and(Tensor([2, 0, 1, 5],"int64"), Tensor([3, 4, 1],"int64"), )

[torch error] paddle.bitwise_and(Tensor([2, 0, 1, 5],"int64"), Tensor([3, 4, 1],"int64"), ) 
 The size of tensor a (0) must match the size of tensor b (3) at non-singleton dimension 1
2025-03-05 15:45:22.628553 test begin: paddle.bitwise_and(Tensor([2, 0, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 0, 3, 3, 4, 1, 5, 2],"bool"), )

[Pass] paddle.bitwise_and(Tensor([2, 0, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 0, 3, 3, 4, 1, 5, 2],"bool"), )
2025-03-05 15:45:22.631099 test begin: paddle.bitwise_and(Tensor([2, 0, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), )

[torch error] paddle.bitwise_and(Tensor([2, 0, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 
 The size of tensor a (0) must match the size of tensor b (3) at non-singleton dimension 1
2025-03-05 15:45:22.632310 test begin: paddle.bitwise_and(Tensor([2, 0, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 0, 3, 3, 4, 1, 5, 2],"int16"), )

[Pass] paddle.bitwise_and(Tensor([2, 0, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 0, 3, 3, 4, 1, 5, 2],"int16"), )
2025-03-05 15:45:22.634411 test begin: paddle.bitwise_and(Tensor([2, 0, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), )

[torch error] paddle.bitwise_and(Tensor([2, 0, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 
 The size of tensor a (0) must match the size of tensor b (3) at non-singleton dimension 1
2025-03-05 15:45:22.635539 test begin: paddle.bitwise_and(Tensor([2, 0, 5],"int32"), Tensor([2, 0, 5],"int32"), )

[Pass] paddle.bitwise_and(Tensor([2, 0, 5],"int32"), Tensor([2, 0, 5],"int32"), )
2025-03-05 15:45:22.637541 test begin: paddle.bitwise_and(Tensor([2, 0, 5],"int32"), Tensor([2, 3, 5],"int32"), )

[torch error] paddle.bitwise_and(Tensor([2, 0, 5],"int32"), Tensor([2, 3, 5],"int32"), ) 
 The size of tensor a (0) must match the size of tensor b (3) at non-singleton dimension 1
2025-03-05 15:45:22.638517 test begin: paddle.bitwise_and(Tensor([2, 3, 0, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 0, 3, 4, 1, 5, 2],"bool"), )

[Pass] paddle.bitwise_and(Tensor([2, 3, 0, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 0, 3, 4, 1, 5, 2],"bool"), )
2025-03-05 15:45:22.640377 test begin: paddle.bitwise_and(Tensor([2, 3, 0, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), )

[torch error] paddle.bitwise_and(Tensor([2, 3, 0, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 
 The size of tensor a (0) must match the size of tensor b (3) at non-singleton dimension 2
2025-03-05 15:45:22.641406 test begin: paddle.bitwise_and(Tensor([2, 3, 0, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 0, 3, 4, 1, 5, 2],"int16"), )

[Pass] paddle.bitwise_and(Tensor([2, 3, 0, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 0, 3, 4, 1, 5, 2],"int16"), )
2025-03-05 15:45:22.643358 test begin: paddle.bitwise_and(Tensor([2, 3, 0, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), )

[torch error] paddle.bitwise_and(Tensor([2, 3, 0, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 
 The size of tensor a (0) must match the size of tensor b (3) at non-singleton dimension 2
2025-03-05 15:45:22.644359 test begin: paddle.bitwise_and(Tensor([2, 3, 0, 5],"int64"), Tensor([3, 4, 1],"int64"), )

[torch error] paddle.bitwise_and(Tensor([2, 3, 0, 5],"int64"), Tensor([3, 4, 1],"int64"), ) 
 The size of tensor a (0) must match the size of tensor b (4) at non-singleton dimension 2
2025-03-05 15:45:22.645784 test begin: paddle.bitwise_and(Tensor([2, 3, 0],"int32"), Tensor([2, 3, 0],"int32"), )

[Pass] paddle.bitwise_and(Tensor([2, 3, 0],"int32"), Tensor([2, 3, 0],"int32"), )
2025-03-05 15:45:22.648349 test begin: paddle.bitwise_and(Tensor([2, 3, 0],"int32"), Tensor([2, 3, 5],"int32"), )

[torch error] paddle.bitwise_and(Tensor([2, 3, 0],"int32"), Tensor([2, 3, 5],"int32"), ) 
 The size of tensor a (0) must match the size of tensor b (5) at non-singleton dimension 2
2025-03-05 15:45:22.649509 test begin: paddle.bitwise_and(Tensor([2, 3, 1, 0],"int64"), Tensor([3, 4, 1],"int64"), )

[Pass] paddle.bitwise_and(Tensor([2, 3, 1, 0],"int64"), Tensor([3, 4, 1],"int64"), )
2025-03-05 15:45:22.651753 test begin: paddle.bitwise_and(Tensor([2, 3, 1, 5],"int64"), Tensor([0, 4, 1],"int64"), )

[torch error] paddle.bitwise_and(Tensor([2, 3, 1, 5],"int64"), Tensor([0, 4, 1],"int64"), ) 
 The size of tensor a (3) must match the size of tensor b (0) at non-singleton dimension 1
2025-03-05 15:45:22.652925 test begin: paddle.bitwise_and(Tensor([2, 3, 1, 5],"int64"), Tensor([3, 0, 1],"int64"), )

[Pass] paddle.bitwise_and(Tensor([2, 3, 1, 5],"int64"), Tensor([3, 0, 1],"int64"), )
2025-03-05 15:45:22.654904 test begin: paddle.bitwise_and(Tensor([2, 3, 1, 5],"int64"), Tensor([3, 4, 0],"int64"), )

[torch error] paddle.bitwise_and(Tensor([2, 3, 1, 5],"int64"), Tensor([3, 4, 0],"int64"), ) 
 The size of tensor a (5) must match the size of tensor b (0) at non-singleton dimension 3
2025-03-05 15:45:22.656176 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 0, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 0, 4, 1, 5, 2],"bool"), )

[Pass] paddle.bitwise_and(Tensor([2, 3, 3, 0, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 0, 4, 1, 5, 2],"bool"), )
2025-03-05 15:45:22.658998 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 0, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), )

[torch error] paddle.bitwise_and(Tensor([2, 3, 3, 0, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 
 The size of tensor a (0) must match the size of tensor b (3) at non-singleton dimension 3
2025-03-05 15:45:22.660162 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 0, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 0, 4, 1, 5, 2],"int16"), )

[Pass] paddle.bitwise_and(Tensor([2, 3, 3, 0, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 0, 4, 1, 5, 2],"int16"), )
2025-03-05 15:45:22.662219 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 0, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), )

[torch error] paddle.bitwise_and(Tensor([2, 3, 3, 0, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 
 The size of tensor a (0) must match the size of tensor b (3) at non-singleton dimension 3
2025-03-05 15:45:22.663249 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 0, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 0, 1, 5, 2],"bool"), )

[Pass] paddle.bitwise_and(Tensor([2, 3, 3, 3, 0, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 0, 1, 5, 2],"bool"), )
2025-03-05 15:45:22.665116 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 0, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), )

[torch error] paddle.bitwise_and(Tensor([2, 3, 3, 3, 0, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 
 The size of tensor a (0) must match the size of tensor b (4) at non-singleton dimension 4
2025-03-05 15:45:22.666177 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 0, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 0, 1, 5, 2],"int16"), )

[Pass] paddle.bitwise_and(Tensor([2, 3, 3, 3, 0, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 0, 1, 5, 2],"int16"), )
2025-03-05 15:45:22.668265 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 0, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), )

[torch error] paddle.bitwise_and(Tensor([2, 3, 3, 3, 0, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 
 The size of tensor a (0) must match the size of tensor b (4) at non-singleton dimension 4
2025-03-05 15:45:22.669320 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 0, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 0, 5, 2],"bool"), )

[Pass] paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 0, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 0, 5, 2],"bool"), )
2025-03-05 15:45:22.671337 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 0, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), )

[Pass] paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 0, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), )
2025-03-05 15:45:22.673638 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 0, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 0, 5, 2],"int16"), )

[Pass] paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 0, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 0, 5, 2],"int16"), )
2025-03-05 15:45:22.675266 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 0, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), )

[Pass] paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 0, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), )
2025-03-05 15:45:22.677485 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 0, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 0, 2],"bool"), )

[Pass] paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 0, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 0, 2],"bool"), )
2025-03-05 15:45:22.679191 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 0, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), )

[torch error] paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 0, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 
 The size of tensor a (0) must match the size of tensor b (5) at non-singleton dimension 6
2025-03-05 15:45:22.681798 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 0, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 0, 2],"int16"), )

[Pass] paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 0, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 0, 2],"int16"), )
2025-03-05 15:45:22.686736 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 0, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), )

[torch error] paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 0, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 
 The size of tensor a (0) must match the size of tensor b (5) at non-singleton dimension 6
2025-03-05 15:45:22.688515 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 0],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 0],"bool"), )

[Pass] paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 0],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 0],"bool"), )
2025-03-05 15:45:22.691560 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 0],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), )

[torch error] paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 0],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 
 The size of tensor a (0) must match the size of tensor b (2) at non-singleton dimension 7
2025-03-05 15:45:22.692718 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 0],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 0],"int16"), )

[Pass] paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 0],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 0],"int16"), )
2025-03-05 15:45:22.694764 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 0],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), )

[torch error] paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 0],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 
 The size of tensor a (0) must match the size of tensor b (2) at non-singleton dimension 7
2025-03-05 15:45:22.695775 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([0, 3, 3, 3, 4, 1, 5, 2],"bool"), )

[torch error] paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([0, 3, 3, 3, 4, 1, 5, 2],"bool"), ) 
 The size of tensor a (2) must match the size of tensor b (0) at non-singleton dimension 0
2025-03-05 15:45:22.696516 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 0, 3, 3, 4, 1, 5, 2],"bool"), )

[torch error] paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 0, 3, 3, 4, 1, 5, 2],"bool"), ) 
 The size of tensor a (3) must match the size of tensor b (0) at non-singleton dimension 1
2025-03-05 15:45:22.697273 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 0, 3, 4, 1, 5, 2],"bool"), )

[torch error] paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 0, 3, 4, 1, 5, 2],"bool"), ) 
 The size of tensor a (3) must match the size of tensor b (0) at non-singleton dimension 2
2025-03-05 15:45:22.698027 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 0, 4, 1, 5, 2],"bool"), )

[torch error] paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 0, 4, 1, 5, 2],"bool"), ) 
 The size of tensor a (3) must match the size of tensor b (0) at non-singleton dimension 3
2025-03-05 15:45:22.698818 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 0, 1, 5, 2],"bool"), )

[torch error] paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 0, 1, 5, 2],"bool"), ) 
 The size of tensor a (4) must match the size of tensor b (0) at non-singleton dimension 4
2025-03-05 15:45:22.699542 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 0, 5, 2],"bool"), )

[Pass] paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 0, 5, 2],"bool"), )
2025-03-05 15:45:22.716431 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 0, 2],"bool"), )

[torch error] paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 0, 2],"bool"), ) 
 The size of tensor a (5) must match the size of tensor b (0) at non-singleton dimension 6
2025-03-05 15:45:22.728355 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 0],"bool"), )

[torch error] paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"bool"), Tensor([2, 3, 3, 3, 4, 1, 5, 0],"bool"), ) 
 The size of tensor a (2) must match the size of tensor b (0) at non-singleton dimension 7
2025-03-05 15:45:22.729628 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([0, 3, 3, 3, 4, 1, 5, 2],"int16"), )

[torch error] paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([0, 3, 3, 3, 4, 1, 5, 2],"int16"), ) 
 The size of tensor a (2) must match the size of tensor b (0) at non-singleton dimension 0
2025-03-05 15:45:22.730496 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 0, 3, 3, 4, 1, 5, 2],"int16"), )

[torch error] paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 0, 3, 3, 4, 1, 5, 2],"int16"), ) 
 The size of tensor a (3) must match the size of tensor b (0) at non-singleton dimension 1
2025-03-05 15:45:22.731286 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 0, 3, 4, 1, 5, 2],"int16"), )

[torch error] paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 0, 3, 4, 1, 5, 2],"int16"), ) 
 The size of tensor a (3) must match the size of tensor b (0) at non-singleton dimension 2
2025-03-05 15:45:22.732038 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 0, 4, 1, 5, 2],"int16"), )

[torch error] paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 0, 4, 1, 5, 2],"int16"), ) 
 The size of tensor a (3) must match the size of tensor b (0) at non-singleton dimension 3
2025-03-05 15:45:22.732782 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 0, 1, 5, 2],"int16"), )

[torch error] paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 0, 1, 5, 2],"int16"), ) 
 The size of tensor a (4) must match the size of tensor b (0) at non-singleton dimension 4
2025-03-05 15:45:22.733487 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 0, 5, 2],"int16"), )

[Pass] paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 0, 5, 2],"int16"), )
2025-03-05 15:45:22.761452 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 0, 2],"int16"), )

[torch error] paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 0, 2],"int16"), ) 
 The size of tensor a (5) must match the size of tensor b (0) at non-singleton dimension 6
2025-03-05 15:45:22.776656 test begin: paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 0],"int16"), )

[torch error] paddle.bitwise_and(Tensor([2, 3, 3, 3, 4, 1, 5, 2],"int16"), Tensor([2, 3, 3, 3, 4, 1, 5, 0],"int16"), ) 
 The size of tensor a (2) must match the size of tensor b (0) at non-singleton dimension 7
2025-03-05 15:45:22.777824 test begin: paddle.bitwise_and(Tensor([2, 3, 5],"int32"), Tensor([0, 3, 5],"int32"), )

[torch error] paddle.bitwise_and(Tensor([2, 3, 5],"int32"), Tensor([0, 3, 5],"int32"), ) 
 The size of tensor a (2) must match the size of tensor b (0) at non-singleton dimension 0
2025-03-05 15:45:22.778668 test begin: paddle.bitwise_and(Tensor([2, 3, 5],"int32"), Tensor([2, 0, 5],"int32"), )

[torch error] paddle.bitwise_and(Tensor([2, 3, 5],"int32"), Tensor([2, 0, 5],"int32"), ) 
 The size of tensor a (3) must match the size of tensor b (0) at non-singleton dimension 1
2025-03-05 15:45:22.779379 test begin: paddle.bitwise_and(Tensor([2, 3, 5],"int32"), Tensor([2, 3, 0],"int32"), )

[torch error] paddle.bitwise_and(Tensor([2, 3, 5],"int32"), Tensor([2, 3, 0],"int32"), ) 
 The size of tensor a (5) must match the size of tensor b (0) at non-singleton dimension 2
2025-03-05 15:45:22.780062 test begin: paddle.bitwise_and(Tensor([3, 0, 1],"int32"), Tensor([3, 0, 1],"int32"), )

[Pass] paddle.bitwise_and(Tensor([3, 0, 1],"int32"), Tensor([3, 0, 1],"int32"), )
2025-03-05 15:45:22.816212 test begin: paddle.bitwise_and(Tensor([3, 0, 1],"int32"), Tensor([3, 4, 1],"int32"), )

[torch error] paddle.bitwise_and(Tensor([3, 0, 1],"int32"), Tensor([3, 4, 1],"int32"), ) 
 The size of tensor a (0) must match the size of tensor b (4) at non-singleton dimension 1
2025-03-05 15:45:22.818358 test begin: paddle.bitwise_and(Tensor([3, 0, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), )

[Pass] paddle.bitwise_and(Tensor([3, 0, 1],"int64"), Tensor([2, 3, 1, 5],"int64"), )
2025-03-05 15:45:22.821424 test begin: paddle.bitwise_and(Tensor([3, 4, 0],"int32"), Tensor([3, 4, 0],"int32"), )

[Pass] paddle.bitwise_and(Tensor([3, 4, 0],"int32"), Tensor([3, 4, 0],"int32"), )
2025-03-05 15:45:22.826646 test begin: paddle.bitwise_and(Tensor([3, 4, 0],"int32"), Tensor([3, 4, 1],"int32"), )

[Pass] paddle.bitwise_and(Tensor([3, 4, 0],"int32"), Tensor([3, 4, 1],"int32"), )
2025-03-05 15:45:22.829893 test begin: paddle.bitwise_and(Tensor([3, 4, 0],"int64"), Tensor([2, 3, 1, 5],"int64"), )

[torch error] paddle.bitwise_and(Tensor([3, 4, 0],"int64"), Tensor([2, 3, 1, 5],"int64"), ) 
 The size of tensor a (0) must match the size of tensor b (5) at non-singleton dimension 3
2025-03-05 15:45:22.831062 test begin: paddle.bitwise_and(Tensor([3, 4, 1],"int32"), Tensor([0, 4, 1],"int32"), )

[torch error] paddle.bitwise_and(Tensor([3, 4, 1],"int32"), Tensor([0, 4, 1],"int32"), ) 
 The size of tensor a (3) must match the size of tensor b (0) at non-singleton dimension 0
2025-03-05 15:45:22.831796 test begin: paddle.bitwise_and(Tensor([3, 4, 1],"int32"), Tensor([3, 0, 1],"int32"), )

[torch error] paddle.bitwise_and(Tensor([3, 4, 1],"int32"), Tensor([3, 0, 1],"int32"), ) 
 The size of tensor a (4) must match the size of tensor b (0) at non-singleton dimension 1
2025-03-05 15:45:22.832485 test begin: paddle.bitwise_and(Tensor([3, 4, 1],"int32"), Tensor([3, 4, 0],"int32"), )

[Pass] paddle.bitwise_and(Tensor([3, 4, 1],"int32"), Tensor([3, 4, 0],"int32"), )
2025-03-05 15:45:22.834343 test begin: paddle.bitwise_and(Tensor([3, 4, 1],"int64"), Tensor([0, 3, 1, 5],"int64"), )

[Pass] paddle.bitwise_and(Tensor([3, 4, 1],"int64"), Tensor([0, 3, 1, 5],"int64"), )
2025-03-05 15:45:22.835930 test begin: paddle.bitwise_and(Tensor([3, 4, 1],"int64"), Tensor([2, 0, 1, 5],"int64"), )

[torch error] paddle.bitwise_and(Tensor([3, 4, 1],"int64"), Tensor([2, 0, 1, 5],"int64"), ) 
 The size of tensor a (3) must match the size of tensor b (0) at non-singleton dimension 1
2025-03-05 15:45:22.837088 test begin: paddle.bitwise_and(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 0, 5],"int64"), )

[torch error] paddle.bitwise_and(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 0, 5],"int64"), ) 
 The size of tensor a (4) must match the size of tensor b (0) at non-singleton dimension 2
2025-03-05 15:45:22.837807 test begin: paddle.bitwise_and(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 1, 0],"int64"), )

[Pass] paddle.bitwise_and(Tensor([3, 4, 1],"int64"), Tensor([2, 3, 1, 0],"int64"), )
2025-03-05 15:45:22.840243 test begin: paddle.bitwise_and(Tensor([3],"int32"), Tensor([0],"int32"), )

[torch error] paddle.bitwise_and(Tensor([3],"int32"), Tensor([0],"int32"), ) 
 The size of tensor a (3) must match the size of tensor b (0) at non-singleton dimension 0
2025-03-05 15:45:22.842142 test begin: paddle.bitwise_left_shift(Tensor([0, 300],"int16"), Tensor([0, 300],"int16"), )

[Pass] paddle.bitwise_left_shift(Tensor([0, 300],"int16"), Tensor([0, 300],"int16"), )
2025-03-05 15:45:22.844218 test begin: paddle.bitwise_left_shift(Tensor([0, 300],"int16"), Tensor([200, 300],"int16"), )

[torch error] paddle.bitwise_left_shift(Tensor([0, 300],"int16"), Tensor([200, 300],"int16"), ) 
 The size of tensor a (0) must match the size of tensor b (200) at non-singleton dimension 0
2025-03-05 15:45:22.846366 test begin: paddle.bitwise_left_shift(Tensor([0, 300],"int16"), Tensor([300],"int16"), )

[Pass] paddle.bitwise_left_shift(Tensor([0, 300],"int16"), Tensor([300],"int16"), )
2025-03-05 15:45:22.848344 test begin: paddle.bitwise_left_shift(Tensor([0, 300],"int32"), Tensor([0, 300],"int32"), )

[Pass] paddle.bitwise_left_shift(Tensor([0, 300],"int32"), Tensor([0, 300],"int32"), )
2025-03-05 15:45:22.849978 test begin: paddle.bitwise_left_shift(Tensor([0, 300],"int32"), Tensor([200, 300],"int32"), )

[torch error] paddle.bitwise_left_shift(Tensor([0, 300],"int32"), Tensor([200, 300],"int32"), ) 
 The size of tensor a (0) must match the size of tensor b (200) at non-singleton dimension 0
2025-03-05 15:45:22.851311 test begin: paddle.bitwise_left_shift(Tensor([0, 300],"int32"), Tensor([300],"int32"), )

[Pass] paddle.bitwise_left_shift(Tensor([0, 300],"int32"), Tensor([300],"int32"), )
2025-03-05 15:45:22.853163 test begin: paddle.bitwise_left_shift(Tensor([0],"int16"), Tensor([0],"int16"), )

[Pass] paddle.bitwise_left_shift(Tensor([0],"int16"), Tensor([0],"int16"), )
2025-03-05 15:45:22.854756 test begin: paddle.bitwise_left_shift(Tensor([0],"int16"), Tensor([1],"int16"), )

[Pass] paddle.bitwise_left_shift(Tensor([0],"int16"), Tensor([1],"int16"), )
2025-03-05 15:45:22.856823 test begin: paddle.bitwise_left_shift(Tensor([0],"int16"), Tensor([200, 300],"int16"), )

[torch error] paddle.bitwise_left_shift(Tensor([0],"int16"), Tensor([200, 300],"int16"), ) 
 The size of tensor a (0) must match the size of tensor b (300) at non-singleton dimension 1
2025-03-05 15:45:22.858352 test begin: paddle.bitwise_left_shift(Tensor([0],"int32"), Tensor([200, 300],"int32"), )

[torch error] paddle.bitwise_left_shift(Tensor([0],"int32"), Tensor([200, 300],"int32"), ) 
 The size of tensor a (0) must match the size of tensor b (300) at non-singleton dimension 1
2025-03-05 15:45:22.859356 test begin: paddle.bitwise_left_shift(Tensor([0],"uint8"), Tensor([0],"uint8"), )

[Pass] paddle.bitwise_left_shift(Tensor([0],"uint8"), Tensor([0],"uint8"), )
2025-03-05 15:45:22.862252 test begin: paddle.bitwise_left_shift(Tensor([0],"uint8"), Tensor([1],"uint8"), )

[Pass] paddle.bitwise_left_shift(Tensor([0],"uint8"), Tensor([1],"uint8"), )
2025-03-05 15:45:22.864292 test begin: paddle.bitwise_left_shift(Tensor([1],"int16"), Tensor([0],"int16"), )

[Pass] paddle.bitwise_left_shift(Tensor([1],"int16"), Tensor([0],"int16"), )
2025-03-05 15:45:22.867028 test begin: paddle.bitwise_left_shift(Tensor([1],"uint8"), Tensor([0],"uint8"), )

[Pass] paddle.bitwise_left_shift(Tensor([1],"uint8"), Tensor([0],"uint8"), )
2025-03-05 15:45:22.869272 test begin: paddle.bitwise_left_shift(Tensor([200, 0],"int16"), Tensor([200, 0],"int16"), )

[Pass] paddle.bitwise_left_shift(Tensor([200, 0],"int16"), Tensor([200, 0],"int16"), )
2025-03-05 15:45:22.871558 test begin: paddle.bitwise_left_shift(Tensor([200, 0],"int16"), Tensor([200, 300],"int16"), )

[torch error] paddle.bitwise_left_shift(Tensor([200, 0],"int16"), Tensor([200, 300],"int16"), ) 
 The size of tensor a (0) must match the size of tensor b (300) at non-singleton dimension 1
2025-03-05 15:45:22.873102 test begin: paddle.bitwise_left_shift(Tensor([200, 0],"int16"), Tensor([300],"int16"), )

[torch error] paddle.bitwise_left_shift(Tensor([200, 0],"int16"), Tensor([300],"int16"), ) 
 The size of tensor a (0) must match the size of tensor b (300) at non-singleton dimension 1
2025-03-05 15:45:22.873869 test begin: paddle.bitwise_left_shift(Tensor([200, 0],"int32"), Tensor([200, 0],"int32"), )

[Pass] paddle.bitwise_left_shift(Tensor([200, 0],"int32"), Tensor([200, 0],"int32"), )
2025-03-05 15:45:22.875866 test begin: paddle.bitwise_left_shift(Tensor([200, 0],"int32"), Tensor([200, 300],"int32"), )

[torch error] paddle.bitwise_left_shift(Tensor([200, 0],"int32"), Tensor([200, 300],"int32"), ) 
 The size of tensor a (0) must match the size of tensor b (300) at non-singleton dimension 1
2025-03-05 15:45:22.877212 test begin: paddle.bitwise_left_shift(Tensor([200, 0],"int32"), Tensor([300],"int32"), )

[torch error] paddle.bitwise_left_shift(Tensor([200, 0],"int32"), Tensor([300],"int32"), ) 
 The size of tensor a (0) must match the size of tensor b (300) at non-singleton dimension 1
2025-03-05 15:45:22.878011 test begin: paddle.bitwise_left_shift(Tensor([200, 300],"int16"), Tensor([0, 300],"int16"), )

[torch error] paddle.bitwise_left_shift(Tensor([200, 300],"int16"), Tensor([0, 300],"int16"), ) 
 The size of tensor a (200) must match the size of tensor b (0) at non-singleton dimension 0
2025-03-05 15:45:22.879029 test begin: paddle.bitwise_left_shift(Tensor([200, 300],"int16"), Tensor([0],"int16"), )

[torch error] paddle.bitwise_left_shift(Tensor([200, 300],"int16"), Tensor([0],"int16"), ) 
 The size of tensor a (300) must match the size of tensor b (0) at non-singleton dimension 1
2025-03-05 15:45:22.880251 test begin: paddle.bitwise_left_shift(Tensor([200, 300],"int16"), Tensor([200, 0],"int16"), )

[torch error] paddle.bitwise_left_shift(Tensor([200, 300],"int16"), Tensor([200, 0],"int16"), ) 
 The size of tensor a (300) must match the size of tensor b (0) at non-singleton dimension 1
2025-03-05 15:45:22.883945 test begin: paddle.bitwise_left_shift(Tensor([200, 300],"int32"), Tensor([0, 300],"int32"), )

[torch error] paddle.bitwise_left_shift(Tensor([200, 300],"int32"), Tensor([0, 300],"int32"), ) 
 The size of tensor a (200) must match the size of tensor b (0) at non-singleton dimension 0
2025-03-05 15:45:22.884960 test begin: paddle.bitwise_left_shift(Tensor([200, 300],"int32"), Tensor([0],"int32"), )

[torch error] paddle.bitwise_left_shift(Tensor([200, 300],"int32"), Tensor([0],"int32"), ) 
 The size of tensor a (300) must match the size of tensor b (0) at non-singleton dimension 1
2025-03-05 15:45:22.885943 test begin: paddle.bitwise_left_shift(Tensor([200, 300],"int32"), Tensor([200, 0],"int32"), )

[torch error] paddle.bitwise_left_shift(Tensor([200, 300],"int32"), Tensor([200, 0],"int32"), ) 
 The size of tensor a (300) must match the size of tensor b (0) at non-singleton dimension 1
2025-03-05 15:45:22.887008 test begin: paddle.bitwise_left_shift(Tensor([300],"int16"), Tensor([0, 300],"int16"), )

[Pass] paddle.bitwise_left_shift(Tensor([300],"int16"), Tensor([0, 300],"int16"), )
2025-03-05 15:45:22.891937 test begin: paddle.bitwise_left_shift(Tensor([300],"int16"), Tensor([200, 0],"int16"), )

[torch error] paddle.bitwise_left_shift(Tensor([300],"int16"), Tensor([200, 0],"int16"), ) 
 The size of tensor a (300) must match the size of tensor b (0) at non-singleton dimension 1
2025-03-05 15:45:22.899772 test begin: paddle.bitwise_left_shift(Tensor([300],"int32"), Tensor([0, 300],"int32"), )

[Pass] paddle.bitwise_left_shift(Tensor([300],"int32"), Tensor([0, 300],"int32"), )
2025-03-05 15:45:22.929052 test begin: paddle.bitwise_left_shift(Tensor([300],"int32"), Tensor([200, 0],"int32"), )

[torch error] paddle.bitwise_left_shift(Tensor([300],"int32"), Tensor([200, 0],"int32"), ) 
 The size of tensor a (300) must match the size of tensor b (0) at non-singleton dimension 1
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 2, in <module>
    from .api_config import TensorConfig, APIConfig, analyse_configs, USE_CACHED_NUMPY, cached_numpy
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/api_config/__init__.py", line 1, in <module>
    from .config_analyzer import TensorConfig, APIConfig, analyse_configs, USE_CACHED_NUMPY, cached_numpy
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/api_config/config_analyzer.py", line 9, in <module>
    import torch
  File "/usr/local/lib/python3.9/dist-packages/torch/__init__.py", line 2665, in <module>
    _import_device_backends()
  File "/usr/local/lib/python3.9/dist-packages/torch/__init__.py", line 2626, in _import_device_backends
    from importlib.metadata import entry_points
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 846, in exec_module
  File "<frozen importlib._bootstrap_external>", line 978, in get_code
  File "<frozen importlib._bootstrap_external>", line 647, in _compile_bytecode
KeyboardInterrupt
2025-03-05 15:57:51.049259 test begin: paddle.digamma(Tensor([0, 10, 10, 2],"float64"), )

W0305 15:58:01.390173 84301 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0305 15:58:01.391225 84301 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.digamma(Tensor([0, 10, 10, 2],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:58:57.896194 test begin: paddle.digamma(Tensor([0, 2, 2],"float32"), )

[cuda error] paddle.digamma(Tensor([0, 2, 2],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:58:57.901308 test begin: paddle.digamma(Tensor([0, 2],"float32"), )

[cuda error] paddle.digamma(Tensor([0, 2],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:58:57.904012 test begin: paddle.digamma(Tensor([0],"float32"), )

[cuda error] paddle.digamma(Tensor([0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:58:57.906478 test begin: paddle.digamma(Tensor([1, 0, 2],"float32"), )

[cuda error] paddle.digamma(Tensor([1, 0, 2],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:58:57.908733 test begin: paddle.digamma(Tensor([1, 0],"float32"), )

[cuda error] paddle.digamma(Tensor([1, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:58:57.910919 test begin: paddle.digamma(Tensor([1, 2, 0],"float32"), )

[cuda error] paddle.digamma(Tensor([1, 2, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:58:57.913005 test begin: paddle.digamma(Tensor([10, 0, 10, 2],"float64"), )

[cuda error] paddle.digamma(Tensor([10, 0, 10, 2],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:58:57.915101 test begin: paddle.digamma(Tensor([10, 10, 0, 2],"float64"), )

[cuda error] paddle.digamma(Tensor([10, 10, 0, 2],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:58:57.917185 test begin: paddle.digamma(Tensor([10, 10, 10, 0],"float64"), )

[cuda error] paddle.digamma(Tensor([10, 10, 10, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:58:57.919222 test begin: paddle.digamma(x=Tensor([0, 3],"float32"), )

[cuda error] paddle.digamma(x=Tensor([0, 3],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:58:57.921274 test begin: paddle.digamma(x=Tensor([0, 6, 6, 6, 6],"float64"), )

[cuda error] paddle.digamma(x=Tensor([0, 6, 6, 6, 6],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:58:57.923428 test begin: paddle.digamma(x=Tensor([0, 6, 6, 6],"float64"), )

[cuda error] paddle.digamma(x=Tensor([0, 6, 6, 6],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:58:57.925511 test begin: paddle.digamma(x=Tensor([0, 6, 6],"float64"), )

[cuda error] paddle.digamma(x=Tensor([0, 6, 6],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:58:57.927591 test begin: paddle.digamma(x=Tensor([3, 0, 6, 6, 6],"float64"), )

[cuda error] paddle.digamma(x=Tensor([3, 0, 6, 6, 6],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:58:57.929625 test begin: paddle.digamma(x=Tensor([3, 0],"float32"), )

[cuda error] paddle.digamma(x=Tensor([3, 0],"float32"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:58:57.931718 test begin: paddle.digamma(x=Tensor([3, 6, 0, 6, 6],"float64"), )

[cuda error] paddle.digamma(x=Tensor([3, 6, 0, 6, 6],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:58:57.933752 test begin: paddle.digamma(x=Tensor([3, 6, 6, 0, 6],"float64"), )

[cuda error] paddle.digamma(x=Tensor([3, 6, 6, 0, 6],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:58:57.935830 test begin: paddle.digamma(x=Tensor([3, 6, 6, 6, 0],"float64"), )

[cuda error] paddle.digamma(x=Tensor([3, 6, 6, 6, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:58:57.937858 test begin: paddle.digamma(x=Tensor([6, 0, 6, 6],"float64"), )

[cuda error] paddle.digamma(x=Tensor([6, 0, 6, 6],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:58:57.939852 test begin: paddle.digamma(x=Tensor([6, 0, 6],"float64"), )

[cuda error] paddle.digamma(x=Tensor([6, 0, 6],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:58:57.941923 test begin: paddle.digamma(x=Tensor([6, 6, 0, 6],"float64"), )

[cuda error] paddle.digamma(x=Tensor([6, 6, 0, 6],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:58:57.943930 test begin: paddle.digamma(x=Tensor([6, 6, 0],"float64"), )

[cuda error] paddle.digamma(x=Tensor([6, 6, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 15:58:57.945939 test begin: paddle.digamma(x=Tensor([6, 6, 6, 0],"float64"), )

[cuda error] paddle.digamma(x=Tensor([6, 6, 6, 0],"float64"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-05 16:01:34.284617 test begin: paddle.diff(x=Tensor([10, 0],"float64"), axis=0, prepend=Tensor([4, 0],"float64"), append=Tensor([4, 0],"float64"), )

W0305 16:01:57.525808 91765 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0305 16:01:57.526901 91765 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.diff(x=Tensor([10, 0],"float64"), axis=0, prepend=Tensor([4, 0],"float64"), append=Tensor([4, 0],"float64"), )
2025-03-05 16:03:46.492853 test begin: paddle.diff(x=Tensor([10, 0],"float64"), axis=0, prepend=Tensor([4, 4],"float64"), append=Tensor([4, 4],"float64"), )

[torch error] paddle.diff(x=Tensor([10, 0],"float64"), axis=0, prepend=Tensor([4, 4],"float64"), append=Tensor([4, 4],"float64"), ) 
 diff expects the shape of tensor to prepend or append to match that of input except along the differencing dimension; input.size(1) = 0, but got tensor.size(1) = 4
2025-03-05 16:03:46.498535 test begin: paddle.diff(x=Tensor([10, 4],"float64"), axis=0, prepend=Tensor([0, 4],"float64"), append=Tensor([4, 4],"float64"), )

[Pass] paddle.diff(x=Tensor([10, 4],"float64"), axis=0, prepend=Tensor([0, 4],"float64"), append=Tensor([4, 4],"float64"), )
2025-03-05 16:03:46.513685 test begin: paddle.diff(x=Tensor([10, 4],"float64"), axis=0, prepend=Tensor([4, 0],"float64"), append=Tensor([4, 4],"float64"), )

[torch error] paddle.diff(x=Tensor([10, 4],"float64"), axis=0, prepend=Tensor([4, 0],"float64"), append=Tensor([4, 4],"float64"), ) 
 diff expects the shape of tensor to prepend or append to match that of input except along the differencing dimension; input.size(1) = 4, but got tensor.size(1) = 0
2025-03-05 16:03:46.523271 test begin: paddle.diff(x=Tensor([10, 4],"float64"), axis=0, prepend=Tensor([4, 4],"float64"), append=Tensor([0, 4],"float64"), )

[Pass] paddle.diff(x=Tensor([10, 4],"float64"), axis=0, prepend=Tensor([4, 4],"float64"), append=Tensor([0, 4],"float64"), )
2025-03-05 16:03:46.592916 test begin: paddle.diff(x=Tensor([10, 4],"float64"), axis=0, prepend=Tensor([4, 4],"float64"), append=Tensor([4, 0],"float64"), )

[torch error] paddle.diff(x=Tensor([10, 4],"float64"), axis=0, prepend=Tensor([4, 4],"float64"), append=Tensor([4, 0],"float64"), ) 
 diff expects the shape of tensor to prepend or append to match that of input except along the differencing dimension; input.size(1) = 4, but got tensor.size(1) = 0
2025-03-05 16:03:46.679855 test begin: paddle.diff(x=Tensor([10],"float64"), prepend=Tensor([0],"float64"), )

[Pass] paddle.diff(x=Tensor([10],"float64"), prepend=Tensor([0],"float64"), )
2025-03-05 16:03:46.695705 test begin: paddle.diff(x=Tensor([10],"float64"), prepend=Tensor([0],"float64"), append=Tensor([4],"float64"), )

[Pass] paddle.diff(x=Tensor([10],"float64"), prepend=Tensor([0],"float64"), append=Tensor([4],"float64"), )
2025-03-05 16:03:46.752895 test begin: paddle.diff(x=Tensor([10],"float64"), prepend=Tensor([4],"float64"), append=Tensor([0],"float64"), )

[Pass] paddle.diff(x=Tensor([10],"float64"), prepend=Tensor([4],"float64"), append=Tensor([0],"float64"), )
2025-03-05 16:03:46.815267 test begin: paddle.diff(x=Tensor([4, 0, 4, 4],"float64"), )

[Pass] paddle.diff(x=Tensor([4, 0, 4, 4],"float64"), )
2025-03-05 16:03:46.926819 test begin: paddle.diff(x=Tensor([4, 0, 4, 4],"float64"), axis=-2, )

[Pass] paddle.diff(x=Tensor([4, 0, 4, 4],"float64"), axis=-2, )
2025-03-05 16:03:46.953410 test begin: paddle.diff(x=Tensor([4, 0, 4, 4],"float64"), axis=2, )

[Pass] paddle.diff(x=Tensor([4, 0, 4, 4],"float64"), axis=2, )
2025-03-05 16:03:47.061550 test begin: paddle.diff(x=Tensor([4, 0, 4],"float64"), )

[Pass] paddle.diff(x=Tensor([4, 0, 4],"float64"), )
2025-03-05 16:03:47.071430 test begin: paddle.diff(x=Tensor([4, 0],"float64"), )

[Pass] paddle.diff(x=Tensor([4, 0],"float64"), )
2025-03-05 16:03:47.088402 test begin: paddle.diff(x=Tensor([4, 4, 0, 4],"float64"), )

[Pass] paddle.diff(x=Tensor([4, 4, 0, 4],"float64"), )
2025-03-05 16:03:47.096742 test begin: paddle.diff(x=Tensor([4, 4, 0, 4],"float64"), axis=-2, )

[Pass] paddle.diff(x=Tensor([4, 4, 0, 4],"float64"), axis=-2, )
2025-03-05 16:03:47.104412 test begin: paddle.diff(x=Tensor([4, 4, 0, 4],"float64"), axis=2, )

[Pass] paddle.diff(x=Tensor([4, 4, 0, 4],"float64"), axis=2, )
2025-03-05 16:03:47.112670 test begin: paddle.diff(x=Tensor([4, 4, 0],"float64"), )

[Pass] paddle.diff(x=Tensor([4, 4, 0],"float64"), )
2025-03-05 16:03:47.119369 test begin: paddle.diff(x=Tensor([4, 4, 4, 0],"float64"), )

[Pass] paddle.diff(x=Tensor([4, 4, 4, 0],"float64"), )
2025-03-05 16:03:47.134266 test begin: paddle.diff(x=Tensor([4, 4, 4, 0],"float64"), axis=-2, )

[Pass] paddle.diff(x=Tensor([4, 4, 4, 0],"float64"), axis=-2, )
2025-03-05 16:03:47.141522 test begin: paddle.diff(x=Tensor([4, 4, 4, 0],"float64"), axis=2, )

[Pass] paddle.diff(x=Tensor([4, 4, 4, 0],"float64"), axis=2, )
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest/PaddleAPITest/engine.py", line 1, in <module>
    from tester import TensorConfig, APIConfig, analyse_configs
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/__init__.py", line 1, in <module>
    from .base import APITestBase
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/base.py", line 2, in <module>
    from .api_config import TensorConfig, APIConfig, analyse_configs, USE_CACHED_NUMPY, cached_numpy
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/api_config/__init__.py", line 1, in <module>
    from .config_analyzer import TensorConfig, APIConfig, analyse_configs, USE_CACHED_NUMPY, cached_numpy
  File "/host_home/wanghuan29/APItest/PaddleAPITest/tester/api_config/config_analyzer.py", line 9, in <module>
    import torch
  File "/usr/local/lib/python3.9/dist-packages/torch/__init__.py", line 2486, in <module>
    from torch import _meta_registrations
  File "/usr/local/lib/python3.9/dist-packages/torch/_meta_registrations.py", line 10, in <module>
    from torch._decomp import (
  File "/usr/local/lib/python3.9/dist-packages/torch/_decomp/__init__.py", line 249, in <module>
    import torch._decomp.decompositions
  File "/usr/local/lib/python3.9/dist-packages/torch/_decomp/decompositions.py", line 15, in <module>
    import torch._prims as prims
  File "/usr/local/lib/python3.9/dist-packages/torch/_prims/__init__.py", line 934, in <module>
    tan = _make_elementwise_unary_prim(
  File "/usr/local/lib/python3.9/dist-packages/torch/_prims/__init__.py", line 491, in _make_elementwise_unary_prim
    return _make_prim(
  File "/usr/local/lib/python3.9/dist-packages/torch/_prims/__init__.py", line 319, in _make_prim
    prim_def = torch.library.custom_op(
  File "/usr/local/lib/python3.9/dist-packages/torch/_library/custom_ops.py", line 157, in custom_op
    return inner(fn)
  File "/usr/local/lib/python3.9/dist-packages/torch/_library/custom_ops.py", line 138, in inner
    result = CustomOpDef(namespace, opname, schema_str, fn)
  File "/usr/local/lib/python3.9/dist-packages/torch/_library/custom_ops.py", line 186, in __init__
    self._register_to_dispatcher()
  File "/usr/local/lib/python3.9/dist-packages/torch/_library/custom_ops.py", line 618, in _register_to_dispatcher
    autograd_impl = autograd.make_autograd_impl(self._opoverload, self)
  File "/usr/local/lib/python3.9/dist-packages/torch/_library/autograd.py", line 29, in make_autograd_impl
    class Metadata:
  File "/usr/lib/python3.9/dataclasses.py", line 1021, in dataclass
    return wrap(cls)
  File "/usr/lib/python3.9/dataclasses.py", line 1013, in wrap
    return _process_class(cls, init, repr, eq, order, unsafe_hash, frozen)
  File "/usr/lib/python3.9/dataclasses.py", line 993, in _process_class
    str(inspect.signature(cls)).replace(' -> None', ''))
  File "/usr/lib/python3.9/inspect.py", line 3070, in __str__
    formatted = str(param)
  File "/usr/lib/python3.9/inspect.py", line 2589, in __str__
    formatannotation(self._annotation))
  File "/usr/lib/python3.9/inspect.py", line 1235, in formatannotation
    return repr(annotation).replace('typing.', '')
  File "/usr/lib/python3.9/typing.py", line 791, in __repr__
    args = ", ".join([_type_repr(a) for a in self.__args__])
  File "/usr/lib/python3.9/typing.py", line 791, in <listcomp>
    args = ", ".join([_type_repr(a) for a in self.__args__])
  File "/usr/lib/python3.9/typing.py", line 178, in _type_repr
    if isinstance(obj, types.GenericAlias):
KeyboardInterrupt
