paddle.nn.functional.flashmask_attention(Tensor([1, 2048, 8, 96],"float16"), Tensor([1, 2048, 8, 96],"float16"), Tensor([1, 2048, 8, 96],"float16"), startend_row_indices=Tensor([1, 1, 2048, 1],"int32"), causal=True, )
paddle.nn.functional.fractional_max_pool2d(Tensor([2, 3, 7, 7],"float32"), output_size=5, kernel_size=None, random_u=0.5, return_mask=False, name=None, )
paddle.nn.functional.fractional_max_pool2d(Tensor([2, 3, 7, 7],"float32"), output_size=list[2,5,], kernel_size=None, random_u=0.7, return_mask=False, name=None, )
paddle.nn.functional.fractional_max_pool2d(Tensor([2, 3, 7, 7],"float32"), output_size=list[3,3,], kernel_size=2, random_u=0.6, return_mask=False, name=None, )
paddle.nn.functional.fractional_max_pool2d(Tensor([2, 3, 7, 7],"float32"), output_size=list[3,3,], kernel_size=None, random_u=0.3, return_mask=False, name=None, )
paddle.nn.functional.fractional_max_pool2d(Tensor([2, 3, 7, 7],"float32"), output_size=list[3,3,], kernel_size=list[2,2,], random_u=0.6, return_mask=False, name=None, )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([2, 3, 7, 7],"float16"), output_size=list[3,3,], random_u=0.3, )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([2, 3, 7, 7],"float32"), kernel_size=2, output_size=list[3,3,], random_u=0.6, )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([2, 3, 7, 7],"float32"), kernel_size=list[2,2,], output_size=list[3,3,], random_u=0.6, )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([2, 3, 7, 7],"float32"), output_size=5, random_u=0.5, )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([2, 3, 7, 7],"float32"), output_size=list[2,5,], random_u=0.7, )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([2, 3, 7, 7],"float32"), output_size=list[3,3,], random_u=0.3, )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([2, 3, 7, 7],"float32"), output_size=list[3,None,], random_u=0.6, )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([2, 3, 7, 7],"float32"), output_size=list[None,3,], random_u=0.6, )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([2, 3, 7, 7],"float32"), return_mask=False, output_size=list[3,3,], random_u=0.3, )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([2, 3, 7, 7],"float64"), output_size=list[3,3,], random_u=0.3, )
paddle.nn.functional.fractional_max_pool2d(x=Tensor([2, 3, 7, 7],"float64"), output_size=list[3,3,], random_u=None, )
paddle.nn.functional.fractional_max_pool3d(Tensor([2, 3, 7, 7, 7],"float32"), output_size=5, kernel_size=None, random_u=0.5, return_mask=False, name=None, )
paddle.nn.functional.fractional_max_pool3d(Tensor([2, 3, 7, 7, 7],"float32"), output_size=list[2,3,5,], kernel_size=None, random_u=0.7, return_mask=False, name=None, )
paddle.nn.functional.fractional_max_pool3d(Tensor([2, 3, 7, 7, 7],"float32"), output_size=list[3,3,3,], kernel_size=2, random_u=0.6, return_mask=False, name=None, )
paddle.nn.functional.fractional_max_pool3d(Tensor([2, 3, 7, 7, 7],"float32"), output_size=list[3,3,3,], kernel_size=None, random_u=0.3, return_mask=False, name=None, )
paddle.nn.functional.fractional_max_pool3d(Tensor([2, 3, 7, 7, 7],"float32"), output_size=list[3,3,3,], kernel_size=list[2,2,2,], random_u=0.6, return_mask=False, name=None, )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([2, 3, 7, 7, 7],"float16"), output_size=list[3,3,3,], random_u=0.3, )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([2, 3, 7, 7, 7],"float32"), kernel_size=2, output_size=list[3,3,3,], random_u=0.6, )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([2, 3, 7, 7, 7],"float32"), kernel_size=list[2,2,2,], output_size=list[3,3,3,], random_u=0.6, )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([2, 3, 7, 7, 7],"float32"), output_size=5, random_u=0.5, )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([2, 3, 7, 7, 7],"float32"), output_size=list[2,3,5,], random_u=0.7, )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([2, 3, 7, 7, 7],"float32"), output_size=list[3,3,3,], random_u=0.3, )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([2, 3, 7, 7, 7],"float32"), output_size=list[3,3,None,], random_u=0.6, )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([2, 3, 7, 7, 7],"float32"), output_size=list[3,None,3,], random_u=0.6, )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([2, 3, 7, 7, 7],"float32"), output_size=list[None,3,3,], random_u=0.6, )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([2, 3, 7, 7, 7],"float64"), output_size=list[3,3,3,], random_u=0.3, )
paddle.nn.functional.fractional_max_pool3d(x=Tensor([2, 3, 7, 7, 7],"float64"), output_size=list[3,3,3,], random_u=None, )
paddle.audio.functional.get_window("triang", 512, )
paddle.nn.functional.class_center_sample(Tensor([10],"int32"), 20, 6, )
paddle.nn.functional.class_center_sample(Tensor([10],"int64"), 20, 6, )
paddle.nn.functional.class_center_sample(Tensor([15],"int32"), 20, 8, )
paddle.nn.functional.class_center_sample(Tensor([20],"int32"), num_classes=10, num_samples=6, group=None, )
paddle.nn.functional.class_center_sample(Tensor([20],"int64"), num_classes=10, num_samples=6, group=None, )
paddle.nn.functional.class_center_sample(Tensor([5],"int32"), 10, 8, )
paddle.nn.functional.max_pool2d(Tensor([2, 256, 1, 1],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.max_pool2d(Tensor([4, 256, 1, 1],"float16"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.max_pool2d(Tensor([4, 256, 1, 1],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.autograd.hessian(Tensor([5, 1],"float32"), Tensor([5, 2],"float32"), batch_axis=0, )
paddle.autograd.hessian(Tensor([5, 1],"float32"), list[Tensor([5, 2],"float32"),Tensor([5, 2],"float32"),], batch_axis=0, )
paddle.autograd.hessian(Tensor([],"float32"), Tensor([4],"float32"), )
paddle.autograd.hessian(Tensor([],"float32"), Tensor([4],"float32"), batch_axis=None, )
paddle.autograd.jacobian(Tensor([2, 3],"float64"), Tensor([2, 3],"float64"), batch_axis=0, )
paddle.autograd.jacobian(Tensor([2],"float64"), Tensor([2],"float64"), batch_axis=None, )
paddle.autograd.jacobian(Tensor([6],"float64"), Tensor([6],"float64"), batch_axis=None, )
paddle.autograd.jacobian(Tensor([],"float64"), list[Tensor([4],"float64"),Tensor([4],"float64"),], batch_axis=None, )
paddle.incubate.nn.functional.masked_multihead_attention(Tensor([2, 12288],"float16"), Tensor([2, 2, 32, 33, 128],"float16"), Tensor([3, 32, 128],"float16"), Tensor([2, 1, 1, 33],"float16"), None, None, None, None, None, None, None, 1, 0, False, "default", -1, 1, 126, -126, )
paddle.incubate.nn.functional.masked_multihead_attention(Tensor([2, 12288],"float16"), Tensor([2, 2, 32, 33, 128],"float16"), Tensor([3, 32, 128],"float16"), Tensor([2, 1, 1, 33],"float16"), None, None, None, None, None, None, None, 1, 0, False, "default", 10, 1, 126, -126, )
paddle.incubate.nn.functional.masked_multihead_attention(Tensor([2, 12288],"int32"), Tensor([2, 2, 32, 33, 128],"float16"), Tensor([3, 32, 128],"float16"), Tensor([2, 1, 1, 33],"float16"), None, None, None, None, Tensor([3, 32, 128],"float32"), None, None, 1, 0, False, "fp16", -1, 1, 126, -126, )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([128, 1536],"float16"), Tensor([4, 8, 64, 64],"float16"), Tensor([4, 8, 64, 64],"float16"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([128],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 2],"int32"), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 64, 64, False, )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([128, 1536],"float16"), Tensor([4, 8, 64, 64],"float16"), Tensor([4, 8, 64, 64],"float16"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([128],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 2],"int32"), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 64, 64, False, out_scale=1.0, )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([128, 1536],"float16"), Tensor([4, 8, 64, 64],"float16"), Tensor([4, 8, 64, 64],"float16"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([128],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 2],"int32"), None, None, None, None, None, None, None, None, None, None, None, None, Tensor([2, 1, 128, 1, 32],"float32"), None, None, 64, 64, False, )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([128, 1536],"float16"), Tensor([4, 8, 64, 64],"float16"), Tensor([4, 8, 64, 64],"float16"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([128],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 2],"int32"), None, None, None, None, None, None, None, None, None, None, Tensor([1],"int32"), Tensor([1],"int32"), None, None, None, 64, 64, False, )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([128, 1536],"float16"), Tensor([4, 8, 64, 64],"uint8"), Tensor([4, 8, 64, 64],"uint8"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([128],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 2],"int32"), None, None, Tensor([2, 8],"float32"), Tensor([2, 8],"float32"), Tensor([2, 8],"float32"), Tensor([2, 8],"float32"), None, None, None, None, None, None, None, None, None, 64, 64, False, use_dynamic_cachekv_quant=True, )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([128, 1536],"float16"), Tensor([4, 8, 64, 64],"uint8"), Tensor([4, 8, 64, 64],"uint8"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([128],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 2],"int32"), None, None, Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), None, None, None, None, None, None, None, None, None, 64, 64, False, use_dynamic_cachekv_quant=False, )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([128, 1536],"float16"), Tensor([6, 8, 64, 64],"float16"), Tensor([6, 8, 64, 64],"float16"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([128],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 3],"int32"), Tensor([2, 8, 64, 64],"float16"), Tensor([2, 8, 64, 64],"float16"), None, None, None, None, None, None, None, None, None, None, None, Tensor([2, 1, 64, 128],"float16"), None, 64, 64, False, )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([128, 1536],"int32"), Tensor([4, 8, 64, 64],"float16"), Tensor([4, 8, 64, 64],"float16"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([128],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 2],"int32"), None, None, None, None, None, None, Tensor([1536],"float32"), Tensor([1536],"float16"), None, None, None, None, None, None, None, 64, 64, False, compute_dtype="fp16", )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([128, 1536],"int32"), Tensor([4, 8, 64, 64],"float16"), Tensor([4, 8, 64, 64],"float16"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([128],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 2],"int32"), None, None, None, None, None, None, Tensor([1536],"float32"), Tensor([1536],"float16"), Tensor([512],"float16"), Tensor([512],"float16"), None, None, None, None, None, 64, 64, False, compute_dtype="fp16", out_scale=1.0, )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([128, 768],"float16"), Tensor([4, 2, 64, 64],"float16"), Tensor([4, 2, 64, 64],"float16"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([128],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 2],"int32"), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 64, 64, False, )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([128, 768],"float16"), Tensor([4, 2, 64, 64],"float16"), Tensor([4, 2, 64, 64],"float16"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([128],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 2],"int32"), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 64, 64, False, out_scale=1.0, )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([128, 768],"float16"), Tensor([4, 2, 64, 64],"float16"), Tensor([4, 2, 64, 64],"float16"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([128],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 2],"int32"), None, None, None, None, None, None, None, None, None, None, None, None, Tensor([2, 1, 128, 1, 32],"float32"), None, None, 64, 64, False, )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([128, 768],"float16"), Tensor([4, 2, 64, 64],"float16"), Tensor([4, 2, 64, 64],"float16"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([128],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 2],"int32"), None, None, None, None, None, None, None, None, None, None, Tensor([1],"int32"), Tensor([1],"int32"), None, None, None, 64, 64, False, )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([128, 768],"float16"), Tensor([4, 2, 64, 64],"uint8"), Tensor([4, 2, 64, 64],"uint8"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([128],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 2],"int32"), None, None, Tensor([2, 2],"float32"), Tensor([2, 2],"float32"), Tensor([2, 2],"float32"), Tensor([2, 2],"float32"), None, None, None, None, None, None, None, None, None, 64, 64, False, use_dynamic_cachekv_quant=True, )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([128, 768],"float16"), Tensor([4, 2, 64, 64],"uint8"), Tensor([4, 2, 64, 64],"uint8"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([128],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 2],"int32"), None, None, Tensor([2],"float32"), Tensor([2],"float32"), Tensor([2],"float32"), Tensor([2],"float32"), None, None, None, None, None, None, None, None, None, 64, 64, False, use_dynamic_cachekv_quant=False, )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([128, 768],"int32"), Tensor([4, 2, 64, 64],"float16"), Tensor([4, 2, 64, 64],"float16"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([128],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 2],"int32"), None, None, None, None, None, None, Tensor([768],"float32"), Tensor([768],"float16"), None, None, None, None, None, None, None, 64, 64, False, compute_dtype="fp16", )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([128, 768],"int32"), Tensor([4, 2, 64, 64],"float16"), Tensor([4, 2, 64, 64],"float16"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([128],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 2],"int32"), None, None, None, None, None, None, Tensor([768],"float32"), Tensor([768],"float16"), Tensor([512],"float16"), Tensor([512],"float16"), None, None, None, None, None, 64, 64, False, compute_dtype="fp16", out_scale=1.0, )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([2, 1536],"float16"), Tensor([4, 8, 64, 64],"float16"), Tensor([4, 8, 64, 64],"float16"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 2],"int32"), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 1, 64, False, out_scale=1.0, )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([2, 1536],"float16"), Tensor([4, 8, 64, 64],"float16"), Tensor([4, 8, 64, 64],"float16"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 2],"int32"), None, None, None, None, None, None, None, None, None, None, None, None, None, None, Tensor([2, 8, 1, 65],"float16"), 1, 64, False, )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([2, 1536],"float16"), Tensor([4, 8, 64, 64],"float16"), Tensor([4, 8, 64, 64],"float16"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 2],"int32"), None, None, None, None, None, None, None, None, None, None, None, None, Tensor([2, 1, 128, 1, 32],"float32"), None, None, 1, 64, False, )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([2, 1536],"float16"), Tensor([4, 8, 64, 64],"float16"), Tensor([4, 8, 64, 64],"float16"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 2],"int32"), None, None, None, None, None, None, None, None, None, None, Tensor([1],"int32"), Tensor([1],"int32"), None, None, Tensor([2, 8, 1, 65],"float16"), 1, 64, False, )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([2, 1536],"float16"), Tensor([4, 8, 64, 64],"uint8"), Tensor([4, 8, 64, 64],"uint8"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 2],"int32"), None, None, Tensor([2, 8],"float32"), Tensor([2, 8],"float32"), Tensor([2, 8],"float32"), Tensor([2, 8],"float32"), None, None, None, None, None, None, None, None, None, 1, 64, False, use_dynamic_cachekv_quant=True, )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([2, 1536],"float16"), Tensor([4, 8, 64, 64],"uint8"), Tensor([4, 8, 64, 64],"uint8"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 2],"int32"), None, None, Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), None, None, None, None, None, None, None, None, None, 1, 64, False, use_dynamic_cachekv_quant=False, )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([2, 1536],"float16"), Tensor([6, 8, 64, 64],"float16"), Tensor([6, 8, 64, 64],"float16"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 3],"int32"), Tensor([2, 8, 64, 64],"float16"), Tensor([2, 8, 64, 64],"float16"), None, None, None, None, None, None, None, None, None, None, None, Tensor([2, 1, 64, 128],"float16"), None, 1, 64, False, )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([2, 1536],"int32"), Tensor([4, 8, 64, 64],"float16"), Tensor([4, 8, 64, 64],"float16"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 2],"int32"), None, None, None, None, None, None, Tensor([1536],"float32"), Tensor([1536],"float16"), None, None, None, None, None, None, None, 1, 64, False, compute_dtype="fp16", )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([2, 1536],"int32"), Tensor([4, 8, 64, 64],"float16"), Tensor([4, 8, 64, 64],"float16"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 2],"int32"), None, None, None, None, None, None, Tensor([1536],"float32"), Tensor([1536],"float16"), Tensor([512],"float16"), Tensor([512],"float16"), None, None, None, None, None, 1, 64, False, compute_dtype="fp16", out_scale=1.0, )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([2, 768],"float16"), Tensor([4, 2, 64, 64],"float16"), Tensor([4, 2, 64, 64],"float16"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 2],"int32"), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, 1, 64, False, out_scale=1.0, )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([2, 768],"float16"), Tensor([4, 2, 64, 64],"float16"), Tensor([4, 2, 64, 64],"float16"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 2],"int32"), None, None, None, None, None, None, None, None, None, None, None, None, None, None, Tensor([2, 8, 1, 65],"float16"), 1, 64, False, )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([2, 768],"float16"), Tensor([4, 2, 64, 64],"float16"), Tensor([4, 2, 64, 64],"float16"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 2],"int32"), None, None, None, None, None, None, None, None, None, None, None, None, Tensor([2, 1, 128, 1, 32],"float32"), None, None, 1, 64, False, )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([2, 768],"float16"), Tensor([4, 2, 64, 64],"float16"), Tensor([4, 2, 64, 64],"float16"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 2],"int32"), None, None, None, None, None, None, None, None, None, None, Tensor([1],"int32"), Tensor([1],"int32"), None, None, Tensor([2, 8, 1, 65],"float16"), 1, 64, False, )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([2, 768],"float16"), Tensor([4, 2, 64, 64],"uint8"), Tensor([4, 2, 64, 64],"uint8"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 2],"int32"), None, None, Tensor([2, 2],"float32"), Tensor([2, 2],"float32"), Tensor([2, 2],"float32"), Tensor([2, 2],"float32"), None, None, None, None, None, None, None, None, None, 1, 64, False, use_dynamic_cachekv_quant=True, )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([2, 768],"float16"), Tensor([4, 2, 64, 64],"uint8"), Tensor([4, 2, 64, 64],"uint8"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 2],"int32"), None, None, Tensor([2],"float32"), Tensor([2],"float32"), Tensor([2],"float32"), Tensor([2],"float32"), None, None, None, None, None, None, None, None, None, 1, 64, False, use_dynamic_cachekv_quant=False, )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([2, 768],"int32"), Tensor([4, 2, 64, 64],"float16"), Tensor([4, 2, 64, 64],"float16"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 2],"int32"), None, None, None, None, None, None, Tensor([768],"float32"), Tensor([768],"float16"), None, None, None, None, None, None, None, 1, 64, False, compute_dtype="fp16", )
paddle.incubate.nn.functional.block_multihead_attention(Tensor([2, 768],"int32"), Tensor([4, 2, 64, 64],"float16"), Tensor([4, 2, 64, 64],"float16"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([2],"int32"), Tensor([3],"int32"), Tensor([3],"int32"), Tensor([2, 2],"int32"), None, None, None, None, None, None, Tensor([768],"float32"), Tensor([768],"float16"), Tensor([512],"float16"), Tensor([512],"float16"), None, None, None, None, None, 1, 64, False, compute_dtype="fp16", out_scale=1.0, )
