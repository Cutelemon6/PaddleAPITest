paddle.diag(x=Tensor([2],"float64"), name="test name", )
paddle.divide(x=Tensor([3, 1],"float64"), y=Tensor([3, 2],"float64"), name="", )
paddle.floor_divide(x=Tensor([3, 1],"int32"), y=Tensor([3, 1],"int32"), name="", )
paddle.tanh(x=Tensor([2, 3, 4],"float64"), name="test name", )
paddle.cumsum(Tensor([5, 6],"float64"), axis=Tensor([1],"int32"), )
paddle.cumsum(Tensor([9, 10, 11],"float32"), axis=Tensor([1],"int64"), )
paddle.clip(x=Tensor([2],"float64"), min=Tensor([1],"float64"), max=Tensor([1],"float64"), name="test name", )
paddle.clip(x=Tensor([3, 3],"float64"), min=1, max=Tensor([1],"float64"), )
paddle.clip(Tensor([107136],"float32"), max=Tensor([],"int64"), )
paddle.clip(Tensor([],"float32"), Tensor([],"float32"), Tensor([],"float32"), )
paddle.clip(x=Tensor([1, 2],"float64"), min=Tensor([1],"float64"), max=Tensor([1],"float64"), )
paddle.clip(x=Tensor([2, 2],"float64"), min=Tensor([1],"float64"), max=Tensor([1],"float64"), )
paddle.clip(x=Tensor([3, 3],"float64"), min=Tensor([1],"float64"), max=None, )
paddle.clip(x=Tensor([3, 3],"float64"), min=Tensor([1],"float64"), max=Tensor([1],"float64"), )
paddle.clip(x=Tensor([3],"float64"), min=Tensor([1],"float64"), max=Tensor([1],"float64"), )
paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float32"), Tensor([1, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float64"), Tensor([1, 3, 8],"int32"), kernel_size=2, stride=2, )
paddle.nn.functional.max_unpool2d(Tensor([2, 4, 11, 11],"float32"), Tensor([2, 4, 11, 11],"int32"), kernel_size=4, stride=None, padding=2, data_format="NCHW", output_size=None, name=None, )
paddle.nn.functional.max_unpool2d(Tensor([2, 4, 11, 11],"float64"), Tensor([2, 4, 11, 11],"int32"), kernel_size=4, stride=None, padding=2, data_format="NCHW", output_size=None, name=None, )
paddle.nn.functional.max_unpool2d(Tensor([2, 4, 20, 10],"float32"), Tensor([2, 4, 20, 10],"int32"), kernel_size=tuple(2,4,), stride=None, padding=0, data_format="NCHW", output_size=None, name=None, )
paddle.nn.functional.max_unpool2d(Tensor([2, 4, 20, 10],"float64"), Tensor([2, 4, 20, 10],"int32"), kernel_size=tuple(2,4,), stride=None, padding=0, data_format="NCHW", output_size=None, name=None, )
paddle.nn.functional.max_unpool2d(Tensor([2, 4, 20, 20],"float32"), Tensor([2, 4, 20, 20],"int32"), kernel_size=2, stride=None, padding=0, data_format="NCHW", output_size=None, name=None, )
paddle.nn.functional.max_unpool2d(Tensor([2, 4, 20, 20],"float32"), Tensor([2, 4, 20, 20],"int32"), kernel_size=2, stride=None, padding=0, data_format="NCHW", output_size=tuple(2,4,40,40,), name=None, )
paddle.nn.functional.max_unpool2d(Tensor([2, 4, 20, 20],"float64"), Tensor([2, 4, 20, 20],"int32"), kernel_size=2, stride=None, padding=0, data_format="NCHW", output_size=None, name=None, )
paddle.nn.functional.max_unpool2d(Tensor([2, 4, 20, 20],"float64"), Tensor([2, 4, 20, 20],"int32"), kernel_size=2, stride=None, padding=0, data_format="NCHW", output_size=tuple(2,4,40,40,), name=None, )
paddle.nn.functional.max_unpool2d(Tensor([2, 4, 21, 21],"float32"), Tensor([2, 4, 21, 21],"int32"), kernel_size=4, stride=2, padding=2, data_format="NCHW", output_size=None, name=None, )
paddle.nn.functional.max_unpool2d(Tensor([2, 4, 21, 21],"float64"), Tensor([2, 4, 21, 21],"int32"), kernel_size=4, stride=2, padding=2, data_format="NCHW", output_size=None, name=None, )
paddle.nn.functional.max_unpool2d(Tensor([2, 4, 7, 8],"float64"), Tensor([2, 4, 7, 8],"int64"), list[2,2,], stride=list[2,2,], padding=list[0,0,], data_format="NCHW", output_size=list[14,16,], name=None, )
paddle.nn.functional.max_unpool2d(Tensor([3, 2, 5, 5],"float64"), Tensor([3, 2, 5, 5],"int32"), list[4,4,], stride=list[2,2,], padding=list[0,0,], data_format="NCHW", output_size=list[12,12,], name=None, )
paddle.nn.functional.max_unpool2d(Tensor([3, 2, 5, 5],"float64"), Tensor([3, 2, 5, 5],"int64"), list[4,4,], stride=list[2,2,], padding=list[0,0,], data_format="NCHW", output_size=list[12,12,], name=None, )
paddle.nn.functional.max_unpool3d(Tensor([1, 1, 4, 5, 6],"float64"), Tensor([1, 1, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
paddle.nn.functional.zeropad2d(Tensor([4, 3, 224, 224],"int32"), list[2,2,2,2,], )
paddle.nn.functional.zeropad2d(Tensor([4, 3, 224, 224],"int64"), list[1,2,3,4,], )
paddle.nn.functional.zeropad2d(Tensor([4, 3, 224, 224],"int64"), list[2,2,2,2,], )
paddle.nn.functional.zeropad2d(Tensor([4, 3, 224, 224],"int64"), list[2,2,4,1,], )
paddle.nn.functional.zeropad2d(Tensor([4, 3, 224, 224],"int64"), Tensor([4],"int32"), )
paddle.nn.functional.zeropad2d(Tensor([4, 3, 224, 224],"int64"), tuple(1,2,3,4,), )
paddle.nn.functional.softmax(Tensor([1, 2, 2048, 2048],"bfloat16"), )
paddle.nn.functional.softmax(Tensor([2, 2],"float32"), axis=Tensor([],"int64"), )
paddle.nn.quant.weight_only_linear(Tensor([1, 1, 64],"float16"), Tensor([128, 64],"int8"), bias=Tensor([256],"float16"), weight_scale=Tensor([256],"float16"), weight_dtype="int4", group_size=-1, )
paddle.nn.quant.weight_only_linear(Tensor([1, 1, 64],"float16"), Tensor([256, 64],"int8"), bias=Tensor([256],"float16"), weight_scale=Tensor([256],"float16"), weight_dtype="int8", group_size=-1, )
paddle.Tensor.__sub__(Tensor([0],"float32"), Tensor([0],"float32"), )