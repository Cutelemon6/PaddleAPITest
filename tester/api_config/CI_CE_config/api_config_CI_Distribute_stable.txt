paddle.add(Tensor([1, 2, 8, 8],"float32"), Tensor([1, 2, 8, 8],"float32"), )
paddle.add(Tensor([1, 4, 8, 8],"float32"), Tensor([1, 4, 8, 8],"float32"), )
paddle.add(Tensor([2, 1, 8, 8],"float32"), Tensor([2, 1, 8, 8],"float32"), )
paddle.add(Tensor([2, 2, 8, 16],"float32"), Tensor([2, 2, 8, 16],"float32"), )
paddle.add(Tensor([2, 4, 8, 8],"float32"), Tensor([2, 4, 8, 8],"float32"), )
paddle.arange(0, 16, 2, dtype="float32", )
paddle.arange(0, 8, 1, dtype="float32", )
paddle.arange(0, 8, 2, dtype="float32", )
paddle.arange(9, )
paddle.assign(Tensor([1, 3, 3, 5],"float32"), )
paddle.cast(Tensor([1, 2, 2, 2],"float64"), Dtype(float32), )
paddle.cast(Tensor([8, 16, 1, 129],"float64"), Dtype(float32), )
paddle.cast(Tensor([8, 16, 128, 128],"float64"), Dtype(float16), )
paddle.cast(Tensor([8, 16, 128, 128],"float64"), Dtype(float32), )
paddle.concat(list[Tensor([1, 2, 8, 4],"float32"),Tensor([1, 2, 8, 4],"float32"),], axis=-1, )
paddle.concat(list[Tensor([1, 4, 8, 4],"float32"),Tensor([1, 4, 8, 4],"float32"),], axis=-1, )
paddle.concat(list[Tensor([2, 1, 8, 4],"float32"),Tensor([2, 1, 8, 4],"float32"),], axis=-1, )
paddle.concat(list[Tensor([2, 2, 8, 8],"float32"),Tensor([2, 2, 8, 8],"float32"),], axis=-1, )
paddle.concat(list[Tensor([2, 4, 8, 4],"float32"),Tensor([2, 4, 8, 4],"float32"),], axis=-1, )
paddle.concat(list[Tensor([8, 16, 128, 64],"float32"),Tensor([8, 16, 1, 64],"float32"),], axis=-2, )
paddle.full(shape=list[1,], dtype=VarType(float64), fill_value=0.2, )
paddle.gather(Tensor([3, 2],"int64"), Tensor([1, 1],"int64"), )
paddle.incubate.nn.functional.fused_bias_dropout_residual_layer_norm(Tensor([8, 128, 1024],"float16"), Tensor([8, 128, 1024],"float16"), Tensor([1024],"float16"), Tensor([1024],"float32"), Tensor([1024],"float32"), 0.0, 1e-05, )
paddle.incubate.nn.functional.fused_bias_dropout_residual_layer_norm(Tensor([8, 128, 1024],"float32"), Tensor([8, 128, 1024],"float32"), None, Tensor([1024],"float32"), Tensor([1024],"float32"), 0.0, 1e-05, )
paddle.incubate.nn.functional.fused_bias_dropout_residual_layer_norm(Tensor([8, 128, 1024],"float32"), Tensor([8, 128, 1024],"float32"), Tensor([1024],"float32"), Tensor([1024],"float32"), Tensor([1024],"float32"), 0.0, 1e-05, )
paddle.incubate.nn.functional.fused_bias_dropout_residual_layer_norm(x=Tensor([1, 2, 4],"float32"), residual=Tensor([1, 2, 4],"float32"), bias=None, ln_scale=Tensor([4],"float32"), ln_bias=None, dropout_rate=0.0, ln_epsilon=1e-05, training=True, mode="upscale_in_train", name=None, )
paddle.incubate.nn.functional.fused_bias_dropout_residual_layer_norm(x=Tensor([1, 2, 4],"float32"), residual=Tensor([1, 2, 4],"float32"), bias=Tensor([4],"float32"), ln_scale=Tensor([4],"float32"), ln_bias=Tensor([4],"float32"), dropout_rate=0.0, ln_epsilon=1e-05, training=True, mode="upscale_in_train", name=None, )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float16"), Tensor([2, 1024, 1, 1],"float16"), p=0.0, training=False, mode="downscale_in_infer", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float16"), Tensor([2, 1024, 1, 1],"float16"), p=0.0, training=False, mode="upscale_in_train", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float16"), Tensor([2, 1024, 1, 1],"float16"), p=0.0, training=True, mode="downscale_in_infer", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float16"), Tensor([2, 1024, 1, 1],"float16"), p=0.0, training=True, mode="upscale_in_train", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float16"), Tensor([2, 1024, 1, 1],"float16"), p=0.5, training=False, mode="downscale_in_infer", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float16"), Tensor([2, 1024, 1, 1],"float16"), p=0.5, training=False, mode="upscale_in_train", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float16"), Tensor([2, 1024, 1, 1],"float16"), p=0.5, training=True, mode="downscale_in_infer", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float16"), Tensor([2, 1024, 1, 1],"float16"), p=0.5, training=True, mode="upscale_in_train", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float16"), Tensor([2, 1024, 1, 1],"float16"), p=0.9, training=False, mode="downscale_in_infer", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float16"), Tensor([2, 1024, 1, 1],"float16"), p=0.9, training=False, mode="upscale_in_train", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float16"), Tensor([2, 1024, 1, 1],"float16"), p=0.9, training=True, mode="downscale_in_infer", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float16"), Tensor([2, 1024, 1, 1],"float16"), p=0.9, training=True, mode="upscale_in_train", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float16"), Tensor([2, 1024, 1, 1],"float16"), p=1.0, training=False, mode="downscale_in_infer", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float16"), Tensor([2, 1024, 1, 1],"float16"), p=1.0, training=False, mode="upscale_in_train", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float16"), Tensor([2, 1024, 1, 1],"float16"), p=1.0, training=True, mode="downscale_in_infer", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float16"), Tensor([2, 1024, 1, 1],"float16"), p=1.0, training=True, mode="upscale_in_train", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float32"), Tensor([2, 1024, 1, 1],"float32"), p=0.0, training=False, mode="downscale_in_infer", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float32"), Tensor([2, 1024, 1, 1],"float32"), p=0.0, training=False, mode="upscale_in_train", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float32"), Tensor([2, 1024, 1, 1],"float32"), p=0.0, training=True, mode="downscale_in_infer", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float32"), Tensor([2, 1024, 1, 1],"float32"), p=0.0, training=True, mode="upscale_in_train", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float32"), Tensor([2, 1024, 1, 1],"float32"), p=0.5, training=False, mode="downscale_in_infer", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float32"), Tensor([2, 1024, 1, 1],"float32"), p=0.5, training=False, mode="upscale_in_train", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float32"), Tensor([2, 1024, 1, 1],"float32"), p=0.5, training=True, mode="downscale_in_infer", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float32"), Tensor([2, 1024, 1, 1],"float32"), p=0.5, training=True, mode="upscale_in_train", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float32"), Tensor([2, 1024, 1, 1],"float32"), p=0.9, training=False, mode="downscale_in_infer", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float32"), Tensor([2, 1024, 1, 1],"float32"), p=0.9, training=False, mode="upscale_in_train", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float32"), Tensor([2, 1024, 1, 1],"float32"), p=0.9, training=True, mode="downscale_in_infer", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float32"), Tensor([2, 1024, 1, 1],"float32"), p=0.9, training=True, mode="upscale_in_train", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float32"), Tensor([2, 1024, 1, 1],"float32"), p=1.0, training=False, mode="downscale_in_infer", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float32"), Tensor([2, 1024, 1, 1],"float32"), p=1.0, training=False, mode="upscale_in_train", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float32"), Tensor([2, 1024, 1, 1],"float32"), p=1.0, training=True, mode="downscale_in_infer", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float32"), Tensor([2, 1024, 1, 1],"float32"), p=1.0, training=True, mode="upscale_in_train", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float64"), Tensor([2, 1024, 1, 1],"float64"), p=0.0, training=False, mode="downscale_in_infer", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float64"), Tensor([2, 1024, 1, 1],"float64"), p=0.0, training=False, mode="upscale_in_train", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float64"), Tensor([2, 1024, 1, 1],"float64"), p=0.0, training=True, mode="downscale_in_infer", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float64"), Tensor([2, 1024, 1, 1],"float64"), p=0.0, training=True, mode="upscale_in_train", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float64"), Tensor([2, 1024, 1, 1],"float64"), p=0.5, training=False, mode="downscale_in_infer", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float64"), Tensor([2, 1024, 1, 1],"float64"), p=0.5, training=False, mode="upscale_in_train", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float64"), Tensor([2, 1024, 1, 1],"float64"), p=0.5, training=True, mode="downscale_in_infer", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float64"), Tensor([2, 1024, 1, 1],"float64"), p=0.5, training=True, mode="upscale_in_train", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float64"), Tensor([2, 1024, 1, 1],"float64"), p=0.9, training=False, mode="downscale_in_infer", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float64"), Tensor([2, 1024, 1, 1],"float64"), p=0.9, training=False, mode="upscale_in_train", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float64"), Tensor([2, 1024, 1, 1],"float64"), p=0.9, training=True, mode="downscale_in_infer", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float64"), Tensor([2, 1024, 1, 1],"float64"), p=0.9, training=True, mode="upscale_in_train", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float64"), Tensor([2, 1024, 1, 1],"float64"), p=1.0, training=False, mode="downscale_in_infer", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float64"), Tensor([2, 1024, 1, 1],"float64"), p=1.0, training=False, mode="upscale_in_train", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float64"), Tensor([2, 1024, 1, 1],"float64"), p=1.0, training=True, mode="downscale_in_infer", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 1, 1],"float64"), Tensor([2, 1024, 1, 1],"float64"), p=1.0, training=True, mode="upscale_in_train", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 1024, 2, 1],"float16"), Tensor([2, 1024, 2, 1],"float16"), p=0.5, training=True, mode="upscale_in_train", )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 80, 8, 2],"float16"), Tensor([2, 80, 8, 2],"float16"), p=0.5, training=True, )
paddle.incubate.nn.functional.fused_dropout_add(Tensor([2, 80, 8, 2],"float16"), Tensor([2, 80, 8, 2],"float16"), p=0.5, training=True, mode="upscale_in_train", name=None, )
paddle.incubate.nn.functional.fused_feedforward(Tensor([1, 1, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8, 8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=True, )
paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )
paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float32"), Tensor([508, 130],"float32"), Tensor([130, 508],"float32"), Tensor([130],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), Tensor([508],"float32"), 0.0, 0.0, activation="relu", pre_layer_norm=False, )
paddle.incubate.nn.functional.fused_feedforward(Tensor([31, 98, 508],"float64"), Tensor([508, 130],"float64"), Tensor([130, 508],"float64"), Tensor([130],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), Tensor([508],"float64"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )
paddle.incubate.nn.functional.fused_feedforward(Tensor([4, 32, 128],"float16"), Tensor([128, 256],"float16"), Tensor([256, 128],"float16"), Tensor([256],"float16"), Tensor([128],"float16"), Tensor([128],"float32"), Tensor([128],"float32"), Tensor([128],"float32"), Tensor([128],"float32"), 0.0, 0.0, activation="gelu", pre_layer_norm=False, )
paddle.incubate.nn.functional.fused_linear(Tensor([30, 40],"float32"), Tensor([40, 50],"float32"), Tensor([50],"float32"), False, )
paddle.incubate.nn.functional.fused_linear(Tensor([30, 40],"float32"), Tensor([40, 50],"float32"), Tensor([50],"float32"), False, None, )
paddle.incubate.nn.functional.fused_linear(Tensor([30, 40],"float32"), Tensor([50, 40],"float32"), Tensor([50],"float32"), True, )
paddle.incubate.nn.functional.fused_linear(Tensor([30, 40],"float32"), Tensor([50, 40],"float32"), Tensor([50],"float32"), True, None, )
paddle.incubate.nn.functional.fused_linear_activation(Tensor([8, 4],"float64"), Tensor([4, 128],"float64"), Tensor([128],"float64"), False, False, "gelu", )
paddle.incubate.nn.functional.fused_linear_activation(Tensor([8, 4],"float64"), Tensor([4, 128],"float64"), Tensor([128],"float64"), False, False, "none", )
paddle.incubate.nn.functional.fused_linear_activation(Tensor([8, 4],"float64"), Tensor([4, 128],"float64"), Tensor([128],"float64"), False, False, "relu", )
paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([40, 50],"float32"), None, False, True, )
paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([40, 50],"float32"), Tensor([40],"float32"), False, True, )
paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([50, 40],"float32"), None, False, False, )
paddle.incubate.nn.functional.fused_matmul_bias(Tensor([30, 50],"float32"), Tensor([50, 40],"float32"), Tensor([40],"float32"), False, False, )
paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 7],"float16"), Tensor([5, 7],"float16"), None, False, True, )
paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 7],"float16"), Tensor([5, 7],"float16"), Tensor([5],"float16"), False, True, )
paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 7],"float16"), Tensor([7, 5],"float16"), None, False, False, )
paddle.incubate.nn.functional.fused_matmul_bias(Tensor([4, 7],"float16"), Tensor([7, 5],"float16"), Tensor([5],"float16"), False, False, )
paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 30],"float32"), Tensor([40, 50],"float32"), None, True, True, )
paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 30],"float32"), Tensor([40, 50],"float32"), Tensor([40],"float32"), True, True, )
paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 30],"float32"), Tensor([50, 40],"float32"), None, True, False, )
paddle.incubate.nn.functional.fused_matmul_bias(Tensor([50, 30],"float32"), Tensor([50, 40],"float32"), Tensor([40],"float32"), True, False, )
paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 4],"float16"), Tensor([5, 7],"float16"), None, True, True, )
paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 4],"float16"), Tensor([5, 7],"float16"), Tensor([5],"float16"), True, True, )
paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 4],"float16"), Tensor([7, 5],"float16"), None, True, False, )
paddle.incubate.nn.functional.fused_matmul_bias(Tensor([7, 4],"float16"), Tensor([7, 5],"float16"), Tensor([5],"float16"), True, False, )
paddle.incubate.nn.functional.fused_multi_head_attention(Tensor([8, 1, 1024],"float32"), Tensor([3, 16, 64, 1024],"float32"), Tensor([1024, 1024],"float32"), False, Tensor([1024],"float32"), Tensor([1024],"float32"), Tensor([1024],"float32"), Tensor([1024],"float32"), 1e-05, Tensor([3, 16, 64],"float32"), Tensor([1024],"float32"), Tensor([2, 8, 16, 128, 64],"float32"), Tensor([8, 16, 1, 129],"float32"), 0.0, 0.0, 1e-05, num_heads=16, transpose_qkv_wb=False, )
paddle.incubate.nn.functional.fused_multi_head_attention(Tensor([8, 128, 1024],"float16"), Tensor([3, 16, 64, 1024],"float16"), Tensor([1024, 1024],"float16"), False, Tensor([1024],"float32"), Tensor([1024],"float32"), Tensor([1024],"float32"), Tensor([1024],"float32"), 1e-05, Tensor([3, 16, 64],"float16"), Tensor([1024],"float16"), None, Tensor([8, 16, 128, 128],"float16"), 0.0, 0.0, 1e-05, num_heads=16, transpose_qkv_wb=False, )
paddle.incubate.nn.functional.fused_multi_head_attention(Tensor([8, 128, 1024],"float32"), Tensor([1024, 3072],"float32"), Tensor([1024, 1024],"float32"), False, Tensor([1024],"float32"), Tensor([1024],"float32"), Tensor([1024],"float32"), Tensor([1024],"float32"), 1e-05, None, None, None, Tensor([8, 16, 128, 128],"float32"), 0.0, 0.0, 1e-05, num_heads=16, transpose_qkv_wb=True, )
paddle.incubate.nn.functional.fused_multi_head_attention(Tensor([8, 128, 1024],"float32"), Tensor([1024, 3072],"float32"), Tensor([1024, 1024],"float32"), False, Tensor([1024],"float32"), Tensor([1024],"float32"), Tensor([1024],"float32"), Tensor([1024],"float32"), 1e-05, Tensor([3072],"float32"), Tensor([1024],"float32"), None, Tensor([8, 16, 128, 128],"float32"), 0.0, 0.0, 1e-05, num_heads=16, transpose_qkv_wb=True, )
paddle.incubate.nn.functional.fused_multi_head_attention(Tensor([8, 128, 1024],"float32"), Tensor([3, 16, 64, 1024],"float32"), Tensor([1024, 1024],"float32"), False, Tensor([1024],"float32"), Tensor([1024],"float32"), Tensor([1024],"float32"), Tensor([1024],"float32"), 1e-05, None, None, None, Tensor([8, 16, 128, 128],"float32"), 0.0, 0.0, 1e-05, num_heads=16, transpose_qkv_wb=False, )
paddle.incubate.nn.functional.fused_multi_head_attention(Tensor([8, 128, 1024],"float32"), Tensor([3, 16, 64, 1024],"float32"), Tensor([1024, 1024],"float32"), False, Tensor([1024],"float32"), Tensor([1024],"float32"), Tensor([1024],"float32"), Tensor([1024],"float32"), 1e-05, Tensor([3, 16, 64],"float32"), Tensor([1024],"float32"), None, Tensor([8, 16, 128, 128],"float32"), 0.0, 0.0, 1e-05, )
paddle.incubate.nn.functional.fused_multi_head_attention(Tensor([8, 128, 1024],"float32"), Tensor([3, 16, 64, 1024],"float32"), Tensor([1024, 1024],"float32"), False, Tensor([1024],"float32"), Tensor([1024],"float32"), Tensor([1024],"float32"), Tensor([1024],"float32"), 1e-05, Tensor([3, 16, 64],"float32"), Tensor([1024],"float32"), None, Tensor([8, 16, 128, 128],"float32"), 0.0, 0.0, 1e-05, num_heads=16, transpose_qkv_wb=False, )
paddle.incubate.nn.functional.fused_multi_head_attention(Tensor([8, 128, 1024],"float32"), Tensor([3, 16, 64, 1024],"float32"), Tensor([1024, 1024],"float32"), True, Tensor([1024],"float32"), Tensor([1024],"float32"), Tensor([1024],"float32"), Tensor([1024],"float32"), 1e-05, Tensor([3, 16, 64],"float32"), Tensor([1024],"float32"), None, None, 0.0, 0.0, 1e-05, num_heads=16, transpose_qkv_wb=False, )
paddle.incubate.nn.functional.fused_multi_head_attention(Tensor([8, 128, 1024],"float32"), Tensor([3, 16, 64, 1024],"float32"), Tensor([1024, 1024],"float32"), True, Tensor([1024],"float32"), Tensor([1024],"float32"), Tensor([1024],"float32"), Tensor([1024],"float32"), 1e-05, Tensor([3, 16, 64],"float32"), Tensor([1024],"float32"), None, Tensor([8, 16, 128, 128],"float32"), 0.0, 0.0, 1e-05, num_heads=16, transpose_qkv_wb=False, )
paddle.incubate.nn.functional.fused_multi_head_attention(x=Tensor([1, 2, 4],"float32"), qkv_weight=Tensor([3, 2, 2, 4],"float32"), linear_weight=Tensor([4, 4],"float32"), pre_layer_norm=False, pre_ln_scale=None, pre_ln_bias=None, ln_scale=Tensor([4],"float32"), ln_bias=None, pre_ln_epsilon=1e-05, qkv_bias=None, linear_bias=None, cache_kv=None, attn_mask=Tensor([1, 2, 2, 2],"float32"), dropout_rate=0.0, attn_dropout_rate=0.0, ln_epsilon=1e-05, training=True, ring_id=-1, num_heads=2, transpose_qkv_wb=False, name=None, )
paddle.incubate.nn.functional.fused_multi_head_attention(x=Tensor([1, 2, 4],"float32"), qkv_weight=Tensor([3, 2, 2, 4],"float32"), linear_weight=Tensor([4, 4],"float32"), pre_layer_norm=False, pre_ln_scale=None, pre_ln_bias=None, ln_scale=Tensor([4],"float32"), ln_bias=Tensor([4],"float32"), pre_ln_epsilon=1e-05, qkv_bias=Tensor([3, 2, 2],"float32"), linear_bias=Tensor([4],"float32"), cache_kv=None, attn_mask=Tensor([1, 2, 2, 2],"float32"), dropout_rate=0.0, attn_dropout_rate=0.0, ln_epsilon=1e-05, training=True, ring_id=-1, num_heads=2, transpose_qkv_wb=False, name=None, )
paddle.incubate.nn.functional.fused_multi_head_attention(x=Tensor([1, 2, 4],"float32"), qkv_weight=Tensor([3, 2, 2, 4],"float32"), linear_weight=Tensor([4, 4],"float32"), pre_layer_norm=True, pre_ln_scale=Tensor([4],"float32"), pre_ln_bias=Tensor([4],"float32"), ln_scale=None, ln_bias=None, pre_ln_epsilon=1e-05, qkv_bias=Tensor([3, 2, 2],"float32"), linear_bias=Tensor([4],"float32"), cache_kv=None, attn_mask=None, dropout_rate=0.0, attn_dropout_rate=0.0, ln_epsilon=1e-05, training=True, ring_id=-1, num_heads=2, transpose_qkv_wb=False, name=None, )
paddle.incubate.nn.functional.fused_multi_head_attention(x=Tensor([1, 2, 4],"float32"), qkv_weight=Tensor([4, 12],"float32"), linear_weight=Tensor([4, 4],"float32"), pre_layer_norm=False, pre_ln_scale=None, pre_ln_bias=None, ln_scale=Tensor([4],"float32"), ln_bias=None, pre_ln_epsilon=1e-05, qkv_bias=None, linear_bias=None, cache_kv=None, attn_mask=Tensor([1, 2, 2, 2],"float32"), dropout_rate=0.0, attn_dropout_rate=0.0, ln_epsilon=1e-05, training=True, ring_id=-1, num_heads=2, transpose_qkv_wb=True, name=None, )
paddle.incubate.nn.functional.fused_multi_head_attention(x=Tensor([1, 2, 4],"float32"), qkv_weight=Tensor([4, 12],"float32"), linear_weight=Tensor([4, 4],"float32"), pre_layer_norm=False, pre_ln_scale=None, pre_ln_bias=None, ln_scale=Tensor([4],"float32"), ln_bias=Tensor([4],"float32"), pre_ln_epsilon=1e-05, qkv_bias=Tensor([12],"float32"), linear_bias=Tensor([4],"float32"), cache_kv=None, attn_mask=Tensor([1, 2, 2, 2],"float32"), dropout_rate=0.0, attn_dropout_rate=0.0, ln_epsilon=1e-05, training=True, ring_id=-1, num_heads=2, transpose_qkv_wb=True, name=None, )
paddle.incubate.nn.functional.fused_multi_head_attention(x=Tensor([8, 128, 256],"float32"), qkv_weight=Tensor([3, 16, 16, 256],"float32"), linear_weight=Tensor([256, 256],"float32"), pre_layer_norm=False, pre_ln_scale=None, pre_ln_bias=None, ln_scale=Tensor([256],"float32"), ln_bias=Tensor([256],"float32"), pre_ln_epsilon=1e-05, qkv_bias=Tensor([3, 16, 16],"float32"), linear_bias=Tensor([256],"float32"), cache_kv=None, attn_mask=None, dropout_rate=0.0, attn_dropout_rate=0.0, ln_epsilon=1e-05, training=True, ring_id=-1, num_heads=16, transpose_qkv_wb=False, name=None, )
paddle.incubate.nn.functional.fused_multi_head_attention(x=Tensor([8, 128, 256],"float32"), qkv_weight=Tensor([3, 16, 16, 256],"float32"), linear_weight=Tensor([256, 256],"float32"), pre_layer_norm=True, pre_ln_scale=Tensor([256],"float32"), pre_ln_bias=Tensor([256],"float32"), ln_scale=None, ln_bias=None, pre_ln_epsilon=1e-05, qkv_bias=Tensor([3, 16, 16],"float32"), linear_bias=Tensor([256],"float32"), cache_kv=None, attn_mask=None, dropout_rate=0.0, attn_dropout_rate=0.0, ln_epsilon=1e-05, training=True, ring_id=-1, num_heads=16, transpose_qkv_wb=False, name=None, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), None, Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), None, Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), None, Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), None, Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), None, Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), None, Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([1, 8, 4, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), None, None, Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), None, None, Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), None, None, Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), None, Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), None, Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), None, Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), None, Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), None, Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), None, Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([2, 8, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 4, 8],"float32"), None, Tensor([2, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 4, 8],"float32"), None, Tensor([2, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 4, 8],"float32"), None, Tensor([2, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 4, 8],"float32"), Tensor([2, 8, 1, 8],"float32"), None, Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 4, 8],"float32"), Tensor([2, 8, 1, 8],"float32"), None, Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 4, 8],"float32"), Tensor([2, 8, 1, 8],"float32"), None, Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 4, 8],"float32"), Tensor([2, 8, 1, 8],"float32"), Tensor([2, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 4, 8],"float32"), Tensor([2, 8, 1, 8],"float32"), Tensor([2, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([2, 8, 4, 8],"float32"), Tensor([2, 8, 1, 8],"float32"), Tensor([2, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=False, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), None, Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), None, Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), None, Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), None, Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), None, Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), None, Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 1, 4, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([8, 1, 2, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([1, 8],"int64"), use_neox_rotary_style=True, time_major=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), None, None, Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), None, None, Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), None, None, Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), None, Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), None, Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), None, Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), None, Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), None, Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), None, Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([8, 2, 2, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), Tensor([1, 8, 1, 16],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 4, 8],"float32"), None, Tensor([8, 2, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 4, 8],"float32"), None, Tensor([8, 2, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 4, 8],"float32"), None, Tensor([8, 2, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 4, 8],"float32"), Tensor([8, 2, 1, 8],"float32"), None, Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 4, 8],"float32"), Tensor([8, 2, 1, 8],"float32"), None, Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 4, 8],"float32"), Tensor([8, 2, 1, 8],"float32"), None, Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 4, 8],"float32"), Tensor([8, 2, 1, 8],"float32"), Tensor([8, 2, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=False, time_major=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 4, 8],"float32"), Tensor([8, 2, 1, 8],"float32"), Tensor([8, 2, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=None, use_neox_rotary_style=True, time_major=True, )
paddle.incubate.nn.functional.fused_rotary_position_embedding(Tensor([8, 2, 4, 8],"float32"), Tensor([8, 2, 1, 8],"float32"), Tensor([8, 2, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), Tensor([1, 8, 1, 8],"float32"), position_ids=Tensor([2, 8],"int64"), use_neox_rotary_style=True, time_major=True, )
paddle.matmul(Tensor([1, 2, 2048, 2048],"bfloat16"), Tensor([1, 2, 2048, 128],"bfloat16"), )
paddle.matmul(Tensor([1, 2, 2048, 2048],"float16"), Tensor([1, 2, 2048, 128],"float16"), )
paddle.matmul(Tensor([12, 32],"float16"), Tensor([12, 128],"float16"), transpose_x=True, )
paddle.matmul(Tensor([12, 32],"float32"), Tensor([12, 128],"float32"), transpose_x=True, )
paddle.matmul(Tensor([2, 2, 1024, 1024],"float16"), Tensor([2, 2, 1024, 128],"float16"), )
paddle.matmul(Tensor([2, 2, 1024, 1024],"float16"), Tensor([2, 2, 1024, 64],"float16"), )
paddle.matmul(Tensor([8, 12, 128, 128],"float16"), Tensor([8, 12, 128, 64],"float16"), )
paddle.matmul(Tensor([8, 128, 256],"float32"), Tensor([768, 256],"float32"), transpose_y=True, )
paddle.matmul(Tensor([8, 16, 128, 128],"float32"), Tensor([8, 16, 128, 16],"float32"), )
paddle.matmul(Tensor([8, 16, 128, 16],"float32"), Tensor([8, 16, 128, 16],"float32"), transpose_y=True, )
paddle.matmul(x=Tensor([1, 2, 2048, 128],"bfloat16"), y=Tensor([1, 2, 2048, 128],"bfloat16"), transpose_x=False, transpose_y=True, )
paddle.matmul(x=Tensor([1, 2, 2048, 128],"float16"), y=Tensor([1, 2, 2048, 128],"float16"), transpose_x=False, transpose_y=True, )
paddle.matmul(x=Tensor([2, 2, 1024, 128],"float16"), y=Tensor([2, 2, 1024, 128],"float16"), transpose_x=False, transpose_y=True, )
paddle.matmul(x=Tensor([2, 2, 1024, 64],"float16"), y=Tensor([2, 2, 1024, 64],"float16"), transpose_x=False, transpose_y=True, )
paddle.matmul(x=Tensor([8, 12, 128, 64],"float16"), y=Tensor([8, 12, 128, 64],"float16"), transpose_x=False, transpose_y=True, )
paddle.matmul(x=Tensor([8, 16, 1, 64],"float32"), y=Tensor([8, 16, 129, 64],"float32"), transpose_y=True, )
paddle.matmul(x=Tensor([8, 16, 128, 64],"float16"), y=Tensor([8, 16, 128, 64],"float16"), transpose_y=True, )
paddle.matmul(x=Tensor([8, 16, 128, 64],"float32"), y=Tensor([8, 16, 128, 64],"float32"), transpose_y=True, )
paddle.mean(Tensor([1, 2, 6, 6],"float32"), )
paddle.mean(Tensor([1, 3, 3, 5],"float32"), )
paddle.mean(Tensor([1, 4, 4, 8],"float64"), )
paddle.mean(Tensor([1, 8, 4, 3],"float64"), )
paddle.mean(Tensor([1, 8, 4, 4],"float64"), )
paddle.mean(Tensor([1, 8, 5, 5],"float64"), )
paddle.mean(Tensor([1, 8, 6, 6],"float64"), )
paddle.mean(Tensor([2, 6, 7, 7],"float64"), )
paddle.multiply(Tensor([1, 2, 8, 8],"float32"), Tensor([1, 1, 8, 8],"float32"), )
paddle.multiply(Tensor([1, 4, 8, 8],"float32"), Tensor([1, 1, 8, 8],"float32"), )
paddle.multiply(Tensor([2, 1, 8, 8],"float32"), Tensor([1, 1, 8, 8],"float32"), )
paddle.multiply(Tensor([2, 1, 8, 8],"float32"), Tensor([2, 1, 8, 8],"float32"), )
paddle.multiply(Tensor([2, 2, 8, 16],"float32"), Tensor([1, 1, 8, 16],"float32"), )
paddle.multiply(Tensor([2, 2, 8, 16],"float32"), Tensor([2, 1, 8, 16],"float32"), )
paddle.multiply(Tensor([2, 4, 8, 8],"float32"), Tensor([1, 1, 8, 8],"float32"), )
paddle.multiply(Tensor([2, 4, 8, 8],"float32"), Tensor([2, 1, 8, 8],"float32"), )
paddle.nn.functional.conv1d(Tensor([4, 16, 3],"float32"), Tensor([6, 1, 3],"float32"), bias=Tensor([6],"float32"), padding="valid", stride=list[1,], dilation=list[1,], groups=3, data_format="NLC", )
paddle.nn.functional.conv1d(Tensor([4, 16, 6],"float32"), Tensor([8, 6, 3],"float32"), bias=Tensor([8],"float32"), padding=2, stride=list[1,], dilation=list[1,], groups=1, data_format="NLC", )
paddle.nn.functional.conv1d(Tensor([4, 16, 6],"float32"), Tensor([8, 6, 3],"float32"), bias=Tensor([8],"float32"), padding=list[1,2,], stride=list[1,], dilation=list[1,], groups=1, data_format="NLC", )
paddle.nn.functional.conv1d(Tensor([4, 6, 16],"float32"), Tensor([512, 6, 3],"float32"), bias=Tensor([512],"float32"), padding="valid", stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv1d(Tensor([4, 6, 16],"float32"), Tensor([512, 6, 3],"float32"), bias=Tensor([512],"float32"), padding=list[1,2,], stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv1d(Tensor([4, 6, 16],"float32"), Tensor([8, 3, 3],"float32"), bias=Tensor([8],"float32"), padding="valid", stride=list[1,], dilation=list[1,], groups=2, data_format="NCL", )
paddle.nn.functional.conv1d(Tensor([4, 6, 16],"float32"), Tensor([8, 6, 3],"float32"), bias=Tensor([8],"float32"), padding="same", stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv1d(Tensor([4, 6, 16],"float32"), Tensor([8, 6, 3],"float32"), bias=Tensor([8],"float32"), padding="valid", stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv1d(Tensor([4, 6, 16],"float32"), Tensor([8, 6, 3],"float32"), bias=Tensor([8],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv1d(Tensor([4, 6, 16],"float32"), Tensor([8, 6, 3],"float32"), bias=Tensor([8],"float32"), padding=0, stride=list[1,], dilation=list[2,], groups=1, data_format="NCL", )
paddle.nn.functional.conv1d(Tensor([4, 6, 16],"float32"), Tensor([8, 6, 3],"float32"), bias=Tensor([8],"float32"), padding=0, stride=list[2,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv1d(Tensor([4, 6, 16],"float32"), Tensor([8, 6, 3],"float32"), bias=Tensor([8],"float32"), padding=2, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv1d(Tensor([4, 6, 16],"float32"), Tensor([8, 6, 3],"float32"), bias=Tensor([8],"float32"), padding=list[1,], stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv1d(Tensor([4, 6, 16],"float32"), Tensor([8, 6, 3],"float32"), bias=Tensor([8],"float32"), padding=list[1,2,], stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv1d_transpose(Tensor([4, 16, 6],"float32"), Tensor([6, 8, 3],"float32"), bias=Tensor([8],"float32"), output_size=18, output_padding=0, padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NLC", )
paddle.nn.functional.conv1d_transpose(Tensor([4, 16, 6],"float32"), Tensor([6, 8, 3],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=0, padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NLC", )
paddle.nn.functional.conv1d_transpose(Tensor([4, 16, 6],"float32"), Tensor([6, 8, 3],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=2, padding=0, stride=list[3,], dilation=list[1,], groups=1, data_format="NLC", )
paddle.nn.functional.conv1d_transpose(Tensor([4, 3, 16],"float32"), Tensor([3, 2, 3],"float32"), bias=Tensor([6],"float32"), output_size=None, output_padding=0, padding="valid", stride=list[1,], dilation=list[1,], groups=3, data_format="NCL", )
paddle.nn.functional.conv1d_transpose(Tensor([4, 6, 16],"float32"), Tensor([6, 4, 3],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=0, padding="valid", stride=list[1,], dilation=list[1,], groups=2, data_format="NCL", )
paddle.nn.functional.conv1d_transpose(Tensor([4, 6, 16],"float32"), Tensor([6, 8, 1],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=0, padding=3, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv1d_transpose(Tensor([4, 6, 16],"float32"), Tensor([6, 8, 3],"float32"), bias=Tensor([8],"float32"), output_size=list[36,], output_padding=0, padding=0, stride=list[2,], dilation=list[2,], groups=1, data_format="NCL", )
paddle.nn.functional.conv1d_transpose(Tensor([4, 6, 16],"float32"), Tensor([6, 8, 3],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=0, padding="valid", stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv1d_transpose(Tensor([4, 6, 16],"float32"), Tensor([6, 8, 3],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=0, padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv1d_transpose(Tensor([4, 6, 16],"float32"), Tensor([6, 8, 3],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=0, padding=0, stride=list[2,], dilation=list[2,], groups=1, data_format="NCL", )
paddle.nn.functional.conv1d_transpose(Tensor([4, 6, 16],"float32"), Tensor([6, 8, 3],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=0, padding=list[1,2,], stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv1d_transpose(Tensor([4, 6, 16],"float32"), Tensor([6, 8, 3],"float32"), bias=Tensor([8],"float32"), output_size=None, output_padding=0, padding=list[2,], stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 3, 3],"float64"), Tensor([2, 2, 1, 1],"float64"), groups=1, padding="SAME", )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 3, 3],"float64"), Tensor([2, 2, 1, 1],"float64"), groups=1, padding="VALID", )
paddle.nn.functional.conv2d_transpose(Tensor([2, 2, 3, 3],"float64"), Tensor([2, 2, 1, 1],"float64"), groups=1, padding=list[1,0,0,1,], )
paddle.nn.functional.conv2d_transpose(Tensor([2, 3, 3, 2],"float64"), Tensor([2, 2, 1, 1],"float64"), groups=1, padding=list[1,1,], data_format="NHWC", )
paddle.nn.functional.conv2d_transpose(Tensor([2, 4, 3, 3],"float64"), Tensor([4, 2, 1, 1],"float64"), groups=1, )
paddle.nn.functional.conv2d_transpose(Tensor([4, 16, 16, 6],"float32"), Tensor([6, 8, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NHWC", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 16, 16, 6],"float32"), Tensor([6, 8, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=list[list[0,0,],list[1,1,],list[2,2,],list[0,0,],], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NHWC", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 3, 16, 16],"float32"), Tensor([3, 2, 3, 3],"float32"), bias=Tensor([6],"float32"), padding="valid", output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=3, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 3, 7, 7],"float32"), Tensor([3, 6, 5, 5],"float32"), bias=Tensor([6],"float32"), padding=2, output_padding=list[1,1,], stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 4, 3, 3],"float32"), bias=Tensor([8],"float32"), padding="valid", output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=2, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 8, 1, 1],"float32"), bias=Tensor([8],"float32"), padding=tuple(2,3,), output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 8, 3, 3],"float32"), bias=Tensor([8],"float32"), padding="same", output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 8, 3, 3],"float32"), bias=Tensor([8],"float32"), padding="valid", output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 8, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 8, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,2,], dilation=list[2,2,], groups=1, output_size=list[20,36,], data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 8, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,2,], dilation=list[2,2,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 8, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[2,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 8, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=list[1,2,2,1,], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 6, 16, 16],"float32"), Tensor([6, 8, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=list[list[0,0,],list[0,0,],list[1,2,],list[2,1,],], output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 8, 8, 8],"float32"), Tensor([3, 2, 3, 3, 3],"float32"), bias=Tensor([6],"float32"), padding="valid", output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=3, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 4, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding="valid", output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=2, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 8, 1, 1, 1],"float32"), bias=Tensor([8],"float32"), padding=tuple(2,3,1,), output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 8, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding="valid", output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 8, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 8, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,2,1,], dilation=list[2,2,2,], groups=1, output_size=list[12,19,12,], data_format="NCDHW", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 8, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,2,1,], dilation=list[2,2,2,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 8, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[2,2,2,], dilation=list[2,1,2,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 8, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=list[1,2,2,3,2,1,], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 6, 8, 8, 8],"float32"), Tensor([6, 8, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=list[list[0,0,],list[0,0,],list[2,3,],list[1,2,],list[2,1,],], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 8, 8, 8, 6],"float32"), Tensor([6, 8, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=0, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NDHWC", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 8, 8, 8, 6],"float32"), Tensor([6, 8, 3, 3, 3],"float32"), bias=Tensor([8],"float32"), padding=list[list[0,0,],list[1,1,],list[2,2,],list[3,3,],list[0,0,],], output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NDHWC", )
paddle.nn.functional.dropout(Tensor([1, 1, 8],"float32"), p=0.0, axis=None, training=True, mode="upscale_in_train", name=None, )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float16"), 0.0, training=False, mode="downscale_in_infer", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float16"), 0.0, training=False, mode="upscale_in_train", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float16"), 0.0, training=True, mode="downscale_in_infer", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float16"), 0.0, training=True, mode="upscale_in_train", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float16"), 0.5, training=False, mode="downscale_in_infer", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float16"), 0.5, training=False, mode="upscale_in_train", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float16"), 0.5, training=True, mode="downscale_in_infer", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float16"), 0.5, training=True, mode="upscale_in_train", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float16"), 0.9, training=False, mode="downscale_in_infer", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float16"), 0.9, training=False, mode="upscale_in_train", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float16"), 0.9, training=True, mode="downscale_in_infer", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float16"), 0.9, training=True, mode="upscale_in_train", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float16"), 1.0, training=False, mode="downscale_in_infer", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float16"), 1.0, training=False, mode="upscale_in_train", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float16"), 1.0, training=True, mode="downscale_in_infer", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float16"), 1.0, training=True, mode="upscale_in_train", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float32"), 0.0, training=False, mode="downscale_in_infer", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float32"), 0.0, training=False, mode="upscale_in_train", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float32"), 0.0, training=True, mode="downscale_in_infer", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float32"), 0.0, training=True, mode="upscale_in_train", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float32"), 0.5, training=False, mode="downscale_in_infer", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float32"), 0.5, training=False, mode="upscale_in_train", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float32"), 0.5, training=True, mode="downscale_in_infer", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float32"), 0.5, training=True, mode="upscale_in_train", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float32"), 0.9, training=False, mode="downscale_in_infer", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float32"), 0.9, training=False, mode="upscale_in_train", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float32"), 0.9, training=True, mode="downscale_in_infer", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float32"), 0.9, training=True, mode="upscale_in_train", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float32"), 1.0, training=False, mode="downscale_in_infer", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float32"), 1.0, training=False, mode="upscale_in_train", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float32"), 1.0, training=True, mode="downscale_in_infer", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float32"), 1.0, training=True, mode="upscale_in_train", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float64"), 0.0, training=False, mode="downscale_in_infer", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float64"), 0.0, training=False, mode="upscale_in_train", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float64"), 0.0, training=True, mode="downscale_in_infer", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float64"), 0.0, training=True, mode="upscale_in_train", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float64"), 0.5, training=False, mode="downscale_in_infer", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float64"), 0.5, training=False, mode="upscale_in_train", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float64"), 0.5, training=True, mode="downscale_in_infer", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float64"), 0.5, training=True, mode="upscale_in_train", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float64"), 0.9, training=False, mode="downscale_in_infer", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float64"), 0.9, training=False, mode="upscale_in_train", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float64"), 0.9, training=True, mode="downscale_in_infer", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float64"), 0.9, training=True, mode="upscale_in_train", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float64"), 1.0, training=False, mode="downscale_in_infer", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float64"), 1.0, training=False, mode="upscale_in_train", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float64"), 1.0, training=True, mode="downscale_in_infer", )
paddle.nn.functional.dropout(Tensor([2, 1024, 1, 1],"float64"), 1.0, training=True, mode="upscale_in_train", )
paddle.nn.functional.dropout(Tensor([2, 1024, 2, 1],"float16"), 0.5, training=True, mode="upscale_in_train", )
paddle.nn.functional.dropout(Tensor([2, 80, 8, 2],"float16"), 0.5, training=True, mode="upscale_in_train", )
paddle.nn.functional.dropout(Tensor([2, 80, 8, 2],"float16"), p=0.5, axis=None, training=True, mode="upscale_in_train", name=None, )
paddle.nn.functional.dropout(Tensor([31, 98, 130],"float32"), p=0.0, axis=None, training=True, mode="upscale_in_train", name=None, )
paddle.nn.functional.dropout(Tensor([31, 98, 130],"float64"), p=0.0, axis=None, training=True, mode="upscale_in_train", name=None, )
paddle.nn.functional.dropout(Tensor([31, 98, 508],"float32"), p=0.0, axis=None, training=True, mode="upscale_in_train", name=None, )
paddle.nn.functional.dropout(Tensor([31, 98, 508],"float64"), p=0.0, axis=None, training=True, mode="upscale_in_train", name=None, )
paddle.nn.functional.dropout(Tensor([4, 32, 128],"float16"), p=0.0, axis=None, training=True, mode="upscale_in_train", name=None, )
paddle.nn.functional.dropout(Tensor([4, 32, 256],"float16"), p=0.0, axis=None, training=True, mode="upscale_in_train", name=None, )
paddle.nn.functional.dropout(Tensor([8, 1, 1024],"float32"), p=0.0, axis=None, training=True, mode="upscale_in_train", name=None, )
paddle.nn.functional.dropout(Tensor([8, 128, 1024],"float16"), p=0.0, axis=None, training=True, mode="upscale_in_train", name=None, )
paddle.nn.functional.dropout(Tensor([8, 128, 1024],"float32"), p=0.0, axis=None, training=True, mode="upscale_in_train", name=None, )
paddle.nn.functional.gelu(Tensor([1, 1, 8],"float32"), )
paddle.nn.functional.gelu(Tensor([31, 98, 130],"float32"), )
paddle.nn.functional.gelu(Tensor([31, 98, 130],"float64"), )
paddle.nn.functional.gelu(Tensor([4, 32, 256],"float16"), )
paddle.nn.functional.layer_norm(Tensor([8, 128, 256],"float32"), list[256,], weight=Tensor([256],"float32"), bias=Tensor([256],"float32"), epsilon=1e-05, )
paddle.nn.functional.linear(Tensor([8, 128, 256],"float32"), weight=Tensor([256, 256],"float32"), bias=Tensor([256],"float32"), )
paddle.nn.functional.linear(x=Tensor([1, 1, 8],"float32"), weight=Tensor([8, 8],"float32"), bias=Tensor([8],"float32"), name=None, )
paddle.nn.functional.linear(x=Tensor([1, 3, 3, 5],"float32"), weight=Tensor([5, 1],"float32"), bias=Tensor([1],"float32"), name=None, )
paddle.nn.functional.linear(x=Tensor([31, 98, 130],"float32"), weight=Tensor([130, 508],"float32"), bias=Tensor([508],"float32"), name=None, )
paddle.nn.functional.linear(x=Tensor([31, 98, 130],"float64"), weight=Tensor([130, 508],"float64"), bias=Tensor([508],"float64"), name=None, )
paddle.nn.functional.linear(x=Tensor([31, 98, 508],"float32"), weight=Tensor([508, 130],"float32"), bias=Tensor([130],"float32"), name=None, )
paddle.nn.functional.linear(x=Tensor([31, 98, 508],"float64"), weight=Tensor([508, 130],"float64"), bias=Tensor([130],"float64"), name=None, )
paddle.nn.functional.linear(x=Tensor([4, 32, 128],"float16"), weight=Tensor([128, 256],"float16"), bias=Tensor([256],"float16"), name=None, )
paddle.nn.functional.linear(x=Tensor([4, 32, 256],"float16"), weight=Tensor([256, 128],"float16"), bias=Tensor([128],"float16"), name=None, )
paddle.nn.functional.linear(x=Tensor([8, 1, 1024],"float32"), weight=Tensor([1024, 1024],"float32"), bias=Tensor([1024],"float32"), name=None, )
paddle.nn.functional.linear(x=Tensor([8, 128, 1024],"float16"), weight=Tensor([1024, 1024],"float16"), bias=Tensor([1024],"float16"), name=None, )
paddle.nn.functional.linear(x=Tensor([8, 128, 1024],"float32"), weight=Tensor([1024, 1024],"float32"), bias=None, name=None, )
paddle.nn.functional.linear(x=Tensor([8, 128, 1024],"float32"), weight=Tensor([1024, 1024],"float32"), bias=Tensor([1024],"float32"), name=None, )
paddle.nn.functional.pad(Tensor([4, 6, 16, 16],"float32"), list[1,1,1,1,], mode="circular", data_format="NCHW", )
paddle.nn.functional.pad(Tensor([4, 6, 16, 16],"float32"), list[1,1,1,1,], mode="reflect", data_format="NCHW", )
paddle.nn.functional.pad(Tensor([4, 6, 16, 16],"float32"), list[1,1,1,1,], mode="replicate", data_format="NCHW", )
paddle.nn.functional.relu(Tensor([1, 3, 3, 1],"float32"), None, )
paddle.nn.functional.relu(Tensor([31, 98, 130],"float32"), )
paddle.nn.functional.softmax(Tensor([1, 2, 2048, 2048],"bfloat16"), )
paddle.nn.functional.softmax(Tensor([1, 2, 2048, 2048],"float16"), )
paddle.nn.functional.softmax(Tensor([2, 2, 1024, 1024],"float16"), )
paddle.nn.functional.softmax(Tensor([8, 12, 128, 128],"float16"), )
paddle.nn.functional.softmax(Tensor([8, 16, 1, 129],"float32"), )
paddle.nn.functional.softmax(Tensor([8, 16, 128, 128],"float16"), )
paddle.nn.functional.softmax(Tensor([8, 16, 128, 128],"float32"), )
paddle.ones(shape=list[2,2,1,1,], dtype=Dtype(float64), )
paddle.ones(shape=list[2,2,1,1,], dtype=VarType(float64), )
paddle.ones(shape=list[2,2,3,3,], dtype=Dtype(float64), )
paddle.ones(shape=list[2,2,3,3,], dtype=VarType(float64), )
paddle.ones(shape=list[2,3,3,2,], dtype=Dtype(float64), )
paddle.ones(shape=list[2,3,3,2,], dtype=VarType(float64), )
paddle.ones(shape=list[2,4,3,3,], dtype=Dtype(float64), )
paddle.ones(shape=list[2,4,3,3,], dtype=VarType(float64), )
paddle.ones(shape=list[4,2,1,1,], dtype=Dtype(float64), )
paddle.ones(shape=list[4,2,1,1,], dtype=VarType(float64), )
paddle.reshape(Tensor([1, 2, 8, 4, 2],"float32"), Tensor([4],"int64"), )
paddle.reshape(Tensor([1, 2, 8, 8],"float32"), Tensor([4],"int64"), )
paddle.reshape(Tensor([1, 4, 8, 4, 2],"float32"), Tensor([4],"int64"), )
paddle.reshape(Tensor([1, 4, 8, 8],"float32"), Tensor([4],"int64"), )
paddle.reshape(Tensor([128],"float32"), list[1,8,1,16,], )
paddle.reshape(Tensor([2, 1, 8, 4, 2],"float32"), Tensor([4],"int64"), )
paddle.reshape(Tensor([2, 1, 8, 8],"float32"), Tensor([4],"int64"), )
paddle.reshape(Tensor([2, 2, 8, 16],"float32"), Tensor([4],"int64"), )
paddle.reshape(Tensor([2, 2, 8, 8, 2],"float32"), Tensor([4],"int64"), )
paddle.reshape(Tensor([2, 4, 8, 4, 2],"float32"), Tensor([4],"int64"), )
paddle.reshape(Tensor([2, 4, 8, 8],"float32"), Tensor([4],"int64"), )
paddle.reshape(Tensor([4, 1],"float32"), list[-1,1,], )
paddle.reshape(Tensor([512],"float32"), list[1,512,1,1,], )
paddle.reshape(Tensor([6],"float32"), list[1,6,1,1,], )
paddle.reshape(Tensor([6],"float32"), list[1,6,1,1,1,], )
paddle.reshape(Tensor([64],"float32"), list[1,8,1,8,], )
paddle.reshape(Tensor([8],"float32"), list[1,8,1,1,], )
paddle.reshape(Tensor([8],"float32"), list[1,8,1,1,1,], )
paddle.roll(Tensor([0, 3],"float32"), shifts=1, )
paddle.roll(Tensor([0, 3],"float32"), shifts=1, axis=0, )
paddle.roll(Tensor([3, 3],"bool"), shifts=1, )
paddle.roll(Tensor([3, 3],"bool"), shifts=1, axis=0, )
paddle.roll(Tensor([3, 3],"float64"), shifts=1, )
paddle.roll(Tensor([3, 3],"float64"), shifts=1, axis=0, )
paddle.roll(Tensor([3, 3],"int64"), shifts=Tensor([2],"int64"), axis=list[0,1,], )
paddle.roll(Tensor([4, 0, 3],"float32"), shifts=1, )
paddle.roll(Tensor([4, 0, 3],"float32"), shifts=1, axis=0, )
paddle.scale(Tensor([8, 16, 1, 129],"float32"), scale=0.125, )
paddle.scale(Tensor([8, 16, 128, 128],"float16"), scale=0.125, )
paddle.scale(Tensor([8, 16, 128, 128],"float32"), scale=0.125, )
paddle.shape(Tensor([1, 2, 8, 8],"float32"), )
paddle.shape(Tensor([1, 4, 8, 8],"float32"), )
paddle.shape(Tensor([2, 1, 8, 8],"float32"), )
paddle.shape(Tensor([2, 2, 8, 16],"float32"), )
paddle.shape(Tensor([2, 4, 8, 8],"float32"), )
paddle.shape(Tensor([3, 3],"int64"), )
paddle.split(Tensor([2, 8, 16, 128, 64],"float32"), 2, )
paddle.split(Tensor([8, 128, 768],"float32"), 3, axis=-1, )
paddle.squeeze(Tensor([1, 8, 16, 128, 64],"float32"), axis=0, )
paddle.stack(list[Tensor([1, 2, 8, 4],"float32"),Tensor([1, 2, 8, 4],"float32"),], axis=-1, )
paddle.stack(list[Tensor([1, 4, 8, 4],"float32"),Tensor([1, 4, 8, 4],"float32"),], axis=-1, )
paddle.stack(list[Tensor([2, 1, 8, 4],"float32"),Tensor([2, 1, 8, 4],"float32"),], axis=-1, )
paddle.stack(list[Tensor([2, 2, 8, 8],"float32"),Tensor([2, 2, 8, 8],"float32"),], axis=-1, )
paddle.stack(list[Tensor([2, 4, 8, 4],"float32"),Tensor([2, 4, 8, 4],"float32"),], axis=-1, )
paddle.sum(Tensor([1, 3, 3, 5],"float32"), )
paddle.Tensor.__add__(Tensor([1, 1, 8],"float32"), Tensor([1, 1, 8],"float32"), )
paddle.Tensor.__add__(Tensor([1, 2, 2048, 2048],"bfloat16"), Tensor([1, 1, 2048, 2048],"bfloat16"), )
paddle.Tensor.__add__(Tensor([1, 2, 2048, 2048],"float16"), Tensor([1, 1, 2048, 2048],"float16"), )
paddle.Tensor.__add__(Tensor([1, 3, 3, 5],"float32"), 1, )
paddle.Tensor.__add__(Tensor([128],"float16"), Tensor([128],"float16"), )
paddle.Tensor.__add__(Tensor([128],"float32"), Tensor([128],"float32"), )
paddle.Tensor.__add__(Tensor([2, 1024, 1, 1],"float16"), Tensor([2, 1024, 1, 1],"float16"), )
paddle.Tensor.__add__(Tensor([2, 1024, 1, 1],"float32"), Tensor([2, 1024, 1, 1],"float32"), )
paddle.Tensor.__add__(Tensor([2, 1024, 1, 1],"float64"), Tensor([2, 1024, 1, 1],"float64"), )
paddle.Tensor.__add__(Tensor([2, 1024, 2, 1],"float16"), Tensor([2, 1024, 2, 1],"float16"), )
paddle.Tensor.__add__(Tensor([2, 2, 1024, 1024],"float16"), Tensor([1, 1, 1024, 1024],"float16"), )
paddle.Tensor.__add__(Tensor([2, 3],"int64"), 1, )
paddle.Tensor.__add__(Tensor([2, 3],"int64"), 2, )
paddle.Tensor.__add__(Tensor([2, 80, 8, 2],"float16"), Tensor([2, 80, 8, 2],"float16"), )
paddle.Tensor.__add__(Tensor([31, 98, 508],"float32"), Tensor([31, 98, 508],"float32"), )
paddle.Tensor.__add__(Tensor([31, 98, 508],"float64"), Tensor([31, 98, 508],"float64"), )
paddle.Tensor.__add__(Tensor([32, 128],"float16"), Tensor([32, 128],"float16"), )
paddle.Tensor.__add__(Tensor([32, 128],"float32"), Tensor([32, 128],"float32"), )
paddle.Tensor.__add__(Tensor([4, 32, 128],"float16"), Tensor([4, 32, 128],"float16"), )
paddle.Tensor.__add__(Tensor([8, 1, 1024],"float32"), Tensor([8, 1, 1024],"float32"), )
paddle.Tensor.__add__(Tensor([8, 12, 128, 128],"float16"), Tensor([1, 1, 128, 128],"float16"), )
paddle.Tensor.__add__(Tensor([8, 128, 1024],"float16"), Tensor([1024],"float16"), )
paddle.Tensor.__add__(Tensor([8, 128, 1024],"float16"), Tensor([8, 128, 1024],"float16"), )
paddle.Tensor.__add__(Tensor([8, 128, 1024],"float32"), Tensor([1024],"float32"), )
paddle.Tensor.__add__(Tensor([8, 128, 1024],"float32"), Tensor([8, 128, 1024],"float32"), )
paddle.Tensor.__add__(Tensor([8, 128, 256],"float32"), Tensor([8, 128, 256],"float32"), )
paddle.Tensor.__add__(Tensor([8, 128, 768],"float32"), Tensor([768],"float32"), )
paddle.Tensor.__add__(Tensor([8, 16, 1, 129],"float32"), Tensor([8, 16, 1, 129],"float32"), )
paddle.Tensor.__add__(Tensor([8, 16, 128, 128],"float16"), Tensor([8, 16, 128, 128],"float16"), )
paddle.Tensor.__add__(Tensor([8, 16, 128, 128],"float32"), Tensor([8, 16, 128, 128],"float32"), )
paddle.Tensor.__floordiv__(Tensor([2],"int64"), 2, )
paddle.Tensor.__getitem__(Tensor([1, 2, 8, 8],"float32"), tuple(Ellipsis,slice(4,None,None),), )
paddle.Tensor.__getitem__(Tensor([1, 2, 8, 8],"float32"), tuple(Ellipsis,slice(None,4,None),), )
paddle.Tensor.__getitem__(Tensor([1, 2, 8, 8],"float32"), tuple(slice(None,None,None),slice(None,None,None),slice(None,None,None),slice(0,None,2),), )
paddle.Tensor.__getitem__(Tensor([1, 2, 8, 8],"float32"), tuple(slice(None,None,None),slice(None,None,None),slice(None,None,None),slice(1,None,2),), )
paddle.Tensor.__getitem__(Tensor([1, 4, 8, 8],"float32"), tuple(Ellipsis,slice(4,None,None),), )
paddle.Tensor.__getitem__(Tensor([1, 4, 8, 8],"float32"), tuple(Ellipsis,slice(None,4,None),), )
paddle.Tensor.__getitem__(Tensor([1, 4, 8, 8],"float32"), tuple(slice(None,None,None),slice(None,None,None),slice(None,None,None),slice(0,None,2),), )
paddle.Tensor.__getitem__(Tensor([1, 4, 8, 8],"float32"), tuple(slice(None,None,None),slice(None,None,None),slice(None,None,None),slice(1,None,2),), )
paddle.Tensor.__getitem__(Tensor([2, 1, 8, 8],"float32"), tuple(Ellipsis,slice(4,None,None),), )
paddle.Tensor.__getitem__(Tensor([2, 1, 8, 8],"float32"), tuple(Ellipsis,slice(None,4,None),), )
paddle.Tensor.__getitem__(Tensor([2, 1, 8, 8],"float32"), tuple(slice(None,None,None),slice(None,None,None),slice(None,None,None),slice(0,None,2),), )
paddle.Tensor.__getitem__(Tensor([2, 1, 8, 8],"float32"), tuple(slice(None,None,None),slice(None,None,None),slice(None,None,None),slice(1,None,2),), )
paddle.Tensor.__getitem__(Tensor([2, 2, 8, 16],"float32"), tuple(Ellipsis,slice(8,None,None),), )
paddle.Tensor.__getitem__(Tensor([2, 2, 8, 16],"float32"), tuple(Ellipsis,slice(None,8,None),), )
paddle.Tensor.__getitem__(Tensor([2, 2, 8, 16],"float32"), tuple(slice(None,None,None),slice(None,None,None),slice(None,None,None),slice(0,None,2),), )
paddle.Tensor.__getitem__(Tensor([2, 2, 8, 16],"float32"), tuple(slice(None,None,None),slice(None,None,None),slice(None,None,None),slice(1,None,2),), )
paddle.Tensor.__getitem__(Tensor([2, 4, 8, 8],"float32"), tuple(Ellipsis,slice(4,None,None),), )
paddle.Tensor.__getitem__(Tensor([2, 4, 8, 8],"float32"), tuple(Ellipsis,slice(None,4,None),), )
paddle.Tensor.__getitem__(Tensor([2, 4, 8, 8],"float32"), tuple(slice(None,None,None),slice(None,None,None),slice(None,None,None),slice(0,None,2),), )
paddle.Tensor.__getitem__(Tensor([2, 4, 8, 8],"float32"), tuple(slice(None,None,None),slice(None,None,None),slice(None,None,None),slice(1,None,2),), )
paddle.Tensor.__getitem__(Tensor([8, 16],"float32"), Tensor([2, 8],"int64"), )
paddle.Tensor.__getitem__(Tensor([8, 8],"float32"), Tensor([1, 8],"int64"), )
paddle.Tensor.__getitem__(Tensor([8, 8],"float32"), Tensor([2, 8],"int64"), )
paddle.Tensor.__mul__(Tensor([1, 2, 2048, 128],"bfloat16"), 0.08838834764831845, )
paddle.Tensor.__mul__(Tensor([1, 2, 2048, 128],"float16"), 0.08838834764831845, )
paddle.Tensor.__mul__(Tensor([2, 2, 1024, 128],"float16"), 0.08838834764831845, )
paddle.Tensor.__mul__(Tensor([2, 2, 1024, 64],"float16"), 0.125, )
paddle.Tensor.__mul__(Tensor([8, 1],"float32"), Tensor([1, 4],"float32"), )
paddle.Tensor.__mul__(Tensor([8, 1],"float32"), Tensor([1, 8],"float32"), )
paddle.Tensor.__mul__(Tensor([8, 12, 128, 64],"float16"), 0.125, )
paddle.Tensor.__mul__(Tensor([8, 16, 128, 16],"float32"), 0.25, )
paddle.Tensor.__neg__(Tensor([1, 2, 8, 4],"float32"), )
paddle.Tensor.__neg__(Tensor([1, 4, 8, 4],"float32"), )
paddle.Tensor.__neg__(Tensor([2, 1, 8, 4],"float32"), )
paddle.Tensor.__neg__(Tensor([2, 2, 8, 8],"float32"), )
paddle.Tensor.__neg__(Tensor([2, 4, 8, 4],"float32"), )
paddle.Tensor.__rpow__(Tensor([4],"float32"), 10000, )
paddle.Tensor.__rpow__(Tensor([8],"float32"), 10000, )
paddle.Tensor.__rtruediv__(Tensor([4],"float32"), 1, )
paddle.Tensor.__rtruediv__(Tensor([8],"float32"), 1, )
paddle.Tensor.__sub__(Tensor([2, 3],"int64"), 1, )
paddle.Tensor.__truediv__(Tensor([4],"float32"), 8, )
paddle.Tensor.__truediv__(Tensor([8],"float32"), 16, )
paddle.Tensor.astype(Tensor([128],"float16"), Dtype(float32), )
paddle.Tensor.astype(Tensor([128],"int64"), Dtype(float16), )
paddle.Tensor.astype(Tensor([3, 4, 128],"float16"), Dtype(float32), )
paddle.Tensor.astype(Tensor([3, 4, 128],"int64"), Dtype(float16), )
paddle.Tensor.astype(Tensor([3, 4, 32],"float16"), Dtype(float32), )
paddle.Tensor.astype(Tensor([3, 4, 32],"int64"), Dtype(float16), )
paddle.Tensor.astype(Tensor([32, 128],"float16"), Dtype(float32), )
paddle.Tensor.astype(Tensor([32, 128],"int64"), Dtype(float16), )
paddle.Tensor.cast(Tensor([1024, 1024],"float32"), dtype=Dtype(float16), )
paddle.Tensor.cast(Tensor([1024, 4096],"float32"), dtype=Dtype(float16), )
paddle.Tensor.cast(Tensor([1024],"float32"), dtype=Dtype(float16), )
paddle.Tensor.cast(Tensor([3, 16, 64, 1024],"float32"), dtype=Dtype(float16), )
paddle.Tensor.cast(Tensor([3, 16, 64],"float32"), dtype=Dtype(float16), )
paddle.Tensor.cast(Tensor([4096, 1024],"float32"), dtype=Dtype(float16), )
paddle.Tensor.cast(Tensor([4096],"float32"), dtype=Dtype(float16), )
paddle.Tensor.clone(Tensor([1, 8, 2, 8],"float32"), )
paddle.Tensor.clone(Tensor([1, 8, 4, 8],"float32"), )
paddle.Tensor.clone(Tensor([2, 8, 1, 8],"float32"), )
paddle.Tensor.clone(Tensor([2, 8, 2, 16],"float32"), )
paddle.Tensor.clone(Tensor([2, 8, 4, 8],"float32"), )
paddle.Tensor.clone(Tensor([8, 1, 2, 8],"float32"), )
paddle.Tensor.clone(Tensor([8, 1, 4, 8],"float32"), )
paddle.Tensor.clone(Tensor([8, 2, 1, 8],"float32"), )
paddle.Tensor.clone(Tensor([8, 2, 2, 16],"float32"), )
paddle.Tensor.clone(Tensor([8, 2, 4, 8],"float32"), )
paddle.Tensor.detach(Tensor([1, 8, 2, 8],"float32"), )
paddle.Tensor.detach(Tensor([1, 8, 4, 8],"float32"), )
paddle.Tensor.detach(Tensor([2, 8, 1, 8],"float32"), )
paddle.Tensor.detach(Tensor([2, 8, 2, 16],"float32"), )
paddle.Tensor.detach(Tensor([2, 8, 4, 8],"float32"), )
paddle.Tensor.detach(Tensor([8, 1, 2, 8],"float32"), )
paddle.Tensor.detach(Tensor([8, 1, 4, 8],"float32"), )
paddle.Tensor.detach(Tensor([8, 2, 1, 8],"float32"), )
paddle.Tensor.detach(Tensor([8, 2, 2, 16],"float32"), )
paddle.Tensor.detach(Tensor([8, 2, 4, 8],"float32"), )
paddle.Tensor.mean(Tensor([8, 128, 256],"float32"), )
paddle.Tensor.reshape(Tensor([2],"float32"), list[1,-1,1,1,], )
paddle.Tensor.reshape(Tensor([3, 16, 16, 256],"float32"), list[768,256,], )
paddle.Tensor.reshape(Tensor([3, 16, 16],"float32"), list[768,], )
paddle.Tensor.reshape(Tensor([3, 4, 128],"float16"), list[-1,128,], )
paddle.Tensor.reshape(Tensor([3, 4, 128],"float32"), list[-1,128,], )
paddle.Tensor.reshape(Tensor([3, 4, 32],"float16"), list[-1,32,], )
paddle.Tensor.reshape(Tensor([3, 4, 32],"float32"), list[-1,32,], )
paddle.Tensor.reshape(Tensor([4],"float32"), list[1,-1,1,1,], )
paddle.Tensor.reshape(Tensor([6],"float32"), list[1,-1,1,1,], )
paddle.Tensor.reshape(Tensor([6],"float32"), list[1,-1,1,1,1,], )
paddle.Tensor.reshape(Tensor([8, 128, 16, 16],"float32"), list[8,-1,256,], )
paddle.Tensor.reshape(Tensor([8, 128, 256],"float32"), list[8,128,16,16,], )
paddle.Tensor.reshape(Tensor([8],"float32"), list[1,1,1,-1,], )
paddle.Tensor.reshape(Tensor([8],"float32"), list[1,-1,1,1,], )
paddle.Tensor.reshape(Tensor([8],"float32"), list[1,1,1,1,-1,], )
paddle.Tensor.reshape(Tensor([8],"float32"), list[1,-1,1,1,1,], )
paddle.Tensor.reshape(Tensor([9],"int64"), list[3,3,], )
paddle.Tensor.squeeze(Tensor([1, 8, 1, 16],"float32"), axis=list[0,2,], )
paddle.Tensor.squeeze(Tensor([1, 8, 1, 8],"float32"), axis=list[0,2,], )
paddle.Tensor.sum(Tensor([12, 128],"float16"), axis=0, )
paddle.Tensor.sum(Tensor([12, 128],"float32"), axis=0, )
paddle.Tensor.transpose(Tensor([8, 128, 16, 16],"float32"), list[0,2,1,3,], )
paddle.Tensor.transpose(Tensor([8, 16, 128, 16],"float32"), list[0,2,1,3,], )
paddle.Tensor.unsqueeze(Tensor([1, 8, 8],"float32"), 2, )
paddle.Tensor.unsqueeze(Tensor([2, 8, 16],"float32"), 2, )
paddle.Tensor.unsqueeze(Tensor([2, 8, 8],"float32"), 2, )
paddle.Tensor.unsqueeze(Tensor([4],"float32"), 0, )
paddle.Tensor.unsqueeze(Tensor([8],"float32"), 0, )
paddle.Tensor.unsqueeze(Tensor([8],"float32"), 1, )
paddle.tolist(Tensor([2, 3],"int64"), )
paddle.topk(Tensor([2, 1030],"float64"), k=1, axis=-1, )
paddle.topk(Tensor([6, 7, 8],"float64"), k=2, )
paddle.topk(Tensor([6, 7, 8],"float64"), k=2, axis=1, )
paddle.topk(Tensor([6, 7, 8],"float64"), k=2, axis=1, largest=False, )
paddle.topk(Tensor([6, 7, 8],"float64"), k=2, axis=-1, largest=False, )
paddle.topk(Tensor([6, 7, 8],"float64"), k=2, axis=1, sorted=False, )
paddle.topk(Tensor([6, 7, 8],"float64"), k=Tensor([1],"int64"), axis=1, )
paddle.transpose(Tensor([1, 2, 2048, 128],"bfloat16"), perm=list[0,2,1,3,], )
paddle.transpose(Tensor([1, 2, 2048, 128],"float16"), perm=list[0,2,1,3,], )
paddle.transpose(Tensor([1, 8, 2, 8],"float32"), perm=list[1,0,], )
paddle.transpose(Tensor([1, 8, 4, 8],"float32"), perm=list[1,0,], )
paddle.transpose(Tensor([2, 2, 1024, 128],"float16"), perm=list[0,2,1,3,], )
paddle.transpose(Tensor([2, 2, 1024, 64],"float16"), perm=list[0,2,1,3,], )
paddle.transpose(Tensor([2, 8, 1, 8],"float32"), perm=list[1,0,], )
paddle.transpose(Tensor([2, 8, 2, 16],"float32"), perm=list[1,0,], )
paddle.transpose(Tensor([2, 8, 4, 8],"float32"), perm=list[1,0,], )
paddle.transpose(Tensor([8, 1, 2, 8],"float32"), perm=list[1,0,], )
paddle.transpose(Tensor([8, 1, 4, 8],"float32"), perm=list[1,0,], )
paddle.transpose(Tensor([8, 12, 128, 64],"float16"), perm=list[0,2,1,3,], )
paddle.transpose(Tensor([8, 2, 1, 8],"float32"), perm=list[1,0,], )
paddle.transpose(Tensor([8, 2, 2, 16],"float32"), perm=list[1,0,], )
paddle.transpose(Tensor([8, 2, 4, 8],"float32"), perm=list[1,0,], )
paddle.transpose(x=Tensor([1, 2, 8, 8],"float32"), perm=list[0,2,1,3,], )
paddle.transpose(x=Tensor([1, 2048, 2, 128],"bfloat16"), perm=list[0,2,1,3,], )
paddle.transpose(x=Tensor([1, 2048, 2, 128],"float16"), perm=list[0,2,1,3,], )
paddle.transpose(x=Tensor([1, 4, 8, 8],"float32"), perm=list[0,2,1,3,], )
paddle.transpose(x=Tensor([1, 8, 1, 16],"float32"), perm=list[0,2,1,3,], )
paddle.transpose(x=Tensor([1, 8, 1, 8],"float32"), perm=list[0,2,1,3,], )
paddle.transpose(x=Tensor([1, 8, 2, 8],"float32"), perm=list[0,2,1,3,], )
paddle.transpose(x=Tensor([1, 8, 4, 8],"float32"), perm=list[0,2,1,3,], )
paddle.transpose(x=Tensor([2, 1, 8, 8],"float32"), perm=list[0,2,1,3,], )
paddle.transpose(x=Tensor([2, 1024, 2, 128],"float16"), perm=list[0,2,1,3,], )
paddle.transpose(x=Tensor([2, 1024, 2, 64],"float16"), perm=list[0,2,1,3,], )
paddle.transpose(x=Tensor([2, 2, 8, 16],"float32"), perm=list[0,2,1,3,], )
paddle.transpose(x=Tensor([2, 4, 8, 8],"float32"), perm=list[0,2,1,3,], )
paddle.transpose(x=Tensor([2, 8, 1, 16],"float32"), perm=list[0,2,1,3,], )
paddle.transpose(x=Tensor([2, 8, 1, 8],"float32"), perm=list[0,2,1,3,], )
paddle.transpose(x=Tensor([2, 8, 2, 16],"float32"), perm=list[0,2,1,3,], )
paddle.transpose(x=Tensor([2, 8, 4, 8],"float32"), perm=list[0,2,1,3,], )
paddle.transpose(x=Tensor([8, 128, 12, 64],"float16"), perm=list[0,2,1,3,], )
paddle.zeros(list[192,], dtype=Dtype(float32), )
paddle.zeros(list[4,1,], dtype="float32", )
paddle.zeros(shape=list[4,1,], dtype="float32", )
