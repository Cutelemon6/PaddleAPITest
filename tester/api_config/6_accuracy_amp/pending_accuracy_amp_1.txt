paddle.incubate.nn.functional.fused_layer_norm(Tensor([101, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([101, 64],"float16"), )
paddle.incubate.nn.functional.fused_layer_norm(Tensor([16, 256],"float16"), Tensor([256],"float32"), Tensor([256],"float32"), 1e-05, begin_norm_axis=1, bias=Tensor([256],"float16"), residual=Tensor([16, 256],"float16"), residual_alpha=0.69204696, )
paddle.incubate.nn.functional.fused_layer_norm(Tensor([2, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([2, 1, 64],"float16"), )
paddle.incubate.nn.functional.fused_layer_norm(Tensor([58, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, residual_alpha=1.4142135623730951, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([58, 64],"float16"), )
paddle.incubate.nn.functional.fused_layer_norm(Tensor([59, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([59, 64],"float16"), )
paddle.incubate.nn.functional.fused_layer_norm(Tensor([60, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([60, 64],"float16"), )
paddle.incubate.nn.functional.fused_layer_norm(Tensor([67, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([67, 64],"float16"), )
paddle.incubate.nn.functional.fused_layer_norm(Tensor([68, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([68, 64],"float16"), )
paddle.incubate.nn.functional.fused_layer_norm(Tensor([76, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([76, 64],"float16"), )
paddle.incubate.nn.functional.fused_layer_norm(Tensor([78, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([78, 64],"float16"), )
paddle.incubate.nn.functional.fused_layer_norm(Tensor([90, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([90, 64],"float16"), )
paddle.incubate.nn.functional.fused_layer_norm(Tensor([91, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([91, 64],"float16"), )
paddle.incubate.nn.functional.fused_layer_norm(Tensor([98, 64],"float16"), norm_weight=Tensor([64],"float32"), norm_bias=Tensor([64],"float32"), epsilon=1e-05, begin_norm_axis=1, bias=Tensor([64],"float16"), residual=Tensor([98, 64],"float16"), )
paddle.incubate.nn.functional.swiglu(Tensor([1, 1024, 11008],"bfloat16"), Tensor([1, 1024, 11008],"bfloat16"), )
paddle.incubate.nn.functional.swiglu(Tensor([1, 1024, 4864],"bfloat16"), Tensor([1, 1024, 4864],"bfloat16"), )
paddle.incubate.nn.functional.swiglu(Tensor([2, 114, 64],"bfloat16"), Tensor([2, 114, 64],"bfloat16"), )
paddle.incubate.nn.functional.swiglu(Tensor([2, 302, 11008],"bfloat16"), Tensor([2, 302, 11008],"bfloat16"), )
paddle.nn.functional.conv2d_transpose(Tensor([2, 128, 124, 108],"float16"), Tensor([128, 128, 2, 2],"float32"), bias=None, padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([2, 256, 62, 54],"float16"), Tensor([256, 128, 4, 4],"float32"), bias=None, padding=0, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([2, 64, 248, 216],"float16"), Tensor([64, 128, 1, 1],"float32"), bias=None, padding=0, output_padding=0, stride=list[1,1,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([20, 128, 38, 68],"float32"), Tensor([128, 1, 4, 4],"float16"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=128, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([20, 256, 19, 34],"float32"), Tensor([256, 1, 4, 4],"float16"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=256, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([20, 64, 38, 68],"float32"), Tensor([64, 1, 8, 8],"float16"), bias=None, padding=2, output_padding=0, stride=list[4,4,], dilation=list[1,1,], groups=64, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([20, 64, 76, 136],"float32"), Tensor([64, 1, 4, 4],"float16"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=64, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([24, 66, 128, 128],"float32"), Tensor([66, 32, 4, 4],"float16"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 256, 124, 108],"float16"), Tensor([256, 128, 2, 2],"float32"), bias=None, padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([8, 256, 64, 64],"float16"), Tensor([256, 128, 2, 2],"float32"), bias=None, padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )