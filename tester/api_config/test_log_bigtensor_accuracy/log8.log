2025-03-11 18:25:09.357773 test begin: paddle.Tensor.amax(Tensor([114085069, 5, 4],"float32"), axis=None, keepdim=False, )

W0311 18:26:24.145479 95720 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 18:26:24.146689 95720 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.Tensor.amax(Tensor([114085069, 5, 4],"float32"), axis=None, keepdim=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 5.368709e+08
Max relative difference: 1.0737418e+09
 x: array(-5.368709e+08, dtype=float32)
 y: array(0.5, dtype=float32)
2025-03-11 18:26:24.511436 test begin: paddle.Tensor.amax(Tensor([2, 1140850690],"float32"), axis=None, keepdim=False, )

[accuracy error] paddle.Tensor.amax(Tensor([2, 1140850690],"float32"), axis=None, keepdim=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 5.368709e+08
Max relative difference: 1.0737418e+09
 x: array(-5.368709e+08, dtype=float32)
 y: array(0.5, dtype=float32)
2025-03-11 18:26:35.125194 test begin: paddle.Tensor.amax(Tensor([2, 285212673, 4],"float32"), axis=None, keepdim=False, )

[accuracy error] paddle.Tensor.amax(Tensor([2, 285212673, 4],"float32"), axis=None, keepdim=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 5.368709e+08
Max relative difference: 1.0737418e+09
 x: array(-5.368709e+08, dtype=float32)
 y: array(0.5, dtype=float32)
2025-03-11 18:26:48.481449 test begin: paddle.Tensor.amax(Tensor([2, 5, 228170138],"float32"), axis=None, keepdim=False, )

[accuracy error] paddle.Tensor.amax(Tensor([2, 5, 228170138],"float32"), axis=None, keepdim=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 5.368709e+08
Max relative difference: 1.0737418e+09
 x: array(-5.368709e+08, dtype=float32)
 y: array(0.5, dtype=float32)
2025-03-11 18:26:59.436933 test begin: paddle.Tensor.amax(Tensor([2281701379],"float32"), axis=None, keepdim=False, )

[accuracy error] paddle.Tensor.amax(Tensor([2281701379],"float32"), axis=None, keepdim=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 5.368709e+08
Max relative difference: 1.0737418e+09
 x: array(-5.368709e+08, dtype=float32)
 y: array(0.5, dtype=float32)
2025-03-11 18:27:11.975709 test begin: paddle.Tensor.amax(Tensor([3, 2, 4, 95070891],"float32"), axis=-1, keepdim=True, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_amax(_object*, _object*, _object*)
1   amax_ad_func(paddle::Tensor const&, std::vector<long, std::allocator<long> >, bool)
2   paddle::experimental::amax(paddle::Tensor const&, std::vector<long, std::allocator<long> > const&, bool)
3   void phi::AMaxRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<long, std::allocator<long> > const&, bool, bool, phi::DenseTensor*)
4   void phi::Reduce<float, phi::kps::MaxFunctor, phi::kps::IdentityFunctor, false>(phi::GPUContext const&, phi::DenseTensor const&, bool, std::vector<long, std::allocator<long> > const&, bool, phi::DataType, phi::DenseTensor*)
5   void phi::funcs::ReduceKernel<float, float, phi::kps::MaxFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741688844 (unix time) try "date -d @1741688844" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x17567) received by PID 95591 (TID 0x7f78067c3700) from PID 95591 ***]

2025-03-11 18:28:07.407500 test begin: paddle.Tensor.amax(Tensor([3, 2, 5, 76056713],"float32"), axis=2, keepdim=True, )

W0311 18:29:38.353961 97977 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 18:29:38.355315 97977 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] backward  paddle.Tensor.amax(Tensor([3, 2, 5, 76056713],"float32"), axis=2, keepdim=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4 / 2281701390 (1.75e-07%)
Max absolute difference: 0.19339661
Max relative difference: 0.5
 x: array([[[[-0.000000e+00, -0.000000e+00, -0.000000e+00, ...,
           0.000000e+00, -0.000000e+00,  0.000000e+00],
         [-0.000000e+00, -0.000000e+00, -0.000000e+00, ...,...
 y: array([[[[-0.000000e+00, -0.000000e+00, -0.000000e+00, ...,
           0.000000e+00, -0.000000e+00,  0.000000e+00],
         [-0.000000e+00, -0.000000e+00, -0.000000e+00, ...,...
2025-03-11 18:31:48.725872 test begin: paddle.Tensor.amax(Tensor([3, 2, 5, 76056713],"float32"), axis=None, keepdim=False, )

[accuracy error] paddle.Tensor.amax(Tensor([3, 2, 5, 76056713],"float32"), axis=None, keepdim=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 2.305843e+18
Max relative difference: 4.611686e+18
 x: array(-2.305843e+18, dtype=float32)
 y: array(0.5, dtype=float32)
2025-03-11 18:32:01.543994 test begin: paddle.Tensor.amax(Tensor([3, 2, 5, 76056713],"float32"), axis=tuple(1,2,), keepdim=True, )

[Pass] paddle.Tensor.amax(Tensor([3, 2, 5, 76056713],"float32"), axis=tuple(1,2,), keepdim=True, )
2025-03-11 18:33:35.496995 test begin: paddle.Tensor.amax(Tensor([3, 2, 76056713, 5],"float32"), axis=-1, keepdim=True, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741689233 (unix time) try "date -d @1741689233" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x17e18) received by PID 97816 (TID 0x7fa0174f4700) from PID 97816 ***]

2025-03-11 18:34:34.631867 test begin: paddle.reshape(Tensor([6, 7605672, 25, 2],"float32"), tuple(-1,2,), )

W0311 18:35:55.386708 103259 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 18:35:55.388033 103259 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.reshape(Tensor([6, 7605672, 25, 2],"float32"), tuple(-1,2,), )
2025-03-11 18:38:14.643315 test begin: paddle.reshape(Tensor([6, 95070891, 4],"float32"), shape=list[6,8,], )

[torch error] paddle.reshape(Tensor([6, 95070891, 4],"float32"), shape=list[6,8,], ) 
 shape '[6, 8]' is invalid for input of size 2281701384
2025-03-11 18:38:18.530995 test begin: paddle.reshape(Tensor([60, 139811, 272],"float32"), shape=tuple(60,-1,), )

2025-03-11 18:40:58.545878 test begin: paddle.Tensor.nansum(Tensor([477218589, 3, 3],"float16"), )

W0311 18:42:28.184484 108811 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 18:42:28.185658 108811 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.Tensor.nansum(Tensor([477218589, 3, 3],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 3774.
Max relative difference: 0.978
 x: array(-84.94, dtype=float16)
 y: array(-3858., dtype=float16)
2025-03-11 18:48:02.746582 test begin: paddle.Tensor.nansum(Tensor([477218589, 3, 3],"float16"), axis=-1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741690263 (unix time) try "date -d @1741690263" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1a83f) received by PID 108607 (TID 0x7f423c949700) from PID 108607 ***]

2025-03-11 18:51:08.760131 test begin: paddle.Tensor.nansum(Tensor([477218589, 3, 3],"float16"), axis=0, keepdim=True, )

W0311 18:52:48.862375 114472 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 18:52:48.864385 114472 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.Tensor.nansum(Tensor([477218589, 3, 3],"float16"), axis=0, keepdim=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 9 / 9 (100%)
Max absolute difference: 11000.
Max relative difference: 14.516
 x: array([[[-935.  ,  -13.74,  152.8 ],
        [1003.5 , -341.2 , -527.5 ],
        [ -43.6 , 1009.  , -952.  ]]], dtype=float16)
 y: array([[[-7124.  , -8368.  ,  7180.  ],
        [-6792.  ,  3506.  ,  6872.  ],
        [-5012.  , 12008.  ,    70.44]]], dtype=float16)
2025-03-11 18:53:10.641239 test begin: paddle.Tensor.outer(x=Tensor([4294967297],"float16"), y=Tensor([4294967297],"float16"), )

[torch error] paddle.Tensor.outer(x=Tensor([4294967297],"float16"), y=Tensor([4294967297],"float16"), ) 
 Storage size calculation overflowed with sizes=[4294967297, 4294967297]
2025-03-11 18:53:18.346826 test begin: paddle.Tensor.outer(x=Tensor([4294967297],"float16"), y=Tensor([4],"float64"), )

[torch error] paddle.Tensor.outer(x=Tensor([4294967297],"float16"), y=Tensor([4],"float64"), ) 
 CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 69.59 GiB is free. Process 41358 has 9.59 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 4.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 18:54:33.758183 test begin: paddle.Tensor.outer(x=Tensor([4],"float64"), y=Tensor([4294967297],"float16"), )

[torch error] paddle.Tensor.outer(x=Tensor([4],"float64"), y=Tensor([4294967297],"float16"), ) 
 CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 69.59 GiB is free. Process 41358 has 9.59 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 4.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 18:54:36.305568 test begin: paddle.Tensor.quantile(Tensor([3, 11930465, 3, 4, 2, 5],"float16"), q=0.5, )

[torch error] paddle.Tensor.quantile(Tensor([3, 11930465, 3, 4, 2, 5],"float16"), q=0.5, ) 
 quantile() input tensor must be either float or double dtype
2025-03-11 18:54:38.257241 test begin: paddle.Tensor.quantile(Tensor([3, 11930465, 3, 4, 2, 5],"float16"), q=0.75, axis=3, keepdim=True, )

[torch error] paddle.Tensor.quantile(Tensor([3, 11930465, 3, 4, 2, 5],"float16"), q=0.75, axis=3, keepdim=True, ) 
 quantile() input tensor must be either float or double dtype
2025-03-11 18:54:40.079180 test begin: paddle.Tensor.quantile(Tensor([3, 11930465, 3, 4, 2, 5],"float16"), q=0.75, axis=5, )

[torch error] paddle.Tensor.quantile(Tensor([3, 11930465, 3, 4, 2, 5],"float16"), q=0.75, axis=5, ) 
 quantile() input tensor must be either float or double dtype
2025-03-11 18:54:42.613440 test begin: paddle.Tensor.quantile(Tensor([3, 3, 477218589],"float16"), q=0.5, axis=0, )

[torch error] paddle.Tensor.quantile(Tensor([3, 3, 477218589],"float16"), q=0.5, axis=0, ) 
 quantile() input tensor must be either float or double dtype
2025-03-11 18:54:45.187398 test begin: paddle.Tensor.quantile(Tensor([3, 477218589, 3],"float16"), q=0.5, axis=0, )

[torch error] paddle.Tensor.quantile(Tensor([3, 477218589, 3],"float16"), q=0.5, axis=0, ) 
 quantile() input tensor must be either float or double dtype
2025-03-11 18:54:47.768594 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 2, 9942054],"float16"), q=0.5, )

[torch error] paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 2, 9942054],"float16"), q=0.5, ) 
 quantile() input tensor must be either float or double dtype
2025-03-11 18:54:50.553992 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 2, 9942054],"float16"), q=0.75, axis=3, keepdim=True, )

[torch error] paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 2, 9942054],"float16"), q=0.75, axis=3, keepdim=True, ) 
 quantile() input tensor must be either float or double dtype
2025-03-11 18:54:53.111203 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 2, 9942054],"float16"), q=0.75, axis=5, )

[torch error] paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 2, 9942054],"float16"), q=0.75, axis=5, ) 
 quantile() input tensor must be either float or double dtype
2025-03-11 18:54:55.778816 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 3976822, 5],"float16"), q=0.5, )

[torch error] paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 3976822, 5],"float16"), q=0.5, ) 
 quantile() input tensor must be either float or double dtype
2025-03-11 18:54:58.298056 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 3976822, 5],"float16"), q=0.75, axis=3, keepdim=True, )

[torch error] paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 3976822, 5],"float16"), q=0.75, axis=3, keepdim=True, ) 
 quantile() input tensor must be either float or double dtype
2025-03-11 18:55:00.824085 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 3976822, 5],"float16"), q=0.75, axis=5, )

[torch error] paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 3976822, 5],"float16"), q=0.75, axis=5, ) 
 quantile() input tensor must be either float or double dtype
2025-03-11 18:55:02.673885 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 7953644, 2, 5],"float16"), q=0.5, )

[torch error] paddle.Tensor.quantile(Tensor([3, 6, 3, 7953644, 2, 5],"float16"), q=0.5, ) 
 quantile() input tensor must be either float or double dtype
2025-03-11 18:55:05.101803 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 7953644, 2, 5],"float16"), q=0.75, axis=3, keepdim=True, )

[torch error] paddle.Tensor.quantile(Tensor([3, 6, 3, 7953644, 2, 5],"float16"), q=0.75, axis=3, keepdim=True, ) 
 quantile() input tensor must be either float or double dtype
2025-03-11 18:55:07.500360 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 7953644, 2, 5],"float16"), q=0.75, axis=5, )

[torch error] paddle.Tensor.quantile(Tensor([3, 6, 3, 7953644, 2, 5],"float16"), q=0.75, axis=5, ) 
 quantile() input tensor must be either float or double dtype
2025-03-11 18:55:09.925309 test begin: paddle.Tensor.quantile(Tensor([3, 6, 5965233, 4, 2, 5],"float16"), q=0.5, )

[torch error] paddle.Tensor.quantile(Tensor([3, 6, 5965233, 4, 2, 5],"float16"), q=0.5, ) 
 quantile() input tensor must be either float or double dtype
2025-03-11 18:55:12.110739 test begin: paddle.Tensor.quantile(Tensor([3, 6, 5965233, 4, 2, 5],"float16"), q=0.75, axis=3, keepdim=True, )

[torch error] paddle.Tensor.quantile(Tensor([3, 6, 5965233, 4, 2, 5],"float16"), q=0.75, axis=3, keepdim=True, ) 
 quantile() input tensor must be either float or double dtype
2025-03-11 18:55:13.921448 test begin: paddle.Tensor.quantile(Tensor([3, 6, 5965233, 4, 2, 5],"float16"), q=0.75, axis=5, )

[torch error] paddle.Tensor.quantile(Tensor([3, 6, 5965233, 4, 2, 5],"float16"), q=0.75, axis=5, ) 
 quantile() input tensor must be either float or double dtype
2025-03-11 18:55:15.837934 test begin: paddle.Tensor.quantile(Tensor([477218589, 3, 3],"float16"), q=0.5, axis=0, )

[torch error] paddle.Tensor.quantile(Tensor([477218589, 3, 3],"float16"), q=0.5, axis=0, ) 
 quantile() input tensor must be either float or double dtype
2025-03-11 18:55:17.340195 test begin: paddle.Tensor.quantile(Tensor([5965233, 6, 3, 4, 2, 5],"float16"), q=0.5, )

[torch error] paddle.Tensor.quantile(Tensor([5965233, 6, 3, 4, 2, 5],"float16"), q=0.5, ) 
 quantile() input tensor must be either float or double dtype
2025-03-11 18:55:19.106428 test begin: paddle.Tensor.quantile(Tensor([5965233, 6, 3, 4, 2, 5],"float16"), q=0.75, axis=3, keepdim=True, )

[torch error] paddle.Tensor.quantile(Tensor([5965233, 6, 3, 4, 2, 5],"float16"), q=0.75, axis=3, keepdim=True, ) 
 quantile() input tensor must be either float or double dtype
2025-03-11 18:55:21.531012 test begin: paddle.Tensor.quantile(Tensor([5965233, 6, 3, 4, 2, 5],"float16"), q=0.75, axis=5, )

[torch error] paddle.Tensor.quantile(Tensor([5965233, 6, 3, 4, 2, 5],"float16"), q=0.75, axis=5, ) 
 quantile() input tensor must be either float or double dtype
2025-03-11 18:55:23.936725 test begin: paddle.Tensor.remainder(Tensor([190141782, 3, 4],"float32"), Tensor([190141782, 3, 4],"float32"), )

[Pass] paddle.Tensor.remainder(Tensor([190141782, 3, 4],"float32"), Tensor([190141782, 3, 4],"float32"), )
2025-03-11 18:57:55.809138 test begin: paddle.Tensor.remainder(Tensor([190141782, 3, 4],"float32"), Tensor([2, 3, 4],"float32"), )

[torch error] paddle.Tensor.remainder(Tensor([190141782, 3, 4],"float32"), Tensor([2, 3, 4],"float32"), ) 
 The size of tensor a (190141782) must match the size of tensor b (2) at non-singleton dimension 0
2025-03-11 18:58:00.603921 test begin: paddle.Tensor.remainder(Tensor([2, 285212673, 4],"float32"), Tensor([2, 285212673, 4],"float32"), )

[Pass] paddle.Tensor.remainder(Tensor([2, 285212673, 4],"float32"), Tensor([2, 285212673, 4],"float32"), )
2025-03-11 19:00:05.312225 test begin: paddle.Tensor.remainder(Tensor([2, 285212673, 4],"float32"), Tensor([2, 3, 4],"float32"), )

[torch error] paddle.Tensor.remainder(Tensor([2, 285212673, 4],"float32"), Tensor([2, 3, 4],"float32"), ) 
 The size of tensor a (285212673) must match the size of tensor b (3) at non-singleton dimension 1
2025-03-11 19:00:12.544190 test begin: paddle.Tensor.remainder(Tensor([2, 3, 380283564],"float32"), Tensor([2, 3, 380283564],"float32"), )

[Pass] paddle.Tensor.remainder(Tensor([2, 3, 380283564],"float32"), Tensor([2, 3, 380283564],"float32"), )
2025-03-11 19:01:44.944415 test begin: paddle.Tensor.remainder(Tensor([2, 3, 380283564],"float32"), Tensor([2, 3, 4],"float32"), )

[torch error] paddle.Tensor.remainder(Tensor([2, 3, 380283564],"float32"), Tensor([2, 3, 4],"float32"), ) 
 The size of tensor a (380283564) must match the size of tensor b (4) at non-singleton dimension 2
2025-03-11 19:01:49.351302 test begin: paddle.Tensor.remainder(Tensor([2, 3, 4],"float32"), Tensor([190141782, 3, 4],"float32"), )

[torch error] paddle.Tensor.remainder(Tensor([2, 3, 4],"float32"), Tensor([190141782, 3, 4],"float32"), ) 
 The size of tensor a (2) must match the size of tensor b (190141782) at non-singleton dimension 0
2025-03-11 19:01:51.809583 test begin: paddle.Tensor.remainder(Tensor([2, 3, 4],"float32"), Tensor([2, 285212673, 4],"float32"), )

[torch error] paddle.Tensor.remainder(Tensor([2, 3, 4],"float32"), Tensor([2, 285212673, 4],"float32"), ) 
 The size of tensor a (3) must match the size of tensor b (285212673) at non-singleton dimension 1
2025-03-11 19:01:54.388533 test begin: paddle.Tensor.remainder(Tensor([2, 3, 4],"float32"), Tensor([2, 3, 380283564],"float32"), )

[torch error] paddle.Tensor.remainder(Tensor([2, 3, 4],"float32"), Tensor([2, 3, 380283564],"float32"), ) 
 The size of tensor a (4) must match the size of tensor b (380283564) at non-singleton dimension 2
2025-03-11 19:01:56.058853 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 1, 1140850690],"float32"), 1, axis=0, )

[Pass] paddle.Tensor.repeat_interleave(Tensor([2, 1, 1, 1140850690],"float32"), 1, axis=0, )
2025-03-11 19:04:52.912173 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 1, 1140850690],"float32"), 2, axis=0, )

2025-03-11 19:04:57.289093 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 1, 1140850690],"float32"), 3, axis=0, )

2025-03-11 19:04:58.787824 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 10, 114085069],"int64"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 10, 114085069],"int64"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:08.085662 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 10, 114085069],"int64"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 10, 114085069],"int64"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 34.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 154.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 958.00 MiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:13.287247 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 10, 114085069],"int64"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 10, 114085069],"int64"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 51.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:17.080651 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 114085069, 10],"int64"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 114085069, 10],"int64"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:20.829485 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 114085069, 10],"int64"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 114085069, 10],"int64"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 34.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:24.560237 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 114085069, 10],"int64"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 114085069, 10],"int64"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 51.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:28.336001 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 380283564, 3],"float32"), 1, axis=0, )

2025-03-11 19:06:32.897270 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 380283564, 3],"float32"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 380283564, 3],"float32"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:32.910043 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 380283564, 3],"float32"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 380283564, 3],"float32"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:32.921756 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1140850690],"int64"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1140850690],"int64"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:32.933406 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1140850690],"int64"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1140850690],"int64"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:32.945739 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1140850690],"int64"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1140850690],"int64"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:32.957405 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 11408507, 10, 10],"int64"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 11408507, 10, 10],"int64"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:32.969236 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 11408507, 10, 10],"int64"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 11408507, 10, 10],"int64"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:32.980689 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 11408507, 10, 10],"int64"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 11408507, 10, 10],"int64"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:32.992086 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 3, 380283564],"float32"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 3, 380283564],"float32"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:33.003349 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 3, 380283564],"float32"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 3, 380283564],"float32"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:33.014547 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 3, 380283564],"float32"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 3, 380283564],"float32"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:33.025685 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 35651585, 32],"float32"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 35651585, 32],"float32"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:33.036650 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 35651585, 32],"float32"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 35651585, 32],"float32"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:33.048727 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 35651585, 32],"float32"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 35651585, 32],"float32"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:33.060370 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 380283564, 1, 3],"float32"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 380283564, 1, 3],"float32"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:33.071831 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 380283564, 1, 3],"float32"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 380283564, 1, 3],"float32"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:33.082802 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 380283564, 1, 3],"float32"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 380283564, 1, 3],"float32"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:33.093740 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 71303169, 16],"float32"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 71303169, 16],"float32"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:33.104802 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 71303169, 16],"float32"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 71303169, 16],"float32"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:33.115754 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 71303169, 16],"float32"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 71303169, 16],"float32"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:33.127484 test begin: paddle.Tensor.repeat_interleave(Tensor([2281701379, 1],"int64"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2281701379, 1],"int64"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:33.138690 test begin: paddle.Tensor.repeat_interleave(Tensor([2281701379, 1],"int64"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2281701379, 1],"int64"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:33.149858 test begin: paddle.Tensor.repeat_interleave(Tensor([2281701379, 1],"int64"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2281701379, 1],"int64"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:33.161507 test begin: paddle.Tensor.repeat_interleave(Tensor([228170138, 10],"int64"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([228170138, 10],"int64"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:33.172775 test begin: paddle.Tensor.repeat_interleave(Tensor([228170138, 10],"int64"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([228170138, 10],"int64"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:33.183789 test begin: paddle.Tensor.repeat_interleave(Tensor([228170138, 10],"int64"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([228170138, 10],"int64"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:33.194657 test begin: paddle.Tensor.repeat_interleave(Tensor([22817014, 1, 10, 10],"int64"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([22817014, 1, 10, 10],"int64"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:33.205548 test begin: paddle.Tensor.repeat_interleave(Tensor([22817014, 1, 10, 10],"int64"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([22817014, 1, 10, 10],"int64"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:33.216386 test begin: paddle.Tensor.repeat_interleave(Tensor([22817014, 1, 10, 10],"int64"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([22817014, 1, 10, 10],"int64"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:33.227043 test begin: paddle.Tensor.repeat_interleave(Tensor([23767723, 3, 32],"float32"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([23767723, 3, 32],"float32"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:33.237967 test begin: paddle.Tensor.repeat_interleave(Tensor([23767723, 3, 32],"float32"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([23767723, 3, 32],"float32"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:33.249128 test begin: paddle.Tensor.repeat_interleave(Tensor([23767723, 3, 32],"float32"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([23767723, 3, 32],"float32"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:33.259851 test begin: paddle.Tensor.repeat_interleave(Tensor([47535446, 3, 16],"float32"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([47535446, 3, 16],"float32"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:33.270461 test begin: paddle.Tensor.repeat_interleave(Tensor([47535446, 3, 16],"float32"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([47535446, 3, 16],"float32"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:33.281015 test begin: paddle.Tensor.repeat_interleave(Tensor([47535446, 3, 16],"float32"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([47535446, 3, 16],"float32"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:33.292147 test begin: paddle.Tensor.repeat_interleave(Tensor([760567127, 1, 1, 3],"float32"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([760567127, 1, 1, 3],"float32"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:33.303123 test begin: paddle.Tensor.repeat_interleave(Tensor([760567127, 1, 1, 3],"float32"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([760567127, 1, 1, 3],"float32"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:33.314035 test begin: paddle.Tensor.repeat_interleave(Tensor([760567127, 1, 1, 3],"float32"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([760567127, 1, 1, 3],"float32"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:33.324941 test begin: paddle.Tensor.repeat_interleave(x=Tensor([107374183, 2, 4, 5],"float16"), repeats=2, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([107374183, 2, 4, 5],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.38 MiB is free. Process 41358 has 78.10 GiB memory in use. Process 148432 has 1.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:06:33.339454 test begin: paddle.Tensor.repeat_interleave(x=Tensor([14260634, 2, 4, 4, 5],"int32"), repeats=2, axis=3, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([14260634, 2, 4, 4, 5],"int32"), repeats=2, axis=3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.339688 test begin: paddle.Tensor.repeat_interleave(x=Tensor([2147483649, 2],"float16"), repeats=2, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([2147483649, 2],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.350567 test begin: paddle.Tensor.repeat_interleave(x=Tensor([2281701379],"float32"), repeats=3, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([2281701379],"float32"), repeats=3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.360663 test begin: paddle.Tensor.repeat_interleave(x=Tensor([26843546, 2, 4, 4, 5],"float16"), repeats=2, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([26843546, 2, 4, 4, 5],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.371095 test begin: paddle.Tensor.repeat_interleave(x=Tensor([26843546, 2, 4, 4, 5],"float16"), repeats=2, axis=1, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([26843546, 2, 4, 4, 5],"float16"), repeats=2, axis=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.387163 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 1073741825],"float16"), repeats=2, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 1073741825],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.397147 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 13421773, 4, 4, 5],"float16"), repeats=2, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 13421773, 4, 4, 5],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.406953 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 13421773, 4, 4, 5],"float16"), repeats=2, axis=1, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 13421773, 4, 4, 5],"float16"), repeats=2, axis=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.416729 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 107374183, 5],"float16"), repeats=2, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 107374183, 5],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.427341 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 14260634, 4, 5],"int32"), repeats=2, axis=3, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 14260634, 4, 5],"int32"), repeats=2, axis=3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.437558 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 26843546, 4, 5],"float16"), repeats=2, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 26843546, 4, 5],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.447712 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 26843546, 4, 5],"float16"), repeats=2, axis=1, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 26843546, 4, 5],"float16"), repeats=2, axis=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.457805 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 134217729],"float16"), repeats=2, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 134217729],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.467660 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 14260634, 5],"int32"), repeats=2, axis=3, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 14260634, 5],"int32"), repeats=2, axis=3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.477360 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 26843546, 5],"float16"), repeats=2, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 26843546, 5],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.487345 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 26843546, 5],"float16"), repeats=2, axis=1, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 26843546, 5],"float16"), repeats=2, axis=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.497017 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 4, 17825793],"int32"), repeats=2, axis=3, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 4, 17825793],"int32"), repeats=2, axis=3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.506799 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 4, 33554433],"float16"), repeats=2, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 4, 33554433],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.516389 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 4, 33554433],"float16"), repeats=2, axis=1, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 4, 33554433],"float16"), repeats=2, axis=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.526476 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 536870913],"float16"), repeats=2, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 536870913],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.536689 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 268435457, 4],"float16"), repeats=2, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 268435457, 4],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.546346 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 53687092, 4, 5],"float16"), repeats=2, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 53687092, 4, 5],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.555756 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 7130317, 4, 4, 5],"int32"), repeats=2, axis=3, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 7130317, 4, 4, 5],"int32"), repeats=2, axis=3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.565232 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4294967297],"float16"), repeats=3, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4294967297],"float16"), repeats=3, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.575259 test begin: paddle.Tensor.repeat_interleave(x=Tensor([536870913, 2, 4],"float16"), repeats=2, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([536870913, 2, 4],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.584966 test begin: paddle.Tensor.rot90(Tensor([1140850690, 2],"float32"), 1, axes=list[0,1,], )

[torch error] paddle.Tensor.rot90(Tensor([1140850690, 2],"float32"), 1, axes=list[0,1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.597944 test begin: paddle.Tensor.rot90(Tensor([3, 760567127],"float32"), 1, axes=list[0,1,], )

[torch error] paddle.Tensor.rot90(Tensor([3, 760567127],"float32"), 1, axes=list[0,1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.611158 test begin: paddle.Tensor.rot90(x=Tensor([1073741825, 4],"float16"), )

[torch error] paddle.Tensor.rot90(x=Tensor([1073741825, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.620970 test begin: paddle.Tensor.rot90(x=Tensor([1073741825, 4],"float16"), k=-1, )

[torch error] paddle.Tensor.rot90(x=Tensor([1073741825, 4],"float16"), k=-1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.630788 test begin: paddle.Tensor.rot90(x=Tensor([1073741825, 4],"float16"), k=4, )

[torch error] paddle.Tensor.rot90(x=Tensor([1073741825, 4],"float16"), k=4, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.640654 test begin: paddle.Tensor.rot90(x=Tensor([2, 2147483649],"float16"), k=-4, )

[torch error] paddle.Tensor.rot90(x=Tensor([2, 2147483649],"float16"), k=-4, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.650395 test begin: paddle.Tensor.rot90(x=Tensor([2147483649, 2],"float16"), k=-4, )

[torch error] paddle.Tensor.rot90(x=Tensor([2147483649, 2],"float16"), k=-4, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.660106 test begin: paddle.Tensor.rot90(x=Tensor([268435457, 4, 4],"float16"), )

[torch error] paddle.Tensor.rot90(x=Tensor([268435457, 4, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.669859 test begin: paddle.Tensor.rot90(x=Tensor([3, 1431655766],"float16"), )

[torch error] paddle.Tensor.rot90(x=Tensor([3, 1431655766],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.679535 test begin: paddle.Tensor.rot90(x=Tensor([4, 1073741825],"float16"), k=-1, )

[torch error] paddle.Tensor.rot90(x=Tensor([4, 1073741825],"float16"), k=-1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.689191 test begin: paddle.Tensor.rot90(x=Tensor([4, 1073741825],"float16"), k=4, )

[torch error] paddle.Tensor.rot90(x=Tensor([4, 1073741825],"float16"), k=4, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.698761 test begin: paddle.Tensor.rot90(x=Tensor([4, 268435457, 4],"float16"), )

[torch error] paddle.Tensor.rot90(x=Tensor([4, 268435457, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.708319 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 268435457],"float16"), )

[torch error] paddle.Tensor.rot90(x=Tensor([4, 4, 268435457],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.717816 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 4, 67108865],"float16"), )

[torch error] paddle.Tensor.rot90(x=Tensor([4, 4, 4, 67108865],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.727333 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 4, 67108865],"float16"), k=-1, axes=list[1,2,], )

[torch error] paddle.Tensor.rot90(x=Tensor([4, 4, 4, 67108865],"float16"), k=-1, axes=list[1,2,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.736890 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 4, 67108865],"float16"), k=-1, axes=tuple(2,3,), )

[torch error] paddle.Tensor.rot90(x=Tensor([4, 4, 4, 67108865],"float16"), k=-1, axes=tuple(2,3,), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.746983 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 67108865, 4],"float16"), )

[torch error] paddle.Tensor.rot90(x=Tensor([4, 4, 67108865, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.756642 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 67108865, 4],"float16"), k=-1, axes=list[1,2,], )

[torch error] paddle.Tensor.rot90(x=Tensor([4, 4, 67108865, 4],"float16"), k=-1, axes=list[1,2,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.766377 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 67108865, 4],"float16"), k=-1, axes=tuple(2,3,), )

[torch error] paddle.Tensor.rot90(x=Tensor([4, 4, 67108865, 4],"float16"), k=-1, axes=tuple(2,3,), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.776100 test begin: paddle.Tensor.rot90(x=Tensor([4, 570425345],"float32"), )

[torch error] paddle.Tensor.rot90(x=Tensor([4, 570425345],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.785895 test begin: paddle.Tensor.rot90(x=Tensor([4, 67108865, 4, 4],"float16"), )

[torch error] paddle.Tensor.rot90(x=Tensor([4, 67108865, 4, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.795491 test begin: paddle.Tensor.rot90(x=Tensor([4, 67108865, 4, 4],"float16"), k=-1, axes=list[1,2,], )

[torch error] paddle.Tensor.rot90(x=Tensor([4, 67108865, 4, 4],"float16"), k=-1, axes=list[1,2,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.805123 test begin: paddle.Tensor.rot90(x=Tensor([4, 67108865, 4, 4],"float16"), k=-1, axes=tuple(2,3,), )

[torch error] paddle.Tensor.rot90(x=Tensor([4, 67108865, 4, 4],"float16"), k=-1, axes=tuple(2,3,), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.814673 test begin: paddle.Tensor.rot90(x=Tensor([570425345, 4],"float32"), )

[torch error] paddle.Tensor.rot90(x=Tensor([570425345, 4],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.824331 test begin: paddle.Tensor.rot90(x=Tensor([67108865, 4, 4, 4],"float16"), )

[torch error] paddle.Tensor.rot90(x=Tensor([67108865, 4, 4, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.833948 test begin: paddle.Tensor.rot90(x=Tensor([67108865, 4, 4, 4],"float16"), k=-1, axes=list[1,2,], )

[torch error] paddle.Tensor.rot90(x=Tensor([67108865, 4, 4, 4],"float16"), k=-1, axes=list[1,2,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.843694 test begin: paddle.Tensor.rot90(x=Tensor([67108865, 4, 4, 4],"float16"), k=-1, axes=tuple(2,3,), )

[torch error] paddle.Tensor.rot90(x=Tensor([67108865, 4, 4, 4],"float16"), k=-1, axes=tuple(2,3,), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.853441 test begin: paddle.Tensor.signbit(Tensor([107374183, 20, 2],"float16"), )

[torch error] paddle.Tensor.signbit(Tensor([107374183, 20, 2],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.863052 test begin: paddle.Tensor.signbit(Tensor([12, 178956971, 2],"float16"), )

[torch error] paddle.Tensor.signbit(Tensor([12, 178956971, 2],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.872825 test begin: paddle.Tensor.signbit(Tensor([12, 20, 17895698],"float16"), )

[torch error] paddle.Tensor.signbit(Tensor([12, 20, 17895698],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.882596 test begin: paddle.Tensor.signbit(Tensor([12, 20, 9507090],"float32"), )

[torch error] paddle.Tensor.signbit(Tensor([12, 20, 9507090],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.892244 test begin: paddle.Tensor.signbit(Tensor([12, 95070891, 2],"float32"), )

[torch error] paddle.Tensor.signbit(Tensor([12, 95070891, 2],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.901926 test begin: paddle.Tensor.signbit(Tensor([57042535, 20, 2],"float32"), )

[torch error] paddle.Tensor.signbit(Tensor([57042535, 20, 2],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.911514 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 1, 2281701379],"float32"), axis=list[0,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 1, 2281701379],"float32"), axis=list[0,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.920861 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 1, 2281701379],"float32"), list[1,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 1, 2281701379],"float32"), list[1,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.930184 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 1, 2281701379],"int64"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 1, 2281701379],"int64"), -2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.939591 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 10, 228170138],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 10, 228170138],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.949766 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 114085069, 20],"int64"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 114085069, 20],"int64"), -2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.959856 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 12, 190141782],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 12, 190141782],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.969547 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 181896, 12544],"float32"), list[1,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 181896, 12544],"float32"), list[1,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.979340 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 21504, 106106],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 21504, 106106],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.989737 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 2281701379],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 2281701379],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:19.999567 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 2281701379],"int32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 2281701379],"int32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.009227 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 24276, 93991],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 24276, 93991],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.018884 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 253522376, 9],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 253522376, 9],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.028523 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 285212673, 8],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 285212673, 8],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.038170 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 285212673, 8],"float32"), axis=list[0,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 285212673, 8],"float32"), axis=list[0,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.047729 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 570425345, 4],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 570425345, 4],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.057450 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 60632, 37632],"float32"), list[1,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 60632, 37632],"float32"), list[1,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.067111 test begin: paddle.Tensor.squeeze(Tensor([1, 10, 1, 228170138],"float32"), axis=list[0,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([1, 10, 1, 228170138],"float32"), axis=list[0,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.077167 test begin: paddle.Tensor.squeeze(Tensor([1, 10, 228170138],"int32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 10, 228170138],"int32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.086665 test begin: paddle.Tensor.squeeze(Tensor([1, 10, 891290, 256],"float32"), axis=list[0,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([1, 10, 891290, 256],"float32"), axis=list[0,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.096047 test begin: paddle.Tensor.squeeze(Tensor([1, 100, 22817014],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 100, 22817014],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.105474 test begin: paddle.Tensor.squeeze(Tensor([1, 114085069, 1, 20],"int64"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 114085069, 1, 20],"int64"), -2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.114938 test begin: paddle.Tensor.squeeze(Tensor([1, 1273271, 1792],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1273271, 1792],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.125020 test begin: paddle.Tensor.squeeze(Tensor([1, 139265, 128, 128],"float32"), )

[torch error] paddle.Tensor.squeeze(Tensor([1, 139265, 128, 128],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.134941 test begin: paddle.Tensor.squeeze(Tensor([1, 17, 1048577, 128],"float32"), )

[torch error] paddle.Tensor.squeeze(Tensor([1, 17, 1048577, 128],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.144716 test begin: paddle.Tensor.squeeze(Tensor([1, 17, 128, 1048577],"float32"), )

[torch error] paddle.Tensor.squeeze(Tensor([1, 17, 128, 1048577],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.154291 test begin: paddle.Tensor.squeeze(Tensor([1, 17, 128, 1973791],"float16"), )

[torch error] paddle.Tensor.squeeze(Tensor([1, 17, 128, 1973791],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.163674 test begin: paddle.Tensor.squeeze(Tensor([1, 17, 1973791, 128],"float16"), )

[torch error] paddle.Tensor.squeeze(Tensor([1, 17, 1973791, 128],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.173667 test begin: paddle.Tensor.squeeze(Tensor([1, 181896, 1, 12544],"float32"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 181896, 1, 12544],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.183289 test begin: paddle.Tensor.squeeze(Tensor([1, 181896, 1, 12544],"float32"), list[1,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([1, 181896, 1, 12544],"float32"), list[1,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.192665 test begin: paddle.Tensor.squeeze(Tensor([1, 2, 32, 32, 2097153],"float16"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 2, 32, 32, 2097153],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.201936 test begin: paddle.Tensor.squeeze(Tensor([1, 2, 32, 524289, 128],"float16"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 2, 32, 524289, 128],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.211230 test begin: paddle.Tensor.squeeze(Tensor([1, 2, 524289, 32, 128],"float16"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 2, 524289, 32, 128],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.220848 test begin: paddle.Tensor.squeeze(Tensor([1, 20, 114085069],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 20, 114085069],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.230281 test begin: paddle.Tensor.squeeze(Tensor([1, 2048, 1114113],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 2048, 1114113],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.239535 test begin: paddle.Tensor.squeeze(Tensor([1, 21126865, 12, 9],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 21126865, 12, 9],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.248733 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379, 1],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 2281701379, 1],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.257964 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 2281701379, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.267334 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379, 1],"int32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 2281701379, 1],"int32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.276816 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379],"float32"), )

[torch error] paddle.Tensor.squeeze(Tensor([1, 2281701379],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.286088 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 2281701379],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.295557 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 2281701379],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.304939 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 2281701379],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.314309 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379],"int32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 2281701379],"int32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.323490 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379],"int32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 2281701379],"int32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.332636 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379],"int64"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 2281701379],"int64"), 1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.342050 test begin: paddle.Tensor.squeeze(Tensor([1, 23498, 24276, 4],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 23498, 24276, 4],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.351411 test begin: paddle.Tensor.squeeze(Tensor([1, 262145, 128, 128],"float16"), )

[torch error] paddle.Tensor.squeeze(Tensor([1, 262145, 128, 128],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.361449 test begin: paddle.Tensor.squeeze(Tensor([1, 26527, 21504, 4],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 26527, 21504, 4],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.371244 test begin: paddle.Tensor.squeeze(Tensor([1, 285212673, 1, 8],"float32"), axis=list[0,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([1, 285212673, 1, 8],"float32"), axis=list[0,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.380915 test begin: paddle.Tensor.squeeze(Tensor([1, 28521268, 10, 8],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 28521268, 10, 8],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.391087 test begin: paddle.Tensor.squeeze(Tensor([1, 300, 1, 7605672],"float32"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 300, 1, 7605672],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.400729 test begin: paddle.Tensor.squeeze(Tensor([1, 300, 607, 12544],"float32"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 300, 607, 12544],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.410230 test begin: paddle.Tensor.squeeze(Tensor([1, 32769, 32, 32, 128],"float16"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 32769, 32, 32, 128],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.419571 test begin: paddle.Tensor.squeeze(Tensor([1, 570425345, 4],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 570425345, 4],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.429550 test begin: paddle.Tensor.squeeze(Tensor([1, 60632, 1, 37632],"float32"), list[1,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([1, 60632, 1, 37632],"float32"), list[1,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.439368 test begin: paddle.Tensor.squeeze(Tensor([1, 8912897, 1, 256],"float32"), axis=list[0,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([1, 8912897, 1, 256],"float32"), axis=list[0,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.449162 test begin: paddle.Tensor.squeeze(Tensor([11, 1, 207427399],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([11, 1, 207427399],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.458739 test begin: paddle.Tensor.squeeze(Tensor([11, 51856850, 4],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([11, 51856850, 4],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.468364 test begin: paddle.Tensor.squeeze(Tensor([1114113, 2048, 1],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([1114113, 2048, 1],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.478119 test begin: paddle.Tensor.squeeze(Tensor([111412, 512, 1, 40],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([111412, 512, 1, 40],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.487818 test begin: paddle.Tensor.squeeze(Tensor([114085069, 1, 1, 20],"int64"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([114085069, 1, 1, 20],"int64"), -2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.497480 test begin: paddle.Tensor.squeeze(Tensor([114085069, 20, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([114085069, 20, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.507010 test begin: paddle.Tensor.squeeze(Tensor([114085069, 20],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([114085069, 20],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.517195 test begin: paddle.Tensor.squeeze(Tensor([1273271, 1, 1792],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1273271, 1, 1792],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.527122 test begin: paddle.Tensor.squeeze(Tensor([1273271, 1, 1792],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([1273271, 1, 1792],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.538127 test begin: paddle.Tensor.squeeze(Tensor([128, 120, 1, 148549],"float32"), axis=2, )

[torch error] paddle.Tensor.squeeze(Tensor([128, 120, 1, 148549],"float32"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.548846 test begin: paddle.Tensor.squeeze(Tensor([128, 120, 1, 279621],"float16"), axis=2, )

[torch error] paddle.Tensor.squeeze(Tensor([128, 120, 1, 279621],"float16"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.560256 test begin: paddle.Tensor.squeeze(Tensor([128, 120, 3714, 40],"float32"), axis=2, )

[torch error] paddle.Tensor.squeeze(Tensor([128, 120, 3714, 40],"float32"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.570813 test begin: paddle.Tensor.squeeze(Tensor([128, 120, 6991, 40],"float16"), axis=2, )

[torch error] paddle.Tensor.squeeze(Tensor([128, 120, 6991, 40],"float16"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.580823 test begin: paddle.Tensor.squeeze(Tensor([128, 445645, 1, 40],"float32"), axis=2, )

[torch error] paddle.Tensor.squeeze(Tensor([128, 445645, 1, 40],"float32"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.590939 test begin: paddle.Tensor.squeeze(Tensor([128, 838861, 1, 40],"float16"), axis=2, )

[torch error] paddle.Tensor.squeeze(Tensor([128, 838861, 1, 40],"float16"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.601111 test begin: paddle.Tensor.squeeze(Tensor([12988, 1, 175678],"float32"), )

[torch error] paddle.Tensor.squeeze(Tensor([12988, 1, 175678],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.611009 test begin: paddle.Tensor.squeeze(Tensor([12988, 1, 330688],"float16"), )

[torch error] paddle.Tensor.squeeze(Tensor([12988, 1, 330688],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.621310 test begin: paddle.Tensor.squeeze(Tensor([12988, 2745, 64],"float32"), )

[torch error] paddle.Tensor.squeeze(Tensor([12988, 2745, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.631262 test begin: paddle.Tensor.squeeze(Tensor([12988, 5167, 64],"float16"), )

[torch error] paddle.Tensor.squeeze(Tensor([12988, 5167, 64],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.641649 test begin: paddle.Tensor.squeeze(Tensor([13, 1, 175515491, 1],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([13, 1, 175515491, 1],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.651605 test begin: paddle.Tensor.squeeze(Tensor([13, 1, 96, 1828287],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([13, 1, 96, 1828287],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.661534 test begin: paddle.Tensor.squeeze(Tensor([13, 175515491, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([13, 175515491, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.671197 test begin: paddle.Tensor.squeeze(Tensor([13, 175515491],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([13, 175515491],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.681134 test begin: paddle.Tensor.squeeze(Tensor([13, 1828287, 96, 1],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([13, 1828287, 96, 1],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.691047 test begin: paddle.Tensor.squeeze(Tensor([13, 2, 64, 1371215],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([13, 2, 64, 1371215],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.700889 test begin: paddle.Tensor.squeeze(Tensor([13, 2, 87757746, 1],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([13, 2, 87757746, 1],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.710703 test begin: paddle.Tensor.squeeze(Tensor([13, 2742430, 64, 1],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([13, 2742430, 64, 1],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.720541 test begin: paddle.Tensor.squeeze(Tensor([13, 7, 25073642],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([13, 7, 25073642],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.730386 test begin: paddle.Tensor.squeeze(Tensor([15421, 17, 128, 128],"float16"), )

[torch error] paddle.Tensor.squeeze(Tensor([15421, 17, 128, 128],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.740670 test begin: paddle.Tensor.squeeze(Tensor([16, 10, 111412, 64, 2, 1],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([16, 10, 111412, 64, 2, 1],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.750546 test begin: paddle.Tensor.squeeze(Tensor([16, 10, 64, 111412, 2, 1],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([16, 10, 64, 111412, 2, 1],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.759914 test begin: paddle.Tensor.squeeze(Tensor([16, 10, 64, 64, 2, 1741],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([16, 10, 64, 64, 2, 1741],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.769772 test begin: paddle.Tensor.squeeze(Tensor([16, 10, 64, 64, 3482, 1],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([16, 10, 64, 64, 3482, 1],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.779837 test begin: paddle.Tensor.squeeze(Tensor([16, 17409, 64, 64, 2, 1],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([16, 17409, 64, 64, 2, 1],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.789710 test begin: paddle.Tensor.squeeze(Tensor([16385, 2, 32, 32, 128],"float16"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([16385, 2, 32, 32, 128],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.799531 test begin: paddle.Tensor.squeeze(Tensor([17825793, 2, 64, 1],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([17825793, 2, 64, 1],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.810090 test begin: paddle.Tensor.squeeze(Tensor([181896, 1, 1, 12544],"float32"), list[1,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([181896, 1, 1, 12544],"float32"), list[1,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.819787 test begin: paddle.Tensor.squeeze(Tensor([2, 1, 1, 1, 1140850690],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([2, 1, 1, 1, 1140850690],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.829184 test begin: paddle.Tensor.squeeze(Tensor([2, 1, 1, 1140850690, 1],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([2, 1, 1, 1140850690, 1],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.838897 test begin: paddle.Tensor.squeeze(Tensor([2, 1, 1140850690, 1, 1],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([2, 1, 1140850690, 1, 1],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.848165 test begin: paddle.Tensor.squeeze(Tensor([2, 1, 1140850690],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([2, 1, 1140850690],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.857412 test begin: paddle.Tensor.squeeze(Tensor([2, 1, 32, 35651585],"float32"), axis=1, )

[torch error] paddle.Tensor.squeeze(Tensor([2, 1, 32, 35651585],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.867846 test begin: paddle.Tensor.squeeze(Tensor([2, 1, 35651585, 32],"float32"), axis=1, )

[torch error] paddle.Tensor.squeeze(Tensor([2, 1, 35651585, 32],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.884043 test begin: paddle.Tensor.squeeze(Tensor([2, 1114113, 32, 32],"float32"), axis=1, )

[torch error] paddle.Tensor.squeeze(Tensor([2, 1114113, 32, 32],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.893924 test begin: paddle.Tensor.squeeze(Tensor([2, 1140850690, 1, 1, 1],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([2, 1140850690, 1, 1, 1],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.903770 test begin: paddle.Tensor.squeeze(Tensor([2, 1140850690],"int64"), axis=1, )

[torch error] paddle.Tensor.squeeze(Tensor([2, 1140850690],"int64"), axis=1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.920216 test begin: paddle.Tensor.squeeze(Tensor([2, 636636, 1792],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([2, 636636, 1792],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.930068 test begin: paddle.Tensor.squeeze(Tensor([209716, 512, 1, 40],"float16"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([209716, 512, 1, 40],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.940063 test begin: paddle.Tensor.squeeze(Tensor([21126865, 1, 12, 9],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([21126865, 1, 12, 9],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.950536 test begin: paddle.Tensor.squeeze(Tensor([21126865, 108],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([21126865, 108],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.960647 test begin: paddle.Tensor.squeeze(Tensor([21126865, 108],"int32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([21126865, 108],"int32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.970568 test begin: paddle.Tensor.squeeze(Tensor([2228225, 1, 32, 32],"float32"), axis=1, )

[torch error] paddle.Tensor.squeeze(Tensor([2228225, 1, 32, 32],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.980497 test begin: paddle.Tensor.squeeze(Tensor([2281701379, 1, 1, 1, 1],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([2281701379, 1, 1, 1, 1],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:20.990355 test begin: paddle.Tensor.squeeze(Tensor([2281701379, 1, 1],"int32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([2281701379, 1, 1],"int32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.000153 test begin: paddle.Tensor.squeeze(Tensor([2281701379, 1],"float32"), )

[torch error] paddle.Tensor.squeeze(Tensor([2281701379, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.010016 test begin: paddle.Tensor.squeeze(Tensor([2281701379, 1],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([2281701379, 1],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.019835 test begin: paddle.Tensor.squeeze(Tensor([2281701379, 1],"int32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([2281701379, 1],"int32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.029534 test begin: paddle.Tensor.squeeze(Tensor([2281701379, 1],"int64"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([2281701379, 1],"int64"), 1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.039269 test begin: paddle.Tensor.squeeze(Tensor([2281701379, 1],"int64"), axis=1, )

[torch error] paddle.Tensor.squeeze(Tensor([2281701379, 1],"int64"), axis=1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.049041 test begin: paddle.Tensor.squeeze(Tensor([2281701379],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([2281701379],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.073932 test begin: paddle.Tensor.squeeze(Tensor([228170138, 10, 1],"int32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([228170138, 10, 1],"int32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.090179 test begin: paddle.Tensor.squeeze(Tensor([2281702, 1000, 1, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([2281702, 1000, 1, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.106146 test begin: paddle.Tensor.squeeze(Tensor([23498, 1, 24276, 4],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([23498, 1, 24276, 4],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.122380 test begin: paddle.Tensor.squeeze(Tensor([23767723, 1, 96, 1],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([23767723, 1, 96, 1],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.134844 test begin: paddle.Tensor.squeeze(Tensor([2527, 7, 126, 1, 1024],"float32"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([2527, 7, 126, 1, 1024],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.150856 test begin: paddle.Tensor.squeeze(Tensor([26527, 1, 21504, 4],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([26527, 1, 21504, 4],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.160584 test begin: paddle.Tensor.squeeze(Tensor([27853, 10, 64, 64, 2, 1],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([27853, 10, 64, 64, 2, 1],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.170158 test begin: paddle.Tensor.squeeze(Tensor([285212673, 1, 1, 8],"float32"), axis=list[0,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([285212673, 1, 1, 8],"float32"), axis=list[0,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.179619 test begin: paddle.Tensor.squeeze(Tensor([28521268, 1, 10, 8],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([28521268, 1, 10, 8],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.189149 test begin: paddle.Tensor.squeeze(Tensor([3, 4, 190141782],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([3, 4, 190141782],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.198693 test begin: paddle.Tensor.squeeze(Tensor([3, 5895, 126, 1, 1024],"float32"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([3, 5895, 126, 1, 1024],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.208247 test begin: paddle.Tensor.squeeze(Tensor([3, 7, 106106, 1, 1024],"float32"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([3, 7, 106106, 1, 1024],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.217815 test begin: paddle.Tensor.squeeze(Tensor([3, 7, 126, 1, 862322],"float32"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([3, 7, 126, 1, 862322],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.227713 test begin: paddle.Tensor.squeeze(Tensor([3, 7, 126, 843, 1024],"float32"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([3, 7, 126, 843, 1024],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.241632 test begin: paddle.Tensor.squeeze(Tensor([3, 760567127, 1],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([3, 760567127, 1],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.251375 test begin: paddle.Tensor.squeeze(Tensor([325957340, 7, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([325957340, 7, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.260873 test begin: paddle.Tensor.squeeze(Tensor([325957340, 7],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([325957340, 7],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.270475 test begin: paddle.Tensor.squeeze(Tensor([35651585, 1, 64],"float32"), )

[torch error] paddle.Tensor.squeeze(Tensor([35651585, 1, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.279999 test begin: paddle.Tensor.squeeze(Tensor([4294968, 1000, 1, 1],"float16"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([4294968, 1000, 1, 1],"float16"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.289390 test begin: paddle.Tensor.squeeze(Tensor([475355, 120, 1, 40],"float32"), axis=2, )

[torch error] paddle.Tensor.squeeze(Tensor([475355, 120, 1, 40],"float32"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.298854 test begin: paddle.Tensor.squeeze(Tensor([570425345, 1, 4],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([570425345, 1, 4],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.308324 test begin: paddle.Tensor.squeeze(Tensor([570425345, 4, 1],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([570425345, 4, 1],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.317734 test begin: paddle.Tensor.squeeze(Tensor([5704254, 100, 4],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([5704254, 100, 4],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.327188 test begin: paddle.Tensor.squeeze(Tensor([60632, 1, 1, 37632],"float32"), list[1,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([60632, 1, 1, 37632],"float32"), list[1,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.336590 test begin: paddle.Tensor.squeeze(Tensor([607, 300, 1, 12544],"float32"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([607, 300, 1, 12544],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.346382 test begin: paddle.Tensor.squeeze(Tensor([64, 1000, 1, 35652],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([64, 1000, 1, 35652],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.355730 test begin: paddle.Tensor.squeeze(Tensor([64, 1000, 1, 67109],"float16"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([64, 1000, 1, 67109],"float16"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.365077 test begin: paddle.Tensor.squeeze(Tensor([64, 1000, 35652, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([64, 1000, 35652, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.375275 test begin: paddle.Tensor.squeeze(Tensor([64, 1000, 67109, 1],"float16"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([64, 1000, 67109, 1],"float16"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.384701 test begin: paddle.Tensor.squeeze(Tensor([64, 1677722, 1, 40],"float16"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([64, 1677722, 1, 40],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.394789 test begin: paddle.Tensor.squeeze(Tensor([64, 35651585, 1, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([64, 35651585, 1, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.405123 test begin: paddle.Tensor.squeeze(Tensor([64, 512, 1, 131073],"float16"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([64, 512, 1, 131073],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.415299 test begin: paddle.Tensor.squeeze(Tensor([64, 512, 1, 69633],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([64, 512, 1, 69633],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.426275 test begin: paddle.Tensor.squeeze(Tensor([64, 512, 1741, 40],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([64, 512, 1741, 40],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.436898 test begin: paddle.Tensor.squeeze(Tensor([64, 512, 3277, 40],"float16"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([64, 512, 3277, 40],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.446886 test begin: paddle.Tensor.squeeze(Tensor([64, 67108865, 1, 1],"float16"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([64, 67108865, 1, 1],"float16"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.456660 test begin: paddle.Tensor.squeeze(Tensor([64, 891290, 1, 40],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([64, 891290, 1, 40],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.466374 test begin: paddle.Tensor.squeeze(Tensor([67108865, 1, 64],"float16"), )

[torch error] paddle.Tensor.squeeze(Tensor([67108865, 1, 64],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.476617 test begin: paddle.Tensor.squeeze(Tensor([8193, 17, 128, 128],"float32"), )

[torch error] paddle.Tensor.squeeze(Tensor([8193, 17, 128, 128],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.489936 test begin: paddle.Tensor.squeeze(Tensor([891290, 10, 1, 256],"float32"), axis=list[0,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([891290, 10, 1, 256],"float32"), axis=list[0,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.501473 test begin: paddle.Tensor.squeeze(Tensor([894785, 120, 1, 40],"float16"), axis=2, )

[torch error] paddle.Tensor.squeeze(Tensor([894785, 120, 1, 40],"float16"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.514889 test begin: paddle.Tensor.std(Tensor([1, 1, 2281701379],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.std(Tensor([1, 1, 2281701379],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.525740 test begin: paddle.Tensor.std(Tensor([1, 50704476, 45],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.std(Tensor([1, 50704476, 45],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.535327 test begin: paddle.Tensor.std(Tensor([1, 63380594, 36],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.std(Tensor([1, 63380594, 36],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.547080 test begin: paddle.Tensor.std(Tensor([1024, 1024, 2177],"float32"), )

[torch error] paddle.Tensor.std(Tensor([1024, 1024, 2177],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.557012 test begin: paddle.Tensor.std(Tensor([1024, 1024, 4097],"float16"), )

[torch error] paddle.Tensor.std(Tensor([1024, 1024, 4097],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.567441 test begin: paddle.Tensor.std(Tensor([1024, 278529, 8],"float32"), )

[torch error] paddle.Tensor.std(Tensor([1024, 278529, 8],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.577022 test begin: paddle.Tensor.std(Tensor([1024, 524289, 8],"float16"), )

[torch error] paddle.Tensor.std(Tensor([1024, 524289, 8],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.587891 test begin: paddle.Tensor.std(Tensor([278529, 1024, 8],"float32"), )

[torch error] paddle.Tensor.std(Tensor([278529, 1024, 8],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.598608 test begin: paddle.Tensor.std(Tensor([50704476, 1, 45],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.std(Tensor([50704476, 1, 45],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.609041 test begin: paddle.Tensor.std(Tensor([524289, 1024, 8],"float16"), )

[torch error] paddle.Tensor.std(Tensor([524289, 1024, 8],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.619242 test begin: paddle.Tensor.std(Tensor([63380594, 1, 36],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.std(Tensor([63380594, 1, 36],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:07:21.628977 test begin: paddle.Tensor.sum(Tensor([1, 1, 1, 2281701379],"bool"), )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 1, 2281701379],"bool"), ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.012537 test begin: paddle.Tensor.sum(Tensor([1, 1, 1, 4294967297],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 1, 4294967297],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.022921 test begin: paddle.Tensor.sum(Tensor([1, 1, 1, 4294967297],"float16"), 1, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 1, 4294967297],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.032641 test begin: paddle.Tensor.sum(Tensor([1, 1, 1, 4294967297],"float16"), 2, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 1, 4294967297],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.042350 test begin: paddle.Tensor.sum(Tensor([1, 1, 1, 4294967297],"float16"), 3, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 1, 4294967297],"float16"), 3, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.051774 test begin: paddle.Tensor.sum(Tensor([1, 1, 1140850690, 2],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 1140850690, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.061329 test begin: paddle.Tensor.sum(Tensor([1, 1, 13, 175515491],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 13, 175515491],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.070744 test begin: paddle.Tensor.sum(Tensor([1, 1, 17, 252645136],"float16"), )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 17, 252645136],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.080077 test begin: paddle.Tensor.sum(Tensor([1, 1, 2, 1140850690],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 2, 1140850690],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.089472 test begin: paddle.Tensor.sum(Tensor([1, 1, 2281701379, 1],"bool"), )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 2281701379, 1],"bool"), ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.098775 test begin: paddle.Tensor.sum(Tensor([1, 1, 2281701379],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 2281701379],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.108128 test begin: paddle.Tensor.sum(Tensor([1, 1, 2281701379],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 2281701379],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.117470 test begin: paddle.Tensor.sum(Tensor([1, 1, 252645136, 17],"float16"), )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 252645136, 17],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.126714 test begin: paddle.Tensor.sum(Tensor([1, 1, 4294967297, 1],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 4294967297, 1],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.136070 test begin: paddle.Tensor.sum(Tensor([1, 1, 4294967297, 1],"float16"), 1, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 4294967297, 1],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.145938 test begin: paddle.Tensor.sum(Tensor([1, 1, 4294967297, 1],"float16"), 2, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 4294967297, 1],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.156414 test begin: paddle.Tensor.sum(Tensor([1, 1, 4294967297, 1],"float16"), 3, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 4294967297, 1],"float16"), 3, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.166221 test begin: paddle.Tensor.sum(Tensor([1, 1, 83837, 27216],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 83837, 27216],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.176040 test begin: paddle.Tensor.sum(Tensor([1, 1, 858993460, 5],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 858993460, 5],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.185439 test begin: paddle.Tensor.sum(Tensor([1, 1, 858993460, 5],"float16"), 1, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 858993460, 5],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.194750 test begin: paddle.Tensor.sum(Tensor([1, 1, 858993460, 5],"float16"), 2, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 858993460, 5],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.204135 test begin: paddle.Tensor.sum(Tensor([1, 1, 858993460, 5],"float16"), 3, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 858993460, 5],"float16"), 3, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.213487 test begin: paddle.Tensor.sum(Tensor([1, 1, 93991, 24276],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 93991, 24276],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.222890 test begin: paddle.Tensor.sum(Tensor([1, 1140850690, 2],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([1, 1140850690, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.232559 test begin: paddle.Tensor.sum(Tensor([1, 128, 128, 3, 46422],"float32"), list[2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([1, 128, 128, 3, 46422],"float32"), list[2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.242104 test begin: paddle.Tensor.sum(Tensor([1, 128, 128, 46422, 3],"float32"), list[2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([1, 128, 128, 46422, 3],"float32"), list[2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.251659 test begin: paddle.Tensor.sum(Tensor([1, 128, 1980644, 3, 3],"float32"), list[2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([1, 128, 1980644, 3, 3],"float32"), list[2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.261249 test begin: paddle.Tensor.sum(Tensor([1, 128, 256, 23211, 3],"float32"), list[2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([1, 128, 256, 23211, 3],"float32"), list[2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.270931 test begin: paddle.Tensor.sum(Tensor([1, 128, 256, 3, 23211],"float32"), list[2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([1, 128, 256, 3, 23211],"float32"), list[2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.281928 test begin: paddle.Tensor.sum(Tensor([1, 142606337, 4, 4, 1, 1],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 142606337, 4, 4, 1, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.291471 test begin: paddle.Tensor.sum(Tensor([1, 14861479, 17, 17],"float16"), )

[torch error] paddle.Tensor.sum(Tensor([1, 14861479, 17, 17],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.300933 test begin: paddle.Tensor.sum(Tensor([1, 190141782, 1, 3, 1, 4],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 190141782, 1, 3, 1, 4],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.310407 test begin: paddle.Tensor.sum(Tensor([1, 1980644, 128, 3, 3],"float32"), list[2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([1, 1980644, 128, 3, 3],"float32"), list[2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.319931 test begin: paddle.Tensor.sum(Tensor([1, 2, 1, 285212673, 1, 4],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 2, 1, 285212673, 1, 4],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.329434 test begin: paddle.Tensor.sum(Tensor([1, 2, 1, 3, 1, 380283564],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 2, 1, 3, 1, 380283564],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.338878 test begin: paddle.Tensor.sum(Tensor([1, 2, 1, 3, 95070891, 4],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 2, 1, 3, 95070891, 4],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.348319 test begin: paddle.Tensor.sum(Tensor([1, 2, 1140850690],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 2, 1140850690],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.358372 test begin: paddle.Tensor.sum(Tensor([1, 2, 1140850690],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([1, 2, 1140850690],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.367776 test begin: paddle.Tensor.sum(Tensor([1, 2, 1140850690],"float32"), -2, )

[torch error] paddle.Tensor.sum(Tensor([1, 2, 1140850690],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.377407 test begin: paddle.Tensor.sum(Tensor([1, 2, 2147483649],"float16"), -1, )

[torch error] paddle.Tensor.sum(Tensor([1, 2, 2147483649],"float16"), -1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.387111 test begin: paddle.Tensor.sum(Tensor([1, 2, 95070891, 3, 1, 4],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 2, 95070891, 3, 1, 4],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.396546 test begin: paddle.Tensor.sum(Tensor([1, 2147483649, 2],"float16"), -1, )

[torch error] paddle.Tensor.sum(Tensor([1, 2147483649, 2],"float16"), -1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.405885 test begin: paddle.Tensor.sum(Tensor([1, 21504, 106106],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 21504, 106106],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.415266 test begin: paddle.Tensor.sum(Tensor([1, 221848, 10285],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([1, 221848, 10285],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.425220 test begin: paddle.Tensor.sum(Tensor([1, 2281701379, 1, 1],"bool"), )

[torch error] paddle.Tensor.sum(Tensor([1, 2281701379, 1, 1],"bool"), ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.434632 test begin: paddle.Tensor.sum(Tensor([1, 2281701379, 1],"float32"), -2, )

[torch error] paddle.Tensor.sum(Tensor([1, 2281701379, 1],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.444317 test begin: paddle.Tensor.sum(Tensor([1, 2281701379],"bool"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([1, 2281701379],"bool"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.453627 test begin: paddle.Tensor.sum(Tensor([1, 2281701379],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 2281701379],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.463497 test begin: paddle.Tensor.sum(Tensor([1, 2281701379],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([1, 2281701379],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.472756 test begin: paddle.Tensor.sum(Tensor([1, 2281701379],"float32"), 1, )

[torch error] paddle.Tensor.sum(Tensor([1, 2281701379],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.482330 test begin: paddle.Tensor.sum(Tensor([1, 2281701379],"float32"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([1, 2281701379],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.492278 test begin: paddle.Tensor.sum(Tensor([1, 2281701379],"float32"), list[-1,], )

[torch error] paddle.Tensor.sum(Tensor([1, 2281701379],"float32"), list[-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.501927 test begin: paddle.Tensor.sum(Tensor([1, 2281701379],"int32"), 0, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([1, 2281701379],"int32"), 0, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.511356 test begin: paddle.Tensor.sum(Tensor([1, 228170138, 10],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 228170138, 10],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.520634 test begin: paddle.Tensor.sum(Tensor([1, 3, 190141782, 4, 1, 1],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 3, 190141782, 4, 1, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.529904 test begin: paddle.Tensor.sum(Tensor([1, 3, 4, 190141782, 1, 1],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 3, 4, 190141782, 1, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.539311 test begin: paddle.Tensor.sum(Tensor([1, 3, 4, 4, 1, 47535446],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 3, 4, 4, 1, 47535446],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.548919 test begin: paddle.Tensor.sum(Tensor([1, 3, 4, 4, 47535446, 1],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 3, 4, 4, 47535446, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.558315 test begin: paddle.Tensor.sum(Tensor([1, 4294967297, 1, 1],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([1, 4294967297, 1, 1],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.568148 test begin: paddle.Tensor.sum(Tensor([1, 4294967297, 1, 1],"float16"), 1, )

[torch error] paddle.Tensor.sum(Tensor([1, 4294967297, 1, 1],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.577668 test begin: paddle.Tensor.sum(Tensor([1, 4294967297, 1, 1],"float16"), 2, )

[torch error] paddle.Tensor.sum(Tensor([1, 4294967297, 1, 1],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.587793 test begin: paddle.Tensor.sum(Tensor([1, 4294967297, 1, 1],"float16"), 3, )

[torch error] paddle.Tensor.sum(Tensor([1, 4294967297, 1, 1],"float16"), 3, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.597111 test begin: paddle.Tensor.sum(Tensor([1, 4294967297],"float16"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([1, 4294967297],"float16"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.606515 test begin: paddle.Tensor.sum(Tensor([1, 4294967297],"float16"), list[-1,], )

[torch error] paddle.Tensor.sum(Tensor([1, 4294967297],"float16"), list[-1,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.616201 test begin: paddle.Tensor.sum(Tensor([1, 45634028, 25, 2],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([1, 45634028, 25, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.625831 test begin: paddle.Tensor.sum(Tensor([1, 570425345, 2, 2],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([1, 570425345, 2, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.635657 test begin: paddle.Tensor.sum(Tensor([1, 6449, 13, 27216],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([1, 6449, 13, 27216],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.647138 test begin: paddle.Tensor.sum(Tensor([1, 65536, 17409, 2],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([1, 65536, 17409, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.656954 test begin: paddle.Tensor.sum(Tensor([1, 65536, 25, 1393],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([1, 65536, 25, 1393],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.666456 test begin: paddle.Tensor.sum(Tensor([1, 7231, 13, 24276],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([1, 7231, 13, 24276],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.676255 test begin: paddle.Tensor.sum(Tensor([1, 760567127, 3],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 760567127, 3],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.685771 test begin: paddle.Tensor.sum(Tensor([1, 83837, 27216],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([1, 83837, 27216],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.695357 test begin: paddle.Tensor.sum(Tensor([1, 858993460, 1, 5],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([1, 858993460, 1, 5],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.705191 test begin: paddle.Tensor.sum(Tensor([1, 858993460, 1, 5],"float16"), 1, )

[torch error] paddle.Tensor.sum(Tensor([1, 858993460, 1, 5],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.714468 test begin: paddle.Tensor.sum(Tensor([1, 858993460, 1, 5],"float16"), 2, )

[torch error] paddle.Tensor.sum(Tensor([1, 858993460, 1, 5],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.723771 test begin: paddle.Tensor.sum(Tensor([1, 858993460, 1, 5],"float16"), 3, )

[torch error] paddle.Tensor.sum(Tensor([1, 858993460, 1, 5],"float16"), 3, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.732915 test begin: paddle.Tensor.sum(Tensor([1, 93991, 24276],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([1, 93991, 24276],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.742951 test begin: paddle.Tensor.sum(Tensor([1, 93991, 24276],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([1, 93991, 24276],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.752517 test begin: paddle.Tensor.sum(Tensor([1, 990322, 256, 3, 3],"float32"), list[2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([1, 990322, 256, 3, 3],"float32"), list[2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.762133 test begin: paddle.Tensor.sum(Tensor([10, 114085069, 1, 2],"float32"), list[0,-1,], )

[torch error] paddle.Tensor.sum(Tensor([10, 114085069, 1, 2],"float32"), list[0,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.771800 test begin: paddle.Tensor.sum(Tensor([10, 114085069, 2],"float32"), list[0,-1,], )

[torch error] paddle.Tensor.sum(Tensor([10, 114085069, 2],"float32"), list[0,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.781188 test begin: paddle.Tensor.sum(Tensor([10, 143165577, 3],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([10, 143165577, 3],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.791237 test begin: paddle.Tensor.sum(Tensor([10, 2, 1, 114085069],"float32"), list[0,-1,], )

[torch error] paddle.Tensor.sum(Tensor([10, 2, 1, 114085069],"float32"), list[0,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.800744 test begin: paddle.Tensor.sum(Tensor([10, 2, 114085069],"float32"), list[0,-1,], )

[torch error] paddle.Tensor.sum(Tensor([10, 2, 114085069],"float32"), list[0,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.810346 test begin: paddle.Tensor.sum(Tensor([10, 2, 214748365],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([10, 2, 214748365],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.819790 test begin: paddle.Tensor.sum(Tensor([10, 2, 57042535, 2],"float32"), list[0,-1,], )

[torch error] paddle.Tensor.sum(Tensor([10, 2, 57042535, 2],"float32"), list[0,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.829343 test begin: paddle.Tensor.sum(Tensor([10, 228170138],"float32"), 0, )

[torch error] paddle.Tensor.sum(Tensor([10, 228170138],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.840593 test begin: paddle.Tensor.sum(Tensor([10, 228170138],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([10, 228170138],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.850628 test begin: paddle.Tensor.sum(Tensor([10, 228170138],"float32"), list[0,-1,], )

[torch error] paddle.Tensor.sum(Tensor([10, 228170138],"float32"), list[0,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.860192 test begin: paddle.Tensor.sum(Tensor([10, 28521268, 8],"float32"), axis=2, )

[torch error] paddle.Tensor.sum(Tensor([10, 28521268, 8],"float32"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.869961 test begin: paddle.Tensor.sum(Tensor([10, 429496730],"float16"), -1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([10, 429496730],"float16"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.880135 test begin: paddle.Tensor.sum(Tensor([10, 429496730],"float16"), list[0,-1,], )

[torch error] paddle.Tensor.sum(Tensor([10, 429496730],"float16"), list[0,-1,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.889822 test begin: paddle.Tensor.sum(Tensor([10, 500, 456341],"float32"), axis=2, )

[torch error] paddle.Tensor.sum(Tensor([10, 500, 456341],"float32"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.899398 test begin: paddle.Tensor.sum(Tensor([10, 5000, 85900],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([10, 5000, 85900],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.909375 test begin: paddle.Tensor.sum(Tensor([10, 57042535, 4],"float32"), axis=2, )

[torch error] paddle.Tensor.sum(Tensor([10, 57042535, 4],"float32"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.918917 test begin: paddle.Tensor.sum(Tensor([100, 42949673],"float16"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([100, 42949673],"float16"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.928268 test begin: paddle.Tensor.sum(Tensor([1000, 1140851, 2],"float32"), list[0,1,], )

[torch error] paddle.Tensor.sum(Tensor([1000, 1140851, 2],"float32"), list[0,1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.939347 test begin: paddle.Tensor.sum(Tensor([1000, 2, 1140851],"float32"), list[0,1,], )

[torch error] paddle.Tensor.sum(Tensor([1000, 2, 1140851],"float32"), list[0,1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.948958 test begin: paddle.Tensor.sum(Tensor([1000, 2, 2, 570426],"float32"), list[0,1,], )

[torch error] paddle.Tensor.sum(Tensor([1000, 2, 2, 570426],"float32"), list[0,1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.958875 test begin: paddle.Tensor.sum(Tensor([1000, 2, 380284, 3],"float32"), list[0,1,], )

[torch error] paddle.Tensor.sum(Tensor([1000, 2, 380284, 3],"float32"), list[0,1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.968313 test begin: paddle.Tensor.sum(Tensor([1000, 380284, 2, 3],"float32"), list[0,1,], )

[torch error] paddle.Tensor.sum(Tensor([1000, 380284, 2, 3],"float32"), list[0,1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.977703 test begin: paddle.Tensor.sum(Tensor([101862, 22400],"float32"), axis=1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([101862, 22400],"float32"), axis=1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.987764 test begin: paddle.Tensor.sum(Tensor([10611, 21504, 10],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([10611, 21504, 10],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:19.997232 test begin: paddle.Tensor.sum(Tensor([1073741825, 2, 2],"float16"), -1, )

[torch error] paddle.Tensor.sum(Tensor([1073741825, 2, 2],"float16"), -1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.006738 test begin: paddle.Tensor.sum(Tensor([1073741825, 4],"float16"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([1073741825, 4],"float16"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.016184 test begin: paddle.Tensor.sum(Tensor([107374183, 40],"float16"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([107374183, 40],"float16"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.025654 test begin: paddle.Tensor.sum(Tensor([1140850690, 1, 2, 1, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([1140850690, 1, 2, 1, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.035103 test begin: paddle.Tensor.sum(Tensor([1140850690, 1, 2, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([1140850690, 1, 2, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.044622 test begin: paddle.Tensor.sum(Tensor([1140850690, 2, 1],"float32"), -2, )

[torch error] paddle.Tensor.sum(Tensor([1140850690, 2, 1],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.054017 test begin: paddle.Tensor.sum(Tensor([1140850690, 2, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([1140850690, 2, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.063357 test begin: paddle.Tensor.sum(Tensor([1140850690, 2],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([1140850690, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.072608 test begin: paddle.Tensor.sum(Tensor([1140850690, 2],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([1140850690, 2],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.081984 test begin: paddle.Tensor.sum(Tensor([1140850690, 2],"float32"), list[-1,], )

[torch error] paddle.Tensor.sum(Tensor([1140850690, 2],"float32"), list[-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.091403 test begin: paddle.Tensor.sum(Tensor([1140850690, 2],"float32"), list[-2,-1,], )

[torch error] paddle.Tensor.sum(Tensor([1140850690, 2],"float32"), list[-2,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.104950 test begin: paddle.Tensor.sum(Tensor([1140851, 500, 4],"float32"), axis=2, )

[torch error] paddle.Tensor.sum(Tensor([1140851, 500, 4],"float32"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.115236 test begin: paddle.Tensor.sum(Tensor([11883862, 3, 8, 8],"float32"), axis=-3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([11883862, 3, 8, 8],"float32"), axis=-3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.125062 test begin: paddle.Tensor.sum(Tensor([12, 190141782],"float32"), axis=0, )

[torch error] paddle.Tensor.sum(Tensor([12, 190141782],"float32"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.134647 test begin: paddle.Tensor.sum(Tensor([12, 1901418, 10, 10, 1],"float32"), list[1,2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([12, 1901418, 10, 10, 1],"float32"), list[1,2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.144255 test begin: paddle.Tensor.sum(Tensor([12, 23768, 10, 10, 80],"float32"), list[1,2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([12, 23768, 10, 10, 80],"float32"), list[1,2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.153729 test begin: paddle.Tensor.sum(Tensor([12, 3, 10, 10, 633806],"float32"), list[1,2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([12, 3, 10, 10, 633806],"float32"), list[1,2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.163646 test begin: paddle.Tensor.sum(Tensor([12, 3, 10, 6338060, 1],"float32"), list[1,2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([12, 3, 10, 6338060, 1],"float32"), list[1,2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.173080 test begin: paddle.Tensor.sum(Tensor([12, 3, 10, 79226, 80],"float32"), list[1,2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([12, 3, 10, 79226, 80],"float32"), list[1,2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.182592 test begin: paddle.Tensor.sum(Tensor([12, 3, 6338060, 10, 1],"float32"), list[1,2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([12, 3, 6338060, 10, 1],"float32"), list[1,2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.192142 test begin: paddle.Tensor.sum(Tensor([12, 3, 79226, 10, 80],"float32"), list[1,2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([12, 3, 79226, 10, 80],"float32"), list[1,2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.202326 test begin: paddle.Tensor.sum(Tensor([12, 357913942],"float16"), axis=0, )

[torch error] paddle.Tensor.sum(Tensor([12, 357913942],"float16"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.211812 test begin: paddle.Tensor.sum(Tensor([126, 369567, 7, 7],"float32"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([126, 369567, 7, 7],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.221421 test begin: paddle.Tensor.sum(Tensor([126, 8, 323371, 7],"float32"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([126, 8, 323371, 7],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.230819 test begin: paddle.Tensor.sum(Tensor([126, 8, 7, 323371],"float32"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([126, 8, 7, 323371],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.240225 test begin: paddle.Tensor.sum(Tensor([128, 17825793, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([128, 17825793, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.250070 test begin: paddle.Tensor.sum(Tensor([128, 2, 8912897],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([128, 2, 8912897],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.263150 test begin: paddle.Tensor.sum(Tensor([1393, 65536, 25],"float32"), 2, )

[torch error] paddle.Tensor.sum(Tensor([1393, 65536, 25],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.276362 test begin: paddle.Tensor.sum(Tensor([14, 12782641, 4, 2, 3],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([14, 12782641, 4, 2, 3],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.285812 test begin: paddle.Tensor.sum(Tensor([14, 2, 25565282, 2, 3],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([14, 2, 25565282, 2, 3],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.295169 test begin: paddle.Tensor.sum(Tensor([14, 2, 4, 12782641, 3],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([14, 2, 4, 12782641, 3],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.304391 test begin: paddle.Tensor.sum(Tensor([14, 2, 4, 2, 19173962],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([14, 2, 4, 2, 19173962],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.313692 test begin: paddle.Tensor.sum(Tensor([1431655766, 3],"float16"), list[0,-1,], )

[torch error] paddle.Tensor.sum(Tensor([1431655766, 3],"float16"), list[0,-1,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.323067 test begin: paddle.Tensor.sum(Tensor([14449, 157920],"bool"), 1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([14449, 157920],"bool"), 1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.333212 test begin: paddle.Tensor.sum(Tensor([14861479, 1, 17, 17],"float16"), )

[torch error] paddle.Tensor.sum(Tensor([14861479, 1, 17, 17],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.343561 test begin: paddle.Tensor.sum(Tensor([15474, 128, 128, 3, 3],"float32"), list[2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([15474, 128, 128, 3, 3],"float32"), list[2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.353306 test begin: paddle.Tensor.sum(Tensor([16, 10, 111412, 64, 2],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([16, 10, 111412, 64, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.362805 test begin: paddle.Tensor.sum(Tensor([16, 10, 122937, 58, 2],"float32"), axis=tuple(2,3,), )

[torch error] paddle.Tensor.sum(Tensor([16, 10, 122937, 58, 2],"float32"), axis=tuple(2,3,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.372286 test begin: paddle.Tensor.sum(Tensor([16, 10, 14260634],"float32"), 2, )

[torch error] paddle.Tensor.sum(Tensor([16, 10, 14260634],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.382012 test begin: paddle.Tensor.sum(Tensor([16, 10, 4, 3565159],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([16, 10, 4, 3565159],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.391442 test begin: paddle.Tensor.sum(Tensor([16, 10, 4240, 3364],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([16, 10, 4240, 3364],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.400796 test begin: paddle.Tensor.sum(Tensor([16, 10, 58, 122937, 2],"float32"), axis=tuple(2,3,), )

[torch error] paddle.Tensor.sum(Tensor([16, 10, 58, 122937, 2],"float32"), axis=tuple(2,3,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.410251 test begin: paddle.Tensor.sum(Tensor([16, 10, 58, 58, 4240],"float32"), axis=tuple(2,3,), )

[torch error] paddle.Tensor.sum(Tensor([16, 10, 58, 58, 4240],"float32"), axis=tuple(2,3,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.420590 test begin: paddle.Tensor.sum(Tensor([16, 10, 64, 111412, 2],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([16, 10, 64, 111412, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.430083 test begin: paddle.Tensor.sum(Tensor([16, 10, 64, 64, 3482],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([16, 10, 64, 64, 3482],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.439643 test begin: paddle.Tensor.sum(Tensor([16, 10598, 4, 3364],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([16, 10598, 4, 3364],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.449153 test begin: paddle.Tensor.sum(Tensor([16, 11, 2, 101283, 64],"float32"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([16, 11, 2, 101283, 64],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.458935 test begin: paddle.Tensor.sum(Tensor([16, 11, 2, 64, 101283],"float32"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([16, 11, 2, 64, 101283],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.468560 test begin: paddle.Tensor.sum(Tensor([16, 11, 3166, 64, 64],"float32"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([16, 11, 3166, 64, 64],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.478002 test begin: paddle.Tensor.sum(Tensor([16, 17409, 2, 64, 64],"float32"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([16, 17409, 2, 64, 64],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.487455 test begin: paddle.Tensor.sum(Tensor([16, 17409, 64, 64, 2],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([16, 17409, 64, 64, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.496898 test begin: paddle.Tensor.sum(Tensor([16, 21196, 58, 58, 2],"float32"), axis=tuple(2,3,), )

[torch error] paddle.Tensor.sum(Tensor([16, 21196, 58, 58, 2],"float32"), axis=tuple(2,3,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.506481 test begin: paddle.Tensor.sum(Tensor([16, 5704254, 25],"float32"), 2, )

[torch error] paddle.Tensor.sum(Tensor([16, 5704254, 25],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.516062 test begin: paddle.Tensor.sum(Tensor([16, 65536, 2177],"float32"), 2, )

[torch error] paddle.Tensor.sum(Tensor([16, 65536, 2177],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.525684 test begin: paddle.Tensor.sum(Tensor([16520, 138120],"int32"), 0, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([16520, 138120],"int32"), 0, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.535802 test begin: paddle.Tensor.sum(Tensor([16957, 10, 4, 3364],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([16957, 10, 4, 3364],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.545841 test begin: paddle.Tensor.sum(Tensor([17409, 4, 4, 4, 4, 4, 128],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([17409, 4, 4, 4, 4, 4, 128],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.555505 test begin: paddle.Tensor.sum(Tensor([17825793, 128],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([17825793, 128],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.565199 test begin: paddle.Tensor.sum(Tensor([17825793, 128],"float32"), axis=0, )

[torch error] paddle.Tensor.sum(Tensor([17825793, 128],"float32"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.574880 test begin: paddle.Tensor.sum(Tensor([181896, 12544],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([181896, 12544],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.584589 test begin: paddle.Tensor.sum(Tensor([181896, 12544],"float32"), 1, )

[torch error] paddle.Tensor.sum(Tensor([181896, 12544],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.594561 test begin: paddle.Tensor.sum(Tensor([18416, 123904],"float32"), 1, )

[torch error] paddle.Tensor.sum(Tensor([18416, 123904],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.603990 test begin: paddle.Tensor.sum(Tensor([190141782, 12],"bool"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([190141782, 12],"bool"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.613361 test begin: paddle.Tensor.sum(Tensor([190141782, 2, 2, 3],"float32"), list[0,1,], )

[torch error] paddle.Tensor.sum(Tensor([190141782, 2, 2, 3],"float32"), list[0,1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.622759 test begin: paddle.Tensor.sum(Tensor([190141782, 3, 1, 4, 1],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([190141782, 3, 1, 4, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.632957 test begin: paddle.Tensor.sum(Tensor([190141782, 3, 4],"bool"), axis=0, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([190141782, 3, 4],"bool"), axis=0, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.642587 test begin: paddle.Tensor.sum(Tensor([190141782, 3, 4],"bool"), axis=1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([190141782, 3, 4],"bool"), axis=1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.652217 test begin: paddle.Tensor.sum(Tensor([19014179, 2, 1, 2, 1, 1, 2, 3, 5],"bool"), 2, )

[torch error] paddle.Tensor.sum(Tensor([19014179, 2, 1, 2, 1, 1, 2, 3, 5],"bool"), 2, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.661786 test begin: paddle.Tensor.sum(Tensor([2, 1, 1140850690, 1, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([2, 1, 1140850690, 1, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.671314 test begin: paddle.Tensor.sum(Tensor([2, 1, 2, 1, 570425345],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([2, 1, 2, 1, 570425345],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.680696 test begin: paddle.Tensor.sum(Tensor([2, 1, 2, 570425345, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([2, 1, 2, 570425345, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.689984 test begin: paddle.Tensor.sum(Tensor([2, 1140850690],"bool"), axis=0, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([2, 1140850690],"bool"), axis=0, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.699320 test begin: paddle.Tensor.sum(Tensor([2, 1140850690],"bool"), axis=1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([2, 1140850690],"bool"), axis=1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.708693 test begin: paddle.Tensor.sum(Tensor([2, 1140850690],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([2, 1140850690],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.717966 test begin: paddle.Tensor.sum(Tensor([2, 1140850690],"float32"), 1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([2, 1140850690],"float32"), 1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.727334 test begin: paddle.Tensor.sum(Tensor([2, 1140850690],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([2, 1140850690],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.736586 test begin: paddle.Tensor.sum(Tensor([2, 1140850690],"float32"), list[-2,-1,], )

[torch error] paddle.Tensor.sum(Tensor([2, 1140850690],"float32"), list[-2,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.746764 test begin: paddle.Tensor.sum(Tensor([2, 17825793, 8, 8],"float32"), axis=-3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([2, 17825793, 8, 8],"float32"), axis=-3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.757426 test begin: paddle.Tensor.sum(Tensor([2, 19014179, 1, 2, 1, 1, 2, 3, 5],"bool"), 2, )

[torch error] paddle.Tensor.sum(Tensor([2, 19014179, 1, 2, 1, 1, 2, 3, 5],"bool"), 2, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.766847 test begin: paddle.Tensor.sum(Tensor([2, 2, 1, 19014179, 1, 1, 2, 3, 5],"bool"), 2, )

[torch error] paddle.Tensor.sum(Tensor([2, 2, 1, 19014179, 1, 1, 2, 3, 5],"bool"), 2, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.776200 test begin: paddle.Tensor.sum(Tensor([2, 2, 1, 2, 1, 1, 19014179, 3, 5],"bool"), 2, )

[torch error] paddle.Tensor.sum(Tensor([2, 2, 1, 2, 1, 1, 19014179, 3, 5],"bool"), 2, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.785540 test begin: paddle.Tensor.sum(Tensor([2, 2, 1, 2, 1, 1, 2, 28521268, 5],"bool"), 2, )

[torch error] paddle.Tensor.sum(Tensor([2, 2, 1, 2, 1, 1, 2, 28521268, 5],"bool"), 2, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.794828 test begin: paddle.Tensor.sum(Tensor([2, 2, 1, 2, 1, 1, 2, 3, 47535446],"bool"), 2, )

[torch error] paddle.Tensor.sum(Tensor([2, 2, 1, 2, 1, 1, 2, 3, 47535446],"bool"), 2, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.804200 test begin: paddle.Tensor.sum(Tensor([2, 2, 1, 2, 1, 9507090, 2, 3, 5],"bool"), 2, )

[torch error] paddle.Tensor.sum(Tensor([2, 2, 1, 2, 1, 9507090, 2, 3, 5],"bool"), 2, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.813491 test begin: paddle.Tensor.sum(Tensor([2, 2, 1, 2, 9507090, 1, 2, 3, 5],"bool"), 2, )

[torch error] paddle.Tensor.sum(Tensor([2, 2, 1, 2, 9507090, 1, 2, 3, 5],"bool"), 2, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.822878 test begin: paddle.Tensor.sum(Tensor([2, 2, 9507090, 2, 1, 1, 2, 3, 5],"bool"), 2, )

[torch error] paddle.Tensor.sum(Tensor([2, 2, 9507090, 2, 1, 1, 2, 3, 5],"bool"), 2, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.832253 test begin: paddle.Tensor.sum(Tensor([2, 285212673, 1, 4, 1],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([2, 285212673, 1, 4, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.841689 test begin: paddle.Tensor.sum(Tensor([2, 285212673, 4],"bool"), axis=0, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([2, 285212673, 4],"bool"), axis=0, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.851215 test begin: paddle.Tensor.sum(Tensor([2, 3, 1, 380283564, 1],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([2, 3, 1, 380283564, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.860605 test begin: paddle.Tensor.sum(Tensor([2, 3, 1, 4, 95070891],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([2, 3, 1, 4, 95070891],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.870161 test begin: paddle.Tensor.sum(Tensor([2, 3, 380283564],"bool"), axis=0, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([2, 3, 380283564],"bool"), axis=0, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.879638 test begin: paddle.Tensor.sum(Tensor([2, 3, 47535446, 8],"float32"), axis=-3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([2, 3, 47535446, 8],"float32"), axis=-3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.889016 test begin: paddle.Tensor.sum(Tensor([2, 3, 715827883],"float16"), list[-2,-1,], )

[torch error] paddle.Tensor.sum(Tensor([2, 3, 715827883],"float16"), list[-2,-1,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.898367 test begin: paddle.Tensor.sum(Tensor([2, 3, 8, 47535446],"float32"), axis=-3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([2, 3, 8, 47535446],"float32"), axis=-3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.907701 test begin: paddle.Tensor.sum(Tensor([2, 3, 95070891, 4, 1],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([2, 3, 95070891, 4, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.916909 test begin: paddle.Tensor.sum(Tensor([2, 570425345, 2, 1, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([2, 570425345, 2, 1, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.926261 test begin: paddle.Tensor.sum(Tensor([2, 715827883, 3],"float16"), list[-2,-1,], )

[torch error] paddle.Tensor.sum(Tensor([2, 715827883, 3],"float16"), list[-2,-1,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.935660 test begin: paddle.Tensor.sum(Tensor([207427399, 11],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([207427399, 11],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.945824 test begin: paddle.Tensor.sum(Tensor([207427399, 11],"float32"), 0, )

[torch error] paddle.Tensor.sum(Tensor([207427399, 11],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.955354 test begin: paddle.Tensor.sum(Tensor([21298, 107136],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([21298, 107136],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.967885 test begin: paddle.Tensor.sum(Tensor([21298, 107136],"float32"), 1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([21298, 107136],"float32"), 1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.977461 test begin: paddle.Tensor.sum(Tensor([2147483649, 2],"float16"), list[-1,], )

[torch error] paddle.Tensor.sum(Tensor([2147483649, 2],"float16"), list[-1,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.986932 test begin: paddle.Tensor.sum(Tensor([214748365, 20],"float16"), -1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([214748365, 20],"float16"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:20.996399 test begin: paddle.Tensor.sum(Tensor([214748365, 4, 5],"float16"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([214748365, 4, 5],"float16"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.005802 test begin: paddle.Tensor.sum(Tensor([21474837, 10, 20],"float16"), list[0,-1,], )

[torch error] paddle.Tensor.sum(Tensor([21474837, 10, 20],"float16"), list[0,-1,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.015269 test begin: paddle.Tensor.sum(Tensor([21559, 105840],"int32"), 0, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([21559, 105840],"int32"), 0, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.025013 test begin: paddle.Tensor.sum(Tensor([221848, 1, 10285],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([221848, 1, 10285],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.034371 test begin: paddle.Tensor.sum(Tensor([2281701379, 1, 1, 1],"bool"), )

[torch error] paddle.Tensor.sum(Tensor([2281701379, 1, 1, 1],"bool"), ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.043744 test begin: paddle.Tensor.sum(Tensor([2281701379, 1],"float32"), list[-2,-1,], )

[torch error] paddle.Tensor.sum(Tensor([2281701379, 1],"float32"), list[-2,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.053106 test begin: paddle.Tensor.sum(Tensor([2281701379],"bool"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([2281701379],"bool"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.062472 test begin: paddle.Tensor.sum(Tensor([2281701379],"bool"), axis=0, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([2281701379],"bool"), axis=0, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.071841 test begin: paddle.Tensor.sum(Tensor([2281701379],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([2281701379],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.081203 test begin: paddle.Tensor.sum(Tensor([2281701379],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([2281701379],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.090563 test begin: paddle.Tensor.sum(Tensor([2281701379],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([2281701379],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.100064 test begin: paddle.Tensor.sum(Tensor([2281701379],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([2281701379],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.110209 test begin: paddle.Tensor.sum(Tensor([2281701379],"float32"), list[-1,], )

[torch error] paddle.Tensor.sum(Tensor([2281701379],"float32"), list[-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.119724 test begin: paddle.Tensor.sum(Tensor([228170138, 10],"float32"), 0, )

[torch error] paddle.Tensor.sum(Tensor([228170138, 10],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.129380 test begin: paddle.Tensor.sum(Tensor([228170138, 10],"float32"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([228170138, 10],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.138915 test begin: paddle.Tensor.sum(Tensor([23283, 280, 350],"float32"), axis=-3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([23283, 280, 350],"float32"), axis=-3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.148471 test begin: paddle.Tensor.sum(Tensor([25321, 11, 2, 64, 64],"float32"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([25321, 11, 2, 64, 64],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.157960 test begin: paddle.Tensor.sum(Tensor([253522376, 3, 3],"bool"), axis=0, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([253522376, 3, 3],"bool"), axis=0, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.167492 test begin: paddle.Tensor.sum(Tensor([27853, 10, 64, 64, 2],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([27853, 10, 64, 64, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.176980 test begin: paddle.Tensor.sum(Tensor([286331154, 3, 5],"float16"), 1, )

[torch error] paddle.Tensor.sum(Tensor([286331154, 3, 5],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.186395 test begin: paddle.Tensor.sum(Tensor([286332, 5000, 3],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([286332, 5000, 3],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.195859 test begin: paddle.Tensor.sum(Tensor([2910334, 28, 28],"float32"), axis=-3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([2910334, 28, 28],"float32"), axis=-3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.205322 test begin: paddle.Tensor.sum(Tensor([3, 1, 2, 1, 380283564],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([3, 1, 2, 1, 380283564],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.215376 test begin: paddle.Tensor.sum(Tensor([3, 1, 2, 380283564, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([3, 1, 2, 380283564, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.225472 test begin: paddle.Tensor.sum(Tensor([3, 1, 2, 380283564],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([3, 1, 2, 380283564],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.235598 test begin: paddle.Tensor.sum(Tensor([3, 1, 760567127, 1, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([3, 1, 760567127, 1, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.245163 test begin: paddle.Tensor.sum(Tensor([3, 1, 760567127, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([3, 1, 760567127, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.254699 test begin: paddle.Tensor.sum(Tensor([3, 2173049, 350],"float32"), axis=-3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 2173049, 350],"float32"), axis=-3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.264252 test begin: paddle.Tensor.sum(Tensor([3, 253522376, 3],"bool"), axis=0, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 253522376, 3],"bool"), axis=0, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.273747 test begin: paddle.Tensor.sum(Tensor([3, 27163112, 28],"float32"), axis=-3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 27163112, 28],"float32"), axis=-3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.283293 test begin: paddle.Tensor.sum(Tensor([3, 28, 27163112],"float32"), axis=-3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 28, 27163112],"float32"), axis=-3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.293289 test begin: paddle.Tensor.sum(Tensor([3, 280, 2716312],"float32"), axis=-3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 280, 2716312],"float32"), axis=-3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.305912 test begin: paddle.Tensor.sum(Tensor([3, 286331154, 5],"float16"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([3, 286331154, 5],"float16"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.315921 test begin: paddle.Tensor.sum(Tensor([3, 3, 253522376],"bool"), axis=0, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 3, 253522376],"bool"), axis=0, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.325530 test begin: paddle.Tensor.sum(Tensor([3, 3, 3, 84507459],"float32"), list[1,2,3,], )

[torch error] paddle.Tensor.sum(Tensor([3, 3, 3, 84507459],"float32"), list[1,2,3,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.335254 test begin: paddle.Tensor.sum(Tensor([3, 3, 84507459, 3],"float32"), list[1,2,3,], )

[torch error] paddle.Tensor.sum(Tensor([3, 3, 84507459, 3],"float32"), list[1,2,3,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.345448 test begin: paddle.Tensor.sum(Tensor([3, 380283564, 2, 1, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([3, 380283564, 2, 1, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.355414 test begin: paddle.Tensor.sum(Tensor([3, 380283564, 2, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([3, 380283564, 2, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.365039 test begin: paddle.Tensor.sum(Tensor([3, 4, 357913942],"float16"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([3, 4, 357913942],"float16"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.374527 test begin: paddle.Tensor.sum(Tensor([3, 6, 3, 4, 2, 5281717],"bool"), axis=3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 6, 3, 4, 2, 5281717],"bool"), axis=3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.384065 test begin: paddle.Tensor.sum(Tensor([3, 6, 3, 4, 2, 5281717],"bool"), axis=4, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 6, 3, 4, 2, 5281717],"bool"), axis=4, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.393968 test begin: paddle.Tensor.sum(Tensor([3, 6, 3, 4, 2, 5281717],"bool"), axis=5, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 6, 3, 4, 2, 5281717],"bool"), axis=5, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.403386 test begin: paddle.Tensor.sum(Tensor([3, 6, 3, 4, 2112687, 5],"bool"), axis=3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 6, 3, 4, 2112687, 5],"bool"), axis=3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.412713 test begin: paddle.Tensor.sum(Tensor([3, 6, 3, 4, 2112687, 5],"bool"), axis=4, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 6, 3, 4, 2112687, 5],"bool"), axis=4, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.422130 test begin: paddle.Tensor.sum(Tensor([3, 6, 3, 4, 2112687, 5],"bool"), axis=5, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 6, 3, 4, 2112687, 5],"bool"), axis=5, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.431762 test begin: paddle.Tensor.sum(Tensor([3, 6, 3, 4225373, 2, 5],"bool"), axis=3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 6, 3, 4225373, 2, 5],"bool"), axis=3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.441250 test begin: paddle.Tensor.sum(Tensor([3, 6, 3, 4225373, 2, 5],"bool"), axis=4, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 6, 3, 4225373, 2, 5],"bool"), axis=4, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.450798 test begin: paddle.Tensor.sum(Tensor([3, 6, 3, 4225373, 2, 5],"bool"), axis=5, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 6, 3, 4225373, 2, 5],"bool"), axis=5, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.460316 test begin: paddle.Tensor.sum(Tensor([3, 6, 3169030, 4, 2, 5],"bool"), axis=3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 6, 3169030, 4, 2, 5],"bool"), axis=3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.469691 test begin: paddle.Tensor.sum(Tensor([3, 6, 3169030, 4, 2, 5],"bool"), axis=4, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 6, 3169030, 4, 2, 5],"bool"), axis=4, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.479607 test begin: paddle.Tensor.sum(Tensor([3, 6, 3169030, 4, 2, 5],"bool"), axis=5, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 6, 3169030, 4, 2, 5],"bool"), axis=5, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.489999 test begin: paddle.Tensor.sum(Tensor([3, 6338060, 3, 4, 2, 5],"bool"), axis=3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 6338060, 3, 4, 2, 5],"bool"), axis=3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.500723 test begin: paddle.Tensor.sum(Tensor([3, 6338060, 3, 4, 2, 5],"bool"), axis=4, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 6338060, 3, 4, 2, 5],"bool"), axis=4, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.511126 test begin: paddle.Tensor.sum(Tensor([3, 6338060, 3, 4, 2, 5],"bool"), axis=5, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 6338060, 3, 4, 2, 5],"bool"), axis=5, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.522641 test begin: paddle.Tensor.sum(Tensor([3, 84507459, 3, 3],"float32"), list[1,2,3,], )

[torch error] paddle.Tensor.sum(Tensor([3, 84507459, 3, 3],"float32"), list[1,2,3,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.532694 test begin: paddle.Tensor.sum(Tensor([3169030, 6, 3, 4, 2, 5],"bool"), axis=3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3169030, 6, 3, 4, 2, 5],"bool"), axis=3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.542140 test begin: paddle.Tensor.sum(Tensor([3169030, 6, 3, 4, 2, 5],"bool"), axis=4, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3169030, 6, 3, 4, 2, 5],"bool"), axis=4, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.551584 test begin: paddle.Tensor.sum(Tensor([3169030, 6, 3, 4, 2, 5],"bool"), axis=5, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3169030, 6, 3, 4, 2, 5],"bool"), axis=5, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.561112 test begin: paddle.Tensor.sum(Tensor([33554433, 128],"float16"), axis=0, )

[torch error] paddle.Tensor.sum(Tensor([33554433, 128],"float16"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.571218 test begin: paddle.Tensor.sum(Tensor([33914, 10, 58, 58, 2],"float32"), axis=tuple(2,3,), )

[torch error] paddle.Tensor.sum(Tensor([33914, 10, 58, 58, 2],"float32"), axis=tuple(2,3,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.580587 test begin: paddle.Tensor.sum(Tensor([380283564, 2, 3],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([380283564, 2, 3],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.589805 test begin: paddle.Tensor.sum(Tensor([4, 214748365, 5],"float16"), 1, )

[torch error] paddle.Tensor.sum(Tensor([4, 214748365, 5],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.599035 test begin: paddle.Tensor.sum(Tensor([4, 3, 357913942],"float16"), 1, )

[torch error] paddle.Tensor.sum(Tensor([4, 3, 357913942],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.608284 test begin: paddle.Tensor.sum(Tensor([4, 570425345],"bool"), 1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([4, 570425345],"bool"), 1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.625321 test begin: paddle.Tensor.sum(Tensor([4, 570425345],"float32"), axis=1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([4, 570425345],"float32"), axis=1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.635841 test begin: paddle.Tensor.sum(Tensor([4, 7, 81489335],"bool"), axis=1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([4, 7, 81489335],"bool"), axis=1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.647118 test begin: paddle.Tensor.sum(Tensor([4, 7, 81489335],"bool"), axis=2, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([4, 7, 81489335],"bool"), axis=2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.658920 test begin: paddle.Tensor.sum(Tensor([4, 95070891, 6],"bool"), axis=1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([4, 95070891, 6],"bool"), axis=1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.668994 test begin: paddle.Tensor.sum(Tensor([4, 95070891, 6],"bool"), axis=2, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([4, 95070891, 6],"bool"), axis=2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.679614 test begin: paddle.Tensor.sum(Tensor([4294967297, 1, 1, 1],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([4294967297, 1, 1, 1],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.689605 test begin: paddle.Tensor.sum(Tensor([4294967297, 1, 1, 1],"float16"), 1, )

[torch error] paddle.Tensor.sum(Tensor([4294967297, 1, 1, 1],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.699489 test begin: paddle.Tensor.sum(Tensor([4294967297, 1, 1, 1],"float16"), 2, )

[torch error] paddle.Tensor.sum(Tensor([4294967297, 1, 1, 1],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.710182 test begin: paddle.Tensor.sum(Tensor([4294967297, 1, 1, 1],"float16"), 3, )

[torch error] paddle.Tensor.sum(Tensor([4294967297, 1, 1, 1],"float16"), 3, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.720640 test begin: paddle.Tensor.sum(Tensor([4294967297],"float16"), -1, )

[torch error] paddle.Tensor.sum(Tensor([4294967297],"float16"), -1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.730988 test begin: paddle.Tensor.sum(Tensor([4294967297],"float16"), -1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([4294967297],"float16"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.740752 test begin: paddle.Tensor.sum(Tensor([4294967297],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([4294967297],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.750525 test begin: paddle.Tensor.sum(Tensor([429496730, 10],"float16"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([429496730, 10],"float16"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.761511 test begin: paddle.Tensor.sum(Tensor([47535446, 3, 4, 4, 1, 1],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([47535446, 3, 4, 4, 1, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.771526 test begin: paddle.Tensor.sum(Tensor([477218589, 3, 3],"float16"), list[-2,-1,], )

[torch error] paddle.Tensor.sum(Tensor([477218589, 3, 3],"float16"), list[-2,-1,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.781707 test begin: paddle.Tensor.sum(Tensor([5, 114085069, 4],"bool"), axis=1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([5, 114085069, 4],"bool"), axis=1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.791437 test begin: paddle.Tensor.sum(Tensor([5, 3, 152113426],"bool"), axis=1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([5, 3, 152113426],"bool"), axis=1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.800838 test begin: paddle.Tensor.sum(Tensor([54326224, 7, 6],"bool"), axis=1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([54326224, 7, 6],"bool"), axis=1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.810266 test begin: paddle.Tensor.sum(Tensor([54326224, 7, 6],"bool"), axis=2, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([54326224, 7, 6],"bool"), axis=2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.820686 test begin: paddle.Tensor.sum(Tensor([570425345, 1, 2, 2],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([570425345, 1, 2, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.830133 test begin: paddle.Tensor.sum(Tensor([570425345, 2, 1, 2],"float32"), list[0,-1,], )

[torch error] paddle.Tensor.sum(Tensor([570425345, 2, 1, 2],"float32"), list[0,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.839607 test begin: paddle.Tensor.sum(Tensor([570425345, 2, 2],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([570425345, 2, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.848934 test begin: paddle.Tensor.sum(Tensor([570425345, 2, 2],"float32"), list[0,-1,], )

[torch error] paddle.Tensor.sum(Tensor([570425345, 2, 2],"float32"), list[0,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.858232 test begin: paddle.Tensor.sum(Tensor([570425345, 2, 2],"float32"), list[0,1,], )

[torch error] paddle.Tensor.sum(Tensor([570425345, 2, 2],"float32"), list[0,1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.867407 test begin: paddle.Tensor.sum(Tensor([570426, 500, 8],"float32"), axis=2, )

[torch error] paddle.Tensor.sum(Tensor([570426, 500, 8],"float32"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.876609 test begin: paddle.Tensor.sum(Tensor([5820667, 8, 7, 7],"float32"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([5820667, 8, 7, 7],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.885751 test begin: paddle.Tensor.sum(Tensor([6449, 1, 13, 27216],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([6449, 1, 13, 27216],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.894878 test begin: paddle.Tensor.sum(Tensor([697, 65536, 25, 2],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([697, 65536, 25, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.904764 test begin: paddle.Tensor.sum(Tensor([715827883, 2, 3],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([715827883, 2, 3],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.914154 test begin: paddle.Tensor.sum(Tensor([7231, 1, 13, 24276],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([7231, 1, 13, 24276],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.923603 test begin: paddle.Tensor.sum(Tensor([760567127, 3],"bool"), axis=0, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([760567127, 3],"bool"), axis=0, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.935572 test begin: paddle.Tensor.sum(Tensor([760567127, 3],"bool"), axis=1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([760567127, 3],"bool"), axis=1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.947687 test begin: paddle.Tensor.sum(Tensor([760567127, 3],"float32"), list[0,-1,], )

[torch error] paddle.Tensor.sum(Tensor([760567127, 3],"float32"), list[0,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.957214 test begin: paddle.Tensor.sum(Tensor([7605672, 3, 10, 10, 1],"float32"), list[1,2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([7605672, 3, 10, 10, 1],"float32"), list[1,2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.966751 test begin: paddle.Tensor.sum(Tensor([7737, 128, 256, 3, 3],"float32"), list[2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([7737, 128, 256, 3, 3],"float32"), list[2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.976270 test begin: paddle.Tensor.sum(Tensor([80, 128, 8, 4, 4, 4, 436],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([80, 128, 8, 4, 4, 4, 436],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.52 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.985644 test begin: paddle.Tensor.sum(Tensor([80, 128, 8, 4, 4, 436, 4],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([80, 128, 8, 4, 4, 436, 4],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.52 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:21.994943 test begin: paddle.Tensor.sum(Tensor([80, 128, 8, 4, 436, 4, 4],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([80, 128, 8, 4, 436, 4, 4],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.52 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:22.004300 test begin: paddle.Tensor.sum(Tensor([80, 128, 8, 436, 4, 4, 4],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([80, 128, 8, 436, 4, 4, 4],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.52 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:22.013612 test begin: paddle.Tensor.sum(Tensor([80, 128, 871, 4, 4, 4, 4],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([80, 128, 871, 4, 4, 4, 4],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:22.022902 test begin: paddle.Tensor.sum(Tensor([80, 13927, 8, 4, 4, 4, 4],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([80, 13927, 8, 4, 4, 4, 4],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:22.032238 test begin: paddle.Tensor.sum(Tensor([80, 4, 4, 4, 4, 4, 27853],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([80, 4, 4, 4, 4, 4, 27853],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:22.042135 test begin: paddle.Tensor.sum(Tensor([80, 4, 4, 4, 4, 871, 128],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([80, 4, 4, 4, 4, 871, 128],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:22.051379 test begin: paddle.Tensor.sum(Tensor([80, 4, 4, 4, 871, 4, 128],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([80, 4, 4, 4, 871, 4, 128],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:22.068758 test begin: paddle.Tensor.sum(Tensor([80, 4, 4, 871, 4, 4, 128],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([80, 4, 4, 871, 4, 4, 128],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:22.078618 test begin: paddle.Tensor.sum(Tensor([80, 4, 871, 4, 4, 4, 128],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([80, 4, 871, 4, 4, 4, 128],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:22.088128 test begin: paddle.Tensor.sum(Tensor([80, 871, 4, 4, 4, 4, 128],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([80, 871, 4, 4, 4, 4, 128],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:22.097365 test begin: paddle.Tensor.sum(Tensor([83837, 1, 27216],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([83837, 1, 27216],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:22.106607 test begin: paddle.Tensor.sum(Tensor([84507459, 3, 3, 3],"float32"), list[1,2,3,], )

[torch error] paddle.Tensor.sum(Tensor([84507459, 3, 3, 3],"float32"), list[1,2,3,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:22.115900 test begin: paddle.Tensor.sum(Tensor([858993460, 1, 1, 5],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([858993460, 1, 1, 5],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:22.126144 test begin: paddle.Tensor.sum(Tensor([858993460, 1, 1, 5],"float16"), 1, )

[torch error] paddle.Tensor.sum(Tensor([858993460, 1, 1, 5],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:22.136061 test begin: paddle.Tensor.sum(Tensor([858993460, 1, 1, 5],"float16"), 2, )

[torch error] paddle.Tensor.sum(Tensor([858993460, 1, 1, 5],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:22.145573 test begin: paddle.Tensor.sum(Tensor([858993460, 1, 1, 5],"float16"), 3, )

[torch error] paddle.Tensor.sum(Tensor([858993460, 1, 1, 5],"float16"), 3, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:22.155910 test begin: paddle.Tensor.sum(Tensor([8705, 128, 8, 4, 4, 4, 4],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([8705, 128, 8, 4, 4, 4, 4],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:22.168973 test begin: paddle.Tensor.sum(Tensor([89478486, 2, 4, 2, 3],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([89478486, 2, 4, 2, 3],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:22.181524 test begin: paddle.Tensor.sum(Tensor([9, 10, 47721859],"float16"), list[0,-1,], )

[torch error] paddle.Tensor.sum(Tensor([9, 10, 47721859],"float16"), list[0,-1,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:22.193542 test begin: paddle.Tensor.sum(Tensor([9, 23860930, 20],"float16"), list[0,-1,], )

[torch error] paddle.Tensor.sum(Tensor([9, 23860930, 20],"float16"), list[0,-1,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:22.206050 test begin: paddle.Tensor.sum(Tensor([9126806, 10, 25],"float32"), 2, )

[torch error] paddle.Tensor.sum(Tensor([9126806, 10, 25],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:22.217491 test begin: paddle.Tensor.sum(Tensor([93991, 1, 24276],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([93991, 1, 24276],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:22.228224 test begin: paddle.Tensor.sum(Tensor([93991, 1, 24276],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([93991, 1, 24276],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:22.239072 test begin: paddle.Tensor.sum(Tensor([95070891, 2, 1, 3, 1, 4],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([95070891, 2, 1, 3, 1, 4],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:22.249465 test begin: paddle.Tensor.sum(Tensor([95071, 3, 10, 10, 80],"float32"), list[1,2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([95071, 3, 10, 10, 80],"float32"), list[1,2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:22.259999 test begin: paddle.Tensor.take_along_axis(Tensor([128, 1000],"float32"), indices=Tensor([128, 17825793],"int32"), axis=-1, )

[torch error] paddle.Tensor.take_along_axis(Tensor([128, 1000],"float32"), indices=Tensor([128, 17825793],"int32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.08 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 11.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:08:51.435112 test begin: paddle.Tensor.take_along_axis(Tensor([128, 1000],"float32"), indices=Tensor([2281701379, 1],"int32"), axis=-1, )

[torch error] paddle.Tensor.take_along_axis(Tensor([128, 1000],"float32"), indices=Tensor([2281701379, 1],"int32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.08 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 11.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:09:19.861881 test begin: paddle.Tensor.take_along_axis(Tensor([128, 17825793],"float32"), indices=Tensor([128, 17825793],"int32"), axis=-1, )

[torch error] paddle.Tensor.take_along_axis(Tensor([128, 17825793],"float32"), indices=Tensor([128, 17825793],"int32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:09:47.598294 test begin: paddle.Tensor.take_along_axis(Tensor([128, 17825793],"float32"), indices=Tensor([128, 1],"int32"), axis=-1, )

[torch error] paddle.Tensor.take_along_axis(Tensor([128, 17825793],"float32"), indices=Tensor([128, 1],"int32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:10:15.617258 test begin: paddle.Tensor.take_along_axis(Tensor([2281702, 1000],"float32"), indices=Tensor([128, 1],"int32"), axis=-1, )

[torch error] paddle.Tensor.take_along_axis(Tensor([2281702, 1000],"float32"), indices=Tensor([128, 1],"int32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:10:44.351944 test begin: paddle.Tensor.take_along_axis(Tensor([2281702, 1000],"float32"), indices=Tensor([2281702, 1],"int32"), axis=-1, )

[torch error] paddle.Tensor.take_along_axis(Tensor([2281702, 1000],"float32"), indices=Tensor([2281702, 1],"int32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:11:17.165677 test begin: paddle.Tensor.take_along_axis(Tensor([2281702, 1000],"float32"), indices=Tensor([80, 1],"int32"), axis=-1, )

[torch error] paddle.Tensor.take_along_axis(Tensor([2281702, 1000],"float32"), indices=Tensor([80, 1],"int32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:11:51.368800 test begin: paddle.Tensor.take_along_axis(Tensor([80, 1000],"float32"), indices=Tensor([2281701379, 1],"int32"), axis=-1, )

[torch error] paddle.Tensor.take_along_axis(Tensor([80, 1000],"float32"), indices=Tensor([2281701379, 1],"int32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.08 GiB is free. Process 41358 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 11.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:12:41.665981 test begin: paddle.Tensor.take_along_axis(Tensor([80, 1000],"float32"), indices=Tensor([80, 28521268],"int32"), axis=-1, )

2025-03-11 19:12:53.726435 test begin: paddle.Tensor.nansum(Tensor([477218589, 3, 3],"float16"), )  

W0311 19:14:34.548800 145490 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 19:14:34.550693 145490 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.Tensor.nansum(Tensor([477218589, 3, 3],"float16"), )   
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 5244.
Max relative difference: 0.851
 x: array(-917., dtype=float16)
 y: array(-6160., dtype=float16)
2025-03-11 19:20:09.231107 test begin: paddle.Tensor.nansum(Tensor([477218589, 3, 3],"float16"), axis=-1, )  

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741692182 (unix time) try "date -d @1741692182" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x237ce) received by PID 145358 (TID 0x7f0abc7c3700) from PID 145358 ***]

2025-03-11 19:23:46.583609 test begin: paddle.Tensor.nansum(Tensor([477218589, 3, 3],"float16"), axis=0, keepdim=True, )  

W0311 19:25:16.952998 151777 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 19:25:16.954150 151777 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.Tensor.nansum(Tensor([477218589, 3, 3],"float16"), axis=0, keepdim=True, )   
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 9 / 9 (100%)
Max absolute difference: 9192.
Max relative difference: 1.229
 x: array([[[ 412.  ,  -32.62,  352.8 ],
        [ 453.  , -804.5 ,  355.2 ],
        [-357.8 ,  685.5 ,  529.  ]]], dtype=float16)
 y: array([[[ 3548.,  2886.,  6144.],
        [-8568., -4296., -5224.],
        [-9552., -3004.,  7136.]]], dtype=float16)
2025-03-11 19:25:38.713506 test begin: paddle.Tensor.outer(x=Tensor([4294967297],"float16"), y=Tensor([4294967297],"float16"), )  

[torch error] paddle.Tensor.outer(x=Tensor([4294967297],"float16"), y=Tensor([4294967297],"float16"), )   
 Storage size calculation overflowed with sizes=[4294967297, 4294967297]
2025-03-11 19:25:46.298378 test begin: paddle.Tensor.outer(x=Tensor([4294967297],"float16"), y=Tensor([4],"float64"), )  

[torch error] paddle.Tensor.outer(x=Tensor([4294967297],"float16"), y=Tensor([4],"float64"), )   
 CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 69.59 GiB is free. Process 54677 has 9.59 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 4.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:27:00.357221 test begin: paddle.Tensor.outer(x=Tensor([4],"float64"), y=Tensor([4294967297],"float16"), )  

[torch error] paddle.Tensor.outer(x=Tensor([4],"float64"), y=Tensor([4294967297],"float16"), )   
 CUDA out of memory. Tried to allocate 128.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 69.59 GiB is free. Process 54677 has 9.59 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 4.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:27:02.673330 test begin: paddle.Tensor.quantile(Tensor([3, 11930465, 3, 4, 2, 5],"float16"), q=0.5, )  

[torch error] paddle.Tensor.quantile(Tensor([3, 11930465, 3, 4, 2, 5],"float16"), q=0.5, )   
 quantile() input tensor must be either float or double dtype
2025-03-11 19:27:04.752566 test begin: paddle.Tensor.quantile(Tensor([3, 11930465, 3, 4, 2, 5],"float16"), q=0.75, axis=3, keepdim=True, )  

[torch error] paddle.Tensor.quantile(Tensor([3, 11930465, 3, 4, 2, 5],"float16"), q=0.75, axis=3, keepdim=True, )   
 quantile() input tensor must be either float or double dtype
2025-03-11 19:27:07.342294 test begin: paddle.Tensor.quantile(Tensor([3, 11930465, 3, 4, 2, 5],"float16"), q=0.75, axis=5, )  

[torch error] paddle.Tensor.quantile(Tensor([3, 11930465, 3, 4, 2, 5],"float16"), q=0.75, axis=5, )   
 quantile() input tensor must be either float or double dtype
2025-03-11 19:27:09.833629 test begin: paddle.Tensor.quantile(Tensor([3, 3, 477218589],"float16"), q=0.5, axis=0, )  

[torch error] paddle.Tensor.quantile(Tensor([3, 3, 477218589],"float16"), q=0.5, axis=0, )   
 quantile() input tensor must be either float or double dtype
2025-03-11 19:27:11.479456 test begin: paddle.Tensor.quantile(Tensor([3, 477218589, 3],"float16"), q=0.5, axis=0, )  

[torch error] paddle.Tensor.quantile(Tensor([3, 477218589, 3],"float16"), q=0.5, axis=0, )   
 quantile() input tensor must be either float or double dtype
2025-03-11 19:27:13.829926 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 2, 9942054],"float16"), q=0.5, )  

[torch error] paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 2, 9942054],"float16"), q=0.5, )   
 quantile() input tensor must be either float or double dtype
2025-03-11 19:27:15.414533 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 2, 9942054],"float16"), q=0.75, axis=3, keepdim=True, )  

[torch error] paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 2, 9942054],"float16"), q=0.75, axis=3, keepdim=True, )   
 quantile() input tensor must be either float or double dtype
2025-03-11 19:27:17.161367 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 2, 9942054],"float16"), q=0.75, axis=5, )  

[torch error] paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 2, 9942054],"float16"), q=0.75, axis=5, )   
 quantile() input tensor must be either float or double dtype
2025-03-11 19:27:19.525976 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 3976822, 5],"float16"), q=0.5, )  

[torch error] paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 3976822, 5],"float16"), q=0.5, )   
 quantile() input tensor must be either float or double dtype
2025-03-11 19:27:21.790476 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 3976822, 5],"float16"), q=0.75, axis=3, keepdim=True, )  

[torch error] paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 3976822, 5],"float16"), q=0.75, axis=3, keepdim=True, )   
 quantile() input tensor must be either float or double dtype
2025-03-11 19:27:23.518731 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 3976822, 5],"float16"), q=0.75, axis=5, )  

[torch error] paddle.Tensor.quantile(Tensor([3, 6, 3, 4, 3976822, 5],"float16"), q=0.75, axis=5, )   
 quantile() input tensor must be either float or double dtype
2025-03-11 19:27:25.493275 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 7953644, 2, 5],"float16"), q=0.5, )  

[torch error] paddle.Tensor.quantile(Tensor([3, 6, 3, 7953644, 2, 5],"float16"), q=0.5, )   
 quantile() input tensor must be either float or double dtype
2025-03-11 19:27:27.868492 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 7953644, 2, 5],"float16"), q=0.75, axis=3, keepdim=True, )  

[torch error] paddle.Tensor.quantile(Tensor([3, 6, 3, 7953644, 2, 5],"float16"), q=0.75, axis=3, keepdim=True, )   
 quantile() input tensor must be either float or double dtype
2025-03-11 19:27:29.992065 test begin: paddle.Tensor.quantile(Tensor([3, 6, 3, 7953644, 2, 5],"float16"), q=0.75, axis=5, )  

[torch error] paddle.Tensor.quantile(Tensor([3, 6, 3, 7953644, 2, 5],"float16"), q=0.75, axis=5, )   
 quantile() input tensor must be either float or double dtype
2025-03-11 19:27:31.521598 test begin: paddle.Tensor.quantile(Tensor([3, 6, 5965233, 4, 2, 5],"float16"), q=0.5, )  

[torch error] paddle.Tensor.quantile(Tensor([3, 6, 5965233, 4, 2, 5],"float16"), q=0.5, )   
 quantile() input tensor must be either float or double dtype
2025-03-11 19:27:33.831361 test begin: paddle.Tensor.quantile(Tensor([3, 6, 5965233, 4, 2, 5],"float16"), q=0.75, axis=3, keepdim=True, )  

[torch error] paddle.Tensor.quantile(Tensor([3, 6, 5965233, 4, 2, 5],"float16"), q=0.75, axis=3, keepdim=True, )   
 quantile() input tensor must be either float or double dtype
2025-03-11 19:27:35.393961 test begin: paddle.Tensor.quantile(Tensor([3, 6, 5965233, 4, 2, 5],"float16"), q=0.75, axis=5, )  

[torch error] paddle.Tensor.quantile(Tensor([3, 6, 5965233, 4, 2, 5],"float16"), q=0.75, axis=5, )   
 quantile() input tensor must be either float or double dtype
2025-03-11 19:27:37.720593 test begin: paddle.Tensor.quantile(Tensor([477218589, 3, 3],"float16"), q=0.5, axis=0, )  

[torch error] paddle.Tensor.quantile(Tensor([477218589, 3, 3],"float16"), q=0.5, axis=0, )   
 quantile() input tensor must be either float or double dtype
2025-03-11 19:27:39.250378 test begin: paddle.Tensor.quantile(Tensor([5965233, 6, 3, 4, 2, 5],"float16"), q=0.5, )  

[torch error] paddle.Tensor.quantile(Tensor([5965233, 6, 3, 4, 2, 5],"float16"), q=0.5, )   
 quantile() input tensor must be either float or double dtype
2025-03-11 19:27:41.461309 test begin: paddle.Tensor.quantile(Tensor([5965233, 6, 3, 4, 2, 5],"float16"), q=0.75, axis=3, keepdim=True, )  

[torch error] paddle.Tensor.quantile(Tensor([5965233, 6, 3, 4, 2, 5],"float16"), q=0.75, axis=3, keepdim=True, )   
 quantile() input tensor must be either float or double dtype
2025-03-11 19:27:42.959695 test begin: paddle.Tensor.quantile(Tensor([5965233, 6, 3, 4, 2, 5],"float16"), q=0.75, axis=5, )  

[torch error] paddle.Tensor.quantile(Tensor([5965233, 6, 3, 4, 2, 5],"float16"), q=0.75, axis=5, )   
 quantile() input tensor must be either float or double dtype
2025-03-11 19:27:45.207605 test begin: paddle.Tensor.remainder(Tensor([190141782, 3, 4],"float32"), Tensor([190141782, 3, 4],"float32"), )  

[Pass] paddle.Tensor.remainder(Tensor([190141782, 3, 4],"float32"), Tensor([190141782, 3, 4],"float32"), )  
2025-03-11 19:30:04.894544 test begin: paddle.Tensor.remainder(Tensor([190141782, 3, 4],"float32"), Tensor([2, 3, 4],"float32"), )  

[torch error] paddle.Tensor.remainder(Tensor([190141782, 3, 4],"float32"), Tensor([2, 3, 4],"float32"), )   
 The size of tensor a (190141782) must match the size of tensor b (2) at non-singleton dimension 0
2025-03-11 19:30:07.002378 test begin: paddle.Tensor.remainder(Tensor([2, 285212673, 4],"float32"), Tensor([2, 285212673, 4],"float32"), )  

[Pass] paddle.Tensor.remainder(Tensor([2, 285212673, 4],"float32"), Tensor([2, 285212673, 4],"float32"), )  
2025-03-11 19:31:30.776050 test begin: paddle.Tensor.remainder(Tensor([2, 285212673, 4],"float32"), Tensor([2, 3, 4],"float32"), )  

[torch error] paddle.Tensor.remainder(Tensor([2, 285212673, 4],"float32"), Tensor([2, 3, 4],"float32"), )   
 The size of tensor a (285212673) must match the size of tensor b (3) at non-singleton dimension 1
2025-03-11 19:31:35.439680 test begin: paddle.Tensor.remainder(Tensor([2, 3, 380283564],"float32"), Tensor([2, 3, 380283564],"float32"), )  

[Pass] paddle.Tensor.remainder(Tensor([2, 3, 380283564],"float32"), Tensor([2, 3, 380283564],"float32"), )  
2025-03-11 19:33:12.396687 test begin: paddle.Tensor.remainder(Tensor([2, 3, 380283564],"float32"), Tensor([2, 3, 4],"float32"), )  

[torch error] paddle.Tensor.remainder(Tensor([2, 3, 380283564],"float32"), Tensor([2, 3, 4],"float32"), )   
 The size of tensor a (380283564) must match the size of tensor b (4) at non-singleton dimension 2
2025-03-11 19:33:16.320879 test begin: paddle.Tensor.remainder(Tensor([2, 3, 4],"float32"), Tensor([190141782, 3, 4],"float32"), )  

[torch error] paddle.Tensor.remainder(Tensor([2, 3, 4],"float32"), Tensor([190141782, 3, 4],"float32"), )   
 The size of tensor a (2) must match the size of tensor b (190141782) at non-singleton dimension 0
2025-03-11 19:33:17.851179 test begin: paddle.Tensor.remainder(Tensor([2, 3, 4],"float32"), Tensor([2, 285212673, 4],"float32"), )  

[torch error] paddle.Tensor.remainder(Tensor([2, 3, 4],"float32"), Tensor([2, 285212673, 4],"float32"), )   
 The size of tensor a (3) must match the size of tensor b (285212673) at non-singleton dimension 1
2025-03-11 19:33:19.490504 test begin: paddle.Tensor.remainder(Tensor([2, 3, 4],"float32"), Tensor([2, 3, 380283564],"float32"), )  

[torch error] paddle.Tensor.remainder(Tensor([2, 3, 4],"float32"), Tensor([2, 3, 380283564],"float32"), )   
 The size of tensor a (4) must match the size of tensor b (380283564) at non-singleton dimension 2
2025-03-11 19:33:20.587224 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 1, 1140850690],"float32"), 1, axis=0, )  

[Pass] paddle.Tensor.repeat_interleave(Tensor([2, 1, 1, 1140850690],"float32"), 1, axis=0, )  
2025-03-11 19:36:19.593911 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 1, 1140850690],"float32"), 2, axis=0, )  

2025-03-11 19:36:23.948687 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 1, 1140850690],"float32"), 3, axis=0, )  

2025-03-11 19:36:25.968503 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 10, 114085069],"int64"), 1, axis=0, )  

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 10, 114085069],"int64"), 1, axis=0, )   
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 54677 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:37:36.179859 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 10, 114085069],"int64"), 2, axis=0, )  

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 10, 114085069],"int64"), 2, axis=0, )   
 CUDA out of memory. Tried to allocate 34.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 54677 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:37:40.917213 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 10, 114085069],"int64"), 3, axis=0, )  

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 10, 114085069],"int64"), 3, axis=0, )   
 CUDA out of memory. Tried to allocate 51.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 54677 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:37:43.880979 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 114085069, 10],"int64"), 1, axis=0, )  

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 114085069, 10],"int64"), 1, axis=0, )   
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 54677 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:37:47.842501 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 114085069, 10],"int64"), 2, axis=0, )  

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 114085069, 10],"int64"), 2, axis=0, )   
 CUDA out of memory. Tried to allocate 34.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 54677 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:37:51.802540 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 114085069, 10],"int64"), 3, axis=0, )  

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 114085069, 10],"int64"), 3, axis=0, )   
 CUDA out of memory. Tried to allocate 51.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.09 GiB is free. Process 54677 has 78.10 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 19:37:56.589910 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 380283564, 3],"float32"), 1, axis=0, )  

2025-03-11 19:38:11.144448 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 380283564, 3],"float32"), 2, axis=0, )  

W0311 19:39:17.307667 159240 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 19:39:17.308528 159240 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
2025-03-11 19:39:17.308852 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 380283564, 3],"float32"), 3, axis=0, )  

2025-03-11 19:39:20.596186 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1140850690],"int64"), 1, axis=0, )  

[Pass] paddle.Tensor.repeat_interleave(Tensor([2, 1140850690],"int64"), 1, axis=0, )  
2025-03-11 19:43:15.734930 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1140850690],"int64"), 2, axis=0, )  

[accuracy error] paddle.Tensor.repeat_interleave(Tensor([2, 1140850690],"int64"), 2, axis=0, )   
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 268433443 / 4563402760 (5.88%)
Max absolute difference: 65535
Max relative difference: 1.
 x: array([[ -2294,  49285,  56194, ..., -42666,  57174, -13796],
       [ -2294,  49285,  56194, ..., -42666,  57174, -13796],
       [-61470,  47931, -42749, ..., -41451,  39285, -31531],
       [-61470,  47931, -42749, ...,      0,      0,      0]])
 y: array([[ -2294,  49285,  56194, ..., -42666,  57174, -13796],
       [ -2294,  49285,  56194, ..., -42666,  57174, -13796],
       [-61470,  47931, -42749, ..., -41451,  39285, -31531],
       [-61470,  47931, -42749, ..., -41451,  39285, -31531]])
2025-03-11 19:52:54.954464 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1140850690],"int64"), 3, axis=0, )  

[paddle error] paddle.Tensor.repeat_interleave(Tensor([2, 1140850690],"int64"), 3, axis=0, )   
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_repeat_interleave(_object*, _object*, _object*)
1   repeat_interleave_ad_func(paddle::Tensor const&, int, int)
2   paddle::experimental::repeat_interleave(paddle::Tensor const&, int, int)
3   void phi::RepeatInterleaveKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, int, phi::DenseTensor*)
4   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
5   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 51.000000GB memory on GPU 0, 35.594727GB memory has been allocated and available memory is only 43.590149GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-11 19:53:50.908228 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 11408507, 10, 10],"int64"), 1, axis=0, )  

[Pass] paddle.Tensor.repeat_interleave(Tensor([2, 11408507, 10, 10],"int64"), 1, axis=0, )  
2025-03-11 19:56:13.094110 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 11408507, 10, 10],"int64"), 2, axis=0, )  

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f5e17419ee0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-11 20:06:36.321958 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 11408507, 10, 10],"int64"), 3, axis=0, )  

W0311 20:08:23.346776 10560 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 20:08:23.347687 10560 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.Tensor.repeat_interleave(Tensor([2, 11408507, 10, 10],"int64"), 3, axis=0, )   
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_repeat_interleave(_object*, _object*, _object*)
1   repeat_interleave_ad_func(paddle::Tensor const&, int, int)
2   paddle::experimental::repeat_interleave(paddle::Tensor const&, int, int)
3   void phi::RepeatInterleaveKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, int, phi::DenseTensor*)
4   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
5   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 51.000001GB memory on GPU 0, 35.594666GB memory has been allocated and available memory is only 43.590210GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-11 20:08:25.626475 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 3, 380283564],"float32"), 1, axis=0, )  

[Pass] paddle.Tensor.repeat_interleave(Tensor([2, 3, 380283564],"float32"), 1, axis=0, )  
2025-03-11 20:12:24.172569 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 3, 380283564],"float32"), 2, axis=0, )  

2025-03-11 20:12:28.778886 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 3, 380283564],"float32"), 3, axis=0, )  

2025-03-11 20:12:31.246490 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 35651585, 32],"float32"), 1, axis=0, )  

2025-03-11 20:12:42.948811 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 35651585, 32],"float32"), 2, axis=0, )  

W0311 20:13:49.747413 12985 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 20:13:49.748548 12985 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
2025-03-11 20:13:49.748952 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 35651585, 32],"float32"), 3, axis=0, )  

2025-03-11 20:13:51.833334 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 380283564, 1, 3],"float32"), 1, axis=0, )  

[Pass] paddle.Tensor.repeat_interleave(Tensor([2, 380283564, 1, 3],"float32"), 1, axis=0, )  
2025-03-11 20:16:44.488624 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 380283564, 1, 3],"float32"), 2, axis=0, )  

2025-03-11 20:16:49.162409 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 380283564, 1, 3],"float32"), 3, axis=0, )  

2025-03-11 20:16:53.299736 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 71303169, 16],"float32"), 1, axis=0, )  

2025-03-11 20:17:02.110780 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 71303169, 16],"float32"), 2, axis=0, )  

W0311 20:18:13.358896 15360 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 20:18:13.359908 15360 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
2025-03-11 20:18:13.360385 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 71303169, 16],"float32"), 3, axis=0, )  

2025-03-11 20:18:15.974716 test begin: paddle.Tensor.repeat_interleave(Tensor([2281701379, 1],"int64"), 1, axis=0, )  

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_repeat_interleave(_object*, _object*, _object*)
1   repeat_interleave_ad_func(paddle::Tensor const&, int, int)
2   paddle::experimental::repeat_interleave(paddle::Tensor const&, int, int)
3   void phi::RepeatInterleaveKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, int, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741695582 (unix time) try "date -d @1741695582" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x3c00) received by PID 15360 (TID 0x7fad7fdc2700) from PID 15360 ***]

2025-03-11 20:20:18.811465 test begin: paddle.Tensor.repeat_interleave(Tensor([2281701379, 1],"int64"), 2, axis=0, )  

W0311 20:22:08.888046 17215 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 20:22:08.888999 17215 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.Tensor.repeat_interleave(Tensor([2281701379, 1],"int64"), 2, axis=0, )   
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_repeat_interleave(_object*, _object*, _object*)
1   repeat_interleave_ad_func(paddle::Tensor const&, int, int)
2   paddle::experimental::repeat_interleave(paddle::Tensor const&, int, int)
3   void phi::RepeatInterleaveKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, int, phi::DenseTensor*)
4   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
5   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 34.000000GB memory on GPU 0, 52.594727GB memory has been allocated and available memory is only 26.590149GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-11 20:22:25.722277 test begin: paddle.Tensor.repeat_interleave(Tensor([2281701379, 1],"int64"), 3, axis=0, )  

[paddle error] paddle.Tensor.repeat_interleave(Tensor([2281701379, 1],"int64"), 3, axis=0, )   
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_repeat_interleave(_object*, _object*, _object*)
1   repeat_interleave_ad_func(paddle::Tensor const&, int, int)
2   paddle::experimental::repeat_interleave(paddle::Tensor const&, int, int)
3   void phi::RepeatInterleaveKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, int, phi::DenseTensor*)
4   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
5   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 51.000000GB memory on GPU 0, 61.094727GB memory has been allocated and available memory is only 18.090149GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-11 20:23:42.757661 test begin: paddle.Tensor.repeat_interleave(Tensor([228170138, 10],"int64"), 1, axis=0, )  

[Pass] paddle.Tensor.repeat_interleave(Tensor([228170138, 10],"int64"), 1, axis=0, )  
2025-03-11 20:26:33.098348 test begin: paddle.Tensor.repeat_interleave(Tensor([228170138, 10],"int64"), 2, axis=0, )  

[accuracy error] paddle.Tensor.repeat_interleave(Tensor([228170138, 10],"int64"), 2, axis=0, )   
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 268433390 / 4563402760 (5.88%)
Max absolute difference: 65535
Max relative difference: 1.
 x: array([[ 63093, -14346,   -386, ..., -58378, -55369,  48812],
       [ 63093, -14346,   -386, ..., -58378, -55369,  48812],
       [ 14630, -48423, -39742, ..., -29543, -51492,  23187],...
 y: array([[ 63093, -14346,   -386, ..., -58378, -55369,  48812],
       [ 63093, -14346,   -386, ..., -58378, -55369,  48812],
       [ 14630, -48423, -39742, ..., -29543, -51492,  23187],...
2025-03-11 20:36:20.332273 test begin: paddle.Tensor.repeat_interleave(Tensor([228170138, 10],"int64"), 3, axis=0, )  

[paddle error] paddle.Tensor.repeat_interleave(Tensor([228170138, 10],"int64"), 3, axis=0, )   
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_repeat_interleave(_object*, _object*, _object*)
1   repeat_interleave_ad_func(paddle::Tensor const&, int, int)
2   paddle::experimental::repeat_interleave(paddle::Tensor const&, int, int)
3   void phi::RepeatInterleaveKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, int, phi::DenseTensor*)
4   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
5   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 51.000000GB memory on GPU 0, 38.143555GB memory has been allocated and available memory is only 41.041321GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-11 20:37:19.425475 test begin: paddle.Tensor.repeat_interleave(Tensor([22817014, 1, 10, 10],"int64"), 1, axis=0, )  

[Pass] paddle.Tensor.repeat_interleave(Tensor([22817014, 1, 10, 10],"int64"), 1, axis=0, )  
2025-03-11 20:41:25.255816 test begin: paddle.Tensor.repeat_interleave(Tensor([22817014, 1, 10, 10],"int64"), 2, axis=0, )  

[accuracy error] paddle.Tensor.repeat_interleave(Tensor([22817014, 1, 10, 10],"int64"), 2, axis=0, )   
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 268433430 / 4563402800 (5.88%)
Max absolute difference: 65535
Max relative difference: 1.
 x: array([[[[ 63093, -14346,   -386, ..., -58378, -55369,  48812],
         [ 14630, -48423, -39742, ..., -29543, -51492,  23187],
         [-21824,  12458,  32978, ...,  38695, -32798,  37378],...
 y: array([[[[ 63093, -14346,   -386, ..., -58378, -55369,  48812],
         [ 14630, -48423, -39742, ..., -29543, -51492,  23187],
         [-21824,  12458,  32978, ...,  38695, -32798,  37378],...
2025-03-11 20:50:48.359048 test begin: paddle.Tensor.repeat_interleave(Tensor([22817014, 1, 10, 10],"int64"), 3, axis=0, )  

[paddle error] paddle.Tensor.repeat_interleave(Tensor([22817014, 1, 10, 10],"int64"), 3, axis=0, )   
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_repeat_interleave(_object*, _object*, _object*)
1   repeat_interleave_ad_func(paddle::Tensor const&, int, int)
2   paddle::experimental::repeat_interleave(paddle::Tensor const&, int, int)
3   void phi::RepeatInterleaveKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, int, int, phi::DenseTensor*)
4   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
5   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 51.000001GB memory on GPU 0, 35.848633GB memory has been allocated and available memory is only 43.336243GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-11 20:51:47.294143 test begin: paddle.Tensor.repeat_interleave(Tensor([23767723, 3, 32],"float32"), 1, axis=0, )  

[Pass] paddle.Tensor.repeat_interleave(Tensor([23767723, 3, 32],"float32"), 1, axis=0, )  
2025-03-11 20:55:31.665263 test begin: paddle.Tensor.repeat_interleave(Tensor([23767723, 3, 32],"float32"), 2, axis=0, )  

2025-03-11 20:55:35.594450 test begin: paddle.Tensor.repeat_interleave(Tensor([23767723, 3, 32],"float32"), 3, axis=0, )  

2025-03-11 20:55:36.629461 test begin: paddle.Tensor.repeat_interleave(Tensor([47535446, 3, 16],"float32"), 1, axis=0, )  

2025-03-11 20:55:46.863719 test begin: paddle.Tensor.repeat_interleave(Tensor([47535446, 3, 16],"float32"), 2, axis=0, )  

W0311 20:57:12.167402 35413 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 20:57:12.168576 35413 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
2025-03-11 20:57:12.169139 test begin: paddle.Tensor.repeat_interleave(Tensor([47535446, 3, 16],"float32"), 3, axis=0, )  

2025-03-11 20:57:17.936246 test begin: paddle.Tensor.repeat_interleave(Tensor([760567127, 1, 1, 3],"float32"), 1, axis=0, )  

[Pass] paddle.Tensor.repeat_interleave(Tensor([760567127, 1, 1, 3],"float32"), 1, axis=0, )  
2025-03-11 21:00:39.482895 test begin: paddle.Tensor.repeat_interleave(Tensor([760567127, 1, 1, 3],"float32"), 2, axis=0, )  

2025-03-11 21:00:43.887135 test begin: paddle.Tensor.repeat_interleave(Tensor([760567127, 1, 1, 3],"float32"), 3, axis=0, )  

2025-03-11 21:00:45.441526 test begin: paddle.Tensor.repeat_interleave(x=Tensor([107374183, 2, 4, 5],"float16"), repeats=2, )  

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([107374183, 2, 4, 5],"float16"), repeats=2, )   
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 10.09 GiB is free. Process 120775 has 69.10 GiB memory in use. Of the allocated memory 67.50 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 21:02:02.802543 test begin: paddle.Tensor.repeat_interleave(x=Tensor([14260634, 2, 4, 4, 5],"int32"), repeats=2, axis=3, )  

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([14260634, 2, 4, 4, 5],"int32"), repeats=2, axis=3, )   
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 9.59 GiB is free. Process 120775 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 21:02:44.123440 test begin: paddle.Tensor.repeat_interleave(x=Tensor([2147483649, 2],"float16"), repeats=2, )  

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([2147483649, 2],"float16"), repeats=2, )   
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 9.59 GiB is free. Process 120775 has 69.60 GiB memory in use. Of the allocated memory 67.50 GiB is allocated by PyTorch, and 522.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 21:02:46.253948 test begin: paddle.Tensor.repeat_interleave(x=Tensor([2281701379],"float32"), repeats=3, )  

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([2281701379],"float32"), repeats=3, )   
 CUDA out of memory. Tried to allocate 25.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 9.59 GiB is free. Process 120775 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 21:02:50.375163 test begin: paddle.Tensor.repeat_interleave(x=Tensor([26843546, 2, 4, 4, 5],"float16"), repeats=2, )  

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([26843546, 2, 4, 4, 5],"float16"), repeats=2, )   
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 9.59 GiB is free. Process 120775 has 69.60 GiB memory in use. Of the allocated memory 67.50 GiB is allocated by PyTorch, and 522.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 21:02:52.319512 test begin: paddle.Tensor.repeat_interleave(x=Tensor([26843546, 2, 4, 4, 5],"float16"), repeats=2, axis=1, )  

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([26843546, 2, 4, 4, 5],"float16"), repeats=2, axis=1, )   
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 9.59 GiB is free. Process 120775 has 69.60 GiB memory in use. Of the allocated memory 67.50 GiB is allocated by PyTorch, and 522.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 21:02:54.591150 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 1073741825],"float16"), repeats=2, )  

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 1073741825],"float16"), repeats=2, )   
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 9.59 GiB is free. Process 120775 has 69.60 GiB memory in use. Of the allocated memory 67.50 GiB is allocated by PyTorch, and 522.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 21:02:56.328062 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 13421773, 4, 4, 5],"float16"), repeats=2, )  

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 13421773, 4, 4, 5],"float16"), repeats=2, )   
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 9.59 GiB is free. Process 120775 has 69.60 GiB memory in use. Of the allocated memory 67.50 GiB is allocated by PyTorch, and 522.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 21:02:58.392073 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 13421773, 4, 4, 5],"float16"), repeats=2, axis=1, )  

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 13421773, 4, 4, 5],"float16"), repeats=2, axis=1, )   
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 9.59 GiB is free. Process 120775 has 69.60 GiB memory in use. Of the allocated memory 67.50 GiB is allocated by PyTorch, and 522.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 21:03:00.643709 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 107374183, 5],"float16"), repeats=2, )  

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 107374183, 5],"float16"), repeats=2, )   
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 9.59 GiB is free. Process 120775 has 69.60 GiB memory in use. Of the allocated memory 67.50 GiB is allocated by PyTorch, and 522.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 21:03:02.229015 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 14260634, 4, 5],"int32"), repeats=2, axis=3, )  

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 14260634, 4, 5],"int32"), repeats=2, axis=3, )   
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 9.59 GiB is free. Process 120775 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 21:03:03.225754 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 26843546, 4, 5],"float16"), repeats=2, )  

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 26843546, 4, 5],"float16"), repeats=2, )   
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 9.59 GiB is free. Process 120775 has 69.60 GiB memory in use. Of the allocated memory 67.50 GiB is allocated by PyTorch, and 522.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 21:03:05.753816 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 26843546, 4, 5],"float16"), repeats=2, axis=1, )  

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 26843546, 4, 5],"float16"), repeats=2, axis=1, )   
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 9.59 GiB is free. Process 120775 has 69.60 GiB memory in use. Of the allocated memory 67.50 GiB is allocated by PyTorch, and 522.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 21:03:07.758712 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 134217729],"float16"), repeats=2, )  

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 134217729],"float16"), repeats=2, )   
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 9.59 GiB is free. Process 120775 has 69.60 GiB memory in use. Of the allocated memory 67.50 GiB is allocated by PyTorch, and 522.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 21:03:09.806370 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 14260634, 5],"int32"), repeats=2, axis=3, )  

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 14260634, 5],"int32"), repeats=2, axis=3, )   
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 9.59 GiB is free. Process 120775 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 21:03:10.800969 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 26843546, 5],"float16"), repeats=2, )  

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 26843546, 5],"float16"), repeats=2, )   
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 9.59 GiB is free. Process 120775 has 69.60 GiB memory in use. Of the allocated memory 67.50 GiB is allocated by PyTorch, and 522.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 21:03:12.966574 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 26843546, 5],"float16"), repeats=2, axis=1, )  

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 26843546, 5],"float16"), repeats=2, axis=1, )   
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 9.59 GiB is free. Process 120775 has 69.60 GiB memory in use. Of the allocated memory 67.50 GiB is allocated by PyTorch, and 522.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 21:03:15.269703 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 4, 17825793],"int32"), repeats=2, axis=3, )  

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 4, 17825793],"int32"), repeats=2, axis=3, )   
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 9.59 GiB is free. Process 120775 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 21:03:16.692345 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 4, 33554433],"float16"), repeats=2, )  

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 4, 33554433],"float16"), repeats=2, )   
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 9.59 GiB is free. Process 120775 has 69.60 GiB memory in use. Of the allocated memory 67.50 GiB is allocated by PyTorch, and 522.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 21:03:19.014895 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 4, 33554433],"float16"), repeats=2, axis=1, )  

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 4, 33554433],"float16"), repeats=2, axis=1, )   
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 9.59 GiB is free. Process 120775 has 69.60 GiB memory in use. Of the allocated memory 67.50 GiB is allocated by PyTorch, and 522.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 21:03:21.317048 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 536870913],"float16"), repeats=2, )  

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 536870913],"float16"), repeats=2, )   
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 9.59 GiB is free. Process 120775 has 69.60 GiB memory in use. Of the allocated memory 67.50 GiB is allocated by PyTorch, and 522.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 21:03:23.612085 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 268435457, 4],"float16"), repeats=2, )  

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 268435457, 4],"float16"), repeats=2, )   
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 9.59 GiB is free. Process 120775 has 69.60 GiB memory in use. Of the allocated memory 67.50 GiB is allocated by PyTorch, and 522.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 21:03:25.760374 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 53687092, 4, 5],"float16"), repeats=2, )  

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 53687092, 4, 5],"float16"), repeats=2, )   
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 9.59 GiB is free. Process 120775 has 69.60 GiB memory in use. Of the allocated memory 67.50 GiB is allocated by PyTorch, and 522.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 21:03:27.889612 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 7130317, 4, 4, 5],"int32"), repeats=2, axis=3, )  

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 7130317, 4, 4, 5],"int32"), repeats=2, axis=3, )   
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 9.59 GiB is free. Process 120775 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 21:03:29.691153 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4294967297],"float16"), repeats=3, )  

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4294967297],"float16"), repeats=3, )   
 CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 9.59 GiB is free. Process 120775 has 69.60 GiB memory in use. Of the allocated memory 67.50 GiB is allocated by PyTorch, and 522.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 21:03:31.814396 test begin: paddle.Tensor.repeat_interleave(x=Tensor([536870913, 2, 4],"float16"), repeats=2, )  

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([536870913, 2, 4],"float16"), repeats=2, )   
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 9.59 GiB is free. Process 120775 has 69.60 GiB memory in use. Of the allocated memory 67.50 GiB is allocated by PyTorch, and 522.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-11 21:03:33.393990 test begin: paddle.Tensor.rot90(Tensor([1140850690, 2],"float32"), 1, axes=list[0,1,], )  

2025-03-11 21:03:45.680355 test begin: paddle.Tensor.rot90(Tensor([3, 760567127],"float32"), 1, axes=list[0,1,], )  

W0311 21:05:10.495071 39853 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 21:05:10.496332 39853 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.Tensor.rot90(Tensor([3, 760567127],"float32"), 1, axes=list[0,1,], )  
2025-03-11 21:08:25.465311 test begin: paddle.Tensor.rot90(x=Tensor([1073741825, 4],"float16"), )  

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f8cbe6f5790>,)) (kwargs={}) timed out after 600.000000 seconds.

Exception in thread Thread-2:
Traceback (most recent call last):
  File "/usr/lib/python3.9/threading.py", line 980, in _bootstrap_inner
2025-03-11 21:18:36.790017 test begin: paddle.Tensor.rot90(x=Tensor([1073741825, 4],"float16"), k=-1, )  

W0311 21:20:32.632208 47993 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 21:20:32.633566 47993 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7fe1945b3bb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-11 21:28:49.883740 test begin: paddle.Tensor.rot90(x=Tensor([1073741825, 4],"float16"), k=4, )  

W0311 21:30:36.358757 53848 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 21:30:36.359952 53848 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f66be42dc10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-11 21:38:59.153383 test begin: paddle.Tensor.rot90(x=Tensor([2, 2147483649],"float16"), k=-4, )  

W0311 21:40:50.759454 59282 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 21:40:50.761188 59282 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7feeaef61250>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-11 21:49:09.566297 test begin: paddle.Tensor.rot90(x=Tensor([2147483649, 2],"float16"), k=-4, )  

W0311 21:50:49.553282 64621 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 21:50:49.554481 64621 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f9e6b19ebb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-11 21:59:17.629748 test begin: paddle.Tensor.rot90(x=Tensor([268435457, 4, 4],"float16"), )  

W0311 22:00:57.393281 69890 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 22:00:57.394382 69890 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f59139b5bb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-11 22:09:27.774003 test begin: paddle.Tensor.rot90(x=Tensor([3, 1431655766],"float16"), )  

W0311 22:11:38.217988 75292 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 22:11:38.219496 75292 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f1501a6c250>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-11 22:19:38.594219 test begin: paddle.Tensor.rot90(x=Tensor([4, 1073741825],"float16"), k=-1, )  

W0311 22:21:19.696882 80566 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 22:21:19.698120 80566 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f5a9ed61250>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-11 22:29:50.256248 test begin: paddle.Tensor.rot90(x=Tensor([4, 1073741825],"float16"), k=4, )  

W0311 22:31:45.431552 85843 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 22:31:45.433290 85843 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7fe1be5b4be0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-11 22:39:58.950987 test begin: paddle.Tensor.rot90(x=Tensor([4, 268435457, 4],"float16"), )  

W0311 22:41:43.978669 91156 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 22:41:43.980007 91156 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f171d9b6250>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   THPVariable_subclass_dealloc(_object*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741704608 (unix time) try "date -d @1741704608" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x163b6) received by PID 91062 (TID 0x7f16ddd0b700) from PID 91062 ***]

2025-03-11 22:50:54.323743 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 268435457],"float16"), )  

W0311 22:52:42.177744 96767 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 22:52:42.178903 96767 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f2509a6c250>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741705255 (unix time) try "date -d @1741705255" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x17995) received by PID 96661 (TID 0x7f24bddc2700) from PID 96661 ***]

2025-03-11 23:01:39.227392 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 4, 67108865],"float16"), )  

W0311 23:03:42.164402 102285 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 23:03:42.165722 102285 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f46da5a9d60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-11 23:11:51.005021 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 4, 67108865],"float16"), k=-1, axes=list[1,2,], )  

W0311 23:14:00.343165 107702 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 23:14:00.344727 107702 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f6355a6cc10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-11 23:22:02.557221 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 4, 67108865],"float16"), k=-1, axes=tuple(2,3,), )  

W0311 23:23:46.851534 113157 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 23:23:46.852891 113157 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f8165bf3c10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-11 23:32:16.525631 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 67108865, 4],"float16"), )  

W0311 23:34:18.485899 118722 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 23:34:18.487068 118722 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7fe0ccff3250>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-11 23:42:31.039517 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 67108865, 4],"float16"), k=-1, axes=list[1,2,], )  

W0311 23:44:21.154003 124091 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 23:44:21.155238 124091 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f448c42ed60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-11 23:52:43.018772 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 67108865, 4],"float16"), k=-1, axes=tuple(2,3,), )  

W0311 23:54:55.874239 129042 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 23:54:55.876396 129042 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7fefd9a6cd60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 00:02:58.546786 test begin: paddle.Tensor.rot90(x=Tensor([4, 570425345],"float32"), )  

W0312 00:04:27.684482 134079 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 00:04:27.685640 134079 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.Tensor.rot90(x=Tensor([4, 570425345],"float32"), )  
2025-03-12 00:07:32.324166 test begin: paddle.Tensor.rot90(x=Tensor([4, 67108865, 4, 4],"float16"), )  

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f6f9cd9abe0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 00:17:45.642781 test begin: paddle.Tensor.rot90(x=Tensor([4, 67108865, 4, 4],"float16"), k=-1, axes=list[1,2,], )  

W0312 00:19:26.724999 142346 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 00:19:26.726351 142346 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7fdbc95dfd60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 00:27:58.169556 test begin: paddle.Tensor.rot90(x=Tensor([4, 67108865, 4, 4],"float16"), k=-1, axes=tuple(2,3,), )  

W0312 00:29:55.225550 147910 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 00:29:55.226931 147910 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f118a42dd60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 00:38:09.789292 test begin: paddle.Tensor.rot90(x=Tensor([570425345, 4],"float32"), )  

W0312 00:39:39.298569 153526 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 00:39:39.299794 153526 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.Tensor.rot90(x=Tensor([570425345, 4],"float32"), )  
2025-03-12 00:42:45.766934 test begin: paddle.Tensor.rot90(x=Tensor([67108865, 4, 4, 4],"float16"), )  

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f005e900790>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 00:52:57.787245 test begin: paddle.Tensor.rot90(x=Tensor([67108865, 4, 4, 4],"float16"), k=-1, axes=list[1,2,], )  

W0312 00:54:43.297178 161568 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 00:54:43.298171 161568 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7fb31b19e250>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 01:03:07.819830 test begin: paddle.Tensor.rot90(x=Tensor([67108865, 4, 4, 4],"float16"), k=-1, axes=tuple(2,3,), )  

W0312 01:04:53.479945  3679 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 01:04:53.481185  3679 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7faee75e0250>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 01:13:16.856792 test begin: paddle.Tensor.signbit(Tensor([107374183, 20, 2],"float16"), )  

W0312 01:16:38.953637  9166 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 01:16:38.954823  9166 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
element 0 of tensors does not require grad and does not have a grad_fn
[Pass] paddle.Tensor.signbit(Tensor([107374183, 20, 2],"float16"), )  
2025-03-12 01:21:49.061687 test begin: paddle.Tensor.signbit(Tensor([12, 178956971, 2],"float16"), )  

element 0 of tensors does not require grad and does not have a grad_fn
[Pass] paddle.Tensor.signbit(Tensor([12, 178956971, 2],"float16"), )  
2025-03-12 01:27:41.998464 test begin: paddle.Tensor.signbit(Tensor([12, 20, 17895698],"float16"), )  

element 0 of tensors does not require grad and does not have a grad_fn
[Pass] paddle.Tensor.signbit(Tensor([12, 20, 17895698],"float16"), )  
2025-03-12 01:33:06.318937 test begin: paddle.Tensor.signbit(Tensor([12, 20, 9507090],"float32"), )  

element 0 of tensors does not require grad and does not have a grad_fn
[Pass] paddle.Tensor.signbit(Tensor([12, 20, 9507090],"float32"), )  
2025-03-12 01:37:17.517542 test begin: paddle.Tensor.signbit(Tensor([12, 95070891, 2],"float32"), )  

element 0 of tensors does not require grad and does not have a grad_fn
[Pass] paddle.Tensor.signbit(Tensor([12, 95070891, 2],"float32"), )  
2025-03-12 01:40:06.457612 test begin: paddle.Tensor.signbit(Tensor([57042535, 20, 2],"float32"), )  

element 0 of tensors does not require grad and does not have a grad_fn
[Pass] paddle.Tensor.signbit(Tensor([57042535, 20, 2],"float32"), )  
2025-03-12 01:43:00.762368 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 1, 2281701379],"float32"), axis=list[0,2,], )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 1, 1, 2281701379],"float32"), axis=list[0,2,], )  
2025-03-12 01:46:03.132484 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 1, 2281701379],"float32"), list[1,2,], )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 1, 1, 2281701379],"float32"), list[1,2,], )  
2025-03-12 01:48:52.885899 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 1, 2281701379],"int64"), -2, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 1, 1, 2281701379],"int64"), -2, )  
2025-03-12 01:54:12.289935 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 10, 228170138],"float32"), 0, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 1, 10, 228170138],"float32"), 0, )  
2025-03-12 01:57:33.657078 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 114085069, 20],"int64"), -2, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 1, 114085069, 20],"int64"), -2, )  
2025-03-12 02:01:00.987462 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 12, 190141782],"float32"), 0, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 1, 12, 190141782],"float32"), 0, )  
2025-03-12 02:04:13.280351 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 181896, 12544],"float32"), list[1,2,], )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 1, 181896, 12544],"float32"), list[1,2,], )  
2025-03-12 02:07:26.944456 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 21504, 106106],"float32"), 1, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 1, 21504, 106106],"float32"), 1, )  
2025-03-12 02:10:34.312633 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 2281701379],"float32"), 0, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 1, 2281701379],"float32"), 0, )  
2025-03-12 02:13:09.986295 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 2281701379],"int32"), -1, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 1, 2281701379],"int32"), -1, )  
2025-03-12 02:16:30.038161 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 24276, 93991],"float32"), 1, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 1, 24276, 93991],"float32"), 1, )  
2025-03-12 02:19:09.695193 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 253522376, 9],"float32"), 0, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 1, 253522376, 9],"float32"), 0, )  
2025-03-12 02:22:14.206916 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 285212673, 8],"float32"), 0, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 1, 285212673, 8],"float32"), 0, )  
2025-03-12 02:25:32.572812 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 285212673, 8],"float32"), axis=list[0,2,], )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 1, 285212673, 8],"float32"), axis=list[0,2,], )  
2025-03-12 02:28:54.733296 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 570425345, 4],"float32"), 1, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 1, 570425345, 4],"float32"), 1, )  
2025-03-12 02:31:55.572916 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 60632, 37632],"float32"), list[1,2,], )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 1, 60632, 37632],"float32"), list[1,2,], )  
2025-03-12 02:35:26.515243 test begin: paddle.Tensor.squeeze(Tensor([1, 10, 1, 228170138],"float32"), axis=list[0,2,], )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 10, 1, 228170138],"float32"), axis=list[0,2,], )  
2025-03-12 02:38:22.914218 test begin: paddle.Tensor.squeeze(Tensor([1, 10, 228170138],"int32"), -1, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 10, 228170138],"int32"), -1, )  
2025-03-12 02:41:04.875891 test begin: paddle.Tensor.squeeze(Tensor([1, 10, 891290, 256],"float32"), axis=list[0,2,], )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 10, 891290, 256],"float32"), axis=list[0,2,], )  
2025-03-12 02:44:09.774474 test begin: paddle.Tensor.squeeze(Tensor([1, 100, 22817014],"float32"), 0, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 100, 22817014],"float32"), 0, )  
2025-03-12 02:47:21.853432 test begin: paddle.Tensor.squeeze(Tensor([1, 114085069, 1, 20],"int64"), -2, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 114085069, 1, 20],"int64"), -2, )  
2025-03-12 02:50:47.141510 test begin: paddle.Tensor.squeeze(Tensor([1, 1273271, 1792],"float32"), 0, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 1273271, 1792],"float32"), 0, )  
2025-03-12 02:54:00.764209 test begin: paddle.Tensor.squeeze(Tensor([1, 139265, 128, 128],"float32"), )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 139265, 128, 128],"float32"), )  
2025-03-12 02:56:52.693814 test begin: paddle.Tensor.squeeze(Tensor([1, 17, 1048577, 128],"float32"), )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 17, 1048577, 128],"float32"), )  
2025-03-12 03:00:04.051643 test begin: paddle.Tensor.squeeze(Tensor([1, 17, 128, 1048577],"float32"), )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 17, 128, 1048577],"float32"), )  
2025-03-12 03:02:55.148811 test begin: paddle.Tensor.squeeze(Tensor([1, 17, 128, 1973791],"float16"), )  

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f292071b790>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 03:13:06.771852 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 114085069, 20],"int64"), -2, )  

W0312 03:14:27.586701 66815 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 03:14:27.588300 66815 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.Tensor.squeeze(Tensor([1, 1, 114085069, 20],"int64"), -2, )  
2025-03-12 03:16:47.501065 test begin: paddle.Tensor.squeeze(Tensor([1, 10, 891290, 256],"float32"), axis=list[0,2,], )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 10, 891290, 256],"float32"), axis=list[0,2,], )  
2025-03-12 03:20:45.251603 test begin: paddle.Tensor.squeeze(Tensor([1, 100, 22817014],"float32"), 0, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 100, 22817014],"float32"), 0, )  
2025-03-12 03:23:34.248944 test begin: paddle.Tensor.squeeze(Tensor([1, 17, 128, 1973791],"float16"), )  

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7fba61c60be0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 03:33:45.746520 test begin: paddle.Tensor.squeeze(Tensor([1, 100, 22817014],"float32"), 0, )  

W0312 03:35:07.998220 76673 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 03:35:07.999380 76673 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.Tensor.squeeze(Tensor([1, 100, 22817014],"float32"), 0, )  
2025-03-12 03:37:23.981402 test begin: paddle.Tensor.squeeze(Tensor([1, 17, 1973791, 128],"float16"), )  

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f2edc1cfbe0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 03:47:33.346219 test begin: paddle.Tensor.squeeze(Tensor([1, 100, 22817014],"float32"), 0, )  

W0312 03:49:08.185772 83678 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 03:49:08.187831 83678 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.Tensor.squeeze(Tensor([1, 100, 22817014],"float32"), 0, )  
2025-03-12 03:51:48.191271 test begin: paddle.Tensor.squeeze(Tensor([1, 181896, 1, 12544],"float32"), -2, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 181896, 1, 12544],"float32"), -2, )  
2025-03-12 03:55:03.918201 test begin: paddle.Tensor.squeeze(Tensor([1, 181896, 1, 12544],"float32"), list[1,2,], )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 181896, 1, 12544],"float32"), list[1,2,], )  
2025-03-12 03:58:10.053608 test begin: paddle.Tensor.squeeze(Tensor([1, 2, 32, 32, 2097153],"float16"), 0, )  

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f990f9b34f0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 04:08:21.820311 test begin: paddle.Tensor.squeeze(Tensor([1, 100, 22817014],"float32"), 0, )  

W0312 04:09:49.100796 93623 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 04:09:49.101960 93623 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.Tensor.squeeze(Tensor([1, 100, 22817014],"float32"), 0, )  
2025-03-12 04:12:23.177042 test begin: paddle.Tensor.squeeze(Tensor([1, 181896, 1, 12544],"float32"), -2, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 181896, 1, 12544],"float32"), -2, )  
2025-03-12 04:15:41.504038 test begin: paddle.Tensor.squeeze(Tensor([1, 2, 32, 32, 2097153],"float16"), 0, )  

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f7b9360f520>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741724746 (unix time) try "date -d @1741724746" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x16d66) received by PID 93542 (TID 0x7f7b707c3700) from PID 93542 ***]

2025-03-12 04:26:33.740337 test begin: paddle.Tensor.squeeze(Tensor([1, 2, 32, 524289, 128],"float16"), 0, )  

W0312 04:28:13.518777 101508 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 04:28:13.519924 101508 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f884daa3be0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 04:36:42.887183 test begin: paddle.Tensor.squeeze(Tensor([1, 2, 524289, 32, 128],"float16"), 0, )  

W0312 04:38:24.843428 105802 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 04:38:24.844630 105802 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f6b5da6cbe0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 04:46:50.258991 test begin: paddle.Tensor.squeeze(Tensor([1, 20, 114085069],"float32"), axis=-1, )  

W0312 04:48:41.017591 110019 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 04:48:41.018788 110019 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.Tensor.squeeze(Tensor([1, 20, 114085069],"float32"), axis=-1, )  
2025-03-12 04:52:10.726649 test begin: paddle.Tensor.squeeze(Tensor([1, 2048, 1114113],"float32"), 2, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 2048, 1114113],"float32"), 2, )  
2025-03-12 04:55:41.143132 test begin: paddle.Tensor.squeeze(Tensor([1, 21126865, 12, 9],"float32"), 0, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 21126865, 12, 9],"float32"), 0, )  
2025-03-12 04:59:48.181181 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379, 1],"float32"), 2, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 2281701379, 1],"float32"), 2, )  
2025-03-12 05:03:37.433591 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379, 1],"float32"), axis=-1, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 2281701379, 1],"float32"), axis=-1, )  
2025-03-12 05:07:23.808092 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379, 1],"int32"), -1, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 2281701379, 1],"int32"), -1, )  
2025-03-12 05:10:49.018204 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379],"float32"), )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 2281701379],"float32"), )  
2025-03-12 05:14:34.954403 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379],"float32"), -1, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 2281701379],"float32"), -1, )  
2025-03-12 05:18:37.937442 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379],"float32"), 0, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 2281701379],"float32"), 0, )  
2025-03-12 05:22:17.749976 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379],"float32"), axis=-1, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 2281701379],"float32"), axis=-1, )  
2025-03-12 05:26:35.415976 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379],"int32"), -1, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 2281701379],"int32"), -1, )  
2025-03-12 05:29:07.888317 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379],"int32"), 0, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 2281701379],"int32"), 0, )  
2025-03-12 05:31:49.168037 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379],"int64"), 1, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 2281701379],"int64"), 1, )  
2025-03-12 05:36:43.055993 test begin: paddle.Tensor.squeeze(Tensor([1, 23498, 24276, 4],"float32"), 1, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 23498, 24276, 4],"float32"), 1, )  
2025-03-12 05:40:18.065478 test begin: paddle.Tensor.squeeze(Tensor([1, 262145, 128, 128],"float16"), )  

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f1265471970>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 05:50:33.550262 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379, 1],"float32"), 2, )  

W0312 05:52:00.374598 135716 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 05:52:00.375833 135716 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.Tensor.squeeze(Tensor([1, 2281701379, 1],"float32"), 2, )  
2025-03-12 05:54:23.097223 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379],"int32"), -1, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 2281701379],"int32"), -1, )  
2025-03-12 05:57:53.801279 test begin: paddle.Tensor.squeeze(Tensor([1, 23498, 24276, 4],"float32"), 1, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 23498, 24276, 4],"float32"), 1, )  
2025-03-12 06:00:42.087861 test begin: paddle.Tensor.squeeze(Tensor([1, 262145, 128, 128],"float16"), )  

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f39d15df310>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 06:10:51.028636 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379],"int32"), -1, )  

W0312 06:11:52.927258 143993 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 06:11:52.928311 143993 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.Tensor.squeeze(Tensor([1, 2281701379],"int32"), -1, )  
2025-03-12 06:13:40.946351 test begin: paddle.Tensor.squeeze(Tensor([1, 26527, 21504, 4],"float32"), 1, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 26527, 21504, 4],"float32"), 1, )  
2025-03-12 06:17:33.924900 test begin: paddle.Tensor.squeeze(Tensor([1, 285212673, 1, 8],"float32"), axis=list[0,2,], )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 285212673, 1, 8],"float32"), axis=list[0,2,], )  
2025-03-12 06:20:35.362468 test begin: paddle.Tensor.squeeze(Tensor([1, 28521268, 10, 8],"float32"), 0, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 28521268, 10, 8],"float32"), 0, )  
2025-03-12 06:23:41.621515 test begin: paddle.Tensor.squeeze(Tensor([1, 300, 1, 7605672],"float32"), -2, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 300, 1, 7605672],"float32"), -2, )  
2025-03-12 06:26:32.118630 test begin: paddle.Tensor.squeeze(Tensor([1, 300, 607, 12544],"float32"), -2, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 300, 607, 12544],"float32"), -2, )  
2025-03-12 06:29:29.916477 test begin: paddle.Tensor.squeeze(Tensor([1, 32769, 32, 32, 128],"float16"), 0, )  

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f545d2b53d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 06:39:41.188434 test begin: paddle.Tensor.squeeze(Tensor([1, 300, 1, 7605672],"float32"), -2, )  

W0312 06:41:32.177124 155798 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 06:41:32.178359 155798 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.Tensor.squeeze(Tensor([1, 300, 1, 7605672],"float32"), -2, )  
2025-03-12 06:44:03.898050 test begin: paddle.Tensor.squeeze(Tensor([1, 300, 607, 12544],"float32"), -2, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 300, 607, 12544],"float32"), -2, )  
2025-03-12 06:46:55.190337 test begin: paddle.Tensor.squeeze(Tensor([1, 570425345, 4],"float32"), 0, )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 570425345, 4],"float32"), 0, )  
2025-03-12 06:49:55.306298 test begin: paddle.Tensor.squeeze(Tensor([1, 60632, 1, 37632],"float32"), list[1,2,], )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 60632, 1, 37632],"float32"), list[1,2,], )  
2025-03-12 06:53:06.011100 test begin: paddle.Tensor.squeeze(Tensor([1, 8912897, 1, 256],"float32"), axis=list[0,2,], )  

[Pass] paddle.Tensor.squeeze(Tensor([1, 8912897, 1, 256],"float32"), axis=list[0,2,], )  
2025-03-12 06:56:37.799854 test begin: paddle.Tensor.squeeze(Tensor([11, 1, 207427399],"float32"), 1, )  

[Pass] paddle.Tensor.squeeze(Tensor([11, 1, 207427399],"float32"), 1, )  
2025-03-12 06:59:25.978441 test begin: paddle.Tensor.squeeze(Tensor([11, 51856850, 4],"float32"), 1, )  

[Pass] paddle.Tensor.squeeze(Tensor([11, 51856850, 4],"float32"), 1, )  
2025-03-12 07:03:57.862574 test begin: paddle.Tensor.squeeze(Tensor([1114113, 2048, 1],"float32"), 2, )  

[Pass] paddle.Tensor.squeeze(Tensor([1114113, 2048, 1],"float32"), 2, )  
2025-03-12 07:07:14.092352 test begin: paddle.Tensor.squeeze(Tensor([111412, 512, 1, 40],"float32"), 2, )  

[Pass] paddle.Tensor.squeeze(Tensor([111412, 512, 1, 40],"float32"), 2, )  
2025-03-12 07:10:34.291860 test begin: paddle.Tensor.squeeze(Tensor([114085069, 1, 1, 20],"int64"), -2, )  

[Pass] paddle.Tensor.squeeze(Tensor([114085069, 1, 1, 20],"int64"), -2, )  
2025-03-12 07:15:09.591321 test begin: paddle.Tensor.squeeze(Tensor([114085069, 20, 1],"float32"), axis=-1, )  

[Pass] paddle.Tensor.squeeze(Tensor([114085069, 20, 1],"float32"), axis=-1, )  
2025-03-12 07:19:23.281212 test begin: paddle.Tensor.squeeze(Tensor([114085069, 20],"float32"), axis=-1, )  

[Pass] paddle.Tensor.squeeze(Tensor([114085069, 20],"float32"), axis=-1, )  
2025-03-12 07:23:03.720458 test begin: paddle.Tensor.squeeze(Tensor([1273271, 1, 1792],"float32"), 0, )  

[Pass] paddle.Tensor.squeeze(Tensor([1273271, 1, 1792],"float32"), 0, )  
2025-03-12 07:27:12.012561 test begin: paddle.Tensor.squeeze(Tensor([1273271, 1, 1792],"float32"), 1, )  

[Pass] paddle.Tensor.squeeze(Tensor([1273271, 1, 1792],"float32"), 1, )  
2025-03-12 07:30:51.532449 test begin: paddle.Tensor.squeeze(Tensor([128, 120, 1, 148549],"float32"), axis=2, )  

[Pass] paddle.Tensor.squeeze(Tensor([128, 120, 1, 148549],"float32"), axis=2, )  
2025-03-12 07:33:47.824645 test begin: paddle.Tensor.squeeze(Tensor([128, 120, 1, 279621],"float16"), axis=2, )  

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f6ab6fa4970>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741736628 (unix time) try "date -d @1741736628" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2603d) received by PID 155709 (TID 0x7f6a69abb700) from PID 155709 ***]

2025-03-12 07:44:36.398125 test begin: paddle.Tensor.squeeze(Tensor([1, 8912897, 1, 256],"float32"), axis=list[0,2,], )  

W0312 07:46:19.621344 19859 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 07:46:19.622568 19859 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.Tensor.squeeze(Tensor([1, 8912897, 1, 256],"float32"), axis=list[0,2,], )  
2025-03-12 07:49:38.013693 test begin: paddle.Tensor.squeeze(Tensor([11, 51856850, 4],"float32"), 1, )  

[Pass] paddle.Tensor.squeeze(Tensor([11, 51856850, 4],"float32"), 1, )  
2025-03-12 07:53:24.995458 test begin: paddle.Tensor.squeeze(Tensor([111412, 512, 1, 40],"float32"), 2, )  

[Pass] paddle.Tensor.squeeze(Tensor([111412, 512, 1, 40],"float32"), 2, )  
2025-03-12 07:57:02.387593 test begin: paddle.Tensor.squeeze(Tensor([114085069, 1, 1, 20],"int64"), -2, )  

[Pass] paddle.Tensor.squeeze(Tensor([114085069, 1, 1, 20],"int64"), -2, )  
2025-03-12 08:01:19.120043 test begin: paddle.Tensor.squeeze(Tensor([114085069, 20],"float32"), axis=-1, )  

[Pass] paddle.Tensor.squeeze(Tensor([114085069, 20],"float32"), axis=-1, )  
2025-03-12 08:05:09.723265 test begin: paddle.Tensor.squeeze(Tensor([1273271, 1, 1792],"float32"), 1, )  

[Pass] paddle.Tensor.squeeze(Tensor([1273271, 1, 1792],"float32"), 1, )  
2025-03-12 08:08:47.341833 test begin: paddle.Tensor.squeeze(Tensor([128, 120, 3714, 40],"float32"), axis=2, )  

[Pass] paddle.Tensor.squeeze(Tensor([128, 120, 3714, 40],"float32"), axis=2, )  
2025-03-12 08:12:09.381282 test begin: paddle.Tensor.squeeze(Tensor([128, 120, 6991, 40],"float16"), axis=2, )  

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7fd684ef7e50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 08:22:23.667642 test begin: paddle.Tensor.squeeze(Tensor([111412, 512, 1, 40],"float32"), 2, )  

W0312 08:23:53.135816 35972 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 08:23:53.136970 35972 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.Tensor.squeeze(Tensor([111412, 512, 1, 40],"float32"), 2, )  
2025-03-12 08:26:50.101708 test begin: paddle.Tensor.squeeze(Tensor([1273271, 1, 1792],"float32"), 1, )  

[Pass] paddle.Tensor.squeeze(Tensor([1273271, 1, 1792],"float32"), 1, )  
2025-03-12 08:30:17.955849 test begin: paddle.Tensor.squeeze(Tensor([128, 120, 3714, 40],"float32"), axis=2, )  

[Pass] paddle.Tensor.squeeze(Tensor([128, 120, 3714, 40],"float32"), axis=2, )  
2025-03-12 08:33:41.962227 test begin: paddle.Tensor.squeeze(Tensor([128, 445645, 1, 40],"float32"), axis=2, )  

[Pass] paddle.Tensor.squeeze(Tensor([128, 445645, 1, 40],"float32"), axis=2, )  
2025-03-12 08:37:07.462107 test begin: paddle.Tensor.squeeze(Tensor([128, 838861, 1, 40],"float16"), axis=2, )  

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f142953d880>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 08:47:19.001912 test begin: paddle.Tensor.squeeze(Tensor([1273271, 1, 1792],"float32"), 1, )  

W0312 08:48:54.932982 46746 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 08:48:54.934263 46746 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.Tensor.squeeze(Tensor([1273271, 1, 1792],"float32"), 1, )  
2025-03-12 08:51:33.676722 test begin: paddle.Tensor.squeeze(Tensor([12988, 1, 175678],"float32"), )  

[Pass] paddle.Tensor.squeeze(Tensor([12988, 1, 175678],"float32"), )  
2025-03-12 08:55:05.523157 test begin: paddle.Tensor.squeeze(Tensor([12988, 1, 330688],"float16"), )  

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7fce71cf66d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 09:05:19.488410 test begin: paddle.Tensor.squeeze(Tensor([1273271, 1, 1792],"float32"), 1, )  

W0312 09:06:47.199868 55688 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 09:06:47.200973 55688 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.Tensor.squeeze(Tensor([1273271, 1, 1792],"float32"), 1, )  
2025-03-12 09:09:21.516039 test begin: paddle.Tensor.squeeze(Tensor([12988, 2745, 64],"float32"), )  

[Pass] paddle.Tensor.squeeze(Tensor([12988, 2745, 64],"float32"), )  
2025-03-12 09:12:30.580954 test begin: paddle.Tensor.squeeze(Tensor([12988, 5167, 64],"float16"), )  

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f2171b79d30>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741742554 (unix time) try "date -d @1741742554" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xd8d2) received by PID 55506 (TID 0x7f22f07c3700) from PID 55506 ***]

2025-03-12 09:23:20.046973 test begin: paddle.Tensor.squeeze(Tensor([1273271, 1, 1792],"float32"), 1, )  

W0312 09:24:49.346437 63339 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 09:24:49.347558 63339 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.Tensor.squeeze(Tensor([1273271, 1, 1792],"float32"), 1, )  
2025-03-12 09:27:39.866486 test begin: paddle.Tensor.squeeze(Tensor([13, 1, 175515491, 1],"float32"), -1, )  

[Pass] paddle.Tensor.squeeze(Tensor([13, 1, 175515491, 1],"float32"), -1, )  
2025-03-12 09:31:22.968444 test begin: paddle.Tensor.squeeze(Tensor([13, 1, 96, 1828287],"float32"), -1, )  

[Pass] paddle.Tensor.squeeze(Tensor([13, 1, 96, 1828287],"float32"), -1, )  
2025-03-12 09:34:42.979565 test begin: paddle.Tensor.squeeze(Tensor([13, 175515491, 1],"float32"), axis=-1, )  

[Pass] paddle.Tensor.squeeze(Tensor([13, 175515491, 1],"float32"), axis=-1, )  
2025-03-12 09:38:38.882593 test begin: paddle.Tensor.squeeze(Tensor([13, 175515491],"float32"), axis=-1, )  

[Pass] paddle.Tensor.squeeze(Tensor([13, 175515491],"float32"), axis=-1, )  
2025-03-12 09:41:44.272369 test begin: paddle.Tensor.squeeze(Tensor([13, 1828287, 96, 1],"float32"), -1, )  

[Pass] paddle.Tensor.squeeze(Tensor([13, 1828287, 96, 1],"float32"), -1, )  
2025-03-12 09:45:20.818860 test begin: paddle.Tensor.squeeze(Tensor([13, 2, 64, 1371215],"float32"), -1, )  

[Pass] paddle.Tensor.squeeze(Tensor([13, 2, 64, 1371215],"float32"), -1, )  
2025-03-12 09:48:27.720321 test begin: paddle.Tensor.squeeze(Tensor([13, 2, 87757746, 1],"float32"), -1, )  

[Pass] paddle.Tensor.squeeze(Tensor([13, 2, 87757746, 1],"float32"), -1, )  
2025-03-12 09:52:14.392248 test begin: paddle.Tensor.squeeze(Tensor([13, 2742430, 64, 1],"float32"), -1, )  

[Pass] paddle.Tensor.squeeze(Tensor([13, 2742430, 64, 1],"float32"), -1, )  
2025-03-12 09:56:42.591733 test begin: paddle.Tensor.squeeze(Tensor([13, 7, 25073642],"float32"), axis=-1, )  

[Pass] paddle.Tensor.squeeze(Tensor([13, 7, 25073642],"float32"), axis=-1, )  
2025-03-12 10:00:46.915678 test begin: paddle.Tensor.squeeze(Tensor([15421, 17, 128, 128],"float16"), )  

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f0f51421970>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 10:10:55.864487 test begin: paddle.Tensor.squeeze(Tensor([1273271, 1, 1792],"float32"), 1, )  

W0312 10:12:31.674966 86737 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 10:12:31.676304 86737 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.Tensor.squeeze(Tensor([1273271, 1, 1792],"float32"), 1, )  
2025-03-12 10:16:01.468351 test begin: paddle.Tensor.squeeze(Tensor([13, 1, 175515491, 1],"float32"), -1, )  

[Pass] paddle.Tensor.squeeze(Tensor([13, 1, 175515491, 1],"float32"), -1, )  
2025-03-12 10:20:04.487605 test begin: paddle.Tensor.squeeze(Tensor([16, 10, 111412, 64, 2, 1],"float32"), -1, )  

[Pass] paddle.Tensor.squeeze(Tensor([16, 10, 111412, 64, 2, 1],"float32"), -1, )  
2025-03-12 10:24:16.797114 test begin: paddle.Tensor.squeeze(Tensor([16, 10, 64, 111412, 2, 1],"float32"), -1, )  

[Pass] paddle.Tensor.squeeze(Tensor([16, 10, 64, 111412, 2, 1],"float32"), -1, )  
2025-03-12 10:28:44.156345 test begin: paddle.Tensor.squeeze(Tensor([16, 10, 64, 64, 2, 1741],"float32"), -1, )  

[Pass] paddle.Tensor.squeeze(Tensor([16, 10, 64, 64, 2, 1741],"float32"), -1, )  
2025-03-12 10:31:28.486081 test begin: paddle.Tensor.squeeze(Tensor([16, 10, 64, 64, 3482, 1],"float32"), -1, )  

[Pass] paddle.Tensor.squeeze(Tensor([16, 10, 64, 64, 3482, 1],"float32"), -1, )  
2025-03-12 10:34:36.012326 test begin: paddle.Tensor.squeeze(Tensor([16, 17409, 64, 64, 2, 1],"float32"), -1, )  

[Pass] paddle.Tensor.squeeze(Tensor([16, 17409, 64, 64, 2, 1],"float32"), -1, )  
2025-03-12 10:37:41.519867 test begin: paddle.Tensor.squeeze(Tensor([16385, 2, 32, 32, 128],"float16"), 0, )  

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 86, in func_timeout
    thread.join(timeout)
  File "/usr/lib/python3.9/threading.py", line 1064, in join
    self._wait_for_tstate_lock(timeout=max(timeout, 0))
  File "/usr/lib/python3.9/threading.py", line 1080, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt
2025-03-12 10:48:34.573957 test begin: paddle.Tensor.amax(Tensor([2281701379],"int32"), axis=None, keepdim=False, )

W0312 10:49:23.288450 104563 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 10:49:23.289541 104563 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.Tensor.amax(Tensor([2281701379],"int32"), axis=None, keepdim=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 32932
Max relative difference: 0.50251778
 x: array(32602, dtype=int32)
 y: array(65534, dtype=int32)
2025-03-12 10:49:23.302308 test begin: paddle.Tensor.amax(Tensor([2281701379],"int64"), axis=None, keepdim=False, )

[accuracy error] paddle.Tensor.amax(Tensor([2281701379],"int64"), axis=None, keepdim=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 139890238881794
Max relative difference: 2.13462079e+09
 x: array(139890238947328)
 y: array(65534)
2025-03-12 10:50:23.109073 test begin: paddle.Tensor.amax(Tensor([3, 2, 4, 95070891],"float32"), axis=-1, keepdim=True, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_amax(_object*, _object*, _object*)
1   amax_ad_func(paddle::Tensor const&, std::vector<long, std::allocator<long> >, bool)
2   paddle::experimental::amax(paddle::Tensor const&, std::vector<long, std::allocator<long> > const&, bool)
3   void phi::AMaxRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<long, std::allocator<long> > const&, bool, bool, phi::DenseTensor*)
4   void phi::Reduce<float, phi::kps::MaxFunctor, phi::kps::IdentityFunctor, false>(phi::GPUContext const&, phi::DenseTensor const&, bool, std::vector<long, std::allocator<long> > const&, bool, phi::DataType, phi::DenseTensor*)
5   void phi::funcs::ReduceKernel<float, float, phi::kps::MaxFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741747906 (unix time) try "date -d @1741747906" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1980d) received by PID 104461 (TID 0x7f5c1f2b7700) from PID 104461 ***]

2025-03-12 10:52:29.881329 test begin: paddle.Tensor.amax(Tensor([3, 2, 95070891, 4],"float32"), axis=2, keepdim=True, )

W0312 10:53:48.957170 106857 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 10:53:48.958266 106857 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_amax(_object*, _object*, _object*)
1   amax_ad_func(paddle::Tensor const&, std::vector<long, std::allocator<long> >, bool)
2   paddle::experimental::amax(paddle::Tensor const&, std::vector<long, std::allocator<long> > const&, bool)
3   void phi::AMaxRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<long, std::allocator<long> > const&, bool, bool, phi::DenseTensor*)
4   void phi::Reduce<float, phi::kps::MaxFunctor, phi::kps::IdentityFunctor, false>(phi::GPUContext const&, phi::DenseTensor const&, bool, std::vector<long, std::allocator<long> > const&, bool, phi::DataType, phi::DenseTensor*)
5   void phi::funcs::ReduceKernel<float, float, phi::kps::MaxFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741748029 (unix time) try "date -d @1741748029" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1a0fc) received by PID 106748 (TID 0x7f74072b7700) from PID 106748 ***]

2025-03-12 10:54:33.585768 test begin: paddle.Tensor.amax(Tensor([3, 2, 95070891, 4],"float32"), axis=tuple(1,2,), keepdim=True, )

W0312 10:55:58.054216 108178 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 10:55:58.055409 108178 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_amax(_object*, _object*, _object*)
1   amax_ad_func(paddle::Tensor const&, std::vector<long, std::allocator<long> >, bool)
2   paddle::experimental::amax(paddle::Tensor const&, std::vector<long, std::allocator<long> > const&, bool)
3   void phi::AMaxRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<long, std::allocator<long> > const&, bool, bool, phi::DenseTensor*)
4   void phi::Reduce<float, phi::kps::MaxFunctor, phi::kps::IdentityFunctor, false>(phi::GPUContext const&, phi::DenseTensor const&, bool, std::vector<long, std::allocator<long> > const&, bool, phi::DataType, phi::DenseTensor*)
5   void phi::funcs::ReduceKernel<float, float, phi::kps::MaxFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741748158 (unix time) try "date -d @1741748158" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1a625) received by PID 108069 (TID 0x7f5d45935700) from PID 108069 ***]

2025-03-12 10:56:40.150368 test begin: paddle.Tensor.amax(Tensor([3, 38028357, 5, 4],"float32"), axis=tuple(1,2,), keepdim=True, )

W0312 10:57:56.723214 109304 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 10:57:56.724422 109304 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_amax(_object*, _object*, _object*)
1   amax_ad_func(paddle::Tensor const&, std::vector<long, std::allocator<long> >, bool)
2   paddle::experimental::amax(paddle::Tensor const&, std::vector<long, std::allocator<long> > const&, bool)
3   void phi::AMaxRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<long, std::allocator<long> > const&, bool, bool, phi::DenseTensor*)
4   void phi::Reduce<float, phi::kps::MaxFunctor, phi::kps::IdentityFunctor, false>(phi::GPUContext const&, phi::DenseTensor const&, bool, std::vector<long, std::allocator<long> > const&, bool, phi::DataType, phi::DenseTensor*)
5   void phi::funcs::ReduceKernel<float, float, phi::kps::MaxFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741748277 (unix time) try "date -d @1741748277" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1aa91) received by PID 109201 (TID 0x7f2da7f48700) from PID 109201 ***]

2025-03-12 10:58:39.929469 test begin: paddle.Tensor.amin(Tensor([2281701379],"int32"), axis=None, keepdim=False, )

W0312 10:59:35.048389 110494 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 10:59:35.049322 110494 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.Tensor.amin(Tensor([2281701379],"int32"), axis=None, keepdim=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 98292
Max relative difference: 1.49983978
 x: array(32757, dtype=int32)
 y: array(-65535, dtype=int32)
2025-03-12 10:59:35.063237 test begin: paddle.Tensor.amin(Tensor([2281701379],"int64"), axis=None, keepdim=False, )

[accuracy error] paddle.Tensor.amin(Tensor([2281701379],"int64"), axis=None, keepdim=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1 / 1 (100%)
Max absolute difference: 140555287855103
Max relative difference: 2.14473622e+09
 x: array(140555287789568)
 y: array(-65535)
2025-03-12 11:00:41.925813 test begin: paddle.Tensor.amin(Tensor([3, 2, 4, 95070891],"float32"), axis=-1, keepdim=True, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_amin(_object*, _object*, _object*)
1   amin_ad_func(paddle::Tensor const&, std::vector<long, std::allocator<long> >, bool)
2   paddle::experimental::amin(paddle::Tensor const&, std::vector<long, std::allocator<long> > const&, bool)
3   void phi::AMinRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<long, std::allocator<long> > const&, bool, bool, phi::DenseTensor*)
4   void phi::Reduce<float, phi::kps::MinFunctor, phi::kps::IdentityFunctor, false>(phi::GPUContext const&, phi::DenseTensor const&, bool, std::vector<long, std::allocator<long> > const&, bool, phi::DataType, phi::DenseTensor*)
5   void phi::funcs::ReduceKernel<float, float, phi::kps::MinFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741748509 (unix time) try "date -d @1741748509" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1af33) received by PID 110387 (TID 0x7ff707b85700) from PID 110387 ***]

2025-03-12 11:02:37.224875 test begin: paddle.Tensor.amin(Tensor([3, 2, 95070891, 4],"float32"), axis=2, keepdim=True, )

W0312 11:04:07.368472 112622 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 11:04:07.369587 112622 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_amin(_object*, _object*, _object*)
1   amin_ad_func(paddle::Tensor const&, std::vector<long, std::allocator<long> >, bool)
2   paddle::experimental::amin(paddle::Tensor const&, std::vector<long, std::allocator<long> > const&, bool)
3   void phi::AMinRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<long, std::allocator<long> > const&, bool, bool, phi::DenseTensor*)
4   void phi::Reduce<float, phi::kps::MinFunctor, phi::kps::IdentityFunctor, false>(phi::GPUContext const&, phi::DenseTensor const&, bool, std::vector<long, std::allocator<long> > const&, bool, phi::DataType, phi::DenseTensor*)
5   void phi::funcs::ReduceKernel<float, float, phi::kps::MinFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741748647 (unix time) try "date -d @1741748647" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1b78a) received by PID 112522 (TID 0x7ff789f48700) from PID 112522 ***]

2025-03-12 11:04:13.768745 test begin: paddle.Tensor.amin(Tensor([3, 2, 95070891, 4],"float32"), axis=tuple(1,2,), keepdim=True, )

W0312 11:05:32.249198 113538 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 11:05:32.250334 113538 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_amin(_object*, _object*, _object*)
1   amin_ad_func(paddle::Tensor const&, std::vector<long, std::allocator<long> >, bool)
2   paddle::experimental::amin(paddle::Tensor const&, std::vector<long, std::allocator<long> > const&, bool)
3   void phi::AMinRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<long, std::allocator<long> > const&, bool, bool, phi::DenseTensor*)
4   void phi::Reduce<float, phi::kps::MinFunctor, phi::kps::IdentityFunctor, false>(phi::GPUContext const&, phi::DenseTensor const&, bool, std::vector<long, std::allocator<long> > const&, bool, phi::DataType, phi::DenseTensor*)
5   void phi::funcs::ReduceKernel<float, float, phi::kps::MinFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741748732 (unix time) try "date -d @1741748732" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1bb1b) received by PID 113435 (TID 0x7f0f99b85700) from PID 113435 ***]

2025-03-12 11:06:17.322363 test begin: paddle.Tensor.amin(Tensor([3, 38028357, 5, 4],"float32"), axis=tuple(1,2,), keepdim=True, )

W0312 11:07:36.968166 114669 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 11:07:36.969432 114669 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_amin(_object*, _object*, _object*)
1   amin_ad_func(paddle::Tensor const&, std::vector<long, std::allocator<long> >, bool)
2   paddle::experimental::amin(paddle::Tensor const&, std::vector<long, std::allocator<long> > const&, bool)
3   void phi::AMinRawKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, std::vector<long, std::allocator<long> > const&, bool, bool, phi::DenseTensor*)
4   void phi::Reduce<float, phi::kps::MinFunctor, phi::kps::IdentityFunctor, false>(phi::GPUContext const&, phi::DenseTensor const&, bool, std::vector<long, std::allocator<long> > const&, bool, phi::DataType, phi::DenseTensor*)
5   void phi::funcs::ReduceKernel<float, float, phi::kps::MinFunctor, phi::kps::IdentityFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::IdentityFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741748857 (unix time) try "date -d @1741748857" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1bf87) received by PID 114567 (TID 0x7fcfe5dc2700) from PID 114567 ***]

2025-03-12 11:08:23.437204 test begin: paddle.Tensor.argmax(Tensor([1, 67908, 33600],"float32"), axis=-2, )

W0312 11:09:35.654349 115938 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 11:09:35.655195 115938 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.Tensor.argmax(Tensor([1, 67908, 33600],"float32"), axis=-2, )
2025-03-12 11:09:35.722109 test begin: paddle.Tensor.argmax(Tensor([1, 75245, 30324],"float32"), axis=-2, )

[Pass] paddle.Tensor.argmax(Tensor([1, 75245, 30324],"float32"), axis=-2, )
2025-03-12 11:09:38.839042 test begin: paddle.Tensor.argmax(Tensor([1, 83837, 27216],"float32"), axis=-2, )

[Pass] paddle.Tensor.argmax(Tensor([1, 83837, 27216],"float32"), axis=-2, )
2025-03-12 11:09:44.732571 test begin: paddle.Tensor.argmax(Tensor([2743, 104, 8000],"float32"), axis=2, )

[Pass] paddle.Tensor.argmax(Tensor([2743, 104, 8000],"float32"), axis=2, )
2025-03-12 11:09:48.133854 test begin: paddle.Tensor.argmax(Tensor([2770, 103, 8000],"float32"), axis=2, )

[Pass] paddle.Tensor.argmax(Tensor([2770, 103, 8000],"float32"), axis=2, )
2025-03-12 11:09:52.028798 test begin: paddle.Tensor.argmax(Tensor([2797, 102, 8000],"float32"), axis=2, )

[Pass] paddle.Tensor.argmax(Tensor([2797, 102, 8000],"float32"), axis=2, )
2025-03-12 11:09:56.673273 test begin: paddle.Tensor.argmax(Tensor([30, 102, 745655],"float32"), axis=2, )

[Pass] paddle.Tensor.argmax(Tensor([30, 102, 745655],"float32"), axis=2, )
2025-03-12 11:10:01.270691 test begin: paddle.Tensor.argmax(Tensor([30, 103, 738415],"float32"), axis=2, )

[Pass] paddle.Tensor.argmax(Tensor([30, 103, 738415],"float32"), axis=2, )
2025-03-12 11:10:07.320016 test begin: paddle.Tensor.argmax(Tensor([30, 104, 731315],"float32"), axis=2, )

[Pass] paddle.Tensor.argmax(Tensor([30, 104, 731315],"float32"), axis=2, )
2025-03-12 11:10:12.230689 test begin: paddle.Tensor.argmax(Tensor([67908, 1, 33600],"float32"), axis=-2, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741749030 (unix time) try "date -d @1741749030" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1c421) received by PID 115745 (TID 0x7fa0e32b7700) from PID 115745 ***]

2025-03-12 11:11:19.620449 test begin: paddle.Tensor.argmax(Tensor([75245, 1, 30324],"float32"), axis=-2, )

W0312 11:13:11.295827 117535 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 11:13:11.298949 117535 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741749193 (unix time) try "date -d @1741749193" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1caae) received by PID 117422 (TID 0x7f1f5a7c3700) from PID 117422 ***]

2025-03-12 11:13:56.318048 test begin: paddle.Tensor.argmax(Tensor([80, 28521268],"int64"), axis=-1, )

W0312 11:15:35.539772 119055 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 11:15:35.541463 119055 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.Tensor.argmax(Tensor([80, 28521268],"int64"), axis=-1, )
2025-03-12 11:15:35.598614 test begin: paddle.Tensor.argmax(Tensor([83837, 1, 27216],"float32"), axis=-2, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741749437 (unix time) try "date -d @1741749437" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1d0a2) received by PID 118946 (TID 0x7fb09ddc2700) from PID 118946 ***]

2025-03-12 11:18:06.988898 test begin: paddle.Tensor.argsort(Tensor([101862, 22400],"int64"), axis=1, )

W0312 11:19:28.524348 120956 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 11:19:28.525861 120956 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.Tensor.argsort(Tensor([101862, 22400],"int64"), axis=1, ) 
 CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 11.19 GiB is free. Process 44290 has 68.00 GiB memory in use. Of the allocated memory 67.00 GiB is allocated by PyTorch, and 5.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-12 11:19:32.245512 test begin: paddle.Tensor.argsort(Tensor([2281701379],"float32"), )

[torch error] paddle.Tensor.argsort(Tensor([2281701379],"float32"), ) 
 The dimension being sorted can not have more than INT_MAX elements.
2025-03-12 11:20:50.933710 test begin: paddle.Tensor.bmm(Tensor([1, 108472, 21035],"float32"), Tensor([1, 3, 21035],"float32"), )

[torch error] paddle.Tensor.bmm(Tensor([1, 108472, 21035],"float32"), Tensor([1, 3, 21035],"float32"), ) 
 Expected size for first two dimensions of batch2 tensor to be: [1, 21035] but got: [1, 3].
2025-03-12 11:20:52.780297 test begin: paddle.Tensor.bmm(Tensor([1, 108472, 21035],"float32"), Tensor([1, 3, 2],"float32"), )

[torch error] paddle.Tensor.bmm(Tensor([1, 108472, 21035],"float32"), Tensor([1, 3, 2],"float32"), ) 
 Expected size for first two dimensions of batch2 tensor to be: [1, 21035] but got: [1, 3].
2025-03-12 11:20:55.249687 test begin: paddle.Tensor.bmm(Tensor([1, 108472, 3],"float32"), Tensor([1, 1140850690, 2],"float32"), )

[torch error] paddle.Tensor.bmm(Tensor([1, 108472, 3],"float32"), Tensor([1, 1140850690, 2],"float32"), ) 
 Expected size for first two dimensions of batch2 tensor to be: [1, 3] but got: [1, 1140850690].
2025-03-12 11:20:57.495015 test begin: paddle.Tensor.bmm(Tensor([1, 108472, 3],"float32"), Tensor([1, 3, 760567127],"float32"), )

[torch error] paddle.Tensor.bmm(Tensor([1, 108472, 3],"float32"), Tensor([1, 3, 760567127],"float32"), ) 
 CUDA out of memory. Tried to allocate 307337.34 GiB. GPU 0 has a total capacity of 79.18 GiB of which 60.59 GiB is free. Process 44290 has 18.59 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-12 11:20:59.642508 test begin: paddle.Tensor.bmm(Tensor([1, 108472, 3],"float32"), Tensor([380283564, 3, 2],"float32"), )

[torch error] paddle.Tensor.bmm(Tensor([1, 108472, 3],"float32"), Tensor([380283564, 3, 2],"float32"), ) 
 Expected size for first two dimensions of batch2 tensor to be: [1, 3] but got: [380283564, 3].
2025-03-12 11:21:01.997941 test begin: paddle.Tensor.bmm(Tensor([1, 1156, 1973791],"float32"), Tensor([1, 3, 1973791],"float32"), )

[torch error] paddle.Tensor.bmm(Tensor([1, 1156, 1973791],"float32"), Tensor([1, 3, 1973791],"float32"), ) 
 Expected size for first two dimensions of batch2 tensor to be: [1, 1973791] but got: [1, 3].
2025-03-12 11:21:04.415893 test begin: paddle.Tensor.bmm(Tensor([1, 1156, 1973791],"float32"), Tensor([1, 3, 2],"float32"), )

[torch error] paddle.Tensor.bmm(Tensor([1, 1156, 1973791],"float32"), Tensor([1, 3, 2],"float32"), ) 
 Expected size for first two dimensions of batch2 tensor to be: [1, 1973791] but got: [1, 3].
2025-03-12 11:21:06.801247 test begin: paddle.Tensor.bmm(Tensor([1, 1156, 3],"float32"), Tensor([1, 1140850690, 2],"float32"), )

[torch error] paddle.Tensor.bmm(Tensor([1, 1156, 3],"float32"), Tensor([1, 1140850690, 2],"float32"), ) 
 Expected size for first two dimensions of batch2 tensor to be: [1, 3] but got: [1, 1140850690].
2025-03-12 11:21:09.231339 test begin: paddle.Tensor.bmm(Tensor([1, 1156, 3],"float32"), Tensor([1, 3, 760567127],"float32"), )

[torch error] paddle.Tensor.bmm(Tensor([1, 1156, 3],"float32"), Tensor([1, 3, 760567127],"float32"), ) 
 CUDA out of memory. Tried to allocate 3275.33 GiB. GPU 0 has a total capacity of 79.18 GiB of which 60.59 GiB is free. Process 44290 has 18.59 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-12 11:21:12.359556 test begin: paddle.Tensor.bmm(Tensor([1, 1156, 3],"float32"), Tensor([380283564, 3, 2],"float32"), )

[torch error] paddle.Tensor.bmm(Tensor([1, 1156, 3],"float32"), Tensor([380283564, 3, 2],"float32"), ) 
 Expected size for first two dimensions of batch2 tensor to be: [1, 3] but got: [380283564, 3].
2025-03-12 11:21:14.545927 test begin: paddle.Tensor.bmm(Tensor([1, 120000, 19015],"float32"), Tensor([1, 3, 19015],"float32"), )

[torch error] paddle.Tensor.bmm(Tensor([1, 120000, 19015],"float32"), Tensor([1, 3, 19015],"float32"), ) 
 Expected size for first two dimensions of batch2 tensor to be: [1, 19015] but got: [1, 3].
2025-03-12 11:21:16.247959 test begin: paddle.Tensor.bmm(Tensor([1, 120000, 19015],"float32"), Tensor([1, 3, 2],"float32"), )

[torch error] paddle.Tensor.bmm(Tensor([1, 120000, 19015],"float32"), Tensor([1, 3, 2],"float32"), ) 
 Expected size for first two dimensions of batch2 tensor to be: [1, 19015] but got: [1, 3].
2025-03-12 11:21:18.290409 test begin: paddle.Tensor.bmm(Tensor([1, 120000, 3],"float32"), Tensor([1, 1140850690, 2],"float32"), )

[torch error] paddle.Tensor.bmm(Tensor([1, 120000, 3],"float32"), Tensor([1, 1140850690, 2],"float32"), ) 
 Expected size for first two dimensions of batch2 tensor to be: [1, 3] but got: [1, 1140850690].
2025-03-12 11:21:20.641890 test begin: paddle.Tensor.bmm(Tensor([1, 120000, 3],"float32"), Tensor([1, 3, 760567127],"float32"), )

[torch error] paddle.Tensor.bmm(Tensor([1, 120000, 3],"float32"), Tensor([1, 3, 760567127],"float32"), ) 
 CUDA out of memory. Tried to allocate 340000.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 60.59 GiB is free. Process 44290 has 18.59 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-12 11:21:23.051364 test begin: paddle.Tensor.bmm(Tensor([1, 120000, 3],"float32"), Tensor([380283564, 3, 2],"float32"), )

[torch error] paddle.Tensor.bmm(Tensor([1, 120000, 3],"float32"), Tensor([380283564, 3, 2],"float32"), ) 
 Expected size for first two dimensions of batch2 tensor to be: [1, 3] but got: [380283564, 3].
2025-03-12 11:21:25.426457 test begin: paddle.Tensor.bmm(Tensor([1, 143264, 15927],"float32"), Tensor([1, 3, 15927],"float32"), )

[torch error] paddle.Tensor.bmm(Tensor([1, 143264, 15927],"float32"), Tensor([1, 3, 15927],"float32"), ) 
 Expected size for first two dimensions of batch2 tensor to be: [1, 15927] but got: [1, 3].
2025-03-12 11:21:27.941754 test begin: paddle.Tensor.bmm(Tensor([1, 143264, 15927],"float32"), Tensor([1, 3, 2],"float32"), )

[torch error] paddle.Tensor.bmm(Tensor([1, 143264, 15927],"float32"), Tensor([1, 3, 2],"float32"), ) 
 Expected size for first two dimensions of batch2 tensor to be: [1, 15927] but got: [1, 3].
2025-03-12 11:21:30.333652 test begin: paddle.Tensor.bmm(Tensor([1, 143264, 3],"float32"), Tensor([1, 1140850690, 2],"float32"), )

[torch error] paddle.Tensor.bmm(Tensor([1, 143264, 3],"float32"), Tensor([1, 1140850690, 2],"float32"), ) 
 Expected size for first two dimensions of batch2 tensor to be: [1, 3] but got: [1, 1140850690].
2025-03-12 11:21:33.118437 test begin: paddle.Tensor.bmm(Tensor([1, 143264, 3],"float32"), Tensor([1, 3, 760567127],"float32"), )

[torch error] paddle.Tensor.bmm(Tensor([1, 143264, 3],"float32"), Tensor([1, 3, 760567127],"float32"), ) 
 CUDA out of memory. Tried to allocate 405914.67 GiB. GPU 0 has a total capacity of 79.18 GiB of which 60.59 GiB is free. Process 44290 has 18.59 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-12 11:21:34.987741 test begin: paddle.Tensor.bmm(Tensor([1, 143264, 3],"float32"), Tensor([380283564, 3, 2],"float32"), )

[torch error] paddle.Tensor.bmm(Tensor([1, 143264, 3],"float32"), Tensor([380283564, 3, 2],"float32"), ) 
 Expected size for first two dimensions of batch2 tensor to be: [1, 3] but got: [380283564, 3].
2025-03-12 11:21:36.879940 test begin: paddle.Tensor.bmm(Tensor([1, 146200, 15607],"float32"), Tensor([1, 3, 15607],"float32"), )

[torch error] paddle.Tensor.bmm(Tensor([1, 146200, 15607],"float32"), Tensor([1, 3, 15607],"float32"), ) 
 Expected size for first two dimensions of batch2 tensor to be: [1, 15607] but got: [1, 3].
2025-03-12 11:21:39.266096 test begin: paddle.Tensor.bmm(Tensor([1, 146200, 15607],"float32"), Tensor([1, 3, 2],"float32"), )

[torch error] paddle.Tensor.bmm(Tensor([1, 146200, 15607],"float32"), Tensor([1, 3, 2],"float32"), ) 
 Expected size for first two dimensions of batch2 tensor to be: [1, 15607] but got: [1, 3].
2025-03-12 11:21:41.479455 test begin: paddle.Tensor.bmm(Tensor([1, 146200, 3],"float32"), Tensor([1, 1140850690, 2],"float32"), )

[torch error] paddle.Tensor.bmm(Tensor([1, 146200, 3],"float32"), Tensor([1, 1140850690, 2],"float32"), ) 
 Expected size for first two dimensions of batch2 tensor to be: [1, 3] but got: [1, 1140850690].
2025-03-12 11:21:43.909277 test begin: paddle.Tensor.bmm(Tensor([1, 146200, 3],"float32"), Tensor([1, 3, 760567127],"float32"), )

[torch error] paddle.Tensor.bmm(Tensor([1, 146200, 3],"float32"), Tensor([1, 3, 760567127],"float32"), ) 
 CUDA out of memory. Tried to allocate 414233.34 GiB. GPU 0 has a total capacity of 79.18 GiB of which 60.59 GiB is free. Process 44290 has 18.59 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-12 11:21:45.697416 test begin: paddle.Tensor.bmm(Tensor([1, 146200, 3],"float32"), Tensor([380283564, 3, 2],"float32"), )

[torch error] paddle.Tensor.bmm(Tensor([1, 146200, 3],"float32"), Tensor([380283564, 3, 2],"float32"), ) 
 Expected size for first two dimensions of batch2 tensor to be: [1, 3] but got: [380283564, 3].
2025-03-12 11:21:47.512842 test begin: paddle.Tensor.bmm(Tensor([1, 760567127, 3],"float32"), Tensor([1, 760567127, 2],"float32"), )

[torch error] paddle.Tensor.bmm(Tensor([1, 760567127, 3],"float32"), Tensor([1, 760567127, 2],"float32"), ) 
 Expected size for first two dimensions of batch2 tensor to be: [1, 3] but got: [1, 760567127].
2025-03-12 11:21:51.366512 test begin: paddle.Tensor.bmm(Tensor([5203, 146200, 3],"float32"), Tensor([1, 3, 2],"float32"), )

[torch error] paddle.Tensor.bmm(Tensor([5203, 146200, 3],"float32"), Tensor([1, 3, 2],"float32"), ) 
 Expected size for first two dimensions of batch2 tensor to be: [5203, 3] but got: [1, 3].
2025-03-12 11:21:53.665579 test begin: paddle.Tensor.bmm(Tensor([5203, 146200, 3],"float32"), Tensor([5203, 3, 2],"float32"), )

/usr/local/lib/python3.9/dist-packages/torch/autograd/graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[accuracy error] backward  paddle.Tensor.bmm(Tensor([5203, 146200, 3],"float32"), Tensor([5203, 3, 2],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 171 / 31218 (0.548%)
Max absolute difference: 0.07515717
Max relative difference: 7.7364936
 x: array([[[  9.700672, -29.541069],
        [-14.000696, -17.672201],
        [ 38.497513,  33.617844]],...
 y: array([[[  9.699971, -29.542145],
        [-14.024817, -17.669855],
        [ 38.503223,  33.6353  ]],...
2025-03-12 11:24:45.539049 test begin: paddle.Tensor.bmm(Tensor([5309, 143264, 3],"float32"), Tensor([1, 3, 2],"float32"), )

[torch error] paddle.Tensor.bmm(Tensor([5309, 143264, 3],"float32"), Tensor([1, 3, 2],"float32"), ) 
 Expected size for first two dimensions of batch2 tensor to be: [5309, 3] but got: [1, 3].
2025-03-12 11:24:50.529417 test begin: paddle.Tensor.bmm(Tensor([5309, 143264, 3],"float32"), Tensor([5309, 3, 2],"float32"), )

[accuracy error] backward  paddle.Tensor.bmm(Tensor([5309, 143264, 3],"float32"), Tensor([5309, 3, 2],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 167 / 31854 (0.524%)
Max absolute difference: 0.0732193
Max relative difference: 15.289176
 x: array([[[ 21.196463,  -3.602156],
        [ 32.622368, -23.486275],
        [ 59.71489 , -49.415188]],...
 y: array([[[ 21.210472,  -3.604038],
        [ 32.650208, -23.484335],
        [ 59.736336, -49.45128 ]],...
2025-03-12 11:27:39.142665 test begin: paddle.Tensor.bmm(Tensor([6339, 120000, 3],"float32"), Tensor([1, 3, 2],"float32"), )

[torch error] paddle.Tensor.bmm(Tensor([6339, 120000, 3],"float32"), Tensor([1, 3, 2],"float32"), ) 
 Expected size for first two dimensions of batch2 tensor to be: [6339, 3] but got: [1, 3].
2025-03-12 11:27:44.870008 test begin: paddle.Tensor.bmm(Tensor([6339, 120000, 3],"float32"), Tensor([6339, 3, 2],"float32"), )

[accuracy error] backward  paddle.Tensor.bmm(Tensor([6339, 120000, 3],"float32"), Tensor([6339, 3, 2],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 127 / 38034 (0.334%)
Max absolute difference: 0.05808258
Max relative difference: 8.115404
 x: array([[[ 28.18752 , -18.88068 ],
        [-17.338022, -12.355803],
        [  8.48315 ,   8.624543]],...
 y: array([[[ 28.190413, -18.89503 ],
        [-17.344336, -12.37042 ],
        [  8.488872,   8.60873 ]],...
2025-03-12 11:30:40.203997 test begin: paddle.Tensor.bmm(Tensor([657931, 1156, 3],"float32"), Tensor([1, 3, 2],"float32"), )

[torch error] paddle.Tensor.bmm(Tensor([657931, 1156, 3],"float32"), Tensor([1, 3, 2],"float32"), ) 
 Expected size for first two dimensions of batch2 tensor to be: [657931, 3] but got: [1, 3].
2025-03-12 11:30:44.645438 test begin: paddle.Tensor.bmm(Tensor([7012, 108472, 3],"float32"), Tensor([1, 3, 2],"float32"), )

[torch error] paddle.Tensor.bmm(Tensor([7012, 108472, 3],"float32"), Tensor([1, 3, 2],"float32"), ) 
 Expected size for first two dimensions of batch2 tensor to be: [7012, 3] but got: [1, 3].
2025-03-12 11:30:47.207452 test begin: paddle.Tensor.broadcast_to(Tensor([2281701379],"float32"), list[3,2,], )

[torch error] paddle.Tensor.broadcast_to(Tensor([2281701379],"float32"), list[3,2,], ) 
 The expanded size of the tensor (2) must match the existing size (2281701379) at non-singleton dimension 1.  Target sizes: [3, 2].  Tensor sizes: [2281701379]
2025-03-12 11:30:50.132537 test begin: paddle.Tensor.cholesky_solve(x=Tensor([1431655766, 3],"float16"), y=Tensor([1431655766, 4],"float16"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([1431655766, 3],"float16"), y=Tensor([1431655766, 4],"float16"), ) 
 cannot reshape array of size 4300000000 into shape (1431655766,4)
2025-03-12 11:32:38.616987 test begin: paddle.Tensor.cholesky_solve(x=Tensor([1431655766, 3],"float16"), y=Tensor([4, 4],"float16"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([1431655766, 3],"float16"), y=Tensor([4, 4],"float16"), ) 
 Incompatible matrix sizes for cholesky_solve: each A matrix is 4 by 4 but each b matrix is 1431655766 by 3
2025-03-12 11:32:40.821785 test begin: paddle.Tensor.cholesky_solve(x=Tensor([178956971, 2, 4, 3],"float16"), y=Tensor([178956971, 2, 4, 4],"float16"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([178956971, 2, 4, 3],"float16"), y=Tensor([178956971, 2, 4, 4],"float16"), ) 
 cannot reshape array of size 4300000000 into shape (178956971,2,4,4)
2025-03-12 11:32:42.704634 test begin: paddle.Tensor.cholesky_solve(x=Tensor([178956971, 2, 4, 3],"float16"), y=Tensor([178956971, 2, 4, 4],"float16"), upper=True, )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([178956971, 2, 4, 3],"float16"), y=Tensor([178956971, 2, 4, 4],"float16"), upper=True, ) 
 cannot reshape array of size 4300000000 into shape (178956971,2,4,4)
2025-03-12 11:32:44.537410 test begin: paddle.Tensor.cholesky_solve(x=Tensor([178956971, 2, 4, 3],"float16"), y=Tensor([5, 2, 4, 4],"float16"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([178956971, 2, 4, 3],"float16"), y=Tensor([5, 2, 4, 4],"float16"), ) 
 The size of tensor a (178956971) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-12 11:32:46.870415 test begin: paddle.Tensor.cholesky_solve(x=Tensor([178956971, 2, 4, 3],"float16"), y=Tensor([5, 2, 4, 4],"float16"), upper=True, )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([178956971, 2, 4, 3],"float16"), y=Tensor([5, 2, 4, 4],"float16"), upper=True, ) 
 The size of tensor a (178956971) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-12 11:32:48.946616 test begin: paddle.Tensor.cholesky_solve(x=Tensor([2281701379, 1],"float32"), y=Tensor([2281701379, 4],"float32"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([2281701379, 1],"float32"), y=Tensor([2281701379, 4],"float32"), ) 
 cannot reshape array of size 4300000000 into shape (2281701379,4)
2025-03-12 11:32:53.726254 test begin: paddle.Tensor.cholesky_solve(x=Tensor([2281701379, 1],"float32"), y=Tensor([4, 4],"float32"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([2281701379, 1],"float32"), y=Tensor([4, 4],"float32"), ) 
 Incompatible matrix sizes for cholesky_solve: each A matrix is 4 by 4 but each b matrix is 2281701379 by 1
2025-03-12 11:32:56.379438 test begin: paddle.Tensor.cholesky_solve(x=Tensor([357913942, 4, 3],"float16"), y=Tensor([357913942, 4, 4],"float16"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([357913942, 4, 3],"float16"), y=Tensor([357913942, 4, 4],"float16"), ) 
 cannot reshape array of size 4300000000 into shape (357913942,4,4)
2025-03-12 11:32:58.950785 test begin: paddle.Tensor.cholesky_solve(x=Tensor([357913942, 4, 3],"float16"), y=Tensor([5, 4, 4],"float16"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([357913942, 4, 3],"float16"), y=Tensor([5, 4, 4],"float16"), ) 
 The size of tensor a (357913942) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-12 11:33:02.573639 test begin: paddle.Tensor.cholesky_solve(x=Tensor([4, 1073741825],"float16"), y=Tensor([4, 1073741825],"float16"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([4, 1073741825],"float16"), y=Tensor([4, 1073741825],"float16"), ) 
 A must be batches of square matrices, but they are 4 by 1073741825 matrices
2025-03-12 11:33:06.954738 test begin: paddle.Tensor.cholesky_solve(x=Tensor([4, 1073741825],"float16"), y=Tensor([4, 4],"float16"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([4, 1073741825],"float16"), y=Tensor([4, 4],"float16"), ) 
 "cholesky_cuda_potrs" not implemented for 'Half'
2025-03-12 11:33:09.203895 test begin: paddle.Tensor.cholesky_solve(x=Tensor([4, 1],"float16"), y=Tensor([1073741825, 4],"float16"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([4, 1],"float16"), y=Tensor([1073741825, 4],"float16"), ) 
 A must be batches of square matrices, but they are 1073741825 by 4 matrices
2025-03-12 11:33:11.162638 test begin: paddle.Tensor.cholesky_solve(x=Tensor([4, 1],"float16"), y=Tensor([4, 1073741825],"float16"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([4, 1],"float16"), y=Tensor([4, 1073741825],"float16"), ) 
 A must be batches of square matrices, but they are 4 by 1073741825 matrices
2025-03-12 11:33:13.572219 test begin: paddle.Tensor.cholesky_solve(x=Tensor([4, 1],"float32"), y=Tensor([4, 570425345],"float32"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([4, 1],"float32"), y=Tensor([4, 570425345],"float32"), ) 
 A must be batches of square matrices, but they are 4 by 570425345 matrices
2025-03-12 11:33:16.201963 test begin: paddle.Tensor.cholesky_solve(x=Tensor([4, 1],"float32"), y=Tensor([570425345, 4],"float32"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([4, 1],"float32"), y=Tensor([570425345, 4],"float32"), ) 
 A must be batches of square matrices, but they are 570425345 by 4 matrices
2025-03-12 11:33:18.367643 test begin: paddle.Tensor.cholesky_solve(x=Tensor([4, 3],"float16"), y=Tensor([1073741825, 4],"float16"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([4, 3],"float16"), y=Tensor([1073741825, 4],"float16"), ) 
 A must be batches of square matrices, but they are 1073741825 by 4 matrices
2025-03-12 11:33:20.313749 test begin: paddle.Tensor.cholesky_solve(x=Tensor([4, 3],"float16"), y=Tensor([4, 1073741825],"float16"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([4, 3],"float16"), y=Tensor([4, 1073741825],"float16"), ) 
 A must be batches of square matrices, but they are 4 by 1073741825 matrices
2025-03-12 11:33:22.797371 test begin: paddle.Tensor.cholesky_solve(x=Tensor([4, 570425345],"float32"), y=Tensor([4, 570425345],"float32"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([4, 570425345],"float32"), y=Tensor([4, 570425345],"float32"), ) 
 A must be batches of square matrices, but they are 4 by 570425345 matrices
2025-03-12 11:33:26.617227 test begin: paddle.Tensor.cholesky_solve(x=Tensor([4294967297, 1],"float16"), y=Tensor([4, 4],"float16"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([4294967297, 1],"float16"), y=Tensor([4, 4],"float16"), ) 
 Incompatible matrix sizes for cholesky_solve: each A matrix is 4 by 4 but each b matrix is 4294967297 by 1
2025-03-12 11:33:28.320177 test begin: paddle.Tensor.cholesky_solve(x=Tensor([4294967297, 1],"float16"), y=Tensor([4294967297, 4],"float16"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([4294967297, 1],"float16"), y=Tensor([4294967297, 4],"float16"), ) 
 cannot reshape array of size 4300000000 into shape (4294967297,4)
2025-03-12 11:33:30.931597 test begin: paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 143165577, 3],"float16"), y=Tensor([5, 2, 143165577, 4],"float16"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 143165577, 3],"float16"), y=Tensor([5, 2, 143165577, 4],"float16"), ) 
 cannot reshape array of size 4300000000 into shape (5,2,143165577,4)
2025-03-12 11:33:32.807878 test begin: paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 143165577, 3],"float16"), y=Tensor([5, 2, 143165577, 4],"float16"), upper=True, )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 143165577, 3],"float16"), y=Tensor([5, 2, 143165577, 4],"float16"), upper=True, ) 
 cannot reshape array of size 4300000000 into shape (5,2,143165577,4)
2025-03-12 11:33:34.603349 test begin: paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 143165577, 3],"float16"), y=Tensor([5, 2, 4, 4],"float16"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 143165577, 3],"float16"), y=Tensor([5, 2, 4, 4],"float16"), ) 
 Incompatible matrix sizes for cholesky_solve: each A matrix is 4 by 4 but each b matrix is 143165577 by 3
2025-03-12 11:33:36.941176 test begin: paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 143165577, 3],"float16"), y=Tensor([5, 2, 4, 4],"float16"), upper=True, )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 143165577, 3],"float16"), y=Tensor([5, 2, 4, 4],"float16"), upper=True, ) 
 Incompatible matrix sizes for cholesky_solve: each A matrix is 4 by 4 but each b matrix is 143165577 by 3
2025-03-12 11:33:39.338285 test begin: paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 4, 107374183],"float16"), y=Tensor([5, 2, 4, 107374183],"float16"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 4, 107374183],"float16"), y=Tensor([5, 2, 4, 107374183],"float16"), ) 
 A must be batches of square matrices, but they are 4 by 107374183 matrices
2025-03-12 11:33:42.763409 test begin: paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 4, 107374183],"float16"), y=Tensor([5, 2, 4, 107374183],"float16"), upper=True, )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 4, 107374183],"float16"), y=Tensor([5, 2, 4, 107374183],"float16"), upper=True, ) 
 A must be batches of square matrices, but they are 4 by 107374183 matrices
2025-03-12 11:33:46.034378 test begin: paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 4, 107374183],"float16"), y=Tensor([5, 2, 4, 4],"float16"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 4, 107374183],"float16"), y=Tensor([5, 2, 4, 4],"float16"), ) 
 "cholesky_solve_cuda" not implemented for 'Half'
2025-03-12 11:33:47.697302 test begin: paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 4, 107374183],"float16"), y=Tensor([5, 2, 4, 4],"float16"), upper=True, )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 4, 107374183],"float16"), y=Tensor([5, 2, 4, 4],"float16"), upper=True, ) 
 "cholesky_solve_cuda" not implemented for 'Half'
2025-03-12 11:33:50.082448 test begin: paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 4, 3],"float16"), y=Tensor([134217729, 2, 4, 4],"float16"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 4, 3],"float16"), y=Tensor([134217729, 2, 4, 4],"float16"), ) 
 The size of tensor a (5) must match the size of tensor b (134217729) at non-singleton dimension 0
2025-03-12 11:33:51.860716 test begin: paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 4, 3],"float16"), y=Tensor([134217729, 2, 4, 4],"float16"), upper=True, )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 4, 3],"float16"), y=Tensor([134217729, 2, 4, 4],"float16"), upper=True, ) 
 The size of tensor a (5) must match the size of tensor b (134217729) at non-singleton dimension 0
2025-03-12 11:33:54.322896 test begin: paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 4, 3],"float16"), y=Tensor([5, 2, 107374183, 4],"float16"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 4, 3],"float16"), y=Tensor([5, 2, 107374183, 4],"float16"), ) 
 A must be batches of square matrices, but they are 107374183 by 4 matrices
2025-03-12 11:33:56.197492 test begin: paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 4, 3],"float16"), y=Tensor([5, 2, 107374183, 4],"float16"), upper=True, )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 4, 3],"float16"), y=Tensor([5, 2, 107374183, 4],"float16"), upper=True, ) 
 A must be batches of square matrices, but they are 107374183 by 4 matrices
2025-03-12 11:33:57.923956 test begin: paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 4, 3],"float16"), y=Tensor([5, 2, 4, 107374183],"float16"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 4, 3],"float16"), y=Tensor([5, 2, 4, 107374183],"float16"), ) 
 A must be batches of square matrices, but they are 4 by 107374183 matrices
2025-03-12 11:34:00.262784 test begin: paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 4, 3],"float16"), y=Tensor([5, 2, 4, 107374183],"float16"), upper=True, )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 4, 3],"float16"), y=Tensor([5, 2, 4, 107374183],"float16"), upper=True, ) 
 A must be batches of square matrices, but they are 4 by 107374183 matrices
2025-03-12 11:34:01.996000 test begin: paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 4, 3],"float16"), y=Tensor([5, 53687092, 4, 4],"float16"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 4, 3],"float16"), y=Tensor([5, 53687092, 4, 4],"float16"), ) 
 The size of tensor a (2) must match the size of tensor b (53687092) at non-singleton dimension 1
2025-03-12 11:34:03.668277 test begin: paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 4, 3],"float16"), y=Tensor([5, 53687092, 4, 4],"float16"), upper=True, )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([5, 2, 4, 3],"float16"), y=Tensor([5, 53687092, 4, 4],"float16"), upper=True, ) 
 The size of tensor a (2) must match the size of tensor b (53687092) at non-singleton dimension 1
2025-03-12 11:34:05.321400 test begin: paddle.Tensor.cholesky_solve(x=Tensor([5, 286331154, 3],"float16"), y=Tensor([5, 286331154, 4],"float16"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([5, 286331154, 3],"float16"), y=Tensor([5, 286331154, 4],"float16"), ) 
 cannot reshape array of size 4300000000 into shape (5,286331154,4)
2025-03-12 11:34:07.360777 test begin: paddle.Tensor.cholesky_solve(x=Tensor([5, 286331154, 3],"float16"), y=Tensor([5, 4, 4],"float16"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([5, 286331154, 3],"float16"), y=Tensor([5, 4, 4],"float16"), ) 
 Incompatible matrix sizes for cholesky_solve: each A matrix is 4 by 4 but each b matrix is 286331154 by 3
2025-03-12 11:34:09.010395 test begin: paddle.Tensor.cholesky_solve(x=Tensor([5, 4, 214748365],"float16"), y=Tensor([5, 4, 214748365],"float16"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([5, 4, 214748365],"float16"), y=Tensor([5, 4, 214748365],"float16"), ) 
 A must be batches of square matrices, but they are 4 by 214748365 matrices
2025-03-12 11:34:12.545175 test begin: paddle.Tensor.cholesky_solve(x=Tensor([5, 4, 214748365],"float16"), y=Tensor([5, 4, 4],"float16"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([5, 4, 214748365],"float16"), y=Tensor([5, 4, 4],"float16"), ) 
 "cholesky_solve_cuda" not implemented for 'Half'
2025-03-12 11:34:14.183837 test begin: paddle.Tensor.cholesky_solve(x=Tensor([5, 4, 3],"float16"), y=Tensor([268435457, 4, 4],"float16"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([5, 4, 3],"float16"), y=Tensor([268435457, 4, 4],"float16"), ) 
 The size of tensor a (5) must match the size of tensor b (268435457) at non-singleton dimension 0
2025-03-12 11:34:16.346480 test begin: paddle.Tensor.cholesky_solve(x=Tensor([5, 4, 3],"float16"), y=Tensor([5, 214748365, 4],"float16"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([5, 4, 3],"float16"), y=Tensor([5, 214748365, 4],"float16"), ) 
 A must be batches of square matrices, but they are 214748365 by 4 matrices
2025-03-12 11:34:18.029294 test begin: paddle.Tensor.cholesky_solve(x=Tensor([5, 4, 3],"float16"), y=Tensor([5, 4, 214748365],"float16"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([5, 4, 3],"float16"), y=Tensor([5, 4, 214748365],"float16"), ) 
 A must be batches of square matrices, but they are 4 by 214748365 matrices
2025-03-12 11:34:19.719473 test begin: paddle.Tensor.cholesky_solve(x=Tensor([5, 71582789, 4, 3],"float16"), y=Tensor([5, 2, 4, 4],"float16"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([5, 71582789, 4, 3],"float16"), y=Tensor([5, 2, 4, 4],"float16"), ) 
 The size of tensor a (71582789) must match the size of tensor b (2) at non-singleton dimension 1
2025-03-12 11:34:21.400047 test begin: paddle.Tensor.cholesky_solve(x=Tensor([5, 71582789, 4, 3],"float16"), y=Tensor([5, 2, 4, 4],"float16"), upper=True, )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([5, 71582789, 4, 3],"float16"), y=Tensor([5, 2, 4, 4],"float16"), upper=True, ) 
 The size of tensor a (71582789) must match the size of tensor b (2) at non-singleton dimension 1
2025-03-12 11:34:22.950153 test begin: paddle.Tensor.cholesky_solve(x=Tensor([5, 71582789, 4, 3],"float16"), y=Tensor([5, 71582789, 4, 4],"float16"), )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([5, 71582789, 4, 3],"float16"), y=Tensor([5, 71582789, 4, 4],"float16"), ) 
 cannot reshape array of size 4300000000 into shape (5,71582789,4,4)
2025-03-12 11:34:24.807970 test begin: paddle.Tensor.cholesky_solve(x=Tensor([5, 71582789, 4, 3],"float16"), y=Tensor([5, 71582789, 4, 4],"float16"), upper=True, )

[torch error] paddle.Tensor.cholesky_solve(x=Tensor([5, 71582789, 4, 3],"float16"), y=Tensor([5, 71582789, 4, 4],"float16"), upper=True, ) 
 cannot reshape array of size 4300000000 into shape (5,71582789,4,4)
2025-03-12 11:34:26.543003 test begin: paddle.Tensor.chunk(Tensor([1, 1, 11109, 205393],"float32"), 2, axis=-1, )

[paddle error] paddle.Tensor.chunk(Tensor([1, 1, 11109, 205393],"float32"), 2, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 2, input(X)'s shape = [1, 1, 11109, 205393], Attr(dim) = 3.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at ../paddle/phi/infermeta/unary.cc:4490)

2025-03-12 11:34:52.608482 test begin: paddle.Tensor.chunk(Tensor([1, 1, 12096, 188633],"float32"), 2, axis=-1, )

[paddle error] paddle.Tensor.chunk(Tensor([1, 1, 12096, 188633],"float32"), 2, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 2, input(X)'s shape = [1, 1, 12096, 188633], Attr(dim) = 3.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at ../paddle/phi/infermeta/unary.cc:4490)

2025-03-12 11:35:16.715285 test begin: paddle.Tensor.chunk(Tensor([1, 1, 1360, 1677722],"float32"), 2, axis=-1, )

[Pass] paddle.Tensor.chunk(Tensor([1, 1, 1360, 1677722],"float32"), 2, axis=-1, )
2025-03-12 11:38:47.799733 test begin: paddle.Tensor.chunk(Tensor([1, 100, 22817014],"float32"), 2, axis=-1, )

[Pass] paddle.Tensor.chunk(Tensor([1, 100, 22817014],"float32"), 2, axis=-1, )
2025-03-12 11:41:47.491582 test begin: paddle.Tensor.chunk(Tensor([1, 101, 1, 22591103],"float32"), 4, axis=-1, )

[paddle error] paddle.Tensor.chunk(Tensor([1, 101, 1, 22591103],"float32"), 4, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 4, input(X)'s shape = [1, 101, 1, 22591103], Attr(dim) = 3.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:3 != 0:0.] (at ../paddle/phi/infermeta/unary.cc:4490)

2025-03-12 11:42:11.758828 test begin: paddle.Tensor.chunk(Tensor([1, 101, 22591103],"float32"), 2, axis=-1, )

[paddle error] paddle.Tensor.chunk(Tensor([1, 101, 22591103],"float32"), 2, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 2, input(X)'s shape = [1, 101, 22591103], Attr(dim) = 2.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at ../paddle/phi/infermeta/unary.cc:4490)

2025-03-12 11:42:34.111314 test begin: paddle.Tensor.chunk(Tensor([1, 101, 5647776, 4],"float32"), 4, axis=-1, )

[Pass] paddle.Tensor.chunk(Tensor([1, 101, 5647776, 4],"float32"), 4, axis=-1, )
2025-03-12 11:46:06.740943 test begin: paddle.Tensor.chunk(Tensor([1, 102, 1, 22369622],"float32"), 4, axis=-1, )

[paddle error] paddle.Tensor.chunk(Tensor([1, 102, 1, 22369622],"float32"), 4, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 4, input(X)'s shape = [1, 102, 1, 22369622], Attr(dim) = 3.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:2 != 0:0.] (at ../paddle/phi/infermeta/unary.cc:4490)

2025-03-12 11:46:33.414980 test begin: paddle.Tensor.chunk(Tensor([1, 102, 22369622],"float32"), 2, axis=-1, )

[Pass] paddle.Tensor.chunk(Tensor([1, 102, 22369622],"float32"), 2, axis=-1, )
2025-03-12 11:49:42.597084 test begin: paddle.Tensor.chunk(Tensor([1, 102, 5592406, 4],"float32"), 4, axis=-1, )

[Pass] paddle.Tensor.chunk(Tensor([1, 102, 5592406, 4],"float32"), 4, axis=-1, )
2025-03-12 11:52:45.604137 test begin: paddle.Tensor.chunk(Tensor([1, 102697, 11109, 2],"float32"), 2, axis=-1, )

[Pass] paddle.Tensor.chunk(Tensor([1, 102697, 11109, 2],"float32"), 2, axis=-1, )
2025-03-12 11:55:47.034128 test begin: paddle.Tensor.chunk(Tensor([1, 103, 1, 22152441],"float32"), 4, axis=-1, )

[paddle error] paddle.Tensor.chunk(Tensor([1, 103, 1, 22152441],"float32"), 4, axis=-1, ) 
 (InvalidArgument) The input's size along the split dimension must be evenly divisible by Attr(num_or_sections). But received Attr(num_or_sections) = 4, input(X)'s shape = [1, 103, 1, 22152441], Attr(dim) = 3.
  [Hint: Expected input_axis_dim % num == 0, but received input_axis_dim % num:1 != 0:0.] (at ../paddle/phi/infermeta/unary.cc:4490)

2025-03-12 11:56:11.873010 test begin: paddle.Tensor.chunk(Tensor([1, 103, 5538111, 4],"float32"), 4, axis=-1, )

[Pass] paddle.Tensor.chunk(Tensor([1, 103, 5538111, 4],"float32"), 4, axis=-1, )
2025-03-12 11:59:13.234356 test begin: paddle.Tensor.chunk(Tensor([1, 285212673, 8],"float32"), 1, )

[Pass] paddle.Tensor.chunk(Tensor([1, 285212673, 8],"float32"), 1, )
2025-03-12 12:02:39.642161 test begin: paddle.Tensor.chunk(Tensor([1, 325957340, 7],"float32"), 1, )

[Pass] paddle.Tensor.chunk(Tensor([1, 325957340, 7],"float32"), 1, )
2025-03-12 12:05:37.577417 test begin: paddle.Tensor.chunk(Tensor([1, 838861, 1360, 2],"float32"), 2, axis=-1, )

[Pass] paddle.Tensor.chunk(Tensor([1, 838861, 1360, 2],"float32"), 2, axis=-1, )
2025-03-12 12:08:41.528938 test begin: paddle.Tensor.chunk(Tensor([1, 94317, 12096, 2],"float32"), 2, axis=-1, )

[Pass] paddle.Tensor.chunk(Tensor([1, 94317, 12096, 2],"float32"), 2, axis=-1, )
2025-03-12 12:11:39.664892 test begin: paddle.Tensor.chunk(Tensor([102697, 1, 11109, 2],"float32"), 2, axis=-1, )

[Pass] paddle.Tensor.chunk(Tensor([102697, 1, 11109, 2],"float32"), 2, axis=-1, )
2025-03-12 12:14:37.625622 test begin: paddle.Tensor.chunk(Tensor([1086525, 300, 7],"float32"), 1, )

[Pass] paddle.Tensor.chunk(Tensor([1086525, 300, 7],"float32"), 1, )
2025-03-12 12:17:43.931310 test begin: paddle.Tensor.chunk(Tensor([10923, 102, 2048],"float32"), 2, axis=-1, )

[Pass] paddle.Tensor.chunk(Tensor([10923, 102, 2048],"float32"), 2, axis=-1, )
2025-03-12 12:20:58.952973 test begin: paddle.Tensor.chunk(Tensor([11031, 101, 2048],"float32"), 2, axis=-1, )

[Pass] paddle.Tensor.chunk(Tensor([11031, 101, 2048],"float32"), 2, axis=-1, )
2025-03-12 12:24:29.828691 test begin: paddle.Tensor.chunk(Tensor([11142, 100, 2048],"float32"), 2, axis=-1, )

[Pass] paddle.Tensor.chunk(Tensor([11142, 100, 2048],"float32"), 2, axis=-1, )
2025-03-12 12:27:22.092393 test begin: paddle.Tensor.chunk(Tensor([128, 11606, 1536],"float32"), 3, axis=-1, )

[Pass] paddle.Tensor.chunk(Tensor([128, 11606, 1536],"float32"), 3, axis=-1, )
2025-03-12 12:30:14.879944 test begin: paddle.Tensor.chunk(Tensor([128, 46422, 384],"float32"), 3, axis=-1, )

[Pass] paddle.Tensor.chunk(Tensor([128, 46422, 384],"float32"), 3, axis=-1, )
2025-03-12 12:33:17.128729 test begin: paddle.Tensor.chunk(Tensor([128, 49, 363792],"float32"), 3, axis=-1, )

[Pass] paddle.Tensor.chunk(Tensor([128, 49, 363792],"float32"), 3, axis=-1, )
2025-03-12 12:36:16.325405 test begin: paddle.Tensor.chunk(Tensor([128, 784, 22737],"float32"), 3, axis=-1, )

[Pass] paddle.Tensor.chunk(Tensor([128, 784, 22737],"float32"), 3, axis=-1, )
2025-03-12 12:39:10.632167 test begin: paddle.Tensor.chunk(Tensor([13, 25073642, 7],"float32"), 2, axis=1, )

[Pass] paddle.Tensor.chunk(Tensor([13, 25073642, 7],"float32"), 2, axis=1, )
2025-03-12 12:42:22.816858 test begin: paddle.Tensor.chunk(Tensor([13, 58505164, 3],"float32"), 2, axis=1, )

[Pass] paddle.Tensor.chunk(Tensor([13, 58505164, 3],"float32"), 2, axis=1, )
2025-03-12 12:45:11.047192 test begin: paddle.Tensor.chunk(Tensor([14, 128, 1273271],"float32"), 2, axis=1, )

[Pass] paddle.Tensor.chunk(Tensor([14, 128, 1273271],"float32"), 2, axis=1, )
2025-03-12 12:48:16.104725 test begin: paddle.Tensor.chunk(Tensor([14, 23282668, 7],"float32"), 2, axis=1, )

[Pass] paddle.Tensor.chunk(Tensor([14, 23282668, 7],"float32"), 2, axis=1, )
2025-03-12 12:51:07.337418 test begin: paddle.Tensor.chunk(Tensor([16, 11606, 128, 96],"float32"), 2, axis=1, )

[Pass] paddle.Tensor.chunk(Tensor([16, 11606, 128, 96],"float32"), 2, axis=1, )
2025-03-12 12:54:05.736190 test begin: paddle.Tensor.chunk(Tensor([16, 11883862, 4, 3],"float32"), 2, axis=1, )

[Pass] paddle.Tensor.chunk(Tensor([16, 11883862, 4, 3],"float32"), 2, axis=1, )
2025-03-12 12:56:58.339061 test begin: paddle.Tensor.chunk(Tensor([16, 32, 128, 34817],"float32"), 2, axis=1, )

[Pass] paddle.Tensor.chunk(Tensor([16, 32, 128, 34817],"float32"), 2, axis=1, )
2025-03-12 12:59:50.897684 test begin: paddle.Tensor.chunk(Tensor([16, 32, 46422, 96],"float32"), 2, axis=1, )

[Pass] paddle.Tensor.chunk(Tensor([16, 32, 46422, 96],"float32"), 2, axis=1, )
2025-03-12 13:02:39.337460 test begin: paddle.Tensor.chunk(Tensor([16, 32, 64, 69633],"float32"), 2, axis=1, )

[Pass] paddle.Tensor.chunk(Tensor([16, 32, 64, 69633],"float32"), 2, axis=1, )
2025-03-12 13:05:37.489277 test begin: paddle.Tensor.chunk(Tensor([16, 32, 92843, 48],"float32"), 2, axis=1, )

[Pass] paddle.Tensor.chunk(Tensor([16, 32, 92843, 48],"float32"), 2, axis=1, )
2025-03-12 13:08:30.835798 test begin: paddle.Tensor.chunk(Tensor([16, 320, 148549, 3],"float32"), 2, axis=1, )

[Pass] paddle.Tensor.chunk(Tensor([16, 320, 148549, 3],"float32"), 2, axis=1, )
2025-03-12 13:11:28.163047 test begin: paddle.Tensor.chunk(Tensor([16, 320, 4, 111412],"float32"), 2, axis=1, )

[Pass] paddle.Tensor.chunk(Tensor([16, 320, 4, 111412],"float32"), 2, axis=1, )
2025-03-12 13:14:45.188329 test begin: paddle.Tensor.chunk(Tensor([16, 46422, 64, 48],"float32"), 2, axis=1, )

[Pass] paddle.Tensor.chunk(Tensor([16, 46422, 64, 48],"float32"), 2, axis=1, )
2025-03-12 13:18:06.328083 test begin: paddle.Tensor.chunk(Tensor([1901418, 300, 4],"float32"), 2, )

[Pass] paddle.Tensor.chunk(Tensor([1901418, 300, 4],"float32"), 2, )
2025-03-12 13:21:26.517866 test begin: paddle.Tensor.chunk(Tensor([2, 285212673, 4],"float32"), 2, )

[Pass] paddle.Tensor.chunk(Tensor([2, 285212673, 4],"float32"), 2, )
2025-03-12 13:25:00.597973 test begin: paddle.Tensor.chunk(Tensor([2, 900, 1267612],"float32"), 2, )

[Pass] paddle.Tensor.chunk(Tensor([2, 900, 1267612],"float32"), 2, )
2025-03-12 13:28:15.680574 test begin: paddle.Tensor.chunk(Tensor([23211, 32, 64, 48],"float32"), 2, axis=1, )

[Pass] paddle.Tensor.chunk(Tensor([23211, 32, 64, 48],"float32"), 2, axis=1, )
2025-03-12 13:31:25.720760 test begin: paddle.Tensor.chunk(Tensor([2546542, 128, 7],"float32"), 2, axis=1, )

[Pass] paddle.Tensor.chunk(Tensor([2546542, 128, 7],"float32"), 2, axis=1, )
2025-03-12 13:34:18.348841 test begin: paddle.Tensor.chunk(Tensor([30316, 49, 1536],"float32"), 3, axis=-1, )

[Pass] paddle.Tensor.chunk(Tensor([30316, 49, 1536],"float32"), 3, axis=-1, )
2025-03-12 13:37:11.746204 test begin: paddle.Tensor.chunk(Tensor([5538111, 103, 1, 4],"float32"), 4, axis=-1, )

[Pass] paddle.Tensor.chunk(Tensor([5538111, 103, 1, 4],"float32"), 4, axis=-1, )
2025-03-12 13:40:05.731366 test begin: paddle.Tensor.chunk(Tensor([5592406, 102, 1, 4],"float32"), 4, axis=-1, )

[Pass] paddle.Tensor.chunk(Tensor([5592406, 102, 1, 4],"float32"), 4, axis=-1, )
2025-03-12 13:42:59.226708 test begin: paddle.Tensor.chunk(Tensor([5647776, 101, 1, 4],"float32"), 4, axis=-1, )

[Pass] paddle.Tensor.chunk(Tensor([5647776, 101, 1, 4],"float32"), 4, axis=-1, )
2025-03-12 13:45:48.650652 test begin: paddle.Tensor.chunk(Tensor([5803, 32, 128, 96],"float32"), 2, axis=1, )

[Pass] paddle.Tensor.chunk(Tensor([5803, 32, 128, 96],"float32"), 2, axis=1, )
2025-03-12 13:48:37.674043 test begin: paddle.Tensor.chunk(Tensor([5941931, 128, 3],"float32"), 2, axis=1, )

[Pass] paddle.Tensor.chunk(Tensor([5941931, 128, 3],"float32"), 2, axis=1, )
2025-03-12 13:51:18.501277 test begin: paddle.Tensor.chunk(Tensor([594194, 320, 4, 3],"float32"), 2, axis=1, )

[Pass] paddle.Tensor.chunk(Tensor([594194, 320, 4, 3],"float32"), 2, axis=1, )
2025-03-12 13:54:02.919401 test begin: paddle.Tensor.chunk(Tensor([633806, 900, 4],"float32"), 2, )

[Pass] paddle.Tensor.chunk(Tensor([633806, 900, 4],"float32"), 2, )
2025-03-12 13:56:54.620982 test begin: paddle.Tensor.chunk(Tensor([7579, 784, 384],"float32"), 3, axis=-1, )

[Pass] paddle.Tensor.chunk(Tensor([7579, 784, 384],"float32"), 3, axis=-1, )
2025-03-12 13:59:52.333607 test begin: paddle.Tensor.chunk(Tensor([838861, 1, 1360, 2],"float32"), 2, axis=-1, )

[Pass] paddle.Tensor.chunk(Tensor([838861, 1, 1360, 2],"float32"), 2, axis=-1, )
2025-03-12 14:02:50.210515 test begin: paddle.Tensor.chunk(Tensor([94317, 1, 12096, 2],"float32"), 2, axis=-1, )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 86, in func_timeout
    thread.join(timeout)
  File "/usr/lib/python3.9/threading.py", line 1064, in join
    self._wait_for_tstate_lock(timeout=max(timeout, 0))
  File "/usr/lib/python3.9/threading.py", line 1080, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt
2025-03-13 10:09:58.254762 test begin: paddle.Tensor.chunk(Tensor([950709, 300, 8],"float32"), 1, )

W0313 10:11:23.083541 11415 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0313 10:11:23.084827 11415 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.Tensor.chunk(Tensor([950709, 300, 8],"float32"), 1, )
2025-03-13 10:14:06.578329 test begin: paddle.Tensor.clip(Tensor([1, 1, 21504, 106106],"float32"), 0, )

[Pass] paddle.Tensor.clip(Tensor([1, 1, 21504, 106106],"float32"), 0, )
2025-03-13 10:17:35.830997 test begin: paddle.Tensor.clip(Tensor([1, 1, 24276, 93991],"float32"), 0, )

[Pass] paddle.Tensor.clip(Tensor([1, 1, 24276, 93991],"float32"), 0, )
2025-03-13 10:20:43.834973 test begin: paddle.Tensor.clip(Tensor([1, 1, 27216, 83837],"float32"), 0, )

[Pass] paddle.Tensor.clip(Tensor([1, 1, 27216, 83837],"float32"), 0, )
2025-03-13 10:23:41.195698 test begin: paddle.Tensor.clip(Tensor([1, 1004, 2272611],"float32"), min=0, )

[Pass] paddle.Tensor.clip(Tensor([1, 1004, 2272611],"float32"), min=0, )
2025-03-13 10:26:40.626544 test begin: paddle.Tensor.clip(Tensor([1, 10285, 221848],"float32"), 0, )

[Pass] paddle.Tensor.clip(Tensor([1, 10285, 221848],"float32"), 0, )
2025-03-13 10:29:50.304203 test begin: paddle.Tensor.clip(Tensor([1, 1031, 2213096],"float32"), min=0, )

[Pass] paddle.Tensor.clip(Tensor([1, 1031, 2213096],"float32"), min=0, )
2025-03-13 10:32:55.623260 test begin: paddle.Tensor.clip(Tensor([1, 1034, 2206675],"float32"), min=0, )

[Pass] paddle.Tensor.clip(Tensor([1, 1034, 2206675],"float32"), min=0, )
2025-03-13 10:36:13.836545 test begin: paddle.Tensor.clip(Tensor([1, 1100, 2074274],"float32"), min=0.0, max=1.0, )

[Pass] paddle.Tensor.clip(Tensor([1, 1100, 2074274],"float32"), min=0.0, max=1.0, )
2025-03-13 10:39:20.655580 test begin: paddle.Tensor.clip(Tensor([1, 1100, 2074274],"float32"), min=1e-05, )

[Pass] paddle.Tensor.clip(Tensor([1, 1100, 2074274],"float32"), min=1e-05, )
2025-03-13 10:42:17.782074 test begin: paddle.Tensor.clip(Tensor([1, 11109, 205393],"float32"), 0, )

[Pass] paddle.Tensor.clip(Tensor([1, 11109, 205393],"float32"), 0, )
2025-03-13 10:45:04.054475 test begin: paddle.Tensor.clip(Tensor([1, 12096, 188633],"float32"), 0, )

[Pass] paddle.Tensor.clip(Tensor([1, 12096, 188633],"float32"), 0, )
2025-03-13 10:48:06.482819 test begin: paddle.Tensor.clip(Tensor([1, 18, 126761188],"float32"), min=0.0, max=1.0, )

[Pass] paddle.Tensor.clip(Tensor([1, 18, 126761188],"float32"), min=0.0, max=1.0, )
2025-03-13 10:51:11.658187 test begin: paddle.Tensor.clip(Tensor([1, 18, 126761188],"float32"), min=1e-05, )

[Pass] paddle.Tensor.clip(Tensor([1, 18, 126761188],"float32"), min=1e-05, )
2025-03-13 10:54:15.204370 test begin: paddle.Tensor.clip(Tensor([1, 192, 11883862],"float32"), min=0.0, max=1.0, )

[Pass] paddle.Tensor.clip(Tensor([1, 192, 11883862],"float32"), min=0.0, max=1.0, )
2025-03-13 10:57:10.148798 test begin: paddle.Tensor.clip(Tensor([1, 192, 11883862],"float32"), min=1e-05, )

[Pass] paddle.Tensor.clip(Tensor([1, 192, 11883862],"float32"), min=1e-05, )
2025-03-13 11:00:11.922973 test begin: paddle.Tensor.clip(Tensor([1, 2, 1073741825, 2],"float16"), min=0.0, max=1.0, )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7fdd8018fbb0>,)) (kwargs={}) timed out after 1200.000000 seconds.

2025-03-13 11:20:21.207538 test begin: paddle.Tensor.clip(Tensor([1, 2, 1073741825, 2],"float16"), min=1e-05, )

W0313 11:22:00.114826 43978 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0313 11:22:00.116117 43978 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f9221527c10>,)) (kwargs={}) timed out after 1200.000000 seconds.

2025-03-13 11:40:30.666809 test begin: paddle.Tensor.clip(Tensor([1, 2, 300, 7158279],"float16"), min=0.0, max=1.0, )

W0313 11:42:19.538748 53714 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0313 11:42:19.540004 53714 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f8a2a42dbb0>,)) (kwargs={}) timed out after 1200.000000 seconds.

2025-03-13 12:00:40.831298 test begin: paddle.Tensor.clip(Tensor([1, 2, 300, 7158279],"float16"), min=1e-05, )

W0313 12:02:34.202647 62664 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0313 12:02:34.204012 62664 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.Tensor.clip(Tensor([1, 2, 300, 7158279],"float16"), min=1e-05, )
2025-03-13 12:22:42.258818 test begin: paddle.Tensor.clip(Tensor([1, 21, 108652447],"float32"), min=0.001, )

[Pass] paddle.Tensor.clip(Tensor([1, 21, 108652447],"float32"), min=0.001, )
2025-03-13 12:27:19.821957 test begin: paddle.Tensor.clip(Tensor([1, 22, 103713700],"float32"), min=0.001, )

[Pass] paddle.Tensor.clip(Tensor([1, 22, 103713700],"float32"), min=0.001, )
2025-03-13 12:30:31.517281 test begin: paddle.Tensor.clip(Tensor([1, 27216, 83837],"float32"), -2, 18.99, )

[Pass] paddle.Tensor.clip(Tensor([1, 27216, 83837],"float32"), -2, 18.99, )
2025-03-13 12:33:38.581944 test begin: paddle.Tensor.clip(Tensor([1, 30324, 75245],"float32"), -2, 18.99, )

[Pass] paddle.Tensor.clip(Tensor([1, 30324, 75245],"float32"), -2, 18.99, )
2025-03-13 12:36:44.204114 test begin: paddle.Tensor.clip(Tensor([1, 33600, 67908],"float32"), -2, 18.99, )

[Pass] paddle.Tensor.clip(Tensor([1, 33600, 67908],"float32"), -2, 18.99, )
2025-03-13 12:39:48.562481 test begin: paddle.Tensor.clip(Tensor([1, 41919, 27216, 2],"float32"), 0, )

[Pass] paddle.Tensor.clip(Tensor([1, 41919, 27216, 2],"float32"), 0, )
2025-03-13 12:43:14.595065 test begin: paddle.Tensor.clip(Tensor([1, 46996, 24276, 2],"float32"), 0, )

[Pass] paddle.Tensor.clip(Tensor([1, 46996, 24276, 2],"float32"), 0, )
2025-03-13 12:47:11.881746 test begin: paddle.Tensor.clip(Tensor([1, 53053, 21504, 2],"float32"), 0, )

[Pass] paddle.Tensor.clip(Tensor([1, 53053, 21504, 2],"float32"), 0, )
2025-03-13 12:50:23.802312 test begin: paddle.Tensor.clip(Tensor([1, 6, 11883862, 32],"float32"), -57344, 57344, )

[Pass] paddle.Tensor.clip(Tensor([1, 6, 11883862, 32],"float32"), -57344, 57344, )
2025-03-13 12:54:08.953794 test begin: paddle.Tensor.clip(Tensor([1, 6, 16, 23767723],"float32"), -57344, 57344, )

[Pass] paddle.Tensor.clip(Tensor([1, 6, 16, 23767723],"float32"), -57344, 57344, )
2025-03-13 12:57:17.177075 test begin: paddle.Tensor.clip(Tensor([1, 6, 23767723, 16],"float32"), -57344, 57344, )

[Pass] paddle.Tensor.clip(Tensor([1, 6, 23767723, 16],"float32"), -57344, 57344, )
2025-03-13 13:00:27.974602 test begin: paddle.Tensor.clip(Tensor([1, 6, 32, 11883862],"float32"), -57344, 57344, )

[Pass] paddle.Tensor.clip(Tensor([1, 6, 32, 11883862],"float32"), -57344, 57344, )
2025-03-13 13:04:08.756251 test begin: paddle.Tensor.clip(Tensor([1, 7158279, 300, 2],"float16"), min=0.0, max=1.0, )

[accuracy error] backward  paddle.Tensor.clip(Tensor([1, 7158279, 300, 2],"float16"), min=0.0, max=1.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 261 / 4294967400 (6.08e-06%)
Max absolute difference: 0.4934
Max relative difference: 1.
 x: array([[[[ 0.     , -0.11005],
         [ 0.     ,  0.     ],
         [ 0.     ,  0.     ],...
 y: array([[[[ 0.     , -0.11005],
         [ 0.     ,  0.     ],
         [ 0.     ,  0.     ],...
2025-03-13 13:27:08.189180 test begin: paddle.Tensor.clip(Tensor([1, 7158279, 300, 2],"float16"), min=1e-05, )

[Pass] paddle.Tensor.clip(Tensor([1, 7158279, 300, 2],"float16"), min=1e-05, )
2025-03-13 13:48:12.074944 test begin: paddle.Tensor.clip(Tensor([1, 8912897, 16, 16],"float32"), -57344, 57344, )

[Pass] paddle.Tensor.clip(Tensor([1, 8912897, 16, 16],"float32"), -57344, 57344, )
2025-03-13 13:51:49.539099 test begin: paddle.Tensor.clip(Tensor([1, 9, 253522376],"float32"), min=0, max=1, )

[Pass] paddle.Tensor.clip(Tensor([1, 9, 253522376],"float32"), min=0, max=1, )
2025-03-13 13:55:20.226742 test begin: paddle.Tensor.clip(Tensor([1, 9, 253522376],"float32"), min=0.001, )

[Pass] paddle.Tensor.clip(Tensor([1, 9, 253522376],"float32"), min=0.001, )
2025-03-13 13:58:39.841243 test begin: paddle.Tensor.clip(Tensor([10, 429496730],"float16"), max=0, )

[accuracy error] backward  paddle.Tensor.clip(Tensor([10, 429496730],"float16"), max=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 265 / 4294967300 (6.17e-06%)
Max absolute difference: 0.4983
Max relative difference: 1.
 x: array([[-0.4316 ,  0.     ,  0.0638 , ...,  0.     ,  0.3472 ,  0.397  ],
       [ 0.     ,  0.02472,  0.     , ..., -0.06256,  0.01158,  0.     ],
       [ 0.     ,  0.     ,  0.     , ...,  0.     , -0.4075 , -0.3052 ],...
 y: array([[-0.4316 ,  0.     ,  0.0638 , ...,  0.     ,  0.3472 ,  0.397  ],
       [ 0.     ,  0.02472,  0.     , ..., -0.06256,  0.01158,  0.     ],
       [ 0.     ,  0.     ,  0.     , ...,  0.     , -0.4075 , -0.3052 ],...
2025-03-13 14:22:08.824007 test begin: paddle.Tensor.clip(Tensor([10, 429496730],"float16"), min=0, )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 86, in func_timeout
    thread.join(timeout)
  File "/usr/lib/python3.9/threading.py", line 1064, in join
    self._wait_for_tstate_lock(timeout=max(timeout, 0))
  File "/usr/lib/python3.9/threading.py", line 1080, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt
2025-03-14 09:07:53.175807 test begin: paddle.Tensor.clip(Tensor([100, 22817014],"float32"), 0, )

W0314 09:09:19.941738 129916 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 09:09:19.943002 129916 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.Tensor.clip(Tensor([100, 22817014],"float32"), 0, )
2025-03-14 09:11:43.171511 test begin: paddle.Tensor.clip(Tensor([1001, 2279422],"float32"), 0, )

[Pass] paddle.Tensor.clip(Tensor([1001, 2279422],"float32"), 0, )
2025-03-14 09:14:48.528203 test begin: paddle.Tensor.clip(Tensor([1002, 2277148],"float32"), 0, )

[Pass] paddle.Tensor.clip(Tensor([1002, 2277148],"float32"), 0, )
2025-03-14 09:17:55.797121 test begin: paddle.Tensor.clip(Tensor([102697, 11109, 2],"float32"), 0, )

[Pass] paddle.Tensor.clip(Tensor([102697, 11109, 2],"float32"), 0, )
2025-03-14 09:21:19.279678 test begin: paddle.Tensor.clip(Tensor([1103338, 1034, 2],"float32"), min=0, )

[Pass] paddle.Tensor.clip(Tensor([1103338, 1034, 2],"float32"), min=0, )
2025-03-14 09:24:18.249937 test begin: paddle.Tensor.clip(Tensor([1106548, 1031, 2],"float32"), min=0, )

[Pass] paddle.Tensor.clip(Tensor([1106548, 1031, 2],"float32"), min=0, )
2025-03-14 09:27:25.318582 test begin: paddle.Tensor.clip(Tensor([110924, 10285, 2],"float32"), 0, )

[Pass] paddle.Tensor.clip(Tensor([110924, 10285, 2],"float32"), 0, )
2025-03-14 09:30:42.561926 test begin: paddle.Tensor.clip(Tensor([1136306, 1004, 2],"float32"), min=0, )

[Pass] paddle.Tensor.clip(Tensor([1136306, 1004, 2],"float32"), min=0, )
2025-03-14 09:33:44.991761 test begin: paddle.Tensor.clip(Tensor([12, 118839, 40, 40, 1],"float32"), 0, )

[Pass] paddle.Tensor.clip(Tensor([12, 118839, 40, 40, 1],"float32"), 0, )
2025-03-14 09:36:39.998933 test begin: paddle.Tensor.clip(Tensor([12, 3, 1584515, 40, 1],"float32"), 0, )

[Pass] paddle.Tensor.clip(Tensor([12, 3, 1584515, 40, 1],"float32"), 0, )
2025-03-14 09:40:03.690563 test begin: paddle.Tensor.clip(Tensor([12, 3, 40, 1584515, 1],"float32"), 0, )

[Pass] paddle.Tensor.clip(Tensor([12, 3, 40, 1584515, 1],"float32"), 0, )
2025-03-14 09:43:06.917335 test begin: paddle.Tensor.clip(Tensor([12, 3, 40, 40, 39613],"float32"), 0, )

[Pass] paddle.Tensor.clip(Tensor([12, 3, 40, 40, 39613],"float32"), 0, )
2025-03-14 09:45:58.062417 test begin: paddle.Tensor.clip(Tensor([1485483, 6, 16, 16],"float32"), -57344, 57344, )

[Pass] paddle.Tensor.clip(Tensor([1485483, 6, 16, 16],"float32"), -57344, 57344, )
2025-03-14 09:49:05.237464 test begin: paddle.Tensor.clip(Tensor([16, 11109, 12838],"float32"), 0, 15.99, )

[Pass] paddle.Tensor.clip(Tensor([16, 11109, 12838],"float32"), 0, 15.99, )
2025-03-14 09:52:16.486394 test begin: paddle.Tensor.clip(Tensor([16, 12096, 11790],"float32"), 0, 15.99, )

[Pass] paddle.Tensor.clip(Tensor([16, 12096, 11790],"float32"), 0, 15.99, )
2025-03-14 09:55:56.509823 test begin: paddle.Tensor.clip(Tensor([16, 2100, 67908],"float32"), 0, 15.99, )

[Pass] paddle.Tensor.clip(Tensor([16, 2100, 67908],"float32"), 0, 15.99, )
2025-03-14 09:58:56.869047 test begin: paddle.Tensor.clip(Tensor([16977, 33600, 4],"float32"), -2, 18.99, )

[Pass] paddle.Tensor.clip(Tensor([16977, 33600, 4],"float32"), -2, 18.99, )
2025-03-14 10:02:14.226018 test begin: paddle.Tensor.clip(Tensor([18812, 30324, 4],"float32"), -2, 18.99, )

[Pass] paddle.Tensor.clip(Tensor([18812, 30324, 4],"float32"), -2, 18.99, )
2025-03-14 10:05:21.071899 test begin: paddle.Tensor.clip(Tensor([2, 12096, 94317],"float32"), -2, 6.99, )

[Pass] paddle.Tensor.clip(Tensor([2, 12096, 94317],"float32"), -2, 6.99, )
2025-03-14 10:08:12.333317 test begin: paddle.Tensor.clip(Tensor([2, 2100, 543263],"float32"), -2, 6.99, )

[Pass] paddle.Tensor.clip(Tensor([2, 2100, 543263],"float32"), -2, 6.99, )
2025-03-14 10:11:32.594551 test begin: paddle.Tensor.clip(Tensor([2, 2541, 448978],"float32"), -2, 6.99, )

[Pass] paddle.Tensor.clip(Tensor([2, 2541, 448978],"float32"), -2, 6.99, )
2025-03-14 10:15:03.180548 test begin: paddle.Tensor.clip(Tensor([20960, 27216, 4],"float32"), -2, 18.99, )

[Pass] paddle.Tensor.clip(Tensor([20960, 27216, 4],"float32"), -2, 18.99, )
2025-03-14 10:18:50.261504 test begin: paddle.Tensor.clip(Tensor([214748365, 20],"float16"), max=0, )

[accuracy error] backward  paddle.Tensor.clip(Tensor([214748365, 20],"float16"), max=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 245 / 4294967300 (5.7e-06%)
Max absolute difference: 0.4983
Max relative difference: 1.
 x: array([[ 0.3965 ,  0.     ,  0.09125, ...,  0.2993 ,  0.3376 ,  0.0999 ],
       [ 0.     , -0.2766 ,  0.     , ...,  0.     , -0.01197,  0.4907 ],
       [ 0.2573 ,  0.     , -0.445  , ...,  0.     ,  0.     , -0.1853 ],...
 y: array([[ 0.3965 ,  0.     ,  0.09125, ...,  0.2993 ,  0.3376 ,  0.0999 ],
       [ 0.     , -0.2766 ,  0.     , ...,  0.     , -0.01197,  0.4907 ],
       [ 0.2573 ,  0.     , -0.445  , ...,  0.     ,  0.     , -0.1853 ],...
2025-03-14 10:43:15.514624 test begin: paddle.Tensor.clip(Tensor([214748365, 20],"float16"), min=0, )

[accuracy error] backward  paddle.Tensor.clip(Tensor([214748365, 20],"float16"), min=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 245 / 4294967300 (5.7e-06%)
Max absolute difference: 0.4983
Max relative difference: 1.
 x: array([[ 0.      , -0.2208  ,  0.      , ...,  0.      ,  0.      ,
         0.      ],
       [-0.1982  ,  0.      , -0.3093  , ..., -0.012085,  0.      ,...
 y: array([[ 0.      , -0.2208  ,  0.      , ...,  0.      ,  0.      ,
         0.      ],
       [-0.1982  ,  0.      , -0.3093  , ..., -0.012085,  0.      ,...
2025-03-14 11:07:00.112329 test begin: paddle.Tensor.clip(Tensor([224489, 2541, 4],"float32"), -2, 6.99, )

[Pass] paddle.Tensor.clip(Tensor([224489, 2541, 4],"float32"), -2, 6.99, )
2025-03-14 11:10:26.500594 test begin: paddle.Tensor.clip(Tensor([2281701379],"float32"), 0, )

[Pass] paddle.Tensor.clip(Tensor([2281701379],"float32"), 0, )
2025-03-14 11:13:30.891565 test begin: paddle.Tensor.clip(Tensor([2281701379],"float32"), 0, 4.605170185988092, )

[Pass] paddle.Tensor.clip(Tensor([2281701379],"float32"), 0, 4.605170185988092, )
2025-03-14 11:16:37.771794 test begin: paddle.Tensor.clip(Tensor([2281701379],"float32"), 0, 89.99, )

[Pass] paddle.Tensor.clip(Tensor([2281701379],"float32"), 0, 89.99, )
2025-03-14 11:19:39.570796 test begin: paddle.Tensor.clip(Tensor([2281701379],"float32"), max=0, )

[Pass] paddle.Tensor.clip(Tensor([2281701379],"float32"), max=0, )
2025-03-14 11:22:52.222366 test begin: paddle.Tensor.clip(Tensor([2281701379],"float32"), min=0, )

[Pass] paddle.Tensor.clip(Tensor([2281701379],"float32"), min=0, )
2025-03-14 11:26:57.807154 test begin: paddle.Tensor.clip(Tensor([2281701379],"float32"), min=0, max=15.9, )

[Pass] paddle.Tensor.clip(Tensor([2281701379],"float32"), min=0, max=15.9, )
2025-03-14 11:30:29.690506 test begin: paddle.Tensor.clip(Tensor([2281701379],"float32"), min=0, max=6.9, )

[Pass] paddle.Tensor.clip(Tensor([2281701379],"float32"), min=0, max=6.9, )
2025-03-14 11:34:09.526453 test begin: paddle.Tensor.clip(Tensor([2281701379],"float32"), min=0.0001, )

[Pass] paddle.Tensor.clip(Tensor([2281701379],"float32"), min=0.0001, )
2025-03-14 11:37:29.348688 test begin: paddle.Tensor.clip(Tensor([2281701379],"int32"), 0, 7, )

[Pass] paddle.Tensor.clip(Tensor([2281701379],"int32"), 0, 7, )
2025-03-14 11:40:34.138321 test begin: paddle.Tensor.clip(Tensor([2281701379],"int32"), 0, 8, )

[Pass] paddle.Tensor.clip(Tensor([2281701379],"int32"), 0, 8, )
2025-03-14 11:43:02.993160 test begin: paddle.Tensor.clip(Tensor([2281701379],"int64"), 0, 512, )

[Pass] paddle.Tensor.clip(Tensor([2281701379],"int64"), 0, 512, )
2025-03-14 11:47:13.956493 test begin: paddle.Tensor.clip(Tensor([228170138, 10],"float32"), -448, 448, )

[Pass] paddle.Tensor.clip(Tensor([228170138, 10],"float32"), -448, 448, )
2025-03-14 11:50:11.496948 test begin: paddle.Tensor.clip(Tensor([23, 99204408],"float32"), max=1, )

[Pass] paddle.Tensor.clip(Tensor([23, 99204408],"float32"), max=1, )
2025-03-14 11:53:14.962564 test begin: paddle.Tensor.clip(Tensor([23, 99204408],"float32"), min=1e-08, )

[Pass] paddle.Tensor.clip(Tensor([23, 99204408],"float32"), min=1e-08, )
2025-03-14 11:56:38.831660 test begin: paddle.Tensor.clip(Tensor([23498, 24276, 4],"float32"), -2, 18.99, )

[Pass] paddle.Tensor.clip(Tensor([23498, 24276, 4],"float32"), -2, 18.99, )
2025-03-14 12:00:33.735616 test begin: paddle.Tensor.clip(Tensor([253522376, 1, 3, 3],"float32"), -448, 448, )

[Pass] paddle.Tensor.clip(Tensor([253522376, 1, 3, 3],"float32"), -448, 448, )
2025-03-14 12:03:51.348923 test begin: paddle.Tensor.clip(Tensor([25928425, 22, 4],"float32"), min=0.001, )

[Pass] paddle.Tensor.clip(Tensor([25928425, 22, 4],"float32"), min=0.001, )
2025-03-14 12:07:09.924210 test begin: paddle.Tensor.clip(Tensor([26527, 21504, 4],"float32"), -2, 18.99, )

[Pass] paddle.Tensor.clip(Tensor([26527, 21504, 4],"float32"), -2, 18.99, )
2025-03-14 12:10:47.095809 test begin: paddle.Tensor.clip(Tensor([27163112, 21, 4],"float32"), min=0.001, )

[Pass] paddle.Tensor.clip(Tensor([27163112, 21, 4],"float32"), min=0.001, )
2025-03-14 12:14:00.995849 test begin: paddle.Tensor.clip(Tensor([27163112, 84],"float32"), -448, 448, )

[Pass] paddle.Tensor.clip(Tensor([27163112, 84],"float32"), -448, 448, )
2025-03-14 12:16:37.820478 test begin: paddle.Tensor.clip(Tensor([27163112, 84],"float32"), -57344, 57344, )

[Pass] paddle.Tensor.clip(Tensor([27163112, 84],"float32"), -57344, 57344, )
2025-03-14 12:19:51.338333 test begin: paddle.Tensor.clip(Tensor([271632, 2100, 4],"float32"), -2, 6.99, )

[Pass] paddle.Tensor.clip(Tensor([271632, 2100, 4],"float32"), -2, 6.99, )
2025-03-14 12:23:33.757826 test begin: paddle.Tensor.clip(Tensor([271632, 2100, 4],"float32"), 0, 15.99, )

[Pass] paddle.Tensor.clip(Tensor([271632, 2100, 4],"float32"), 0, 15.99, )
2025-03-14 12:26:40.954240 test begin: paddle.Tensor.clip(Tensor([2910334, 1, 28, 28],"float32"), -128, 127, )

[Pass] paddle.Tensor.clip(Tensor([2910334, 1, 28, 28],"float32"), -128, 127, )
2025-03-14 12:30:32.577279 test begin: paddle.Tensor.clip(Tensor([2910334, 28, 28],"float32"), 0, 1.0, )

[Pass] paddle.Tensor.clip(Tensor([2910334, 28, 28],"float32"), 0, 1.0, )
2025-03-14 12:33:46.568136 test begin: paddle.Tensor.clip(Tensor([2970966, 192, 4],"float32"), min=0.0, max=1.0, )

[Pass] paddle.Tensor.clip(Tensor([2970966, 192, 4],"float32"), min=0.0, max=1.0, )
2025-03-14 12:37:02.199851 test begin: paddle.Tensor.clip(Tensor([2970966, 192, 4],"float32"), min=1e-05, )

[Pass] paddle.Tensor.clip(Tensor([2970966, 192, 4],"float32"), min=1e-05, )
2025-03-14 12:40:09.676306 test begin: paddle.Tensor.clip(Tensor([3, 27163112, 28],"float32"), 0, 1.0, )

[Pass] paddle.Tensor.clip(Tensor([3, 27163112, 28],"float32"), 0, 1.0, )
2025-03-14 12:43:39.943325 test begin: paddle.Tensor.clip(Tensor([3, 28, 27163112],"float32"), 0, 1.0, )

[Pass] paddle.Tensor.clip(Tensor([3, 28, 27163112],"float32"), 0, 1.0, )
2025-03-14 12:47:18.758018 test begin: paddle.Tensor.clip(Tensor([3, 8, 95070891],"float32"), 0, 1.0, )

[Pass] paddle.Tensor.clip(Tensor([3, 8, 95070891],"float32"), 0, 1.0, )
2025-03-14 12:50:20.201606 test begin: paddle.Tensor.clip(Tensor([3, 95070891, 8],"float32"), 0, 1.0, )

[Pass] paddle.Tensor.clip(Tensor([3, 95070891, 8],"float32"), 0, 1.0, )
2025-03-14 12:53:39.444281 test begin: paddle.Tensor.clip(Tensor([31690297, 18, 4],"float32"), min=0.0, max=1.0, )

[Pass] paddle.Tensor.clip(Tensor([31690297, 18, 4],"float32"), min=0.0, max=1.0, )
2025-03-14 12:56:44.565418 test begin: paddle.Tensor.clip(Tensor([31690297, 18, 4],"float32"), min=0.001, )

[Pass] paddle.Tensor.clip(Tensor([31690297, 18, 4],"float32"), min=0.001, )
2025-03-14 12:59:49.755564 test begin: paddle.Tensor.clip(Tensor([31690297, 18, 4],"float32"), min=1e-05, )

[Pass] paddle.Tensor.clip(Tensor([31690297, 18, 4],"float32"), min=1e-05, )
2025-03-14 13:03:05.324236 test begin: paddle.Tensor.clip(Tensor([32, 71303169],"float32"), max=1, )

[Pass] paddle.Tensor.clip(Tensor([32, 71303169],"float32"), max=1, )
2025-03-14 13:07:05.419537 test begin: paddle.Tensor.clip(Tensor([32, 71303169],"float32"), min=1e-08, )

[Pass] paddle.Tensor.clip(Tensor([32, 71303169],"float32"), min=1e-08, )
2025-03-14 13:11:03.406975 test begin: paddle.Tensor.clip(Tensor([325957340, 7],"int32"), 2, )

[Pass] paddle.Tensor.clip(Tensor([325957340, 7],"int32"), 2, )
2025-03-14 13:13:47.410346 test begin: paddle.Tensor.clip(Tensor([325957340, 7],"int64"), 3, )

[Pass] paddle.Tensor.clip(Tensor([325957340, 7],"int64"), 3, )
2025-03-14 13:17:58.177602 test begin: paddle.Tensor.clip(Tensor([35651585, 8, 8],"float32"), 0, 1.0, )

[Pass] paddle.Tensor.clip(Tensor([35651585, 8, 8],"float32"), 0, 1.0, )
2025-03-14 13:21:42.029851 test begin: paddle.Tensor.clip(Tensor([3579140, 2, 300, 2],"float16"), min=0.0, max=1.0, )

[accuracy error] backward  paddle.Tensor.clip(Tensor([3579140, 2, 300, 2],"float16"), min=0.0, max=1.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 243 / 4294968000 (5.66e-06%)
Max absolute difference: 0.4998
Max relative difference: 1.
 x: array([[[[ 0.     , -0.02585],
         [ 0.     , -0.2169 ],
         [ 0.     , -0.0898 ],...
 y: array([[[[ 0.     , -0.02585],
         [ 0.     , -0.2169 ],
         [ 0.     , -0.0898 ],...
2025-03-14 13:46:48.250919 test begin: paddle.Tensor.clip(Tensor([3579140, 2, 300, 2],"float16"), min=1e-05, )

[Pass] paddle.Tensor.clip(Tensor([3579140, 2, 300, 2],"float16"), min=1e-05, )
2025-03-14 14:08:48.781781 test begin: paddle.Tensor.clip(Tensor([371371, 6, 32, 32],"float32"), -57344, 57344, )

[Pass] paddle.Tensor.clip(Tensor([371371, 6, 32, 32],"float32"), -57344, 57344, )
2025-03-14 14:12:59.337276 test begin: paddle.Tensor.clip(Tensor([380283564, 6],"float32"), -128, 127, )

[Pass] paddle.Tensor.clip(Tensor([380283564, 6],"float32"), -128, 127, )
2025-03-14 14:16:35.354383 test begin: paddle.Tensor.clip(Tensor([3961288, 576],"float32"), -57344, 57344, )

[Pass] paddle.Tensor.clip(Tensor([3961288, 576],"float32"), -57344, 57344, )
2025-03-14 14:19:45.638659 test begin: paddle.Tensor.clip(Tensor([4, 570425345],"float32"), max=1, )

[Pass] paddle.Tensor.clip(Tensor([4, 570425345],"float32"), max=1, )
2025-03-14 14:22:46.484840 test begin: paddle.Tensor.clip(Tensor([4, 570425345],"float32"), min=1e-08, )

[Pass] paddle.Tensor.clip(Tensor([4, 570425345],"float32"), min=1e-08, )
2025-03-14 14:25:44.396226 test begin: paddle.Tensor.clip(Tensor([41919, 1, 27216, 2],"float32"), 0, )

[Pass] paddle.Tensor.clip(Tensor([41919, 1, 27216, 2],"float32"), 0, )
2025-03-14 14:28:26.089721 test begin: paddle.Tensor.clip(Tensor([4294967297],"float16"), max=0, )

[accuracy error] backward  paddle.Tensor.clip(Tensor([4294967297],"float16"), max=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 246 / 4294967297 (5.73e-06%)
Max absolute difference: 0.4912
Max relative difference: 1.
 x: array([ 0.4834,  0.    , -0.2114, ..., -0.4722,  0.1757,  0.    ],
      dtype=float16)
 y: array([ 0.4834,  0.    , -0.2114, ..., -0.4722,  0.1757,  0.    ],
      dtype=float16)
2025-03-14 14:51:38.528408 test begin: paddle.Tensor.clip(Tensor([4294967297],"float16"), min=0, )

[accuracy error] backward  paddle.Tensor.clip(Tensor([4294967297],"float16"), min=0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 246 / 4294967297 (5.73e-06%)
Max absolute difference: 0.4912
Max relative difference: 1.
 x: array([ 0.    ,  0.2324,  0.    , ...,  0.    ,  0.    , -0.4163],
      dtype=float16)
 y: array([ 0.    ,  0.2324,  0.    , ...,  0.    ,  0.    , -0.4163],
      dtype=float16)
2025-03-14 15:14:58.623998 test begin: paddle.Tensor.clip(Tensor([46996, 1, 24276, 2],"float32"), 0, )

[Pass] paddle.Tensor.clip(Tensor([46996, 1, 24276, 2],"float32"), 0, )
2025-03-14 15:18:21.676214 test begin: paddle.Tensor.clip(Tensor([47159, 12096, 4],"float32"), -2, 6.99, )

[Pass] paddle.Tensor.clip(Tensor([47159, 12096, 4],"float32"), -2, 6.99, )
2025-03-14 15:21:23.427789 test begin: paddle.Tensor.clip(Tensor([47159, 12096, 4],"float32"), 0, 15.99, )

[Pass] paddle.Tensor.clip(Tensor([47159, 12096, 4],"float32"), 0, 15.99, )
2025-03-14 15:24:31.323676 test begin: paddle.Tensor.clip(Tensor([475355, 3, 40, 40, 1],"float32"), 0, )

[Pass] paddle.Tensor.clip(Tensor([475355, 3, 40, 40, 1],"float32"), 0, )
2025-03-14 15:27:52.184566 test begin: paddle.Tensor.clip(Tensor([51349, 11109, 4],"float32"), -2, 6.99, )

[Pass] paddle.Tensor.clip(Tensor([51349, 11109, 4],"float32"), -2, 6.99, )
2025-03-14 15:31:22.218667 test begin: paddle.Tensor.clip(Tensor([51349, 11109, 4],"float32"), 0, 15.99, )

[Pass] paddle.Tensor.clip(Tensor([51349, 11109, 4],"float32"), 0, 15.99, )
2025-03-14 15:34:42.107989 test begin: paddle.Tensor.clip(Tensor([51856850, 11, 4],"float32"), min=0, max=1, )

[Pass] paddle.Tensor.clip(Tensor([51856850, 11, 4],"float32"), min=0, max=1, )
2025-03-14 15:38:16.765006 test begin: paddle.Tensor.clip(Tensor([51856850, 11, 4],"float32"), min=0.0, max=1.0, )

[Pass] paddle.Tensor.clip(Tensor([51856850, 11, 4],"float32"), min=0.0, max=1.0, )
2025-03-14 15:41:06.505757 test begin: paddle.Tensor.clip(Tensor([51856850, 11, 4],"float32"), min=1e-05, )

[Pass] paddle.Tensor.clip(Tensor([51856850, 11, 4],"float32"), min=1e-05, )
2025-03-14 15:44:08.302320 test begin: paddle.Tensor.clip(Tensor([518569, 1100, 4],"float32"), min=0.0, max=1.0, )

[Pass] paddle.Tensor.clip(Tensor([518569, 1100, 4],"float32"), min=0.0, max=1.0, )
2025-03-14 15:47:14.431661 test begin: paddle.Tensor.clip(Tensor([518569, 1100, 4],"float32"), min=0.001, )

[Pass] paddle.Tensor.clip(Tensor([518569, 1100, 4],"float32"), min=0.001, )
2025-03-14 15:50:26.205868 test begin: paddle.Tensor.clip(Tensor([518569, 1100, 4],"float32"), min=1e-05, )

[Pass] paddle.Tensor.clip(Tensor([518569, 1100, 4],"float32"), min=1e-05, )
2025-03-14 15:53:29.694769 test begin: paddle.Tensor.clip(Tensor([53053, 1, 21504, 2],"float32"), 0, )

[Pass] paddle.Tensor.clip(Tensor([53053, 1, 21504, 2],"float32"), 0, )
2025-03-14 15:56:44.871645 test begin: paddle.Tensor.clip(Tensor([56123, 10164, 4],"float32"), -2, 6.99, )

[Pass] paddle.Tensor.clip(Tensor([56123, 10164, 4],"float32"), -2, 6.99, )
2025-03-14 16:00:06.450396 test begin: paddle.Tensor.clip(Tensor([56123, 10164, 4],"float32"), 0, 15.99, )

[Pass] paddle.Tensor.clip(Tensor([56123, 10164, 4],"float32"), 0, 15.99, )
2025-03-14 16:03:19.940794 test begin: paddle.Tensor.clip(Tensor([5704254, 100, 4],"float32"), min=0.0, max=1.0, )

[Pass] paddle.Tensor.clip(Tensor([5704254, 100, 4],"float32"), min=0.0, max=1.0, )
2025-03-14 16:06:32.716217 test begin: paddle.Tensor.clip(Tensor([5704254, 100, 4],"float32"), min=1e-05, )

[Pass] paddle.Tensor.clip(Tensor([5704254, 100, 4],"float32"), min=1e-05, )
2025-03-14 16:09:46.615352 test begin: paddle.Tensor.clip(Tensor([576, 3961288],"float32"), -448, 448, )

[Pass] paddle.Tensor.clip(Tensor([576, 3961288],"float32"), -448, 448, )
2025-03-14 16:13:05.743077 test begin: paddle.Tensor.clip(Tensor([6, 1, 126761188, 3],"float32"), -448, 448, )

[Pass] paddle.Tensor.clip(Tensor([6, 1, 126761188, 3],"float32"), -448, 448, )
2025-03-14 16:16:38.875487 test begin: paddle.Tensor.clip(Tensor([6, 1, 3, 126761188],"float32"), -448, 448, )

[Pass] paddle.Tensor.clip(Tensor([6, 1, 3, 126761188],"float32"), -448, 448, )
2025-03-14 16:19:52.498310 test begin: paddle.Tensor.clip(Tensor([6, 42253730, 3, 3],"float32"), -448, 448, )

[Pass] paddle.Tensor.clip(Tensor([6, 42253730, 3, 3],"float32"), -448, 448, )
2025-03-14 16:22:57.554717 test begin: paddle.Tensor.clip(Tensor([63380594, 9, 4],"float32"), min=0, max=1, )

[Pass] paddle.Tensor.clip(Tensor([63380594, 9, 4],"float32"), min=0, max=1, )
2025-03-14 16:26:00.099394 test begin: paddle.Tensor.clip(Tensor([63380594, 9, 4],"float32"), min=0.001, )

[Pass] paddle.Tensor.clip(Tensor([63380594, 9, 4],"float32"), min=0.001, )
2025-03-14 16:28:58.896200 test begin: paddle.Tensor.clip(Tensor([64, 1, 1273271, 28],"float32"), -128, 127, )

[Pass] paddle.Tensor.clip(Tensor([64, 1, 1273271, 28],"float32"), -128, 127, )
2025-03-14 16:32:02.878969 test begin: paddle.Tensor.clip(Tensor([64, 1, 28, 1273271],"float32"), -128, 127, )

[Pass] paddle.Tensor.clip(Tensor([64, 1, 28, 1273271],"float32"), -128, 127, )
2025-03-14 16:35:12.552231 test begin: paddle.Tensor.clip(Tensor([64, 35651585],"float32"), max=1, )

[Pass] paddle.Tensor.clip(Tensor([64, 35651585],"float32"), max=1, )
2025-03-14 16:39:20.152214 test begin: paddle.Tensor.clip(Tensor([64, 35651585],"float32"), min=1e-08, )

[Pass] paddle.Tensor.clip(Tensor([64, 35651585],"float32"), min=1e-08, )
2025-03-14 16:43:01.717934 test begin: paddle.Tensor.clip(Tensor([64, 45474, 28, 28],"float32"), -128, 127, )

[Pass] paddle.Tensor.clip(Tensor([64, 45474, 28, 28],"float32"), -128, 127, )
2025-03-14 16:46:15.109128 test begin: paddle.Tensor.clip(Tensor([67908, 8400, 4],"float32"), 0, 15.99, )

[Pass] paddle.Tensor.clip(Tensor([67908, 8400, 4],"float32"), 0, 15.99, )
2025-03-14 16:49:28.453691 test begin: paddle.Tensor.clip(Tensor([69142467, 33],"float32"), max=1, )

[Pass] paddle.Tensor.clip(Tensor([69142467, 33],"float32"), max=1, )
2025-03-14 16:52:58.071306 test begin: paddle.Tensor.clip(Tensor([69142467, 33],"float32"), min=1e-08, )

[Pass] paddle.Tensor.clip(Tensor([69142467, 33],"float32"), min=1e-08, )
2025-03-14 16:56:29.293071 test begin: paddle.Tensor.clip(Tensor([760567127, 3],"int32"), 2, )

[Pass] paddle.Tensor.clip(Tensor([760567127, 3],"int32"), 2, )
2025-03-14 16:59:02.151979 test begin: paddle.Tensor.clip(Tensor([7605672, 3, 10, 10, 1],"float32"), 0, )

[Pass] paddle.Tensor.clip(Tensor([7605672, 3, 10, 10, 1],"float32"), 0, )
2025-03-14 17:02:13.596031 test begin: paddle.Tensor.clip(Tensor([84, 27163112],"float32"), -448, 448, )

[Pass] paddle.Tensor.clip(Tensor([84, 27163112],"float32"), -448, 448, )
2025-03-14 17:05:28.217046 test begin: paddle.Tensor.clip(Tensor([9, 253522376],"float32"), -128, 127, )

[Pass] paddle.Tensor.clip(Tensor([9, 253522376],"float32"), -128, 127, )
2025-03-14 17:08:14.164655 test begin: paddle.Tensor.clip(Tensor([94317, 12096, 2],"float32"), 0, )

[Pass] paddle.Tensor.clip(Tensor([94317, 12096, 2],"float32"), 0, )
2025-03-14 17:11:35.947513 test begin: paddle.Tensor.clip(Tensor([990322, 16, 12, 12],"float32"), -57344, 57344, )

[Pass] paddle.Tensor.clip(Tensor([990322, 16, 12, 12],"float32"), -57344, 57344, )
2025-03-14 17:14:44.178717 test begin: paddle.Tensor.clone(Tensor([1, 100, 22817014],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([1, 100, 22817014],"float32"), )
2025-03-14 17:17:53.527141 test begin: paddle.Tensor.clone(Tensor([1, 10000, 228171],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([1, 10000, 228171],"float32"), )
2025-03-14 17:20:55.468514 test begin: paddle.Tensor.clone(Tensor([1, 1024, 30948, 72],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([1, 1024, 30948, 72],"float32"), )
2025-03-14 17:24:03.134127 test begin: paddle.Tensor.clone(Tensor([1, 1024, 48, 46422],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([1, 1024, 48, 46422],"float32"), )
2025-03-14 17:27:55.643933 test begin: paddle.Tensor.clone(Tensor([1, 12, 1, 190141782],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([1, 12, 1, 190141782],"float32"), )
2025-03-14 17:31:01.839384 test begin: paddle.Tensor.clone(Tensor([1, 12, 11, 17285617],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([1, 12, 11, 17285617],"float32"), )
2025-03-14 17:34:05.250606 test begin: paddle.Tensor.clone(Tensor([1, 12, 11883862, 16],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([1, 12, 11883862, 16],"float32"), )
2025-03-14 17:36:58.958399 test begin: paddle.Tensor.clone(Tensor([1, 12, 168, 1131797],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([1, 12, 168, 1131797],"float32"), )
2025-03-14 17:40:03.658877 test begin: paddle.Tensor.clone(Tensor([1, 12, 21, 9054371],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([1, 12, 21, 9054371],"float32"), )
2025-03-14 17:43:11.776783 test begin: paddle.Tensor.clone(Tensor([1, 12, 2970966, 64],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([1, 12, 2970966, 64],"float32"), )
2025-03-14 17:45:54.336991 test begin: paddle.Tensor.clone(Tensor([1, 12, 5941931, 32],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([1, 12, 5941931, 32],"float32"), )
2025-03-14 17:49:07.232823 test begin: paddle.Tensor.clone(Tensor([1, 12, 742742, 256],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([1, 12, 742742, 256],"float32"), )
2025-03-14 17:52:06.455341 test begin: paddle.Tensor.clone(Tensor([1, 12964213, 11, 16],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([1, 12964213, 11, 16],"float32"), )
2025-03-14 17:55:08.223624 test begin: paddle.Tensor.clone(Tensor([1, 2281701379, 1],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([1, 2281701379, 1],"float32"), )
2025-03-14 17:58:11.838670 test begin: paddle.Tensor.clone(Tensor([1, 2281701379],"bool"), )

[Pass] paddle.Tensor.clone(Tensor([1, 2281701379],"bool"), )
2025-03-14 17:59:52.455394 test begin: paddle.Tensor.clone(Tensor([1, 2281701379],"int64"), )

[Pass] paddle.Tensor.clone(Tensor([1, 2281701379],"int64"), )
2025-03-14 18:03:02.232724 test begin: paddle.Tensor.clone(Tensor([1, 247581, 96, 96, 1],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([1, 247581, 96, 96, 1],"float32"), )
2025-03-14 18:06:07.142192 test begin: paddle.Tensor.clone(Tensor([1, 28521268, 80],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([1, 28521268, 80],"float32"), )
2025-03-14 18:09:23.472425 test begin: paddle.Tensor.clone(Tensor([1, 3, 7922575, 96, 1],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([1, 3, 7922575, 96, 1],"float32"), )
2025-03-14 18:12:10.241975 test begin: paddle.Tensor.clone(Tensor([1, 3, 96, 7922575, 1],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([1, 3, 96, 7922575, 1],"float32"), )
2025-03-14 18:15:12.506899 test begin: paddle.Tensor.clone(Tensor([1, 3, 96, 96, 82527],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([1, 3, 96, 96, 82527],"float32"), )
2025-03-14 18:18:40.453813 test begin: paddle.Tensor.clone(Tensor([1, 3395389, 21, 32],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([1, 3395389, 21, 32],"float32"), )
2025-03-14 18:22:10.279994 test begin: paddle.Tensor.clone(Tensor([1, 35651585, 1, 64],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([1, 35651585, 1, 64],"float32"), )
2025-03-14 18:25:52.444230 test begin: paddle.Tensor.clone(Tensor([1, 53053, 168, 256],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([1, 53053, 168, 256],"float32"), )
2025-03-14 18:29:12.532719 test begin: paddle.Tensor.clone(Tensor([1, 570425345, 4],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([1, 570425345, 4],"float32"), )
2025-03-14 18:32:34.145504 test begin: paddle.Tensor.clone(Tensor([1, 660215, 48, 72],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([1, 660215, 48, 72],"float32"), )
2025-03-14 18:35:38.626352 test begin: paddle.Tensor.clone(Tensor([1, 8912897, 256],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([1, 8912897, 256],"float32"), )
2025-03-14 18:38:24.346350 test begin: paddle.Tensor.clone(Tensor([1080352, 12, 11, 16],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([1080352, 12, 11, 16],"float32"), )
2025-03-14 18:41:31.735905 test begin: paddle.Tensor.clone(Tensor([108652447, 21],"int64"), )

[Pass] paddle.Tensor.clone(Tensor([108652447, 21],"int64"), )
2025-03-14 18:44:21.206671 test begin: paddle.Tensor.clone(Tensor([120089547, 19],"int64"), )

[Pass] paddle.Tensor.clone(Tensor([120089547, 19],"int64"), )
2025-03-14 18:47:22.972119 test begin: paddle.Tensor.clone(Tensor([2281701379, 1],"int64"), )

[Pass] paddle.Tensor.clone(Tensor([2281701379, 1],"int64"), )
2025-03-14 18:50:17.739892 test begin: paddle.Tensor.clone(Tensor([2281701379],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([2281701379],"float32"), )
2025-03-14 18:53:07.986770 test begin: paddle.Tensor.clone(Tensor([2281701379],"int64"), )

[Pass] paddle.Tensor.clone(Tensor([2281701379],"int64"), )
2025-03-14 18:56:00.284212 test begin: paddle.Tensor.clone(Tensor([228171, 10000, 1],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([228171, 10000, 1],"float32"), )
2025-03-14 18:58:54.498149 test begin: paddle.Tensor.clone(Tensor([282950, 12, 21, 32],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([282950, 12, 21, 32],"float32"), )
2025-03-14 19:01:53.570418 test begin: paddle.Tensor.clone(Tensor([285213, 100, 80],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([285213, 100, 80],"float32"), )
2025-03-14 19:04:50.579831 test begin: paddle.Tensor.clone(Tensor([2970966, 12, 1, 64],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([2970966, 12, 1, 64],"float32"), )
2025-03-14 19:08:52.682605 test begin: paddle.Tensor.clone(Tensor([4294967297],"float16"), )

[Pass] paddle.Tensor.clone(Tensor([4294967297],"float16"), )
2025-03-14 19:25:01.356416 test begin: paddle.Tensor.clone(Tensor([4422, 12, 168, 256],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([4422, 12, 168, 256],"float32"), )
2025-03-14 19:28:20.605229 test begin: paddle.Tensor.clone(Tensor([5704254, 100, 4],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([5704254, 100, 4],"float32"), )
2025-03-14 19:31:34.472105 test begin: paddle.Tensor.clone(Tensor([57043, 10000, 4],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([57043, 10000, 4],"float32"), )
2025-03-14 19:34:37.476491 test begin: paddle.Tensor.clone(Tensor([645, 1024, 48, 72],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([645, 1024, 48, 72],"float32"), )
2025-03-14 19:37:37.367091 test begin: paddle.Tensor.clone(Tensor([76060, 29999],"int64"), )

[Pass] paddle.Tensor.clone(Tensor([76060, 29999],"int64"), )
2025-03-14 19:40:47.971964 test begin: paddle.Tensor.clone(Tensor([82527, 3, 96, 96, 1],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([82527, 3, 96, 96, 1],"float32"), )
2025-03-14 19:43:52.712549 test begin: paddle.Tensor.clone(Tensor([89129, 100, 256],"float32"), )

[Pass] paddle.Tensor.clone(Tensor([89129, 100, 256],"float32"), )
2025-03-14 19:47:27.563549 test begin: paddle.Tensor.clone(Tensor([897955, 2541],"bool"), )

[Pass] paddle.Tensor.clone(Tensor([897955, 2541],"bool"), )
2025-03-14 19:48:03.738883 test begin: paddle.Tensor.cumsum(Tensor([1, 10, 228170138],"float32"), 1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741952915 (unix time) try "date -d @1741952915" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1fb13) received by PID 129811 (TID 0x7fe2667c3700) from PID 129811 ***]

2025-03-14 19:49:24.890957 test begin: paddle.Tensor.cumsum(Tensor([1, 10, 228170138],"float32"), 2, )

W0314 19:51:19.009824 140579 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 19:51:19.011161 140579 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.Tensor.cumsum(Tensor([1, 10, 228170138],"float32"), 2, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1457825 / 2281701380 (0.0639%)
Max absolute difference: 0.24316406
Max relative difference: 9589.298
 x: array([[[ 2.883038e-01, -1.855359e-01,  9.007332e-02, ...,
         -4.172986e+02, -4.169965e+02, -4.166406e+02],
        [ 2.921735e-01,  2.126660e-01, -1.916986e-01, ...,...
 y: array([[[ 2.883038e-01, -1.855359e-01,  9.007332e-02, ...,
         -4.173292e+02, -4.170272e+02, -4.166713e+02],
        [ 2.921735e-01,  2.126660e-01, -1.916986e-01, ...,...
2025-03-14 19:53:01.876547 test begin: paddle.Tensor.cumsum(Tensor([1, 11408507, 200],"float32"), 1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741953223 (unix time) try "date -d @1741953223" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x224cc) received by PID 140492 (TID 0x7f02827c3700) from PID 140492 ***]

2025-03-14 19:54:33.277031 test begin: paddle.Tensor.cumsum(Tensor([1, 11408507, 200],"float32"), 2, )

W0314 19:56:12.398475 140677 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 19:56:12.400624 140677 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741953372 (unix time) try "date -d @1741953372" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2252e) received by PID 140590 (TID 0x7feac07c3700) from PID 140590 ***]

2025-03-14 19:56:58.728176 test begin: paddle.Tensor.cumsum(Tensor([1, 12, 190141782],"float32"), 1, )

W0314 19:58:29.910966 140771 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 19:58:29.912125 140771 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741953510 (unix time) try "date -d @1741953510" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2258c) received by PID 140684 (TID 0x7fc64c949700) from PID 140684 ***]

2025-03-14 19:59:12.939164 test begin: paddle.Tensor.cumsum(Tensor([1, 12, 190141782],"float32"), 2, )

W0314 20:00:55.235728 140862 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 20:00:55.236994 140862 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.Tensor.cumsum(Tensor([1, 12, 190141782],"float32"), 2, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1521460 / 2281701384 (0.0667%)
Max absolute difference: 0.18115234
Max relative difference: 12121.341
 x: array([[[-3.102454e-01, -5.264149e-01, -7.840613e-01, ...,
         -4.301405e+03, -4.300930e+03, -4.301035e+03],
        [-1.866510e-01, -2.457626e-01, -6.678932e-01, ...,...
 y: array([[[-3.102454e-01, -5.264149e-01, -7.840613e-01, ...,
         -4.301377e+03, -4.300902e+03, -4.301007e+03],
        [-1.866510e-01, -2.457626e-01, -6.678932e-01, ...,...
2025-03-14 20:03:03.362746 test begin: paddle.Tensor.cumsum(Tensor([1, 144, 15845149],"float32"), 1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741953810 (unix time) try "date -d @1741953810" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x225ea) received by PID 140778 (TID 0x7f269a949700) from PID 140778 ***]

2025-03-14 20:04:15.545497 test begin: paddle.Tensor.cumsum(Tensor([1, 144, 15845149],"float32"), 2, )

W0314 20:05:43.357543 140969 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 20:05:43.358675 140969 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741953943 (unix time) try "date -d @1741953943" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x22652) received by PID 140882 (TID 0x7f626534a700) from PID 140882 ***]

2025-03-14 20:06:25.015957 test begin: paddle.Tensor.cumsum(Tensor([1, 15845149, 144],"float32"), 1, )

W0314 20:08:25.630409 141061 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 20:08:25.631628 141061 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741954105 (unix time) try "date -d @1741954105" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x226ae) received by PID 140974 (TID 0x7f4710949700) from PID 140974 ***]

2025-03-14 20:09:09.281052 test begin: paddle.Tensor.cumsum(Tensor([1, 15845149, 144],"float32"), 2, )

W0314 20:10:34.846447 141157 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 20:10:34.847707 141157 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741954235 (unix time) try "date -d @1741954235" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2270e) received by PID 141070 (TID 0x7ff889dc2700) from PID 141070 ***]

2025-03-14 20:11:19.102489 test begin: paddle.Tensor.cumsum(Tensor([1, 18, 126761188],"float32"), 1, )

W0314 20:12:56.003973 141253 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 20:12:56.005173 141253 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741954376 (unix time) try "date -d @1741954376" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2276e) received by PID 141166 (TID 0x7f3c91f48700) from PID 141166 ***]

2025-03-14 20:13:42.310377 test begin: paddle.Tensor.cumsum(Tensor([1, 18, 126761188],"float32"), 2, )

W0314 20:15:22.577332 141347 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 20:15:22.578667 141347 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741954522 (unix time) try "date -d @1741954522" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x227c9) received by PID 141257 (TID 0x7f4cf4949700) from PID 141257 ***]

2025-03-14 20:16:10.075684 test begin: paddle.Tensor.cumsum(Tensor([1, 192, 11883862],"float32"), 1, )

W0314 20:17:42.668592 141442 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 20:17:42.669835 141442 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741954663 (unix time) try "date -d @1741954663" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2282b) received by PID 141355 (TID 0x7f1de5d0b700) from PID 141355 ***]

2025-03-14 20:18:26.965866 test begin: paddle.Tensor.cumsum(Tensor([1, 192, 11883862],"float32"), 2, )

W0314 20:19:56.188901 141541 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 20:19:56.190130 141541 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741954796 (unix time) try "date -d @1741954796" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2288e) received by PID 141454 (TID 0x7ff687dc2700) from PID 141454 ***]

2025-03-14 20:20:42.061085 test begin: paddle.Tensor.cumsum(Tensor([1, 2281701379],"float32"), axis=-1, )

W0314 20:22:11.142056 141632 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 20:22:11.143298 141632 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] backward  paddle.Tensor.cumsum(Tensor([1, 2281701379],"float32"), axis=-1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 150615 / 2281701379 (0.0066%)
Max absolute difference: 0.05078125
Max relative difference: 4723.1045
 x: array([[ 1.129984e+04,  1.129972e+04,  1.129933e+04, ...,  4.395730e-01,
         2.137924e-01, -1.956542e-01]], dtype=float32)
 y: array([[ 1.129988e+04,  1.129976e+04,  1.129937e+04, ...,  4.395730e-01,
         2.137924e-01, -1.956542e-01]], dtype=float32)
2025-03-14 20:25:16.159673 test begin: paddle.Tensor.cumsum(Tensor([1, 253522376, 9],"float32"), 1, )

[cuda error] paddle.Tensor.cumsum(Tensor([1, 253522376, 9],"float32"), 1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-14 20:28:10.906794 test begin: paddle.Tensor.cumsum(Tensor([1, 253522376, 9],"float32"), 2, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741955317 (unix time) try "date -d @1741955317" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x228e9) received by PID 141545 (TID 0x7f735a949700) from PID 141545 ***]

2025-03-14 20:29:19.337655 test begin: paddle.Tensor.cumsum(Tensor([1, 285212673, 8],"float32"), 1, )

W0314 20:33:25.613143 141742 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 20:33:25.614527 141742 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.Tensor.cumsum(Tensor([1, 285212673, 8],"float32"), 1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-14 20:33:27.157328 test begin: paddle.Tensor.cumsum(Tensor([1, 285212673, 8],"float32"), 2, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741955636 (unix time) try "date -d @1741955636" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x22957) received by PID 141655 (TID 0x7f8107abb700) from PID 141655 ***]

2025-03-14 20:34:40.222875 test begin: paddle.Tensor.cumsum(Tensor([1, 91268056, 25],"float32"), 1, )

W0314 20:37:32.426048 141840 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 20:37:32.427222 141840 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741955852 (unix time) try "date -d @1741955852" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x229b9) received by PID 141753 (TID 0x7fa7b5f48700) from PID 141753 ***]

2025-03-14 20:38:17.390409 test begin: paddle.Tensor.cumsum(Tensor([1, 91268056, 25],"float32"), 2, )

W0314 20:39:59.944950 141936 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 20:39:59.946327 141936 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741956000 (unix time) try "date -d @1741956000" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x22a16) received by PID 141846 (TID 0x7f0be534a700) from PID 141846 ***]

2025-03-14 20:40:43.424794 test begin: paddle.Tensor.cumsum(Tensor([114085069, 20],"float32"), axis=-1, )

W0314 20:42:22.598495 142029 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 20:42:22.599581 142029 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741956143 (unix time) try "date -d @1741956143" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x22a76) received by PID 141942 (TID 0x7fdd2534a700) from PID 141942 ***]

2025-03-14 20:43:05.978657 test begin: paddle.Tensor.cumsum(Tensor([1140850690, 2],"float32"), axis=-1, )

W0314 20:44:52.643771 142121 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 20:44:52.645032 142121 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741956294 (unix time) try "date -d @1741956294" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x22ad2) received by PID 142034 (TID 0x7ff460949700) from PID 142034 ***]

2025-03-14 20:45:40.079244 test begin: paddle.Tensor.cumsum(Tensor([13, 175515491],"int32"), -1, )

W0314 20:46:44.626392 142216 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 20:46:44.627431 142216 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.Tensor.cumsum(Tensor([13, 175515491],"int32"), -1, )
2025-03-14 20:48:48.718398 test begin: paddle.Tensor.cumsum(Tensor([13, 175515491],"int32"), axis=-1, )

[Pass] paddle.Tensor.cumsum(Tensor([13, 175515491],"int32"), axis=-1, )
2025-03-14 20:51:17.623975 test begin: paddle.Tensor.cumsum(Tensor([162978670, 14],"int32"), -1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741956700 (unix time) try "date -d @1741956700" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x22b31) received by PID 142129 (TID 0x7fc5b27c3700) from PID 142129 ***]

2025-03-14 20:52:22.598295 test begin: paddle.Tensor.cumsum(Tensor([2, 1140850690],"int64"), axis=1, )

W0314 20:54:09.904558 142316 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 20:54:09.905632 142316 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.Tensor.cumsum(Tensor([2, 1140850690],"int64"), axis=1, )
2025-03-14 20:57:01.103498 test begin: paddle.Tensor.cumsum(Tensor([21126865, 12, 9],"float32"), 1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741957118 (unix time) try "date -d @1741957118" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x22b94) received by PID 142228 (TID 0x7fef37dc2700) from PID 142228 ***]

2025-03-14 20:59:25.112330 test begin: paddle.Tensor.cumsum(Tensor([21126865, 12, 9],"float32"), 2, )

W0314 21:00:57.755129 142413 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 21:00:57.756374 142413 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741957258 (unix time) try "date -d @1741957258" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x22bf6) received by PID 142326 (TID 0x7fcf532b7700) from PID 142326 ***]

2025-03-14 21:01:41.957203 test begin: paddle.Tensor.cumsum(Tensor([2228225, 1024],"int32"), axis=-1, )

W0314 21:02:54.023795 142508 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 21:02:54.024873 142508 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741957374 (unix time) try "date -d @1741957374" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x22c52) received by PID 142418 (TID 0x7f53f934a700) from PID 142418 ***]

2025-03-14 21:03:37.750947 test begin: paddle.Tensor.cumsum(Tensor([2281701379, 1],"float32"), axis=-1, )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7fa54d3a2bb0>,)) (kwargs={}) timed out after 1800.000000 seconds.

2025-03-14 21:33:44.016570 test begin: paddle.Tensor.cumsum(Tensor([2281701379],"float32"), -1, )

W0314 21:35:10.292351 142726 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 21:35:10.293761 142726 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] backward  paddle.Tensor.cumsum(Tensor([2281701379],"float32"), -1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 224838 / 2281701379 (0.00985%)
Max absolute difference: 0.03320312
Max relative difference: 3277.166
 x: array([-8.419052e+03, -8.418854e+03, -8.419315e+03, ...,  3.782174e-01,
        6.813864e-01,  3.662082e-01], dtype=float32)
 y: array([-8.419039e+03, -8.418841e+03, -8.419303e+03, ...,  3.782174e-01,
        6.813864e-01,  3.662082e-01], dtype=float32)
2025-03-14 21:38:05.090915 test begin: paddle.Tensor.cumsum(Tensor([2281701379],"int64"), 0, )

[Pass] paddle.Tensor.cumsum(Tensor([2281701379],"int64"), 0, )
2025-03-14 21:41:33.940671 test begin: paddle.Tensor.cumsum(Tensor([228170138, 10],"int64"), axis=1, )

[accuracy error] paddle.Tensor.cumsum(Tensor([228170138, 10],"int64"), axis=1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 134217043 / 2281701380 (5.88%)
Max absolute difference: 550690
Max relative difference: 1.
 x: array([[ -47021,    5065,  -46824, ..., -115387, -124948, -184754],
       [  12166,  -18728,  -27736, ...,   16386,    4071,   47204],
       [ -43192,  -47660,  -51464, ...,  -60134,  -68115,  -22530],...
 y: array([[ -47021,    5065,  -46824, ..., -115387, -124948, -184754],
       [  12166,  -18728,  -27736, ...,   16386,    4071,   47204],
       [ -43192,  -47660,  -51464, ...,  -60134,  -68115,  -22530],...
2025-03-14 21:45:46.381872 test begin: paddle.Tensor.cumsum(Tensor([22817014, 100],"int64"), axis=1, )

[accuracy error] paddle.Tensor.cumsum(Tensor([22817014, 100],"int64"), axis=1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 134217458 / 2281701400 (5.88%)
Max absolute difference: 1862749
Max relative difference: 1.
 x: array([[-47021,   5065, -46824, ..., -59803,  -6605, -65036],
       [-38547, -27470, -62357, ..., -58269, -71643, -43627],
       [-38413, -27321,  27185, ..., 249621, 224151, 226707],...
 y: array([[ -47021,    5065,  -46824, ...,  -59803,   -6605,  -65036],
       [ -38547,  -27470,  -62357, ...,  -58269,  -71643,  -43627],
       [ -38413,  -27321,   27185, ...,  249621,  224151,  226707],...
2025-03-14 21:50:16.886012 test begin: paddle.Tensor.cumsum(Tensor([285212673, 4, 2],"int64"), axis=0, )

[cuda error] paddle.Tensor.cumsum(Tensor([285212673, 4, 2],"int64"), axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-14 21:52:42.112659 test begin: paddle.Tensor.cumsum(Tensor([285212673, 4, 2],"int64"), axis=1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741960390 (unix time) try "date -d @1741960390" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x22d3a) received by PID 142650 (TID 0x7f9333dc2700) from PID 142650 ***]

2025-03-14 21:53:58.693016 test begin: paddle.Tensor.cumsum(Tensor([285212673, 4, 2],"int64"), axis=2, )

W0314 21:55:44.696066 143087 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 21:55:44.697212 143087 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741960547 (unix time) try "date -d @1741960547" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x22e98) received by PID 143000 (TID 0x7f0ac5f48700) from PID 143000 ***]

2025-03-14 21:56:31.058453 test begin: paddle.Tensor.cumsum(Tensor([28521268, 10, 8],"float32"), 1, )

W0314 21:57:55.539945 143186 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 21:57:55.541158 143186 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741960676 (unix time) try "date -d @1741960676" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x22ef8) received by PID 143096 (TID 0x7ff136949700) from PID 143096 ***]

2025-03-14 21:58:40.242664 test begin: paddle.Tensor.cumsum(Tensor([28521268, 10, 8],"float32"), 2, )

W0314 22:00:19.818799 143368 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 22:00:19.819943 143368 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741960820 (unix time) try "date -d @1741960820" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x22f5d) received by PID 143197 (TID 0x7fa6d7dc2700) from PID 143197 ***]

2025-03-14 22:01:09.280845 test begin: paddle.Tensor.cumsum(Tensor([3, 380283564, 2],"int64"), axis=0, )

W0314 22:02:34.608279 143471 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 22:02:34.609457 143471 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741960956 (unix time) try "date -d @1741960956" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x23017) received by PID 143383 (TID 0x7f6e17f48700) from PID 143383 ***]

2025-03-14 22:03:19.642011 test begin: paddle.Tensor.cumsum(Tensor([3, 380283564, 2],"int64"), axis=1, )

W0314 22:06:05.365900 143561 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 22:06:05.366930 143561 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.Tensor.cumsum(Tensor([3, 380283564, 2],"int64"), axis=1, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-14 22:06:07.394390 test begin: paddle.Tensor.cumsum(Tensor([3, 380283564, 2],"int64"), axis=2, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741961207 (unix time) try "date -d @1741961207" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x23072) received by PID 143474 (TID 0x7f64534f4700) from PID 143474 ***]

2025-03-14 22:07:31.878699 test begin: paddle.Tensor.cumsum(Tensor([3, 4, 190141782],"int64"), axis=0, )

W0314 22:08:53.101635 143663 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 22:08:53.102591 143663 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741961335 (unix time) try "date -d @1741961335" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x230d8) received by PID 143576 (TID 0x7f204df48700) from PID 143576 ***]

2025-03-14 22:09:44.185226 test begin: paddle.Tensor.cumsum(Tensor([3, 4, 190141782],"int64"), axis=1, )

W0314 22:11:12.121021 143767 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 22:11:12.121907 143767 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741961473 (unix time) try "date -d @1741961473" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2313a) received by PID 143674 (TID 0x7efc047c3700) from PID 143674 ***]

2025-03-14 22:12:02.099682 test begin: paddle.Tensor.cumsum(Tensor([3, 4, 190141782],"int64"), axis=2, )

W0314 22:13:31.341099 143860 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 22:13:31.342156 143860 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.Tensor.cumsum(Tensor([3, 4, 190141782],"int64"), axis=2, )
2025-03-14 22:16:10.552912 test begin: paddle.Tensor.cumsum(Tensor([3, 760567127],"int64"), axis=0, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741961801 (unix time) try "date -d @1741961801" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2319d) received by PID 143773 (TID 0x7fc173dc2700) from PID 143773 ***]

2025-03-14 22:17:31.142044 test begin: paddle.Tensor.cumsum(Tensor([3, 760567127],"int64"), axis=1, )

W0314 22:19:06.664513 143963 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 22:19:06.665645 143963 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.Tensor.cumsum(Tensor([3, 760567127],"int64"), axis=1, )
2025-03-14 22:21:40.185699 test begin: paddle.Tensor.cumsum(Tensor([4294967297],"float16"), -1, )

[accuracy error] paddle.Tensor.cumsum(Tensor([4294967297],"float16"), -1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4294965632 / 4294967297 (100%)
Max absolute difference: 31056.
Max relative difference: 1.
 x: array([-0.3774,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],
      dtype=float16)
 y: array([-3.774e-01, -7.676e-01, -3.042e-01, ..., -1.525e+04, -1.525e+04,
       -1.525e+04], dtype=float16)
2025-03-14 22:36:32.912287 test begin: paddle.Tensor.cumsum(Tensor([5, 456340276],"int64"), axis=0, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741963022 (unix time) try "date -d @1741963022" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x23204) received by PID 143876 (TID 0x7f0ae9abb700) from PID 143876 ***]

2025-03-14 22:37:47.545755 test begin: paddle.Tensor.cumsum(Tensor([5, 456340276],"int64"), axis=1, )

W0314 22:39:10.822454 144280 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 22:39:10.824216 144280 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.Tensor.cumsum(Tensor([5, 456340276],"int64"), axis=1, )
2025-03-14 22:42:50.560010 test begin: paddle.Tensor.cumsum(Tensor([5070448, 18, 25],"float32"), 1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741963455 (unix time) try "date -d @1741963455" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x23341) received by PID 144193 (TID 0x7f9792949700) from PID 144193 ***]

2025-03-14 22:45:00.628272 test begin: paddle.Tensor.cumsum(Tensor([5070448, 18, 25],"float32"), 2, )

W0314 22:46:28.335196 144487 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 22:46:28.336313 144487 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741963588 (unix time) try "date -d @1741963588" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x23410) received by PID 144400 (TID 0x7f2c3c949700) from PID 144400 ***]

2025-03-14 22:47:16.178402 test begin: paddle.Tensor.cumsum(Tensor([570425345, 4],"int64"), axis=0, )

W0314 22:51:44.209123 144666 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 22:51:44.210254 144666 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[cuda error] paddle.Tensor.cumsum(Tensor([570425345, 4],"int64"), axis=0, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-14 22:51:46.599239 test begin: paddle.Tensor.cumsum(Tensor([570425345, 4],"int64"), axis=1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   phi::DenseTensor::~DenseTensor()
4   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
5   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741963938 (unix time) try "date -d @1741963938" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x234c3) received by PID 144579 (TID 0x7f4abff48700) from PID 144579 ***]

2025-03-14 22:53:03.981967 test begin: paddle.Tensor.cumsum(Tensor([760567127, 3],"float32"), axis=-1, )

W0314 22:55:04.788699 144864 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 22:55:04.790130 144864 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741964105 (unix time) try "date -d @1741964105" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x23589) received by PID 144777 (TID 0x7f6cd2949700) from PID 144777 ***]

2025-03-14 22:55:49.141455 test begin: paddle.Tensor.cumsum(Tensor([79226, 144, 200],"float32"), 1, )

W0314 22:57:14.136648 144958 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 22:57:14.137917 144958 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741964234 (unix time) try "date -d @1741964234" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x235e7) received by PID 144871 (TID 0x7fc7df744700) from PID 144871 ***]

2025-03-14 22:57:59.589155 test begin: paddle.Tensor.cumsum(Tensor([79226, 144, 200],"float32"), 2, )

W0314 22:59:30.551568 145053 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 22:59:30.552848 145053 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741964370 (unix time) try "date -d @1741964370" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x23643) received by PID 144963 (TID 0x7f8090949700) from PID 144963 ***]

2025-03-14 23:00:16.817299 test begin: paddle.Tensor.cumsum(Tensor([82527, 192, 144],"float32"), 1, )

W0314 23:01:44.388679 145233 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 23:01:44.389921 145233 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741964504 (unix time) try "date -d @1741964504" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x236fa) received by PID 145146 (TID 0x7f6260949700) from PID 145146 ***]

2025-03-14 23:02:27.884407 test begin: paddle.Tensor.cumsum(Tensor([82527, 192, 144],"float32"), 2, )

W0314 23:04:00.842373 145328 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 23:04:00.844238 145328 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_cumsum(_object*, _object*, _object*)
1   cumsum_ad_func(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor>, bool, bool, bool)
2   paddle::experimental::cumsum(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool, bool, bool)
3   void phi::ScanKernel<float, phi::GPUContext, cub::Sum>(phi::GPUContext const&, phi::DenseTensor const&, int, bool, bool, bool, cub::Sum, phi::DenseTensor*)
4   phi::DenseTensor::~DenseTensor()
5   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
6   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741964641 (unix time) try "date -d @1741964641" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x23759) received by PID 145241 (TID 0x7f39547c3700) from PID 145241 ***]

2025-03-14 23:04:48.138199 test begin: paddle.Tensor.diag_embed(Tensor([1, 1, 2281701379],"float32"), )

W0314 23:06:11.846406 145338 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 23:06:11.847821 145338 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.Tensor.diag_embed(Tensor([1, 1, 2281701379],"float32"), ) 
 Storage size calculation overflowed with sizes=[1, 1, 2281701379, 2281701379]
2025-03-14 23:06:13.826257 test begin: paddle.Tensor.diag_embed(Tensor([1, 1140850690, 2],"float32"), )

[Pass] paddle.Tensor.diag_embed(Tensor([1, 1140850690, 2],"float32"), )
2025-03-14 23:09:27.331616 test begin: paddle.Tensor.diag_embed(Tensor([1140850690, 1, 2],"float32"), )

[Pass] paddle.Tensor.diag_embed(Tensor([1140850690, 1, 2],"float32"), )
2025-03-14 23:13:17.397313 test begin: paddle.Tensor.diagonal(Tensor([1140850690, 2],"float32"), axis1=-2, axis2=-1, )

[Pass] paddle.Tensor.diagonal(Tensor([1140850690, 2],"float32"), axis1=-2, axis2=-1, )
2025-03-14 23:14:59.062908 test begin: paddle.Tensor.diagonal(Tensor([1431655766, 3],"float16"), axis1=-2, axis2=-1, )

[Pass] paddle.Tensor.diagonal(Tensor([1431655766, 3],"float16"), axis1=-2, axis2=-1, )
2025-03-14 23:24:23.270544 test begin: paddle.Tensor.diagonal(Tensor([2, 1140850690],"float32"), axis1=-2, axis2=-1, )

[Pass] paddle.Tensor.diagonal(Tensor([2, 1140850690],"float32"), axis1=-2, axis2=-1, )
2025-03-14 23:26:16.651676 test begin: paddle.Tensor.diagonal(Tensor([3, 1431655766],"float16"), axis1=-2, axis2=-1, )

[Pass] paddle.Tensor.diagonal(Tensor([3, 1431655766],"float16"), axis1=-2, axis2=-1, )
2025-03-14 23:34:25.903470 test begin: paddle.Tensor.diff(Tensor([2281701379],"float32"), )

[accuracy error] paddle.Tensor.diff(Tensor([2281701379],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2235834673 / 2281701378 (98%)
Max absolute difference: 0.9999933
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)
 y: array([ 0.394564, -0.569381,  0.322047, ...,  0.233371, -0.682198,
        0.602916], dtype=float32)
2025-03-14 23:36:46.017356 test begin: paddle.Tensor.diff(Tensor([4294967297],"float16"), )

[accuracy error] paddle.Tensor.diff(Tensor([4294967297],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208559478 / 4294967296 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float16)
 y: array([ 0.2278,  0.1649, -0.832 , ..., -0.1841,  0.0977, -0.4165],
      dtype=float16)
2025-03-14 23:49:33.605686 test begin: paddle.Tensor.diff(x=Tensor([10, 429496730],"float16"), axis=0, prepend=Tensor([4, 429496730],"float16"), append=Tensor([4, 429496730],"float16"), )

[accuracy error] paddle.Tensor.diff(x=Tensor([10, 429496730],"float16"), axis=0, prepend=Tensor([4, 429496730],"float16"), append=Tensor([4, 429496730],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 7154549141 / 7301444410 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[ 0.3064 ,  0.00891, -0.4675 , ..., -0.2654 , -0.04727,  0.2634 ],
       [-0.11914, -0.07324,  0.07935, ..., -0.3062 ,  0.0631 , -0.6846 ],
       [-0.287  ,  0.308  , -0.0702 , ..., -0.0332 , -0.0758 ,  0.303  ],...
2025-03-15 00:12:01.306124 test begin: paddle.Tensor.diff(x=Tensor([10, 429496730],"float16"), axis=0, prepend=Tensor([4, 4],"float16"), append=Tensor([4, 4],"float16"), )

[torch error] paddle.Tensor.diff(x=Tensor([10, 429496730],"float16"), axis=0, prepend=Tensor([4, 4],"float16"), append=Tensor([4, 4],"float16"), ) 
 diff expects the shape of tensor to prepend or append to match that of input except along the differencing dimension; input.size(1) = 429496730, but got tensor.size(1) = 4
2025-03-15 00:12:05.752233 test begin: paddle.Tensor.diff(x=Tensor([10, 4],"float16"), axis=0, prepend=Tensor([1073741825, 4],"float16"), append=Tensor([4, 4],"float16"), )

[accuracy error] paddle.Tensor.diff(x=Tensor([10, 4],"float16"), axis=0, prepend=Tensor([1073741825, 4],"float16"), append=Tensor([4, 4],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208559908 / 4294967352 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],...
 y: array([[ 0.2874  ,  0.009155, -0.6826  ,  0.7124  ],
       [-0.2515  , -0.00537 , -0.01758 , -0.72    ],
       [ 0.1443  , -0.597   ,  0.53    ,  0.8213  ],...
2025-03-15 00:24:57.622558 test begin: paddle.Tensor.diff(x=Tensor([10, 4],"float16"), axis=0, prepend=Tensor([4, 1073741825],"float16"), append=Tensor([4, 4],"float16"), )

[torch error] paddle.Tensor.diff(x=Tensor([10, 4],"float16"), axis=0, prepend=Tensor([4, 1073741825],"float16"), append=Tensor([4, 4],"float16"), ) 
 diff expects the shape of tensor to prepend or append to match that of input except along the differencing dimension; input.size(1) = 4, but got tensor.size(1) = 1073741825
2025-03-15 00:25:01.955787 test begin: paddle.Tensor.diff(x=Tensor([10, 4],"float16"), axis=0, prepend=Tensor([4, 4],"float16"), append=Tensor([1073741825, 4],"float16"), )

[accuracy error] paddle.Tensor.diff(x=Tensor([10, 4],"float16"), axis=0, prepend=Tensor([4, 4],"float16"), append=Tensor([1073741825, 4],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208559908 / 4294967352 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],...
 y: array([[ 0.2874  ,  0.009155, -0.6826  ,  0.7124  ],
       [-0.2515  , -0.00537 , -0.01758 , -0.72    ],
       [ 0.1443  , -0.597   ,  0.53    ,  0.8213  ],...
2025-03-15 00:37:52.438657 test begin: paddle.Tensor.diff(x=Tensor([10, 4],"float16"), axis=0, prepend=Tensor([4, 4],"float16"), append=Tensor([4, 1073741825],"float16"), )

[torch error] paddle.Tensor.diff(x=Tensor([10, 4],"float16"), axis=0, prepend=Tensor([4, 4],"float16"), append=Tensor([4, 1073741825],"float16"), ) 
 diff expects the shape of tensor to prepend or append to match that of input except along the differencing dimension; input.size(1) = 4, but got tensor.size(1) = 1073741825
2025-03-15 00:37:56.758172 test begin: paddle.Tensor.diff(x=Tensor([1073741825, 4],"float16"), )

[Pass] paddle.Tensor.diff(x=Tensor([1073741825, 4],"float16"), )
2025-03-15 00:44:11.071295 test begin: paddle.Tensor.diff(x=Tensor([1073741825, 4],"float16"), axis=0, prepend=Tensor([1073741825, 4],"float16"), append=Tensor([1073741825, 4],"float16"), )

[paddle error] paddle.Tensor.diff(x=Tensor([1073741825, 4],"float16"), axis=0, prepend=Tensor([1073741825, 4],"float16"), append=Tensor([1073741825, 4],"float16"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_subtract(_object*, _object*, _object*)
1   subtract_ad_func(paddle::Tensor const&, paddle::Tensor const&)
2   paddle::experimental::subtract(paddle::Tensor const&, paddle::Tensor const&)
3   void phi::SubtractRawKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
4   phi::dtype::float16* phi::DeviceContext::Alloc<phi::dtype::float16>(phi::TensorBase*, unsigned long, bool) const
5   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 24.000000GB memory on GPU 0, 57.598572GB memory has been allocated and available memory is only 21.586304GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-15 00:44:46.351318 test begin: paddle.Tensor.diff(x=Tensor([1073741825, 4],"float16"), axis=0, prepend=Tensor([4, 4],"float16"), append=Tensor([4, 4],"float16"), )

[accuracy error] paddle.Tensor.diff(x=Tensor([1073741825, 4],"float16"), axis=0, prepend=Tensor([4, 4],"float16"), append=Tensor([4, 4],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208559884 / 4294967328 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],...
 y: array([[ 0.2874  ,  0.009155, -0.6826  ,  0.7124  ],
       [-0.2515  , -0.00537 , -0.01758 , -0.72    ],
       [ 0.1443  , -0.597   ,  0.53    ,  0.8213  ],...
2025-03-15 00:57:28.576946 test begin: paddle.Tensor.diff(x=Tensor([10],"float16"), prepend=Tensor([4294967297],"float16"), )

[accuracy error] paddle.Tensor.diff(x=Tensor([10],"float16"), prepend=Tensor([4294967297],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208559488 / 4294967306 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float16)
 y: array([ 0.2278,  0.1649, -0.832 , ...,  0.563 , -0.237 ,  0.1956],
      dtype=float16)
2025-03-15 01:10:35.256293 test begin: paddle.Tensor.diff(x=Tensor([10],"float16"), prepend=Tensor([4294967297],"float16"), append=Tensor([4],"float16"), )

[accuracy error] paddle.Tensor.diff(x=Tensor([10],"float16"), prepend=Tensor([4294967297],"float16"), append=Tensor([4],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208559492 / 4294967310 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float16)
 y: array([ 0.2278,  0.1649, -0.832 , ...,  0.2278,  0.1649, -0.832 ],
      dtype=float16)
2025-03-15 01:23:22.504595 test begin: paddle.Tensor.diff(x=Tensor([10],"float16"), prepend=Tensor([4],"float16"), append=Tensor([4294967297],"float16"), )

[accuracy error] paddle.Tensor.diff(x=Tensor([10],"float16"), prepend=Tensor([4],"float16"), append=Tensor([4294967297],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208559492 / 4294967310 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float16)
 y: array([ 0.2278,  0.1649, -0.832 , ..., -0.1841,  0.0977, -0.4165],
      dtype=float16)
2025-03-15 01:35:44.281188 test begin: paddle.Tensor.diff(x=Tensor([2281701379],"float32"), )

[accuracy error] paddle.Tensor.diff(x=Tensor([2281701379],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2235834673 / 2281701378 (98%)
Max absolute difference: 0.9999933
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)
 y: array([ 0.394564, -0.569381,  0.322047, ...,  0.233371, -0.682198,
        0.602916], dtype=float32)
2025-03-15 01:37:50.509547 test begin: paddle.Tensor.diff(x=Tensor([2281701379],"int32"), )

[accuracy error] paddle.Tensor.diff(x=Tensor([2281701379],"int32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281684018 / 2281701378 (100%)
Max absolute difference: 131069
Max relative difference: 1.
 x: array([0, 0, 0, ..., 0, 0, 0], dtype=int32)
 y: array([-13541,  24789, -61912, ..., -68357,  13889,  28338], dtype=int32)
2025-03-15 01:41:53.726100 test begin: paddle.Tensor.diff(x=Tensor([2281701379],"int64"), )

[accuracy error] paddle.Tensor.diff(x=Tensor([2281701379],"int64"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281683872 / 2281701378 (100%)
Max absolute difference: 131068
Max relative difference: 1.
 x: array([0, 0, 0, ..., 0, 0, 0])
 y: array([  5006,  95032, -37414, ...,    351, -93699,  35177])
2025-03-15 01:48:23.330964 test begin: paddle.Tensor.diff(x=Tensor([268435457, 4, 4],"float16"), )

[Pass] paddle.Tensor.diff(x=Tensor([268435457, 4, 4],"float16"), )
2025-03-15 01:54:21.034535 test begin: paddle.Tensor.diff(x=Tensor([4, 1073741825],"float16"), )

[cuda error] paddle.Tensor.diff(x=Tensor([4, 1073741825],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-15 01:54:34.840163 test begin: paddle.Tensor.diff(x=Tensor([4, 268435457, 4],"float16"), )

[Pass] paddle.Tensor.diff(x=Tensor([4, 268435457, 4],"float16"), )
2025-03-15 02:00:45.457012 test begin: paddle.Tensor.diff(x=Tensor([4, 4, 268435457],"float16"), )

[cuda error] paddle.Tensor.diff(x=Tensor([4, 4, 268435457],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-15 02:01:00.732090 test begin: paddle.Tensor.diff(x=Tensor([4, 4, 4, 67108865],"float16"), )

[Pass] paddle.Tensor.diff(x=Tensor([4, 4, 4, 67108865],"float16"), )
2025-03-15 02:09:07.958616 test begin: paddle.Tensor.diff(x=Tensor([4, 4, 4, 67108865],"float16"), axis=-2, )

[Pass] paddle.Tensor.diff(x=Tensor([4, 4, 4, 67108865],"float16"), axis=-2, )
2025-03-15 02:15:00.147361 test begin: paddle.Tensor.diff(x=Tensor([4, 4, 4, 67108865],"float16"), axis=2, )

[Pass] paddle.Tensor.diff(x=Tensor([4, 4, 4, 67108865],"float16"), axis=2, )
2025-03-15 02:21:12.786801 test begin: paddle.Tensor.diff(x=Tensor([4, 4, 67108865, 4],"float16"), )

[Pass] paddle.Tensor.diff(x=Tensor([4, 4, 67108865, 4],"float16"), )
2025-03-15 02:27:24.738050 test begin: paddle.Tensor.diff(x=Tensor([4, 4, 67108865, 4],"float16"), axis=-2, )

[Pass] paddle.Tensor.diff(x=Tensor([4, 4, 67108865, 4],"float16"), axis=-2, )
2025-03-15 02:35:49.413556 test begin: paddle.Tensor.diff(x=Tensor([4, 4, 67108865, 4],"float16"), axis=2, )

[Pass] paddle.Tensor.diff(x=Tensor([4, 4, 67108865, 4],"float16"), axis=2, )
2025-03-15 02:44:06.445056 test begin: paddle.Tensor.diff(x=Tensor([4, 67108865, 4, 4],"float16"), )

[Pass] paddle.Tensor.diff(x=Tensor([4, 67108865, 4, 4],"float16"), )
2025-03-15 02:50:06.116463 test begin: paddle.Tensor.diff(x=Tensor([4, 67108865, 4, 4],"float16"), axis=-2, )

[Pass] paddle.Tensor.diff(x=Tensor([4, 67108865, 4, 4],"float16"), axis=-2, )
2025-03-15 02:56:08.572793 test begin: paddle.Tensor.diff(x=Tensor([4, 67108865, 4, 4],"float16"), axis=2, )

[Pass] paddle.Tensor.diff(x=Tensor([4, 67108865, 4, 4],"float16"), axis=2, )
2025-03-15 03:02:00.860082 test begin: paddle.Tensor.diff(x=Tensor([4294967297],"float16"), )

[accuracy error] paddle.Tensor.diff(x=Tensor([4294967297],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208559478 / 4294967296 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float16)
 y: array([ 0.2278,  0.1649, -0.832 , ..., -0.1841,  0.0977, -0.4165],
      dtype=float16)
2025-03-15 03:14:52.499679 test begin: paddle.Tensor.diff(x=Tensor([4294967297],"float16"), prepend=Tensor([4294967297],"float16"), )

[accuracy error] paddle.Tensor.diff(x=Tensor([4294967297],"float16"), prepend=Tensor([4294967297],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 8417118957 / 8589934593 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float16)
 y: array([ 0.2278,  0.1649, -0.832 , ..., -0.1841,  0.0977, -0.4165],
      dtype=float16)
2025-03-15 03:41:10.567943 test begin: paddle.Tensor.diff(x=Tensor([4294967297],"float16"), prepend=Tensor([4294967297],"float16"), append=Tensor([4294967297],"float16"), )

[paddle error] paddle.Tensor.diff(x=Tensor([4294967297],"float16"), prepend=Tensor([4294967297],"float16"), append=Tensor([4294967297],"float16"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_subtract(_object*, _object*, _object*)
1   subtract_ad_func(paddle::Tensor const&, paddle::Tensor const&)
2   paddle::experimental::subtract(paddle::Tensor const&, paddle::Tensor const&)
3   void phi::SubtractRawKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
4   phi::dtype::float16* phi::DeviceContext::Alloc<phi::dtype::float16>(phi::TensorBase*, unsigned long, bool) const
5   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 24.000000GB memory on GPU 0, 66.598572GB memory has been allocated and available memory is only 12.586304GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-15 03:41:45.099506 test begin: paddle.Tensor.diff(x=Tensor([4294967297],"float16"), prepend=Tensor([4],"float16"), )

[accuracy error] paddle.Tensor.diff(x=Tensor([4294967297],"float16"), prepend=Tensor([4],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208559482 / 4294967300 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float16)
 y: array([ 0.2278,  0.1649, -0.832 , ..., -0.1841,  0.0977, -0.4165],
      dtype=float16)
2025-03-15 03:54:40.311210 test begin: paddle.Tensor.diff(x=Tensor([4294967297],"float16"), prepend=Tensor([4],"float16"), append=Tensor([4],"float16"), )

[accuracy error] paddle.Tensor.diff(x=Tensor([4294967297],"float16"), prepend=Tensor([4],"float16"), append=Tensor([4],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208559486 / 4294967304 (98%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float16)
 y: array([ 0.2278,  0.1649, -0.832 , ...,  0.2278,  0.1649, -0.832 ],
      dtype=float16)
2025-03-15 04:07:14.347428 test begin: paddle.Tensor.diff(x=Tensor([67108865, 4, 4, 4],"float16"), )

[cuda error] paddle.Tensor.diff(x=Tensor([67108865, 4, 4, 4],"float16"), ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-15 04:07:24.590521 test begin: paddle.Tensor.diff(x=Tensor([67108865, 4, 4, 4],"float16"), axis=-2, )

[cuda error] paddle.Tensor.diff(x=Tensor([67108865, 4, 4, 4],"float16"), axis=-2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-15 04:07:33.270986 test begin: paddle.Tensor.diff(x=Tensor([67108865, 4, 4, 4],"float16"), axis=2, )

[cuda error] paddle.Tensor.diff(x=Tensor([67108865, 4, 4, 4],"float16"), axis=2, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-15 04:07:42.586740 test begin: paddle.Tensor.dot(Tensor([1],"float32"), Tensor([2281701379],"float32"), )

[torch error] paddle.Tensor.dot(Tensor([1],"float32"), Tensor([2281701379],"float32"), ) 
 inconsistent tensor size, expected tensor [1] and src [2281701379] to have the same number of elements, but got 1 and 2281701379 elements respectively
2025-03-15 04:07:46.772420 test begin: paddle.Tensor.dot(Tensor([2281701379],"float32"), Tensor([1],"float32"), )

[torch error] paddle.Tensor.dot(Tensor([2281701379],"float32"), Tensor([1],"float32"), ) 
 inconsistent tensor size, expected tensor [2281701379] and src [1] to have the same number of elements, but got 2281701379 and 1 elements respectively
2025-03-15 04:07:48.665696 test begin: paddle.Tensor.dot(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), )

[torch error] paddle.Tensor.dot(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), ) 
 dot only supports n, incx, incy with the bound [val] <= %d2147483647
2025-03-15 04:07:52.454951 test begin: paddle.Tensor.dot(Tensor([2281701379],"float32"), Tensor([2],"float32"), )

[torch error] paddle.Tensor.dot(Tensor([2281701379],"float32"), Tensor([2],"float32"), ) 
 inconsistent tensor size, expected tensor [2281701379] and src [2] to have the same number of elements, but got 2281701379 and 2 elements respectively
2025-03-15 04:07:54.362188 test begin: paddle.Tensor.dot(Tensor([2],"float32"), Tensor([2281701379],"float32"), )

[torch error] paddle.Tensor.dot(Tensor([2],"float32"), Tensor([2281701379],"float32"), ) 
 inconsistent tensor size, expected tensor [2] and src [2281701379] to have the same number of elements, but got 2 and 2281701379 elements respectively
2025-03-15 04:07:56.865991 test begin: paddle.Tensor.equal(Tensor([128, 128],"int64"), Tensor([128, 17825793],"int64"), )

[torch error] paddle.Tensor.equal(Tensor([128, 128],"int64"), Tensor([128, 17825793],"int64"), ) 
 The size of tensor a (128) must match the size of tensor b (17825793) at non-singleton dimension 1
2025-03-15 04:08:05.580405 test begin: paddle.Tensor.equal(Tensor([128, 128],"int64"), Tensor([17825793, 128],"int64"), )

[torch error] paddle.Tensor.equal(Tensor([128, 128],"int64"), Tensor([17825793, 128],"int64"), ) 
 The size of tensor a (128) must match the size of tensor b (17825793) at non-singleton dimension 0
2025-03-15 04:08:12.092700 test begin: paddle.Tensor.equal(Tensor([128, 17825793],"int64"), Tensor([128, 128],"int64"), )

[torch error] paddle.Tensor.equal(Tensor([128, 17825793],"int64"), Tensor([128, 128],"int64"), ) 
 The size of tensor a (17825793) must match the size of tensor b (128) at non-singleton dimension 1
2025-03-15 04:08:16.236898 test begin: paddle.Tensor.equal(Tensor([128, 17825793],"int64"), Tensor([128, 17825793],"int64"), )

[Pass] paddle.Tensor.equal(Tensor([128, 17825793],"int64"), Tensor([128, 17825793],"int64"), )
2025-03-15 04:09:05.436892 test begin: paddle.Tensor.equal(Tensor([17825793, 128],"int64"), Tensor([128, 128],"int64"), )

[torch error] paddle.Tensor.equal(Tensor([17825793, 128],"int64"), Tensor([128, 128],"int64"), ) 
 The size of tensor a (17825793) must match the size of tensor b (128) at non-singleton dimension 0
2025-03-15 04:09:10.152731 test begin: paddle.Tensor.equal(Tensor([17825793, 128],"int64"), Tensor([17825793, 128],"int64"), )

[Pass] paddle.Tensor.equal(Tensor([17825793, 128],"int64"), Tensor([17825793, 128],"int64"), )
2025-03-15 04:10:10.295750 test begin: paddle.Tensor.equal(Tensor([2, 1140850690],"int64"), 3, )

[Pass] paddle.Tensor.equal(Tensor([2, 1140850690],"int64"), 3, )
2025-03-15 04:10:54.015043 test begin: paddle.Tensor.equal(Tensor([228170138, 10],"int64"), 3, )

[Pass] paddle.Tensor.equal(Tensor([228170138, 10],"int64"), 3, )
2025-03-15 04:11:39.272388 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 1973791, 34, 34],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[Pass] paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 1973791, 34, 34],"float32"), )
2025-03-15 04:13:19.684740 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 21035, 298, 364],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[Pass] paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 21035, 298, 364],"float32"), )
2025-03-15 04:14:42.346998 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 23283, 280, 350],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[Pass] paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 23283, 280, 350],"float32"), )
2025-03-15 04:16:13.324746 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 2742430, 32, 26],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[Pass] paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 2742430, 32, 26],"float32"), )
2025-03-15 04:17:35.211650 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 2910334, 28, 28],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[Pass] paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 2910334, 28, 28],"float32"), )
2025-03-15 04:19:00.285805 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 2089471, 364],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[Pass] paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 2089471, 364],"float32"), )
2025-03-15 04:20:34.716199 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 2173049, 350],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[Pass] paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 2173049, 350],"float32"), )
2025-03-15 04:22:14.151456 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 22369622, 34],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[Pass] paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 22369622, 34],"float32"), )
2025-03-15 04:24:09.996126 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 27163112, 28],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[Pass] paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 27163112, 28],"float32"), )
2025-03-15 04:26:16.141375 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 28, 27163112],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[Pass] paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 28, 27163112],"float32"), )
2025-03-15 04:27:43.093166 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 280, 2716312],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[Pass] paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 280, 2716312],"float32"), )
2025-03-15 04:29:17.038110 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 29252582, 26],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[Pass] paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 29252582, 26],"float32"), )
2025-03-15 04:30:57.824848 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 298, 2552239],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[Pass] paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 298, 2552239],"float32"), )
2025-03-15 04:32:37.227328 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 32, 23767723],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[Pass] paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 32, 23767723],"float32"), )
2025-03-15 04:34:27.323957 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 34, 22369622],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[Pass] paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([1, 3, 34, 22369622],"float32"), )
2025-03-15 04:36:11.274775 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([657931, 3, 34, 34],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[Pass] paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([657931, 3, 34, 34],"float32"), )
2025-03-15 04:37:55.326622 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([7012, 3, 298, 364],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[Pass] paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([7012, 3, 298, 364],"float32"), )
2025-03-15 04:39:40.522790 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([7761, 3, 280, 350],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[Pass] paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([7761, 3, 280, 350],"float32"), )
2025-03-15 04:41:33.046253 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([914144, 3, 32, 26],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[Pass] paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([914144, 3, 32, 26],"float32"), )
2025-03-15 04:43:25.799730 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([970112, 3, 28, 28],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[Pass] paddle.Tensor.expand_as(Tensor([1, 1, 1, 1],"float32"), Tensor([970112, 3, 28, 28],"float32"), )
2025-03-15 04:45:02.645736 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 2281701379],"float32"), Tensor([1, 3, 28, 2281701379],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 1, 1, 2281701379],"float32"), Tensor([1, 3, 28, 2281701379],"float32"), ) 
 cannot reshape array of size 4300000000 into shape (1,3,28,2281701379)
2025-03-15 04:45:06.904154 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 2281701379],"float32"), Tensor([1, 3, 28, 28],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 1, 1, 2281701379],"float32"), Tensor([1, 3, 28, 28],"float32"), ) 
 The expanded size of the tensor (28) must match the existing size (2281701379) at non-singleton dimension 3.  Target sizes: [1, 3, 28, 28].  Tensor sizes: [1, 1, 1, 2281701379]
2025-03-15 04:45:09.396699 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 2281701379],"float32"), Tensor([1, 3, 280, 2281701379],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 1, 1, 2281701379],"float32"), Tensor([1, 3, 280, 2281701379],"float32"), ) 
 cannot reshape array of size 4300000000 into shape (1,3,280,2281701379)
2025-03-15 04:45:11.347627 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 2281701379],"float32"), Tensor([1, 3, 280, 350],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 1, 1, 2281701379],"float32"), Tensor([1, 3, 280, 350],"float32"), ) 
 The expanded size of the tensor (350) must match the existing size (2281701379) at non-singleton dimension 3.  Target sizes: [1, 3, 280, 350].  Tensor sizes: [1, 1, 1, 2281701379]
2025-03-15 04:45:13.295553 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 2281701379],"float32"), Tensor([1, 3, 298, 2281701379],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 1, 1, 2281701379],"float32"), Tensor([1, 3, 298, 2281701379],"float32"), ) 
 cannot reshape array of size 4300000000 into shape (1,3,298,2281701379)
2025-03-15 04:45:15.796288 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 2281701379],"float32"), Tensor([1, 3, 298, 364],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 1, 1, 2281701379],"float32"), Tensor([1, 3, 298, 364],"float32"), ) 
 The expanded size of the tensor (364) must match the existing size (2281701379) at non-singleton dimension 3.  Target sizes: [1, 3, 298, 364].  Tensor sizes: [1, 1, 1, 2281701379]
2025-03-15 04:45:17.415083 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 2281701379],"float32"), Tensor([1, 3, 32, 2281701379],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 1, 1, 2281701379],"float32"), Tensor([1, 3, 32, 2281701379],"float32"), ) 
 cannot reshape array of size 4300000000 into shape (1,3,32,2281701379)
2025-03-15 04:45:18.793437 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 2281701379],"float32"), Tensor([1, 3, 32, 26],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 1, 1, 2281701379],"float32"), Tensor([1, 3, 32, 26],"float32"), ) 
 The expanded size of the tensor (26) must match the existing size (2281701379) at non-singleton dimension 3.  Target sizes: [1, 3, 32, 26].  Tensor sizes: [1, 1, 1, 2281701379]
2025-03-15 04:45:20.719179 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 2281701379],"float32"), Tensor([1, 3, 34, 2281701379],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 1, 1, 2281701379],"float32"), Tensor([1, 3, 34, 2281701379],"float32"), ) 
 cannot reshape array of size 4300000000 into shape (1,3,34,2281701379)
2025-03-15 04:45:22.614001 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 1, 2281701379],"float32"), Tensor([1, 3, 34, 34],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 1, 1, 2281701379],"float32"), Tensor([1, 3, 34, 34],"float32"), ) 
 The expanded size of the tensor (34) must match the existing size (2281701379) at non-singleton dimension 3.  Target sizes: [1, 3, 34, 34].  Tensor sizes: [1, 1, 1, 2281701379]
2025-03-15 04:45:24.208423 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 2281701379, 1],"float32"), Tensor([1, 3, 2281701379, 26],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 1, 2281701379, 1],"float32"), Tensor([1, 3, 2281701379, 26],"float32"), ) 
 cannot reshape array of size 4300000000 into shape (1,3,2281701379,26)
2025-03-15 04:45:25.583083 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 2281701379, 1],"float32"), Tensor([1, 3, 2281701379, 28],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 1, 2281701379, 1],"float32"), Tensor([1, 3, 2281701379, 28],"float32"), ) 
 cannot reshape array of size 4300000000 into shape (1,3,2281701379,28)
2025-03-15 04:45:27.505704 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 2281701379, 1],"float32"), Tensor([1, 3, 2281701379, 34],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 1, 2281701379, 1],"float32"), Tensor([1, 3, 2281701379, 34],"float32"), ) 
 cannot reshape array of size 4300000000 into shape (1,3,2281701379,34)
2025-03-15 04:45:29.428276 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 2281701379, 1],"float32"), Tensor([1, 3, 2281701379, 350],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 1, 2281701379, 1],"float32"), Tensor([1, 3, 2281701379, 350],"float32"), ) 
 cannot reshape array of size 4300000000 into shape (1,3,2281701379,350)
2025-03-15 04:45:31.314914 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 2281701379, 1],"float32"), Tensor([1, 3, 2281701379, 364],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 1, 2281701379, 1],"float32"), Tensor([1, 3, 2281701379, 364],"float32"), ) 
 cannot reshape array of size 4300000000 into shape (1,3,2281701379,364)
2025-03-15 04:45:33.247219 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 2281701379, 1],"float32"), Tensor([1, 3, 28, 28],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 1, 2281701379, 1],"float32"), Tensor([1, 3, 28, 28],"float32"), ) 
 The expanded size of the tensor (28) must match the existing size (2281701379) at non-singleton dimension 2.  Target sizes: [1, 3, 28, 28].  Tensor sizes: [1, 1, 2281701379, 1]
2025-03-15 04:45:34.840667 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 2281701379, 1],"float32"), Tensor([1, 3, 280, 350],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 1, 2281701379, 1],"float32"), Tensor([1, 3, 280, 350],"float32"), ) 
 The expanded size of the tensor (280) must match the existing size (2281701379) at non-singleton dimension 2.  Target sizes: [1, 3, 280, 350].  Tensor sizes: [1, 1, 2281701379, 1]
2025-03-15 04:45:36.182148 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 2281701379, 1],"float32"), Tensor([1, 3, 298, 364],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 1, 2281701379, 1],"float32"), Tensor([1, 3, 298, 364],"float32"), ) 
 The expanded size of the tensor (298) must match the existing size (2281701379) at non-singleton dimension 2.  Target sizes: [1, 3, 298, 364].  Tensor sizes: [1, 1, 2281701379, 1]
2025-03-15 04:45:38.078577 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 2281701379, 1],"float32"), Tensor([1, 3, 32, 26],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 1, 2281701379, 1],"float32"), Tensor([1, 3, 32, 26],"float32"), ) 
 The expanded size of the tensor (32) must match the existing size (2281701379) at non-singleton dimension 2.  Target sizes: [1, 3, 32, 26].  Tensor sizes: [1, 1, 2281701379, 1]
2025-03-15 04:45:40.650711 test begin: paddle.Tensor.expand_as(Tensor([1, 1, 2281701379, 1],"float32"), Tensor([1, 3, 34, 34],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 1, 2281701379, 1],"float32"), Tensor([1, 3, 34, 34],"float32"), ) 
 The expanded size of the tensor (34) must match the existing size (2281701379) at non-singleton dimension 2.  Target sizes: [1, 3, 34, 34].  Tensor sizes: [1, 1, 2281701379, 1]
2025-03-15 04:45:42.771036 test begin: paddle.Tensor.expand_as(Tensor([1, 128],"int32"), Tensor([17825793, 128],"int64"), )

[Pass] paddle.Tensor.expand_as(Tensor([1, 128],"int32"), Tensor([17825793, 128],"int64"), )
2025-03-15 04:48:01.759300 test begin: paddle.Tensor.expand_as(Tensor([1, 128],"int32"), Tensor([5, 456340276],"int64"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 128],"int32"), Tensor([5, 456340276],"int64"), ) 
 The expanded size of the tensor (456340276) must match the existing size (128) at non-singleton dimension 1.  Target sizes: [5, 456340276].  Tensor sizes: [1, 128]
2025-03-15 04:48:09.901977 test begin: paddle.Tensor.expand_as(Tensor([1, 16],"float32"), Tensor([142606337, 16],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[Pass] paddle.Tensor.expand_as(Tensor([1, 16],"float32"), Tensor([142606337, 16],"float32"), )
2025-03-15 04:50:15.071056 test begin: paddle.Tensor.expand_as(Tensor([1, 16],"float32"), Tensor([64, 35651585],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 16],"float32"), Tensor([64, 35651585],"float32"), ) 
 The expanded size of the tensor (35651585) must match the existing size (16) at non-singleton dimension 1.  Target sizes: [64, 35651585].  Tensor sizes: [1, 16]
2025-03-15 04:50:19.936796 test begin: paddle.Tensor.expand_as(Tensor([1, 1],"int32"), Tensor([2281701379, 1],"int64"), )

[paddle error] paddle.Tensor.expand_as(Tensor([1, 1],"int32"), Tensor([2281701379, 1],"int64"), ) 
 (InvalidArgument) When the value in shape is negative for expand_as_v2 op, only -1 is supported, but the value received is -2013265917.
  [Hint: Expected target_shape[i] == -1, but received target_shape[i]:-2013265917 != -1:-1.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:76)

2025-03-15 04:50:39.878990 test begin: paddle.Tensor.expand_as(Tensor([1, 1],"int32"), Tensor([5, 456340276],"int64"), )

[Pass] paddle.Tensor.expand_as(Tensor([1, 1],"int32"), Tensor([5, 456340276],"int64"), )
2025-03-15 04:53:24.742805 test begin: paddle.Tensor.expand_as(Tensor([1, 2281701379, 1, 1],"float32"), Tensor([1, 2281701379, 28, 28],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 2281701379, 1, 1],"float32"), Tensor([1, 2281701379, 28, 28],"float32"), ) 
 cannot reshape array of size 4300000000 into shape (1,2281701379,28,28)
2025-03-15 04:53:29.482653 test begin: paddle.Tensor.expand_as(Tensor([1, 2281701379, 1, 1],"float32"), Tensor([1, 2281701379, 280, 350],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 2281701379, 1, 1],"float32"), Tensor([1, 2281701379, 280, 350],"float32"), ) 
 cannot reshape array of size 4300000000 into shape (1,2281701379,280,350)
2025-03-15 04:53:31.117872 test begin: paddle.Tensor.expand_as(Tensor([1, 2281701379, 1, 1],"float32"), Tensor([1, 2281701379, 298, 364],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 2281701379, 1, 1],"float32"), Tensor([1, 2281701379, 298, 364],"float32"), ) 
 cannot reshape array of size 4300000000 into shape (1,2281701379,298,364)
2025-03-15 04:53:32.603120 test begin: paddle.Tensor.expand_as(Tensor([1, 2281701379, 1, 1],"float32"), Tensor([1, 2281701379, 32, 26],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 2281701379, 1, 1],"float32"), Tensor([1, 2281701379, 32, 26],"float32"), ) 
 cannot reshape array of size 4300000000 into shape (1,2281701379,32,26)
2025-03-15 04:53:34.236881 test begin: paddle.Tensor.expand_as(Tensor([1, 2281701379, 1, 1],"float32"), Tensor([1, 2281701379, 34, 34],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 2281701379, 1, 1],"float32"), Tensor([1, 2281701379, 34, 34],"float32"), ) 
 cannot reshape array of size 4300000000 into shape (1,2281701379,34,34)
2025-03-15 04:53:35.609660 test begin: paddle.Tensor.expand_as(Tensor([1, 2281701379, 1, 1],"float32"), Tensor([1, 3, 28, 28],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 2281701379, 1, 1],"float32"), Tensor([1, 3, 28, 28],"float32"), ) 
 The expanded size of the tensor (3) must match the existing size (2281701379) at non-singleton dimension 1.  Target sizes: [1, 3, 28, 28].  Tensor sizes: [1, 2281701379, 1, 1]
2025-03-15 04:53:37.504545 test begin: paddle.Tensor.expand_as(Tensor([1, 2281701379, 1, 1],"float32"), Tensor([1, 3, 280, 350],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 2281701379, 1, 1],"float32"), Tensor([1, 3, 280, 350],"float32"), ) 
 The expanded size of the tensor (3) must match the existing size (2281701379) at non-singleton dimension 1.  Target sizes: [1, 3, 280, 350].  Tensor sizes: [1, 2281701379, 1, 1]
2025-03-15 04:53:39.432962 test begin: paddle.Tensor.expand_as(Tensor([1, 2281701379, 1, 1],"float32"), Tensor([1, 3, 298, 364],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 2281701379, 1, 1],"float32"), Tensor([1, 3, 298, 364],"float32"), ) 
 The expanded size of the tensor (3) must match the existing size (2281701379) at non-singleton dimension 1.  Target sizes: [1, 3, 298, 364].  Tensor sizes: [1, 2281701379, 1, 1]
2025-03-15 04:53:41.350266 test begin: paddle.Tensor.expand_as(Tensor([1, 2281701379, 1, 1],"float32"), Tensor([1, 3, 32, 26],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 2281701379, 1, 1],"float32"), Tensor([1, 3, 32, 26],"float32"), ) 
 The expanded size of the tensor (3) must match the existing size (2281701379) at non-singleton dimension 1.  Target sizes: [1, 3, 32, 26].  Tensor sizes: [1, 2281701379, 1, 1]
2025-03-15 04:53:43.264357 test begin: paddle.Tensor.expand_as(Tensor([1, 2281701379, 1, 1],"float32"), Tensor([1, 3, 34, 34],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 2281701379, 1, 1],"float32"), Tensor([1, 3, 34, 34],"float32"), ) 
 The expanded size of the tensor (3) must match the existing size (2281701379) at non-singleton dimension 1.  Target sizes: [1, 3, 34, 34].  Tensor sizes: [1, 2281701379, 1, 1]
2025-03-15 04:53:45.206528 test begin: paddle.Tensor.expand_as(Tensor([1, 2281701379],"float32"), Tensor([64, 16],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 2281701379],"float32"), Tensor([64, 16],"float32"), ) 
 The expanded size of the tensor (16) must match the existing size (2281701379) at non-singleton dimension 1.  Target sizes: [64, 16].  Tensor sizes: [1, 2281701379]
2025-03-15 04:53:47.195905 test begin: paddle.Tensor.expand_as(Tensor([1, 2281701379],"float32"), Tensor([64, 2281701379],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 2281701379],"float32"), Tensor([64, 2281701379],"float32"), ) 
 cannot reshape array of size 4300000000 into shape (64,2281701379)
2025-03-15 04:53:48.798229 test begin: paddle.Tensor.expand_as(Tensor([1, 2281701379],"int32"), Tensor([5, 128],"int64"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 2281701379],"int32"), Tensor([5, 128],"int64"), ) 
 The expanded size of the tensor (128) must match the existing size (2281701379) at non-singleton dimension 1.  Target sizes: [5, 128].  Tensor sizes: [1, 2281701379]
2025-03-15 04:53:53.381829 test begin: paddle.Tensor.expand_as(Tensor([1, 2281701379],"int32"), Tensor([5, 1],"int64"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 2281701379],"int32"), Tensor([5, 1],"int64"), ) 
 The expanded size of the tensor (1) must match the existing size (2281701379) at non-singleton dimension 1.  Target sizes: [5, 1].  Tensor sizes: [1, 2281701379]
2025-03-15 04:53:55.250479 test begin: paddle.Tensor.expand_as(Tensor([1, 2281701379],"int32"), Tensor([5, 2281701379],"int64"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 2281701379],"int32"), Tensor([5, 2281701379],"int64"), ) 
 cannot reshape array of size 4300000000 into shape (5,2281701379)
2025-03-15 04:53:57.116109 test begin: paddle.Tensor.expand_as(Tensor([1, 2281701379],"int64"), Tensor([26, 2281701379],"int64"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 2281701379],"int64"), Tensor([26, 2281701379],"int64"), ) 
 cannot reshape array of size 4300000000 into shape (26,2281701379)
2025-03-15 04:54:06.269309 test begin: paddle.Tensor.expand_as(Tensor([1, 2281701379],"int64"), Tensor([26, 64],"int64"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 2281701379],"int64"), Tensor([26, 64],"int64"), ) 
 The expanded size of the tensor (64) must match the existing size (2281701379) at non-singleton dimension 1.  Target sizes: [26, 64].  Tensor sizes: [1, 2281701379]
2025-03-15 04:54:10.314579 test begin: paddle.Tensor.expand_as(Tensor([1, 64],"int64"), Tensor([26, 87757746],"int64"), )

[torch error] paddle.Tensor.expand_as(Tensor([1, 64],"int64"), Tensor([26, 87757746],"int64"), ) 
 The expanded size of the tensor (87757746) must match the existing size (64) at non-singleton dimension 1.  Target sizes: [26, 87757746].  Tensor sizes: [1, 64]
2025-03-15 04:54:14.130314 test begin: paddle.Tensor.expand_as(Tensor([1, 64],"int64"), Tensor([35651585, 64],"int64"), )

[Pass] paddle.Tensor.expand_as(Tensor([1, 64],"int64"), Tensor([35651585, 64],"int64"), )
2025-03-15 04:57:38.448191 test begin: paddle.Tensor.expand_as(Tensor([142606337, 16],"float32"), Tensor([142606337, 16],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[Pass] paddle.Tensor.expand_as(Tensor([142606337, 16],"float32"), Tensor([142606337, 16],"float32"), )
2025-03-15 04:59:23.340549 test begin: paddle.Tensor.expand_as(Tensor([142606337, 16],"float32"), Tensor([64, 16],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([142606337, 16],"float32"), Tensor([64, 16],"float32"), ) 
 The expanded size of the tensor (64) must match the existing size (142606337) at non-singleton dimension 0.  Target sizes: [64, 16].  Tensor sizes: [142606337, 16]
2025-03-15 04:59:28.177791 test begin: paddle.Tensor.expand_as(Tensor([17825793, 128],"int32"), Tensor([17825793, 128],"int64"), )

[Pass] paddle.Tensor.expand_as(Tensor([17825793, 128],"int32"), Tensor([17825793, 128],"int64"), )
2025-03-15 05:01:59.295553 test begin: paddle.Tensor.expand_as(Tensor([17825793, 128],"int32"), Tensor([5, 128],"int64"), )

[torch error] paddle.Tensor.expand_as(Tensor([17825793, 128],"int32"), Tensor([5, 128],"int64"), ) 
 The expanded size of the tensor (5) must match the existing size (17825793) at non-singleton dimension 0.  Target sizes: [5, 128].  Tensor sizes: [17825793, 128]
2025-03-15 05:02:03.518494 test begin: paddle.Tensor.expand_as(Tensor([2, 1, 1140850690],"float32"), Tensor([2, 4, 1140850690],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([2, 1, 1140850690],"float32"), Tensor([2, 4, 1140850690],"float32"), ) 
 cannot reshape array of size 4300000000 into shape (2,4,1140850690)
2025-03-15 05:02:08.150626 test begin: paddle.Tensor.expand_as(Tensor([2, 1, 1140850690],"float32"), Tensor([2, 4, 32],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([2, 1, 1140850690],"float32"), Tensor([2, 4, 32],"float32"), ) 
 The expanded size of the tensor (32) must match the existing size (1140850690) at non-singleton dimension 2.  Target sizes: [2, 4, 32].  Tensor sizes: [2, 1, 1140850690]
2025-03-15 05:02:10.056407 test begin: paddle.Tensor.expand_as(Tensor([2, 1, 32],"float32"), Tensor([17825793, 4, 32],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([2, 1, 32],"float32"), Tensor([17825793, 4, 32],"float32"), ) 
 The expanded size of the tensor (17825793) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [17825793, 4, 32].  Tensor sizes: [2, 1, 32]
2025-03-15 05:02:11.961702 test begin: paddle.Tensor.expand_as(Tensor([2, 1, 32],"float32"), Tensor([2, 35651585, 32],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[Pass] paddle.Tensor.expand_as(Tensor([2, 1, 32],"float32"), Tensor([2, 35651585, 32],"float32"), )
2025-03-15 05:03:47.281799 test begin: paddle.Tensor.expand_as(Tensor([2, 1, 32],"float32"), Tensor([2, 4, 285212673],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([2, 1, 32],"float32"), Tensor([2, 4, 285212673],"float32"), ) 
 The expanded size of the tensor (285212673) must match the existing size (32) at non-singleton dimension 2.  Target sizes: [2, 4, 285212673].  Tensor sizes: [2, 1, 32]
2025-03-15 05:03:50.352431 test begin: paddle.Tensor.expand_as(Tensor([2, 35651585, 32],"float32"), Tensor([2, 35651585, 32],"float32"), )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[Pass] paddle.Tensor.expand_as(Tensor([2, 35651585, 32],"float32"), Tensor([2, 35651585, 32],"float32"), )
2025-03-15 05:05:14.945507 test begin: paddle.Tensor.expand_as(Tensor([2, 35651585, 32],"float32"), Tensor([2, 4, 32],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([2, 35651585, 32],"float32"), Tensor([2, 4, 32],"float32"), ) 
 The expanded size of the tensor (4) must match the existing size (35651585) at non-singleton dimension 1.  Target sizes: [2, 4, 32].  Tensor sizes: [2, 35651585, 32]
2025-03-15 05:05:19.070463 test begin: paddle.Tensor.expand_as(Tensor([2281701379, 1, 1, 1],"float32"), Tensor([1, 3, 28, 28],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([2281701379, 1, 1, 1],"float32"), Tensor([1, 3, 28, 28],"float32"), ) 
 The expanded size of the tensor (1) must match the existing size (2281701379) at non-singleton dimension 0.  Target sizes: [1, 3, 28, 28].  Tensor sizes: [2281701379, 1, 1, 1]
2025-03-15 05:05:20.980595 test begin: paddle.Tensor.expand_as(Tensor([2281701379, 1, 1, 1],"float32"), Tensor([1, 3, 280, 350],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([2281701379, 1, 1, 1],"float32"), Tensor([1, 3, 280, 350],"float32"), ) 
 The expanded size of the tensor (1) must match the existing size (2281701379) at non-singleton dimension 0.  Target sizes: [1, 3, 280, 350].  Tensor sizes: [2281701379, 1, 1, 1]
2025-03-15 05:05:22.902231 test begin: paddle.Tensor.expand_as(Tensor([2281701379, 1, 1, 1],"float32"), Tensor([1, 3, 298, 364],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([2281701379, 1, 1, 1],"float32"), Tensor([1, 3, 298, 364],"float32"), ) 
 The expanded size of the tensor (1) must match the existing size (2281701379) at non-singleton dimension 0.  Target sizes: [1, 3, 298, 364].  Tensor sizes: [2281701379, 1, 1, 1]
2025-03-15 05:05:25.397928 test begin: paddle.Tensor.expand_as(Tensor([2281701379, 1, 1, 1],"float32"), Tensor([1, 3, 32, 26],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([2281701379, 1, 1, 1],"float32"), Tensor([1, 3, 32, 26],"float32"), ) 
 The expanded size of the tensor (1) must match the existing size (2281701379) at non-singleton dimension 0.  Target sizes: [1, 3, 32, 26].  Tensor sizes: [2281701379, 1, 1, 1]
2025-03-15 05:05:27.304873 test begin: paddle.Tensor.expand_as(Tensor([2281701379, 1, 1, 1],"float32"), Tensor([1, 3, 34, 34],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([2281701379, 1, 1, 1],"float32"), Tensor([1, 3, 34, 34],"float32"), ) 
 The expanded size of the tensor (1) must match the existing size (2281701379) at non-singleton dimension 0.  Target sizes: [1, 3, 34, 34].  Tensor sizes: [2281701379, 1, 1, 1]
2025-03-15 05:05:29.780228 test begin: paddle.Tensor.expand_as(Tensor([2281701379, 1, 1, 1],"float32"), Tensor([2281701379, 3, 28, 28],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([2281701379, 1, 1, 1],"float32"), Tensor([2281701379, 3, 28, 28],"float32"), ) 
 cannot reshape array of size 4300000000 into shape (2281701379,3,28,28)
2025-03-15 05:05:31.388427 test begin: paddle.Tensor.expand_as(Tensor([2281701379, 1, 1, 1],"float32"), Tensor([2281701379, 3, 280, 350],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([2281701379, 1, 1, 1],"float32"), Tensor([2281701379, 3, 280, 350],"float32"), ) 
 cannot reshape array of size 4300000000 into shape (2281701379,3,280,350)
2025-03-15 05:05:33.570601 test begin: paddle.Tensor.expand_as(Tensor([2281701379, 1, 1, 1],"float32"), Tensor([2281701379, 3, 298, 364],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([2281701379, 1, 1, 1],"float32"), Tensor([2281701379, 3, 298, 364],"float32"), ) 
 cannot reshape array of size 4300000000 into shape (2281701379,3,298,364)
2025-03-15 05:05:35.466518 test begin: paddle.Tensor.expand_as(Tensor([2281701379, 1, 1, 1],"float32"), Tensor([2281701379, 3, 32, 26],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([2281701379, 1, 1, 1],"float32"), Tensor([2281701379, 3, 32, 26],"float32"), ) 
 cannot reshape array of size 4300000000 into shape (2281701379,3,32,26)
2025-03-15 05:05:37.381658 test begin: paddle.Tensor.expand_as(Tensor([2281701379, 1, 1, 1],"float32"), Tensor([2281701379, 3, 34, 34],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([2281701379, 1, 1, 1],"float32"), Tensor([2281701379, 3, 34, 34],"float32"), ) 
 cannot reshape array of size 4300000000 into shape (2281701379,3,34,34)
2025-03-15 05:05:39.873902 test begin: paddle.Tensor.expand_as(Tensor([2281701379, 1],"float32"), Tensor([2281701379, 22400],"int64"), )

[torch error] paddle.Tensor.expand_as(Tensor([2281701379, 1],"float32"), Tensor([2281701379, 22400],"int64"), ) 
 cannot reshape array of size 4300000000 into shape (2281701379,22400)
2025-03-15 05:05:42.375390 test begin: paddle.Tensor.expand_as(Tensor([2281701379, 1],"float32"), Tensor([4, 22400],"int64"), )

[torch error] paddle.Tensor.expand_as(Tensor([2281701379, 1],"float32"), Tensor([4, 22400],"int64"), ) 
 The expanded size of the tensor (4) must match the existing size (2281701379) at non-singleton dimension 0.  Target sizes: [4, 22400].  Tensor sizes: [2281701379, 1]
2025-03-15 05:05:44.262183 test begin: paddle.Tensor.expand_as(Tensor([2281701379, 1],"int32"), Tensor([2281701379, 1],"int64"), )

[paddle error] paddle.Tensor.expand_as(Tensor([2281701379, 1],"int32"), Tensor([2281701379, 1],"int64"), ) 
 (InvalidArgument) When the value in shape is negative for expand_as_v2 op, only -1 is supported, but the value received is -2013265917.
  [Hint: Expected target_shape[i] == -1, but received target_shape[i]:-2013265917 != -1:-1.] (at ../paddle/phi/kernels/gpu/expand_as_kernel.cu:76)

2025-03-15 05:06:10.345020 test begin: paddle.Tensor.expand_as(Tensor([2281701379, 1],"int32"), Tensor([5, 1],"int64"), )

[torch error] paddle.Tensor.expand_as(Tensor([2281701379, 1],"int32"), Tensor([5, 1],"int64"), ) 
 The expanded size of the tensor (5) must match the existing size (2281701379) at non-singleton dimension 0.  Target sizes: [5, 1].  Tensor sizes: [2281701379, 1]
2025-03-15 05:06:12.528722 test begin: paddle.Tensor.expand_as(Tensor([35651585, 64],"int64"), Tensor([26, 64],"int64"), )

[torch error] paddle.Tensor.expand_as(Tensor([35651585, 64],"int64"), Tensor([26, 64],"int64"), ) 
 The expanded size of the tensor (26) must match the existing size (35651585) at non-singleton dimension 0.  Target sizes: [26, 64].  Tensor sizes: [35651585, 64]
2025-03-15 05:06:15.469754 test begin: paddle.Tensor.expand_as(Tensor([35651585, 64],"int64"), Tensor([35651585, 64],"int64"), )

[Pass] paddle.Tensor.expand_as(Tensor([35651585, 64],"int64"), Tensor([35651585, 64],"int64"), )
2025-03-15 05:09:45.625484 test begin: paddle.Tensor.expand_as(Tensor([4, 1],"float32"), Tensor([101862, 22400],"int64"), )

[torch error] paddle.Tensor.expand_as(Tensor([4, 1],"float32"), Tensor([101862, 22400],"int64"), ) 
 The expanded size of the tensor (101862) must match the existing size (4) at non-singleton dimension 0.  Target sizes: [101862, 22400].  Tensor sizes: [4, 1]
2025-03-15 05:09:54.564249 test begin: paddle.Tensor.expand_as(Tensor([4, 1],"float32"), Tensor([4, 570425345],"int64"), )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.expand_as(Tensor([4, 1],"float32"), Tensor([4, 570425345],"int64"), )
2025-03-15 05:11:37.436489 test begin: paddle.Tensor.expand_as(Tensor([4, 570425345],"float32"), Tensor([4, 22400],"int64"), )

[torch error] paddle.Tensor.expand_as(Tensor([4, 570425345],"float32"), Tensor([4, 22400],"int64"), ) 
 The expanded size of the tensor (22400) must match the existing size (570425345) at non-singleton dimension 1.  Target sizes: [4, 22400].  Tensor sizes: [4, 570425345]
2025-03-15 05:11:42.230169 test begin: paddle.Tensor.expand_as(Tensor([4, 570425345],"float32"), Tensor([4, 570425345],"int64"), )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.expand_as(Tensor([4, 570425345],"float32"), Tensor([4, 570425345],"int64"), )
2025-03-15 05:13:37.959913 test begin: paddle.Tensor.expand_as(Tensor([71303169, 1, 32],"float32"), Tensor([2, 4, 32],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([71303169, 1, 32],"float32"), Tensor([2, 4, 32],"float32"), ) 
 The expanded size of the tensor (2) must match the existing size (71303169) at non-singleton dimension 0.  Target sizes: [2, 4, 32].  Tensor sizes: [71303169, 1, 32]
2025-03-15 05:13:42.918174 test begin: paddle.Tensor.expand_as(Tensor([71303169, 1, 32],"float32"), Tensor([71303169, 4, 32],"float32"), )

[torch error] paddle.Tensor.expand_as(Tensor([71303169, 1, 32],"float32"), Tensor([71303169, 4, 32],"float32"), ) 
 cannot reshape array of size 4300000000 into shape (71303169,4,32)
2025-03-15 05:13:45.116531 test begin: paddle.Tensor.fill_(Tensor([10, 228170138],"float32"), 1, )

[Pass] paddle.Tensor.fill_(Tensor([10, 228170138],"float32"), 1, )
2025-03-15 05:16:34.261845 test begin: paddle.Tensor.fill_(Tensor([134217729, 32],"float16"), 1.0, )

[Pass] paddle.Tensor.fill_(Tensor([134217729, 32],"float16"), 1.0, )
2025-03-15 05:32:59.152691 test begin: paddle.Tensor.fill_(Tensor([20, 114085069],"float32"), 1, )

[Pass] paddle.Tensor.fill_(Tensor([20, 114085069],"float32"), 1, )
2025-03-15 05:37:13.908346 test begin: paddle.Tensor.fill_(Tensor([2281701379],"float32"), 0, )

[Pass] paddle.Tensor.fill_(Tensor([2281701379],"float32"), 0, )
2025-03-15 05:40:23.167649 test begin: paddle.Tensor.fill_(Tensor([2281701379],"float32"), 1, )

[Pass] paddle.Tensor.fill_(Tensor([2281701379],"float32"), 1, )
2025-03-15 05:43:32.281809 test begin: paddle.Tensor.fill_(Tensor([2281701379],"float32"), 1.0, )

[Pass] paddle.Tensor.fill_(Tensor([2281701379],"float32"), 1.0, )
2025-03-15 05:47:05.954295 test begin: paddle.Tensor.fill_(Tensor([228170138, 10],"float32"), 1, )

[Pass] paddle.Tensor.fill_(Tensor([228170138, 10],"float32"), 1, )
2025-03-15 05:50:27.244564 test begin: paddle.Tensor.fill_(Tensor([29632486, 77],"float32"), value=-math.inf, )

[Pass] paddle.Tensor.fill_(Tensor([29632486, 77],"float32"), value=-math.inf, )
2025-03-15 05:52:43.249420 test begin: paddle.Tensor.fill_(Tensor([32, 134217729],"float16"), 1.0, )

[Pass] paddle.Tensor.fill_(Tensor([32, 134217729],"float16"), 1.0, )
2025-03-15 06:08:37.888654 test begin: paddle.Tensor.fill_(Tensor([32, 71303169],"float32"), 1, )

[Pass] paddle.Tensor.fill_(Tensor([32, 71303169],"float32"), 1, )
2025-03-15 06:11:10.881701 test begin: paddle.Tensor.fill_(Tensor([32, 71303169],"float32"), 1.0, )

[Pass] paddle.Tensor.fill_(Tensor([32, 71303169],"float32"), 1.0, )
2025-03-15 06:14:13.014562 test begin: paddle.Tensor.fill_(Tensor([32, 71303169],"int32"), 1.0, )

[Pass] paddle.Tensor.fill_(Tensor([32, 71303169],"int32"), 1.0, )
2025-03-15 06:16:21.389410 test begin: paddle.Tensor.fill_(Tensor([32, 71303169],"int64"), 1.0, )

[Pass] paddle.Tensor.fill_(Tensor([32, 71303169],"int64"), 1.0, )
2025-03-15 06:19:18.386290 test begin: paddle.Tensor.fill_(Tensor([380283564, 2, 3],"float32"), 0, )

[Pass] paddle.Tensor.fill_(Tensor([380283564, 2, 3],"float32"), 0, )
2025-03-15 06:22:43.314268 test begin: paddle.Tensor.fill_(Tensor([4, 190141782, 3],"float32"), 0, )

[Pass] paddle.Tensor.fill_(Tensor([4, 190141782, 3],"float32"), 0, )
2025-03-15 06:25:58.317737 test begin: paddle.Tensor.fill_(Tensor([4, 2, 285212673],"float32"), 0, )

[Pass] paddle.Tensor.fill_(Tensor([4, 2, 285212673],"float32"), 0, )
2025-03-15 06:28:54.965207 test begin: paddle.Tensor.fill_(Tensor([4294967297],"float16"), 1.0, )

[Pass] paddle.Tensor.fill_(Tensor([4294967297],"float16"), 1.0, )
2025-03-15 06:44:41.089080 test begin: paddle.Tensor.fill_(Tensor([71303169, 32],"float32"), 1, )

[Pass] paddle.Tensor.fill_(Tensor([71303169, 32],"float32"), 1, )
2025-03-15 06:48:06.806942 test begin: paddle.Tensor.fill_(Tensor([71303169, 32],"float32"), 1.0, )

[Pass] paddle.Tensor.fill_(Tensor([71303169, 32],"float32"), 1.0, )
2025-03-15 06:50:40.869732 test begin: paddle.Tensor.fill_(Tensor([71303169, 32],"int32"), 1.0, )

[Pass] paddle.Tensor.fill_(Tensor([71303169, 32],"int32"), 1.0, )
2025-03-15 06:52:45.347487 test begin: paddle.Tensor.fill_(Tensor([71303169, 32],"int64"), 1.0, )

[Pass] paddle.Tensor.fill_(Tensor([71303169, 32],"int64"), 1.0, )
2025-03-15 06:55:39.919500 test begin: paddle.Tensor.fill_(Tensor([76056713, 30],"float32"), 1, )

[Pass] paddle.Tensor.fill_(Tensor([76056713, 30],"float32"), 1, )
2025-03-15 06:58:19.063123 test begin: paddle.Tensor.fill_(Tensor([77, 29632486],"float32"), value=-math.inf, )

[Pass] paddle.Tensor.fill_(Tensor([77, 29632486],"float32"), value=-math.inf, )
2025-03-15 07:00:58.092118 test begin: paddle.Tensor.fill_(x=Tensor([10, 16, 26843546],"float16"), value=41.2, )

[Pass] paddle.Tensor.fill_(x=Tensor([10, 16, 26843546],"float16"), value=41.2, )
2025-03-15 07:17:12.195672 test begin: paddle.Tensor.fill_(x=Tensor([10, 26843546, 16],"float16"), value=41.2, )

[Pass] paddle.Tensor.fill_(x=Tensor([10, 26843546, 16],"float16"), value=41.2, )
2025-03-15 07:33:43.301094 test begin: paddle.Tensor.fill_(x=Tensor([1073741825, 4],"float16"), value=4, )

[Pass] paddle.Tensor.fill_(x=Tensor([1073741825, 4],"float16"), value=4, )
2025-03-15 07:49:41.074260 test begin: paddle.Tensor.fill_(x=Tensor([16, 268435457],"float16"), value=41.2, )

[Pass] paddle.Tensor.fill_(x=Tensor([16, 268435457],"float16"), value=41.2, )
2025-03-15 08:05:33.351272 test begin: paddle.Tensor.fill_(x=Tensor([16777217, 16, 16],"float16"), value=41.2, )

[Pass] paddle.Tensor.fill_(x=Tensor([16777217, 16, 16],"float16"), value=41.2, )
2025-03-15 08:21:23.256140 test begin: paddle.Tensor.fill_(x=Tensor([268435457, 16],"float16"), value=41.2, )

[Pass] paddle.Tensor.fill_(x=Tensor([268435457, 16],"float16"), value=41.2, )
2025-03-15 08:37:46.729623 test begin: paddle.Tensor.fill_(x=Tensor([4, 1073741825],"float16"), value=4, )

[Pass] paddle.Tensor.fill_(x=Tensor([4, 1073741825],"float16"), value=4, )
2025-03-15 08:54:11.786275 test begin: paddle.Tensor.fill_(x=Tensor([4, 570425345],"float32"), value=4, )

[Pass] paddle.Tensor.fill_(x=Tensor([4, 570425345],"float32"), value=4, )
2025-03-15 08:57:40.031188 test begin: paddle.Tensor.fill_(x=Tensor([4, 570425345],"int32"), value=4, )

element 0 of tensors does not require grad and does not have a grad_fn
[Pass] paddle.Tensor.fill_(x=Tensor([4, 570425345],"int32"), value=4, )
2025-03-15 08:59:40.603756 test begin: paddle.Tensor.fill_(x=Tensor([4, 570425345],"int64"), value=4, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.fill_(x=Tensor([4, 570425345],"int64"), value=4, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_assign(_object*, _object*, _object*)
1   assign_ad_func(paddle::Tensor const&)
2   paddle::experimental::assign(paddle::Tensor const&)
3   void phi::Copy<phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::Place, bool, phi::DenseTensor*)
4   phi::DeviceContext::Alloc(phi::TensorBase*, phi::DataType, unsigned long, bool, bool) const
5   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.596619GB memory has been allocated and available memory is only 9.588257GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-15 09:00:18.699113 test begin: paddle.Tensor.fill_(x=Tensor([4294967297],"float16"), value=4.2, )

[Pass] paddle.Tensor.fill_(x=Tensor([4294967297],"float16"), value=4.2, )
2025-03-15 09:17:03.121005 test begin: paddle.Tensor.fill_(x=Tensor([570425345, 4],"float32"), value=4, )

[Pass] paddle.Tensor.fill_(x=Tensor([570425345, 4],"float32"), value=4, )
2025-03-15 09:20:42.587857 test begin: paddle.Tensor.fill_(x=Tensor([570425345, 4],"int32"), value=4, )

element 0 of tensors does not require grad and does not have a grad_fn
[Pass] paddle.Tensor.fill_(x=Tensor([570425345, 4],"int32"), value=4, )
2025-03-15 09:23:25.046174 test begin: paddle.Tensor.fill_(x=Tensor([570425345, 4],"int64"), value=4, )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.fill_(x=Tensor([570425345, 4],"int64"), value=4, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_assign(_object*, _object*, _object*)
1   assign_ad_func(paddle::Tensor const&)
2   paddle::experimental::assign(paddle::Tensor const&)
3   void phi::Copy<phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::Place, bool, phi::DenseTensor*)
4   phi::DeviceContext::Alloc(phi::TensorBase*, phi::DataType, unsigned long, bool, bool) const
5   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.596619GB memory has been allocated and available memory is only 9.588257GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-15 09:24:04.297651 test begin: paddle.Tensor.fill_diagonal_(Tensor([128, 17825793],"float32"), 0, wrap=False, )

[Pass] paddle.Tensor.fill_diagonal_(Tensor([128, 17825793],"float32"), 0, wrap=False, )
2025-03-15 09:27:03.621106 test begin: paddle.Tensor.fill_diagonal_(Tensor([17825793, 128],"float32"), 0, wrap=False, )

[Pass] paddle.Tensor.fill_diagonal_(Tensor([17825793, 128],"float32"), 0, wrap=False, )
2025-03-15 09:30:11.345312 test begin: paddle.Tensor.flatten(Tensor([1, 1, 12, 190141782],"float32"), start_axis=2, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 1, 12, 190141782],"float32"), start_axis=2, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, 1, -2013265912], shape[2] = -2013265912.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265912 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 09:30:34.439735 test begin: paddle.Tensor.flatten(Tensor([1, 1, 16, 142606337],"float32"), start_axis=2, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 1, 16, 142606337],"float32"), start_axis=2, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, 1, -2013265904], shape[2] = -2013265904.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265904 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 09:30:54.661083 test begin: paddle.Tensor.flatten(Tensor([1, 1, 2, 2, 570425345],"float32"), 3, )

[Pass] paddle.Tensor.flatten(Tensor([1, 1, 2, 2, 570425345],"float32"), 3, )
2025-03-15 09:33:42.506117 test begin: paddle.Tensor.flatten(Tensor([1, 1, 2, 570425345, 2],"float32"), 3, )

[Pass] paddle.Tensor.flatten(Tensor([1, 1, 2, 570425345, 2],"float32"), 3, )
2025-03-15 09:36:22.879379 test begin: paddle.Tensor.flatten(Tensor([1, 1, 20, 114085069],"float32"), start_axis=2, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 1, 20, 114085069],"float32"), start_axis=2, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, 1, -2013265916], shape[2] = -2013265916.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265916 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 09:36:45.905572 test begin: paddle.Tensor.flatten(Tensor([1, 1, 2281701379],"float32"), stop_axis=-2, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 1, 2281701379],"float32"), stop_axis=-2, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265917], shape[1] = -2013265917.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265917 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 09:37:06.565193 test begin: paddle.Tensor.flatten(Tensor([1, 1, 2281701379],"int32"), )

[paddle error] paddle.Tensor.flatten(Tensor([1, 1, 2281701379],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265917], shape[0] = -2013265917.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265917 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 09:37:20.005088 test begin: paddle.Tensor.flatten(Tensor([1, 1, 2281701379],"int64"), stop_axis=-2, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 1, 2281701379],"int64"), stop_axis=-2, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265917], shape[1] = -2013265917.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265917 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 09:37:45.303744 test begin: paddle.Tensor.flatten(Tensor([1, 1, 35651585, 64],"float32"), start_axis=2, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 1, 35651585, 64],"float32"), start_axis=2, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, 1, -2013265856], shape[2] = -2013265856.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265856 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 09:38:07.557209 test begin: paddle.Tensor.flatten(Tensor([1, 1, 4, 2, 285212673],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 1, 4, 2, 285212673],"float32"), -2, )
2025-03-15 09:40:38.524901 test begin: paddle.Tensor.flatten(Tensor([1, 1, 4, 2, 285212673],"float32"), 3, )

[Pass] paddle.Tensor.flatten(Tensor([1, 1, 4, 2, 285212673],"float32"), 3, )
2025-03-15 09:43:12.897753 test begin: paddle.Tensor.flatten(Tensor([1, 1, 4, 285212673, 2],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 1, 4, 285212673, 2],"float32"), -2, )
2025-03-15 09:45:48.667738 test begin: paddle.Tensor.flatten(Tensor([1, 1, 4, 285212673, 2],"float32"), 3, )

[Pass] paddle.Tensor.flatten(Tensor([1, 1, 4, 285212673, 2],"float32"), 3, )
2025-03-15 09:49:04.457923 test begin: paddle.Tensor.flatten(Tensor([1, 1, 570425345, 2, 2],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 1, 570425345, 2, 2],"float32"), -2, )
2025-03-15 09:52:25.587391 test begin: paddle.Tensor.flatten(Tensor([1, 1, 570425345, 2, 2],"float32"), 3, )

[Pass] paddle.Tensor.flatten(Tensor([1, 1, 570425345, 2, 2],"float32"), 3, )
2025-03-15 09:55:22.176103 test begin: paddle.Tensor.flatten(Tensor([1, 1, 6, 380283564],"float32"), start_axis=2, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 1, 6, 380283564],"float32"), start_axis=2, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, 1, -2013265912], shape[2] = -2013265912.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265912 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 09:55:48.737703 test begin: paddle.Tensor.flatten(Tensor([1, 1, 8, 285212673],"float32"), start_axis=2, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 1, 8, 285212673],"float32"), start_axis=2, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, 1, -2013265912], shape[2] = -2013265912.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265912 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 09:56:15.506298 test begin: paddle.Tensor.flatten(Tensor([1, 10, 100, 2281702],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 10, 100, 2281702],"float32"), 2, )
2025-03-15 09:58:59.209281 test begin: paddle.Tensor.flatten(Tensor([1, 10, 104, 2193944],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 10, 104, 2193944],"float32"), 2, )
2025-03-15 10:02:20.869144 test begin: paddle.Tensor.flatten(Tensor([1, 10, 108, 2112687],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 10, 108, 2112687],"float32"), 2, )
2025-03-15 10:05:52.903925 test begin: paddle.Tensor.flatten(Tensor([1, 10, 112, 2037234],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 10, 112, 2037234],"float32"), 2, )
2025-03-15 10:08:36.014123 test begin: paddle.Tensor.flatten(Tensor([1, 10, 116, 1966984],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 10, 116, 1966984],"float32"), 2, )
2025-03-15 10:11:38.721500 test begin: paddle.Tensor.flatten(Tensor([1, 10, 1782580, 64, 2],"float32"), 3, )

[Pass] paddle.Tensor.flatten(Tensor([1, 10, 1782580, 64, 2],"float32"), 3, )
2025-03-15 10:14:55.394943 test begin: paddle.Tensor.flatten(Tensor([1, 10, 1966984, 116],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 10, 1966984, 116],"float32"), 2, )
2025-03-15 10:17:32.745453 test begin: paddle.Tensor.flatten(Tensor([1, 10, 2037234, 112],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 10, 2037234, 112],"float32"), 2, )
2025-03-15 10:20:30.011915 test begin: paddle.Tensor.flatten(Tensor([1, 10, 2112687, 108],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 10, 2112687, 108],"float32"), 2, )
2025-03-15 10:24:37.043839 test begin: paddle.Tensor.flatten(Tensor([1, 10, 2193944, 104],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 10, 2193944, 104],"float32"), 2, )
2025-03-15 10:27:44.230989 test begin: paddle.Tensor.flatten(Tensor([1, 10, 228170138],"float32"), 1, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 10, 228170138],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265916], shape[1] = -2013265916.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265916 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 10:28:10.996862 test begin: paddle.Tensor.flatten(Tensor([1, 10, 228170138],"int32"), )

[paddle error] paddle.Tensor.flatten(Tensor([1, 10, 228170138],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265916], shape[0] = -2013265916.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265916 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 10:28:24.249767 test begin: paddle.Tensor.flatten(Tensor([1, 10, 2281702, 100],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 10, 2281702, 100],"float32"), 2, )
2025-03-15 10:31:15.204401 test begin: paddle.Tensor.flatten(Tensor([1, 10, 8, 14260634, 2],"float32"), 3, )

[Pass] paddle.Tensor.flatten(Tensor([1, 10, 8, 14260634, 2],"float32"), 3, )
2025-03-15 10:34:58.635659 test begin: paddle.Tensor.flatten(Tensor([1, 10, 8, 64, 445645],"float32"), 3, )

[Pass] paddle.Tensor.flatten(Tensor([1, 10, 8, 64, 445645],"float32"), 3, )
2025-03-15 10:37:50.296366 test begin: paddle.Tensor.flatten(Tensor([1, 101, 22591103],"int32"), )

[paddle error] paddle.Tensor.flatten(Tensor([1, 101, 22591103],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265893], shape[0] = -2013265893.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265893 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 10:38:04.645550 test begin: paddle.Tensor.flatten(Tensor([1, 102, 22369622],"int32"), )

[paddle error] paddle.Tensor.flatten(Tensor([1, 102, 22369622],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265852], shape[0] = -2013265852.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265852 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 10:38:15.864608 test begin: paddle.Tensor.flatten(Tensor([1, 103, 22152441],"int32"), )

[paddle error] paddle.Tensor.flatten(Tensor([1, 103, 22152441],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265873], shape[0] = -2013265873.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265873 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 10:38:26.703674 test begin: paddle.Tensor.flatten(Tensor([1, 11, 1620527, 64, 2],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 11, 1620527, 64, 2],"float32"), 2, )
2025-03-15 10:41:40.637338 test begin: paddle.Tensor.flatten(Tensor([1, 11, 4, 25928425, 2],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 11, 4, 25928425, 2],"float32"), 2, )
2025-03-15 10:45:03.016124 test begin: paddle.Tensor.flatten(Tensor([1, 11, 4, 64, 810264],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 11, 4, 64, 810264],"float32"), 2, )
2025-03-15 10:47:51.816750 test begin: paddle.Tensor.flatten(Tensor([1, 1140850690, 2],"float32"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([1, 1140850690, 2],"float32"), 0, 1, )
2025-03-15 10:50:55.071632 test begin: paddle.Tensor.flatten(Tensor([1, 11408507, 200],"float32"), 1, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 11408507, 200],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265896], shape[1] = -2013265896.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265896 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 10:51:18.810572 test begin: paddle.Tensor.flatten(Tensor([1, 12, 1485483, 64, 2],"float32"), 3, )

[Pass] paddle.Tensor.flatten(Tensor([1, 12, 1485483, 64, 2],"float32"), 3, )
2025-03-15 10:54:59.524068 test begin: paddle.Tensor.flatten(Tensor([1, 12, 190141782],"float32"), 1, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 12, 190141782],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265912], shape[1] = -2013265912.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265912 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 10:55:29.502808 test begin: paddle.Tensor.flatten(Tensor([1, 12, 9, 10563433, 2],"float32"), 3, )

[Pass] paddle.Tensor.flatten(Tensor([1, 12, 9, 10563433, 2],"float32"), 3, )
2025-03-15 10:58:59.146342 test begin: paddle.Tensor.flatten(Tensor([1, 12, 9, 64, 330108],"float32"), 3, )

[Pass] paddle.Tensor.flatten(Tensor([1, 12, 9, 64, 330108],"float32"), 3, )
2025-03-15 11:02:23.823607 test begin: paddle.Tensor.flatten(Tensor([1, 12964213, 22, 4, 2],"float32"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([1, 12964213, 22, 4, 2],"float32"), 0, 1, )
2025-03-15 11:05:23.627760 test begin: paddle.Tensor.flatten(Tensor([1, 13581556, 21, 4, 2],"float32"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([1, 13581556, 21, 4, 2],"float32"), 0, 1, )
2025-03-15 11:09:17.034473 test begin: paddle.Tensor.flatten(Tensor([1, 142606337, 4, 2, 2],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 142606337, 4, 2, 2],"float32"), -2, )
2025-03-15 11:12:24.347286 test begin: paddle.Tensor.flatten(Tensor([1, 142606337, 4, 2, 2],"float32"), 3, )

[Pass] paddle.Tensor.flatten(Tensor([1, 142606337, 4, 2, 2],"float32"), 3, )
2025-03-15 11:15:56.511990 test begin: paddle.Tensor.flatten(Tensor([1, 142606337, 4, 4],"float32"), start_axis=1, stop_axis=3, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 142606337, 4, 4],"float32"), start_axis=1, stop_axis=3, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265904], shape[1] = -2013265904.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265904 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 11:16:23.798844 test begin: paddle.Tensor.flatten(Tensor([1, 144, 123791, 64, 2],"float32"), 3, )

[Pass] paddle.Tensor.flatten(Tensor([1, 144, 123791, 64, 2],"float32"), 3, )
2025-03-15 11:19:55.125776 test begin: paddle.Tensor.flatten(Tensor([1, 144, 15845149],"float32"), 1, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 144, 15845149],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265840], shape[1] = -2013265840.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265840 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 11:20:22.975155 test begin: paddle.Tensor.flatten(Tensor([1, 144, 200, 39613, 2],"float32"), 3, )

[Pass] paddle.Tensor.flatten(Tensor([1, 144, 200, 39613, 2],"float32"), 3, )
2025-03-15 11:23:15.295405 test begin: paddle.Tensor.flatten(Tensor([1, 144, 200, 64, 1238],"float32"), 3, )

[Pass] paddle.Tensor.flatten(Tensor([1, 144, 200, 64, 1238],"float32"), 3, )
2025-03-15 11:26:43.425403 test begin: paddle.Tensor.flatten(Tensor([1, 15845149, 144],"float32"), 1, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 15845149, 144],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265840], shape[1] = -2013265840.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265840 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 11:27:04.524341 test begin: paddle.Tensor.flatten(Tensor([1, 15845149, 18, 4, 2],"float32"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([1, 15845149, 18, 4, 2],"float32"), 0, 1, )
2025-03-15 11:29:43.127590 test begin: paddle.Tensor.flatten(Tensor([1, 169568, 116, 116],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 169568, 116, 116],"float32"), 2, )
2025-03-15 11:32:23.109565 test begin: paddle.Tensor.flatten(Tensor([1, 1782580, 20, 64],"float32"), start_axis=2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 1782580, 20, 64],"float32"), start_axis=2, )
2025-03-15 11:35:51.824237 test begin: paddle.Tensor.flatten(Tensor([1, 18, 126761188],"float32"), 1, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 18, 126761188],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265912], shape[1] = -2013265912.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265912 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 11:36:14.082306 test begin: paddle.Tensor.flatten(Tensor([1, 18, 4, 15845149, 2],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 18, 4, 15845149, 2],"float32"), 2, )
2025-03-15 11:39:30.313120 test begin: paddle.Tensor.flatten(Tensor([1, 18, 4, 64, 495161],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 18, 4, 64, 495161],"float32"), 2, )
2025-03-15 11:42:39.519724 test begin: paddle.Tensor.flatten(Tensor([1, 18, 990322, 64, 2],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 18, 990322, 64, 2],"float32"), 2, )
2025-03-15 11:46:19.279621 test begin: paddle.Tensor.flatten(Tensor([1, 181896, 112, 112],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 181896, 112, 112],"float32"), 2, )
2025-03-15 11:49:29.971763 test begin: paddle.Tensor.flatten(Tensor([1, 181896, 12544],"float32"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([1, 181896, 12544],"float32"), 0, 1, )
2025-03-15 11:53:40.115829 test begin: paddle.Tensor.flatten(Tensor([1, 192, 11883862],"float32"), 1, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 192, 11883862],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265792], shape[1] = -2013265792.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265792 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 11:54:04.842289 test begin: paddle.Tensor.flatten(Tensor([1, 195620, 108, 108],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 195620, 108, 108],"float32"), 2, )
2025-03-15 11:57:32.881340 test begin: paddle.Tensor.flatten(Tensor([1, 1980644, 9, 64, 2],"float32"), 3, )

[Pass] paddle.Tensor.flatten(Tensor([1, 1980644, 9, 64, 2],"float32"), 3, )
2025-03-15 12:00:36.262056 test begin: paddle.Tensor.flatten(Tensor([1, 207427399, 11],"float32"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([1, 207427399, 11],"float32"), 0, 1, )
2025-03-15 12:04:02.643783 test begin: paddle.Tensor.flatten(Tensor([1, 21, 4, 13581556, 2],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 21, 4, 13581556, 2],"float32"), 2, )
2025-03-15 12:07:36.734109 test begin: paddle.Tensor.flatten(Tensor([1, 21, 4, 64, 424424],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 21, 4, 64, 424424],"float32"), 2, )
2025-03-15 12:10:58.613802 test begin: paddle.Tensor.flatten(Tensor([1, 21, 848848, 64, 2],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 21, 848848, 64, 2],"float32"), 2, )
2025-03-15 12:14:30.747599 test begin: paddle.Tensor.flatten(Tensor([1, 210957, 104, 104],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 210957, 104, 104],"float32"), 2, )
2025-03-15 12:17:43.989328 test begin: paddle.Tensor.flatten(Tensor([1, 22, 4, 12964213, 2],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 22, 4, 12964213, 2],"float32"), 2, )
2025-03-15 12:21:36.124365 test begin: paddle.Tensor.flatten(Tensor([1, 22, 4, 64, 405132],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 22, 4, 64, 405132],"float32"), 2, )
2025-03-15 12:24:14.337507 test begin: paddle.Tensor.flatten(Tensor([1, 22, 810264, 64, 2],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 22, 810264, 64, 2],"float32"), 2, )
2025-03-15 12:27:35.764877 test begin: paddle.Tensor.flatten(Tensor([1, 221848, 10285],"float32"), stop_axis=-2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 221848, 10285],"float32"), stop_axis=-2, )
2025-03-15 12:30:52.166930 test begin: paddle.Tensor.flatten(Tensor([1, 2228225, 16, 64],"float32"), start_axis=2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 2228225, 16, 64],"float32"), start_axis=2, )
2025-03-15 12:34:15.991081 test begin: paddle.Tensor.flatten(Tensor([1, 2228225, 8, 64, 2],"float32"), 3, )

[Pass] paddle.Tensor.flatten(Tensor([1, 2228225, 8, 64, 2],"float32"), 3, )
2025-03-15 12:37:13.242684 test begin: paddle.Tensor.flatten(Tensor([1, 2281701379, 1],"int32"), )

[paddle error] paddle.Tensor.flatten(Tensor([1, 2281701379, 1],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265917], shape[0] = -2013265917.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265917 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 12:37:27.027064 test begin: paddle.Tensor.flatten(Tensor([1, 2281701379],"float32"), 1, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 2281701379],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265917], shape[1] = -2013265917.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265917 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 12:37:48.147223 test begin: paddle.Tensor.flatten(Tensor([1, 2281701379],"int32"), )

[paddle error] paddle.Tensor.flatten(Tensor([1, 2281701379],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265917], shape[0] = -2013265917.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265917 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 12:38:01.046887 test begin: paddle.Tensor.flatten(Tensor([1, 2281701379],"int64"), )

[paddle error] paddle.Tensor.flatten(Tensor([1, 2281701379],"int64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265917], shape[0] = -2013265917.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265917 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 12:38:26.488182 test begin: paddle.Tensor.flatten(Tensor([1, 228171, 100, 100],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 228171, 100, 100],"float32"), 2, )
2025-03-15 12:41:44.395964 test begin: paddle.Tensor.flatten(Tensor([1, 24, 24, 3961288],"float32"), start_axis=1, stop_axis=2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 24, 24, 3961288],"float32"), start_axis=1, stop_axis=2, )
2025-03-15 12:45:14.125444 test begin: paddle.Tensor.flatten(Tensor([1, 24, 495161, 192],"float32"), start_axis=1, stop_axis=2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 24, 495161, 192],"float32"), start_axis=1, stop_axis=2, )
2025-03-15 12:48:07.311645 test begin: paddle.Tensor.flatten(Tensor([1, 253522376, 9],"float32"), 1, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 253522376, 9],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265912], shape[1] = -2013265912.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265912 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 12:48:29.256135 test begin: paddle.Tensor.flatten(Tensor([1, 25928425, 11, 4, 2],"float32"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([1, 25928425, 11, 4, 2],"float32"), 0, 1, )
2025-03-15 12:52:00.733693 test begin: paddle.Tensor.flatten(Tensor([1, 259285, 1100, 4, 2],"float32"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([1, 259285, 1100, 4, 2],"float32"), 0, 1, )
2025-03-15 12:55:13.476047 test begin: paddle.Tensor.flatten(Tensor([1, 268435457, 4, 4],"float16"), start_axis=1, stop_axis=3, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 268435457, 4, 4],"float16"), start_axis=1, stop_axis=3, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 268435457, 4, 4], X's size = 4294967312, 'shape' is [1, 16], the capacity of 'shape' is 16.
  [Hint: Expected capacity == in_size, but received capacity:16 != in_size:4294967312.] (at ../paddle/phi/infermeta/unary.cc:2258)

2025-03-15 12:55:33.311922 test begin: paddle.Tensor.flatten(Tensor([1, 285212673, 2, 2, 2],"float32"), 3, )

[Pass] paddle.Tensor.flatten(Tensor([1, 285212673, 2, 2, 2],"float32"), 3, )
2025-03-15 12:59:33.655903 test begin: paddle.Tensor.flatten(Tensor([1, 285212673, 8],"float32"), 1, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 285212673, 8],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265912], shape[1] = -2013265912.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265912 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 12:59:55.461634 test begin: paddle.Tensor.flatten(Tensor([1, 2970966, 12, 64],"float32"), start_axis=2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 2970966, 12, 64],"float32"), start_axis=2, )
2025-03-15 13:03:37.828362 test begin: paddle.Tensor.flatten(Tensor([1, 300, 176, 43215],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 300, 176, 43215],"float32"), -2, )
2025-03-15 13:07:33.201681 test begin: paddle.Tensor.flatten(Tensor([1, 300, 184, 41336],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 300, 184, 41336],"float32"), -2, )
2025-03-15 13:10:52.204335 test begin: paddle.Tensor.flatten(Tensor([1, 300, 192, 39613],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 300, 192, 39613],"float32"), -2, )
2025-03-15 13:14:25.623722 test begin: paddle.Tensor.flatten(Tensor([1, 300, 39613, 192],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 300, 39613, 192],"float32"), -2, )
2025-03-15 13:17:36.874853 test begin: paddle.Tensor.flatten(Tensor([1, 300, 4, 64, 29710],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 300, 4, 64, 29710],"float32"), 2, )
2025-03-15 13:22:01.723182 test begin: paddle.Tensor.flatten(Tensor([1, 300, 4, 950709, 2],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 300, 4, 950709, 2],"float32"), 2, )
2025-03-15 13:26:24.474165 test begin: paddle.Tensor.flatten(Tensor([1, 300, 41336, 184],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 300, 41336, 184],"float32"), -2, )
2025-03-15 13:29:52.367273 test begin: paddle.Tensor.flatten(Tensor([1, 300, 43215, 176],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 300, 43215, 176],"float32"), -2, )
2025-03-15 13:33:38.541353 test begin: paddle.Tensor.flatten(Tensor([1, 300, 59420, 64, 2],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 300, 59420, 64, 2],"float32"), 2, )
2025-03-15 13:37:32.644496 test begin: paddle.Tensor.flatten(Tensor([1, 300, 7605672],"float32"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([1, 300, 7605672],"float32"), 0, 1, )
2025-03-15 13:40:28.320751 test begin: paddle.Tensor.flatten(Tensor([1, 4456449, 4, 64, 2],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 4456449, 4, 64, 2],"float32"), 2, )
2025-03-15 13:44:06.051992 test begin: paddle.Tensor.flatten(Tensor([1, 4456449, 8, 64],"float32"), start_axis=2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 4456449, 8, 64],"float32"), start_axis=2, )
2025-03-15 13:47:14.982620 test begin: paddle.Tensor.flatten(Tensor([1, 495161, 24, 192],"float32"), start_axis=1, stop_axis=2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 495161, 24, 192],"float32"), start_axis=1, stop_axis=2, )
2025-03-15 13:50:20.825358 test begin: paddle.Tensor.flatten(Tensor([1, 570425345, 4],"float32"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([1, 570425345, 4],"float32"), 0, 1, )
2025-03-15 13:54:10.617855 test begin: paddle.Tensor.flatten(Tensor([1, 59417, 38402],"float32"), stop_axis=-2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 59417, 38402],"float32"), stop_axis=-2, )
2025-03-15 13:57:41.920736 test begin: paddle.Tensor.flatten(Tensor([1, 5941931, 6, 64],"float32"), start_axis=2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 5941931, 6, 64],"float32"), start_axis=2, )
2025-03-15 14:01:15.039291 test begin: paddle.Tensor.flatten(Tensor([1, 6, 178956971, 4],"float16"), start_axis=1, stop_axis=3, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 6, 178956971, 4],"float16"), start_axis=1, stop_axis=3, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 6, 178956971, 4], X's size = 4294967304, 'shape' is [1, 8], the capacity of 'shape' is 8.
  [Hint: Expected capacity == in_size, but received capacity:8 != in_size:4294967304.] (at ../paddle/phi/infermeta/unary.cc:2258)

2025-03-15 14:01:37.312353 test begin: paddle.Tensor.flatten(Tensor([1, 6, 4, 178956971],"float16"), start_axis=1, stop_axis=3, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 6, 4, 178956971],"float16"), start_axis=1, stop_axis=3, ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [1, 6, 4, 178956971], X's size = 4294967304, 'shape' is [1, 8], the capacity of 'shape' is 8.
  [Hint: Expected capacity == in_size, but received capacity:8 != in_size:4294967304.] (at ../paddle/phi/infermeta/unary.cc:2258)

2025-03-15 14:01:56.387524 test begin: paddle.Tensor.flatten(Tensor([1, 6, 4, 95070891],"float32"), start_axis=1, stop_axis=3, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 6, 4, 95070891],"float32"), start_axis=1, stop_axis=3, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265912], shape[1] = -2013265912.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265912 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 14:02:19.332807 test begin: paddle.Tensor.flatten(Tensor([1, 6, 95070891, 4],"float32"), start_axis=1, stop_axis=3, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 6, 95070891, 4],"float32"), start_axis=1, stop_axis=3, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265912], shape[1] = -2013265912.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265912 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 14:02:36.076155 test begin: paddle.Tensor.flatten(Tensor([1, 61896, 192, 192],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 61896, 192, 192],"float32"), -2, )
2025-03-15 14:06:32.478764 test begin: paddle.Tensor.flatten(Tensor([1, 61906, 36858],"float32"), stop_axis=-2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 61906, 36858],"float32"), stop_axis=-2, )
2025-03-15 14:09:18.972684 test begin: paddle.Tensor.flatten(Tensor([1, 63380594, 36],"int64"), stop_axis=-2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 63380594, 36],"int64"), stop_axis=-2, )
2025-03-15 14:12:57.019897 test begin: paddle.Tensor.flatten(Tensor([1, 634159, 3598],"float32"), stop_axis=-2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 634159, 3598],"float32"), stop_axis=-2, )
2025-03-15 14:16:07.511178 test begin: paddle.Tensor.flatten(Tensor([1, 67395, 184, 184],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 67395, 184, 184],"float32"), -2, )
2025-03-15 14:19:38.097285 test begin: paddle.Tensor.flatten(Tensor([1, 73661, 176, 176],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([1, 73661, 176, 176],"float32"), -2, )
2025-03-15 14:22:47.167339 test begin: paddle.Tensor.flatten(Tensor([1, 8, 11, 12964213, 2],"float32"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([1, 8, 11, 12964213, 2],"float32"), 0, 1, )
2025-03-15 14:26:54.538308 test begin: paddle.Tensor.flatten(Tensor([1, 8, 11, 4, 6482107],"float32"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([1, 8, 11, 4, 6482107],"float32"), 0, 1, )
2025-03-15 14:30:23.839684 test begin: paddle.Tensor.flatten(Tensor([1, 8, 1100, 129643, 2],"float32"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([1, 8, 1100, 129643, 2],"float32"), 0, 1, )
2025-03-15 14:33:10.629507 test begin: paddle.Tensor.flatten(Tensor([1, 8, 1100, 4, 64822],"float32"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([1, 8, 1100, 4, 64822],"float32"), 0, 1, )
2025-03-15 14:36:47.867883 test begin: paddle.Tensor.flatten(Tensor([1, 8, 18, 4, 3961288],"float32"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([1, 8, 18, 4, 3961288],"float32"), 0, 1, )
2025-03-15 14:39:54.377034 test begin: paddle.Tensor.flatten(Tensor([1, 8, 18, 7922575, 2],"float32"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([1, 8, 18, 7922575, 2],"float32"), 0, 1, )
2025-03-15 14:43:00.108071 test begin: paddle.Tensor.flatten(Tensor([1, 8, 21, 4, 3395389],"float32"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([1, 8, 21, 4, 3395389],"float32"), 0, 1, )
2025-03-15 14:45:41.898407 test begin: paddle.Tensor.flatten(Tensor([1, 8, 21, 6790778, 2],"float32"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([1, 8, 21, 6790778, 2],"float32"), 0, 1, )
2025-03-15 14:48:41.057975 test begin: paddle.Tensor.flatten(Tensor([1, 8, 22, 4, 3241054],"float32"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([1, 8, 22, 4, 3241054],"float32"), 0, 1, )
2025-03-15 14:51:51.266256 test begin: paddle.Tensor.flatten(Tensor([1, 8, 22, 6482107, 2],"float32"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([1, 8, 22, 6482107, 2],"float32"), 0, 1, )
2025-03-15 14:55:12.517852 test begin: paddle.Tensor.flatten(Tensor([1, 8, 35651585, 4, 2],"float32"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([1, 8, 35651585, 4, 2],"float32"), 0, 1, )
2025-03-15 14:58:02.324013 test begin: paddle.Tensor.flatten(Tensor([1, 89129, 200, 64, 2],"float32"), 3, )

[Pass] paddle.Tensor.flatten(Tensor([1, 89129, 200, 64, 2],"float32"), 3, )
2025-03-15 15:01:10.504646 test begin: paddle.Tensor.flatten(Tensor([1, 91268056, 25],"float32"), 1, )

[paddle error] paddle.Tensor.flatten(Tensor([1, 91268056, 25],"float32"), 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [1, -2013265896], shape[1] = -2013265896.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265896 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 15:01:38.288986 test begin: paddle.Tensor.flatten(Tensor([10, 228170138],"float32"), 0, 1, )

[paddle error] paddle.Tensor.flatten(Tensor([10, 228170138],"float32"), 0, 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265916], shape[0] = -2013265916.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265916 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 15:01:59.956696 test begin: paddle.Tensor.flatten(Tensor([100, 22817014],"float32"), 0, 1, )

[paddle error] paddle.Tensor.flatten(Tensor([100, 22817014],"float32"), 0, 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265896], shape[0] = -2013265896.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265896 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 15:02:29.861610 test begin: paddle.Tensor.flatten(Tensor([100, 22817014],"float32"), 1, )

[Pass] paddle.Tensor.flatten(Tensor([100, 22817014],"float32"), 1, )
2025-03-15 15:06:22.833674 test begin: paddle.Tensor.flatten(Tensor([10186167, 4, 7, 4, 2],"float32"), -2, -1, )

[Pass] paddle.Tensor.flatten(Tensor([10186167, 4, 7, 4, 2],"float32"), -2, -1, )
2025-03-15 15:09:31.272943 test begin: paddle.Tensor.flatten(Tensor([1020, 76, 115, 256],"float32"), 1, 2, )

[Pass] paddle.Tensor.flatten(Tensor([1020, 76, 115, 256],"float32"), 1, 2, )
2025-03-15 15:12:22.732293 test begin: paddle.Tensor.flatten(Tensor([1114113, 2, 16, 4, 16],"float32"), start_axis=2, stop_axis=3, )

[Pass] paddle.Tensor.flatten(Tensor([1114113, 2, 16, 4, 16],"float32"), start_axis=2, stop_axis=3, )
2025-03-15 15:15:37.163141 test begin: paddle.Tensor.flatten(Tensor([11369, 64, 56, 56],"float32"), start_axis=2, stop_axis=-1, )

[Pass] paddle.Tensor.flatten(Tensor([11369, 64, 56, 56],"float32"), start_axis=2, stop_axis=-1, )
2025-03-15 15:19:18.601841 test begin: paddle.Tensor.flatten(Tensor([12, 18397, 76, 136],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([12, 18397, 76, 136],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013150592], shape[0] = -2013150592.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013150592 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 15:19:41.291262 test begin: paddle.Tensor.flatten(Tensor([12, 19, 10007463, 1],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([12, 19, 10007463, 1],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265732], shape[0] = -2013265732.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265732 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 15:20:00.538358 test begin: paddle.Tensor.flatten(Tensor([12, 19, 34, 294338],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([12, 19, 34, 294338],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013259120], shape[0] = -2013259120.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013259120 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 15:20:21.204183 test begin: paddle.Tensor.flatten(Tensor([12, 2796203, 68, 1],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([12, 2796203, 68, 1],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265648], shape[0] = -2013265648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265648 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 15:20:38.534742 test begin: paddle.Tensor.flatten(Tensor([12, 294338, 19, 34],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([12, 294338, 19, 34],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013259120], shape[0] = -2013259120.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013259120 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 15:20:57.906953 test begin: paddle.Tensor.flatten(Tensor([12, 38, 5003732, 1],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([12, 38, 5003732, 1],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265504], shape[0] = -2013265504.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265504 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 15:21:17.061325 test begin: paddle.Tensor.flatten(Tensor([12, 38, 68, 73585],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([12, 38, 68, 73585],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013243616], shape[0] = -2013243616.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013243616 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 15:21:38.555694 test begin: paddle.Tensor.flatten(Tensor([12, 4, 1398102, 34],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([12, 4, 1398102, 34],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013264832], shape[0] = -2013264832.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013264832 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 15:21:59.696179 test begin: paddle.Tensor.flatten(Tensor([12, 4, 19, 2501866],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([12, 4, 19, 2501866],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265504], shape[0] = -2013265504.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265504 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 15:22:17.819655 test begin: paddle.Tensor.flatten(Tensor([12, 4, 349526, 136],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([12, 4, 349526, 136],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013261568], shape[0] = -2013261568.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013261568 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 15:22:36.549078 test begin: paddle.Tensor.flatten(Tensor([12, 4, 38, 1250933],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([12, 4, 38, 1250933],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265504], shape[0] = -2013265504.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265504 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 15:22:57.829000 test begin: paddle.Tensor.flatten(Tensor([12, 4, 699051, 68],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([12, 4, 699051, 68],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013264832], shape[0] = -2013264832.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013264832 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 15:23:18.642383 test begin: paddle.Tensor.flatten(Tensor([12, 4, 76, 625467],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([12, 4, 76, 625467],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013263680], shape[0] = -2013263680.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013263680 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 15:23:45.168669 test begin: paddle.Tensor.flatten(Tensor([12, 5592406, 34, 1],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([12, 5592406, 34, 1],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265648], shape[0] = -2013265648.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265648 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 15:24:10.869809 test begin: paddle.Tensor.flatten(Tensor([12, 73585, 38, 68],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([12, 73585, 38, 68],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013243616], shape[0] = -2013243616.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013243616 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 15:24:30.864577 test begin: paddle.Tensor.flatten(Tensor([12294, 25, 29, 256],"float32"), 1, 2, )

[Pass] paddle.Tensor.flatten(Tensor([12294, 25, 29, 256],"float32"), 1, 2, )
2025-03-15 15:28:16.541482 test begin: paddle.Tensor.flatten(Tensor([12484, 32, 476, 3, 4],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([12484, 32, 476, 3, 4],"float32"), -2, )
2025-03-15 15:31:21.907692 test begin: paddle.Tensor.flatten(Tensor([128, 3121, 476, 3, 4],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([128, 3121, 476, 3, 4],"float32"), -2, )
2025-03-15 15:34:31.287421 test begin: paddle.Tensor.flatten(Tensor([128, 32, 46422, 3, 4],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([128, 32, 46422, 3, 4],"float32"), -2, )
2025-03-15 15:37:23.872142 test begin: paddle.Tensor.flatten(Tensor([128, 32, 476, 293, 4],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([128, 32, 476, 293, 4],"float32"), -2, )
2025-03-15 15:40:40.217392 test begin: paddle.Tensor.flatten(Tensor([128, 32, 476, 3, 391],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([128, 32, 476, 3, 391],"float32"), -2, )
2025-03-15 15:43:34.659507 test begin: paddle.Tensor.flatten(Tensor([128, 5685, 56, 56],"float32"), start_axis=2, stop_axis=-1, )

[Pass] paddle.Tensor.flatten(Tensor([128, 5685, 56, 56],"float32"), start_axis=2, stop_axis=-1, )
2025-03-15 15:46:37.016652 test begin: paddle.Tensor.flatten(Tensor([128, 64, 4974, 56],"float32"), start_axis=2, stop_axis=-1, )

[Pass] paddle.Tensor.flatten(Tensor([128, 64, 4974, 56],"float32"), start_axis=2, stop_axis=-1, )
2025-03-15 15:49:27.531831 test begin: paddle.Tensor.flatten(Tensor([128, 64, 56, 4974],"float32"), start_axis=2, stop_axis=-1, )

[Pass] paddle.Tensor.flatten(Tensor([128, 64, 56, 4974],"float32"), start_axis=2, stop_axis=-1, )
2025-03-15 15:52:03.694152 test begin: paddle.Tensor.flatten(Tensor([13, 1371215, 64, 2],"float32"), -2, -1, )

[Pass] paddle.Tensor.flatten(Tensor([13, 1371215, 64, 2],"float32"), -2, -1, )
2025-03-15 15:55:08.615071 test begin: paddle.Tensor.flatten(Tensor([13, 171402, 16, 4, 16],"float32"), start_axis=2, stop_axis=3, )

[Pass] paddle.Tensor.flatten(Tensor([13, 171402, 16, 4, 16],"float32"), start_axis=2, stop_axis=3, )
2025-03-15 15:58:16.982153 test begin: paddle.Tensor.flatten(Tensor([13, 2, 1371215, 4, 16],"float32"), start_axis=2, stop_axis=3, )

[Pass] paddle.Tensor.flatten(Tensor([13, 2, 1371215, 4, 16],"float32"), start_axis=2, stop_axis=3, )
2025-03-15 16:01:19.646052 test begin: paddle.Tensor.flatten(Tensor([13, 2, 16, 342804, 16],"float32"), start_axis=2, stop_axis=3, )

[Pass] paddle.Tensor.flatten(Tensor([13, 2, 16, 342804, 16],"float32"), start_axis=2, stop_axis=3, )
2025-03-15 16:04:46.637822 test begin: paddle.Tensor.flatten(Tensor([13, 2, 16, 4, 1371215],"float32"), start_axis=2, stop_axis=3, )

[Pass] paddle.Tensor.flatten(Tensor([13, 2, 16, 4, 1371215],"float32"), start_axis=2, stop_axis=3, )
2025-03-15 16:07:28.056108 test begin: paddle.Tensor.flatten(Tensor([13, 2, 16, 5484860, 1],"float32"), start_axis=2, stop_axis=3, )

[Pass] paddle.Tensor.flatten(Tensor([13, 2, 16, 5484860, 1],"float32"), start_axis=2, stop_axis=3, )
2025-03-15 16:10:26.155011 test begin: paddle.Tensor.flatten(Tensor([13, 2, 21939437, 4, 1],"float32"), start_axis=2, stop_axis=3, )

[Pass] paddle.Tensor.flatten(Tensor([13, 2, 21939437, 4, 1],"float32"), start_axis=2, stop_axis=3, )
2025-03-15 16:13:17.867314 test begin: paddle.Tensor.flatten(Tensor([13, 2, 4, 21939437],"int64"), start_axis=2, stop_axis=3, )

[Pass] paddle.Tensor.flatten(Tensor([13, 2, 4, 21939437],"int64"), start_axis=2, stop_axis=3, )
2025-03-15 16:16:43.680945 test begin: paddle.Tensor.flatten(Tensor([13, 2, 5484860, 16],"int64"), start_axis=2, stop_axis=3, )

[Pass] paddle.Tensor.flatten(Tensor([13, 2, 5484860, 16],"int64"), start_axis=2, stop_axis=3, )
2025-03-15 16:20:09.211208 test begin: paddle.Tensor.flatten(Tensor([13, 2, 8, 4, 2742430],"float32"), start_axis=2, stop_axis=3, )

[Pass] paddle.Tensor.flatten(Tensor([13, 2, 8, 4, 2742430],"float32"), start_axis=2, stop_axis=3, )
2025-03-15 16:23:20.186258 test begin: paddle.Tensor.flatten(Tensor([13, 2, 8, 685608, 16],"float32"), start_axis=2, stop_axis=3, )

[Pass] paddle.Tensor.flatten(Tensor([13, 2, 8, 685608, 16],"float32"), start_axis=2, stop_axis=3, )
2025-03-15 16:26:34.930139 test begin: paddle.Tensor.flatten(Tensor([13, 2, 87757746],"int64"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([13, 2, 87757746],"int64"), 0, 1, )
2025-03-15 16:30:00.851175 test begin: paddle.Tensor.flatten(Tensor([13, 2742430, 16, 4, 1],"float32"), start_axis=2, stop_axis=3, )

[Pass] paddle.Tensor.flatten(Tensor([13, 2742430, 16, 4, 1],"float32"), start_axis=2, stop_axis=3, )
2025-03-15 16:33:36.357755 test begin: paddle.Tensor.flatten(Tensor([13, 2742430, 4, 16],"int64"), start_axis=2, stop_axis=3, )

[Pass] paddle.Tensor.flatten(Tensor([13, 2742430, 4, 16],"int64"), start_axis=2, stop_axis=3, )
2025-03-15 16:36:51.304798 test begin: paddle.Tensor.flatten(Tensor([13, 2742430, 64],"int64"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([13, 2742430, 64],"int64"), 0, 1, )
2025-03-15 16:40:03.966429 test begin: paddle.Tensor.flatten(Tensor([13, 3134206, 7, 4, 2],"float32"), -2, -1, )

[Pass] paddle.Tensor.flatten(Tensor([13, 3134206, 7, 4, 2],"float32"), -2, -1, )
2025-03-15 16:43:25.942966 test begin: paddle.Tensor.flatten(Tensor([13, 342804, 8, 4, 16],"float32"), start_axis=2, stop_axis=3, )

[Pass] paddle.Tensor.flatten(Tensor([13, 342804, 8, 4, 16],"float32"), start_axis=2, stop_axis=3, )
2025-03-15 16:46:09.911297 test begin: paddle.Tensor.flatten(Tensor([13, 4, 5484860, 4, 2],"float32"), -2, -1, )

[Pass] paddle.Tensor.flatten(Tensor([13, 4, 5484860, 4, 2],"float32"), -2, -1, )
2025-03-15 16:49:01.786389 test begin: paddle.Tensor.flatten(Tensor([13, 4, 7, 3134206, 2],"float32"), -2, -1, )

[Pass] paddle.Tensor.flatten(Tensor([13, 4, 7, 3134206, 2],"float32"), -2, -1, )
2025-03-15 16:52:23.594854 test begin: paddle.Tensor.flatten(Tensor([13, 4, 7, 4, 1567103],"float32"), -2, -1, )

[Pass] paddle.Tensor.flatten(Tensor([13, 4, 7, 4, 1567103],"float32"), -2, -1, )
2025-03-15 16:55:41.239253 test begin: paddle.Tensor.flatten(Tensor([13, 7, 12536821, 2],"float32"), -2, -1, )

[Pass] paddle.Tensor.flatten(Tensor([13, 7, 12536821, 2],"float32"), -2, -1, )
2025-03-15 16:58:41.691149 test begin: paddle.Tensor.flatten(Tensor([13, 7, 64, 391776],"float32"), -2, -1, )

[Pass] paddle.Tensor.flatten(Tensor([13, 7, 64, 391776],"float32"), -2, -1, )
2025-03-15 17:02:17.163265 test begin: paddle.Tensor.flatten(Tensor([14, 1, 4, 2, 20372334],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([14, 1, 4, 2, 20372334],"float32"), -2, )
2025-03-15 17:05:30.735006 test begin: paddle.Tensor.flatten(Tensor([14, 1, 4, 20372334, 2],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([14, 1, 4, 20372334, 2],"float32"), -2, )
2025-03-15 17:08:56.355934 test begin: paddle.Tensor.flatten(Tensor([14, 1, 40744668, 2, 2],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([14, 1, 40744668, 2, 2],"float32"), -2, )
2025-03-15 17:11:50.300362 test begin: paddle.Tensor.flatten(Tensor([14, 10, 4, 2, 2037234],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([14, 10, 4, 2, 2037234],"float32"), -2, )
2025-03-15 17:14:59.617417 test begin: paddle.Tensor.flatten(Tensor([14, 10, 4, 2037234, 2],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([14, 10, 4, 2037234, 2],"float32"), -2, )
2025-03-15 17:18:34.914665 test begin: paddle.Tensor.flatten(Tensor([14, 10, 4074467, 2, 2],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([14, 10, 4074467, 2, 2],"float32"), -2, )
2025-03-15 17:21:17.711179 test begin: paddle.Tensor.flatten(Tensor([14, 10186167, 4, 2, 2],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([14, 10186167, 4, 2, 2],"float32"), -2, )
2025-03-15 17:24:18.438068 test begin: paddle.Tensor.flatten(Tensor([14, 3, 13581556, 2, 2],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([14, 3, 13581556, 2, 2],"float32"), -2, )
2025-03-15 17:27:18.925174 test begin: paddle.Tensor.flatten(Tensor([14, 3, 4, 2, 6790778],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([14, 3, 4, 2, 6790778],"float32"), -2, )
2025-03-15 17:30:08.312957 test begin: paddle.Tensor.flatten(Tensor([14, 3, 4, 6790778, 2],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([14, 3, 4, 6790778, 2],"float32"), -2, )
2025-03-15 17:33:04.980385 test begin: paddle.Tensor.flatten(Tensor([14, 64, 25, 376, 280],"float32"), start_axis=1, stop_axis=2, )

[Pass] paddle.Tensor.flatten(Tensor([14, 64, 25, 376, 280],"float32"), start_axis=1, stop_axis=2, )
2025-03-15 17:36:26.095844 test begin: paddle.Tensor.flatten(Tensor([142606337, 1, 4, 2, 2],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([142606337, 1, 4, 2, 2],"float32"), -2, )
2025-03-15 17:39:40.035929 test begin: paddle.Tensor.flatten(Tensor([142606337, 1, 4, 2, 2],"float32"), 3, )

[Pass] paddle.Tensor.flatten(Tensor([142606337, 1, 4, 2, 2],"float32"), 3, )
2025-03-15 17:43:10.330009 test begin: paddle.Tensor.flatten(Tensor([14260634, 10, 4, 2, 2],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([14260634, 10, 4, 2, 2],"float32"), -2, )
2025-03-15 17:46:27.542597 test begin: paddle.Tensor.flatten(Tensor([14855, 300, 4, 64, 2],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([14855, 300, 4, 64, 2],"float32"), 2, )
2025-03-15 17:49:08.402845 test begin: paddle.Tensor.flatten(Tensor([16, 142606337, 1, 1],"float32"), 1, )

[Pass] paddle.Tensor.flatten(Tensor([16, 142606337, 1, 1],"float32"), 1, )
2025-03-15 17:51:48.148072 test begin: paddle.Tensor.flatten(Tensor([16, 33, 1, 4321405],"float32"), 1, )

[Pass] paddle.Tensor.flatten(Tensor([16, 33, 1, 4321405],"float32"), 1, )
2025-03-15 17:54:40.691350 test begin: paddle.Tensor.flatten(Tensor([16, 33, 4321405, 1],"float32"), 1, )

[Pass] paddle.Tensor.flatten(Tensor([16, 33, 4321405, 1],"float32"), 1, )
2025-03-15 17:57:39.501918 test begin: paddle.Tensor.flatten(Tensor([16176, 19, 29, 256],"float32"), 1, 2, )

[Pass] paddle.Tensor.flatten(Tensor([16176, 19, 29, 256],"float32"), 1, 2, )
2025-03-15 18:00:58.938320 test begin: paddle.Tensor.flatten(Tensor([1620527, 8, 22, 4, 2],"float32"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([1620527, 8, 22, 4, 2],"float32"), 0, 1, )
2025-03-15 18:03:30.900080 test begin: paddle.Tensor.flatten(Tensor([165054, 12, 9, 64, 2],"float32"), 3, )

[Pass] paddle.Tensor.flatten(Tensor([165054, 12, 9, 64, 2],"float32"), 3, )
2025-03-15 18:06:30.053568 test begin: paddle.Tensor.flatten(Tensor([16957, 10, 116, 116],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([16957, 10, 116, 116],"float32"), 2, )
2025-03-15 18:09:11.533567 test begin: paddle.Tensor.flatten(Tensor([1697695, 8, 21, 4, 2],"float32"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([1697695, 8, 21, 4, 2],"float32"), 0, 1, )
2025-03-15 18:11:50.627245 test begin: paddle.Tensor.flatten(Tensor([17825793, 2, 16, 4, 1],"float32"), start_axis=2, stop_axis=3, )

[Pass] paddle.Tensor.flatten(Tensor([17825793, 2, 16, 4, 1],"float32"), start_axis=2, stop_axis=3, )
2025-03-15 18:14:51.156360 test begin: paddle.Tensor.flatten(Tensor([17825793, 2, 4, 16],"int64"), start_axis=2, stop_axis=3, )

[Pass] paddle.Tensor.flatten(Tensor([17825793, 2, 4, 16],"int64"), start_axis=2, stop_axis=3, )
2025-03-15 18:17:51.379311 test begin: paddle.Tensor.flatten(Tensor([17825793, 2, 64],"int64"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([17825793, 2, 64],"int64"), 0, 1, )
2025-03-15 18:20:49.837181 test begin: paddle.Tensor.flatten(Tensor([1782580, 1, 20, 64],"float32"), start_axis=2, )

[Pass] paddle.Tensor.flatten(Tensor([1782580, 1, 20, 64],"float32"), start_axis=2, )
2025-03-15 18:23:22.498330 test begin: paddle.Tensor.flatten(Tensor([181896, 12544],"float32"), 1, )

[Pass] paddle.Tensor.flatten(Tensor([181896, 12544],"float32"), 1, )
2025-03-15 18:26:48.189966 test begin: paddle.Tensor.flatten(Tensor([18190, 10, 112, 112],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([18190, 10, 112, 112],"float32"), 2, )
2025-03-15 18:29:20.775812 test begin: paddle.Tensor.flatten(Tensor([1901418, 300, 4],"float32"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([1901418, 300, 4],"float32"), 0, 1, )
2025-03-15 18:32:31.297011 test begin: paddle.Tensor.flatten(Tensor([19562, 10, 108, 108],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([19562, 10, 108, 108],"float32"), 2, )
2025-03-15 18:35:29.672739 test begin: paddle.Tensor.flatten(Tensor([1980644, 8, 18, 4, 2],"float32"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([1980644, 8, 18, 4, 2],"float32"), 0, 1, )
2025-03-15 18:38:30.729320 test begin: paddle.Tensor.flatten(Tensor([2, 10, 15, 7605672],"float32"), 1, 2, )

[Pass] paddle.Tensor.flatten(Tensor([2, 10, 15, 7605672],"float32"), 1, 2, )
2025-03-15 18:41:26.832073 test begin: paddle.Tensor.flatten(Tensor([2, 10, 445645, 256],"float32"), 1, 2, )

[Pass] paddle.Tensor.flatten(Tensor([2, 10, 445645, 256],"float32"), 1, 2, )
2025-03-15 18:44:38.482840 test begin: paddle.Tensor.flatten(Tensor([2, 1140850690],"float32"), 1, )

[Pass] paddle.Tensor.flatten(Tensor([2, 1140850690],"float32"), 1, )
2025-03-15 18:47:40.826059 test begin: paddle.Tensor.flatten(Tensor([2, 153671, 29, 256],"float32"), 1, 2, )

[Pass] paddle.Tensor.flatten(Tensor([2, 153671, 29, 256],"float32"), 1, 2, )
2025-03-15 18:51:02.120960 test begin: paddle.Tensor.flatten(Tensor([2, 16, 71303169],"int64"), start_axis=1, )

[Pass] paddle.Tensor.flatten(Tensor([2, 16, 71303169],"int64"), start_axis=1, )
2025-03-15 18:53:50.724798 test begin: paddle.Tensor.flatten(Tensor([2, 19, 234550, 256],"float32"), 1, 2, )

[Pass] paddle.Tensor.flatten(Tensor([2, 19, 234550, 256],"float32"), 1, 2, )
2025-03-15 18:56:23.354016 test begin: paddle.Tensor.flatten(Tensor([2, 19, 29, 2070510],"float32"), 1, 2, )

[Pass] paddle.Tensor.flatten(Tensor([2, 19, 29, 2070510],"float32"), 1, 2, )
2025-03-15 18:59:24.610689 test begin: paddle.Tensor.flatten(Tensor([2, 19014179, 10, 3, 2],"float32"), -2, -1, )

[Pass] paddle.Tensor.flatten(Tensor([2, 19014179, 10, 3, 2],"float32"), -2, -1, )
2025-03-15 19:02:42.462998 test begin: paddle.Tensor.flatten(Tensor([2, 27163112, 7, 3, 2],"float32"), -2, -1, )

[Pass] paddle.Tensor.flatten(Tensor([2, 27163112, 7, 3, 2],"float32"), -2, -1, )
2025-03-15 19:05:45.964288 test begin: paddle.Tensor.flatten(Tensor([2, 297097, 15, 256],"float32"), 1, 2, )

[Pass] paddle.Tensor.flatten(Tensor([2, 297097, 15, 256],"float32"), 1, 2, )
2025-03-15 19:08:49.691604 test begin: paddle.Tensor.flatten(Tensor([2, 3, 15845149, 6, 4],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([2, 3, 15845149, 6, 4],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265840], shape[0] = -2013265840.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265840 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 19:09:15.063837 test begin: paddle.Tensor.flatten(Tensor([2, 3, 6, 15845149, 4],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([2, 3, 6, 15845149, 4],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265840], shape[0] = -2013265840.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265840 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 19:09:36.329522 test begin: paddle.Tensor.flatten(Tensor([2, 3, 6, 6, 10563433],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([2, 3, 6, 6, 10563433],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265768], shape[0] = -2013265768.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265768 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 19:10:05.440198 test begin: paddle.Tensor.flatten(Tensor([2, 300, 136, 27963],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([2, 300, 136, 27963],"float32"), -2, )
2025-03-15 19:13:25.983592 test begin: paddle.Tensor.flatten(Tensor([2, 300, 27963, 136],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([2, 300, 27963, 136],"float32"), -2, )
2025-03-15 19:17:03.120639 test begin: paddle.Tensor.flatten(Tensor([2, 38, 117275, 256],"float32"), 1, 2, )

[Pass] paddle.Tensor.flatten(Tensor([2, 38, 117275, 256],"float32"), 1, 2, )
2025-03-15 19:20:09.537439 test begin: paddle.Tensor.flatten(Tensor([2, 38, 58, 517628],"float32"), 1, 2, )

[Pass] paddle.Tensor.flatten(Tensor([2, 38, 58, 517628],"float32"), 1, 2, )
2025-03-15 19:23:08.390360 test begin: paddle.Tensor.flatten(Tensor([2, 38752, 115, 256],"float32"), 1, 2, )

[Pass] paddle.Tensor.flatten(Tensor([2, 38752, 115, 256],"float32"), 1, 2, )
2025-03-15 19:25:59.429033 test begin: paddle.Tensor.flatten(Tensor([2, 6, 10, 3, 6338060],"float32"), -2, -1, )

[Pass] paddle.Tensor.flatten(Tensor([2, 6, 10, 3, 6338060],"float32"), -2, -1, )
2025-03-15 19:29:01.382130 test begin: paddle.Tensor.flatten(Tensor([2, 6, 10, 9507090, 2],"float32"), -2, -1, )

[Pass] paddle.Tensor.flatten(Tensor([2, 6, 10, 9507090, 2],"float32"), -2, -1, )
2025-03-15 19:31:49.711676 test begin: paddle.Tensor.flatten(Tensor([2, 6, 3, 3, 21126865],"float32"), -2, -1, )

[Pass] paddle.Tensor.flatten(Tensor([2, 6, 3, 3, 21126865],"float32"), -2, -1, )
2025-03-15 19:35:27.266135 test begin: paddle.Tensor.flatten(Tensor([2, 6, 3, 31690297, 2],"float32"), -2, -1, )

[Pass] paddle.Tensor.flatten(Tensor([2, 6, 3, 31690297, 2],"float32"), -2, -1, )
2025-03-15 19:39:44.042790 test begin: paddle.Tensor.flatten(Tensor([2, 6, 31690297, 3, 2],"float32"), -2, -1, )

[Pass] paddle.Tensor.flatten(Tensor([2, 6, 31690297, 3, 2],"float32"), -2, -1, )
2025-03-15 19:43:19.732700 test begin: paddle.Tensor.flatten(Tensor([2, 6, 7, 13581556, 2],"float32"), -2, -1, )

[Pass] paddle.Tensor.flatten(Tensor([2, 6, 7, 13581556, 2],"float32"), -2, -1, )
2025-03-15 19:47:39.831249 test begin: paddle.Tensor.flatten(Tensor([2, 6, 7, 3, 9054371],"float32"), -2, -1, )

[Pass] paddle.Tensor.flatten(Tensor([2, 6, 7, 3, 9054371],"float32"), -2, -1, )
2025-03-15 19:50:54.091186 test begin: paddle.Tensor.flatten(Tensor([2, 61681, 136, 136],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([2, 61681, 136, 136],"float32"), -2, )
2025-03-15 19:53:50.613185 test begin: paddle.Tensor.flatten(Tensor([2, 63380594, 3, 3, 2],"float32"), -2, -1, )

[Pass] paddle.Tensor.flatten(Tensor([2, 63380594, 3, 3, 2],"float32"), -2, -1, )
2025-03-15 19:57:53.805668 test begin: paddle.Tensor.flatten(Tensor([2, 71303169, 16],"int64"), start_axis=1, )

[Pass] paddle.Tensor.flatten(Tensor([2, 71303169, 16],"int64"), start_axis=1, )
2025-03-15 20:01:25.746751 test begin: paddle.Tensor.flatten(Tensor([2, 76, 115, 130533],"float32"), 1, 2, )

[Pass] paddle.Tensor.flatten(Tensor([2, 76, 115, 130533],"float32"), 1, 2, )
2025-03-15 20:04:41.915409 test begin: paddle.Tensor.flatten(Tensor([2, 76, 58638, 256],"float32"), 1, 2, )

[Pass] paddle.Tensor.flatten(Tensor([2, 76, 58638, 256],"float32"), 1, 2, )
2025-03-15 20:08:23.716854 test begin: paddle.Tensor.flatten(Tensor([2, 76836, 58, 256],"float32"), 1, 2, )

[Pass] paddle.Tensor.flatten(Tensor([2, 76836, 58, 256],"float32"), 1, 2, )
2025-03-15 20:13:02.545696 test begin: paddle.Tensor.flatten(Tensor([2, 7922575, 6, 6, 4],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([2, 7922575, 6, 6, 4],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265696], shape[0] = -2013265696.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265696 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 20:13:29.023764 test begin: paddle.Tensor.flatten(Tensor([200, 11408507],"float32"), 1, )

[Pass] paddle.Tensor.flatten(Tensor([200, 11408507],"float32"), 1, )
2025-03-15 20:16:45.962867 test begin: paddle.Tensor.flatten(Tensor([202566, 22, 4, 64, 2],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([202566, 22, 4, 64, 2],"float32"), 2, )
2025-03-15 20:20:46.497374 test begin: paddle.Tensor.flatten(Tensor([20632, 24, 24, 192],"float32"), start_axis=1, stop_axis=2, )

[Pass] paddle.Tensor.flatten(Tensor([20632, 24, 24, 192],"float32"), start_axis=1, stop_axis=2, )
2025-03-15 20:24:25.127803 test begin: paddle.Tensor.flatten(Tensor([207, 300, 192, 192],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([207, 300, 192, 192],"float32"), -2, )
2025-03-15 20:28:36.195830 test begin: paddle.Tensor.flatten(Tensor([21096, 10, 104, 104],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([21096, 10, 104, 104],"float32"), 2, )
2025-03-15 20:32:00.872552 test begin: paddle.Tensor.flatten(Tensor([21126865, 12, 9],"float32"), 1, )

[Pass] paddle.Tensor.flatten(Tensor([21126865, 12, 9],"float32"), 1, )
2025-03-15 20:35:37.332769 test begin: paddle.Tensor.flatten(Tensor([21126865, 6, 3, 3, 2],"float32"), -2, -1, )

[Pass] paddle.Tensor.flatten(Tensor([21126865, 6, 3, 3, 2],"float32"), -2, -1, )
2025-03-15 20:39:15.794256 test begin: paddle.Tensor.flatten(Tensor([212212, 21, 4, 64, 2],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([212212, 21, 4, 64, 2],"float32"), 2, )
2025-03-15 20:42:52.116495 test begin: paddle.Tensor.flatten(Tensor([220753, 4, 38, 68],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([220753, 4, 38, 68],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013264288], shape[0] = -2013264288.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013264288 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 20:43:14.661487 test begin: paddle.Tensor.flatten(Tensor([22152441, 103, 1],"int32"), )

[paddle error] paddle.Tensor.flatten(Tensor([22152441, 103, 1],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265873], shape[0] = -2013265873.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265873 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 20:43:27.986338 test begin: paddle.Tensor.flatten(Tensor([221848, 1, 10285],"float32"), stop_axis=-2, )

[Pass] paddle.Tensor.flatten(Tensor([221848, 1, 10285],"float32"), stop_axis=-2, )
2025-03-15 20:46:45.377356 test begin: paddle.Tensor.flatten(Tensor([2228225, 1, 16, 64],"float32"), start_axis=2, )

[Pass] paddle.Tensor.flatten(Tensor([2228225, 1, 16, 64],"float32"), start_axis=2, )
2025-03-15 20:50:16.648399 test begin: paddle.Tensor.flatten(Tensor([2228225, 2, 8, 4, 16],"float32"), start_axis=2, stop_axis=3, )

[Pass] paddle.Tensor.flatten(Tensor([2228225, 2, 8, 4, 16],"float32"), start_axis=2, stop_axis=3, )
2025-03-15 20:53:28.264923 test begin: paddle.Tensor.flatten(Tensor([222823, 10, 8, 64, 2],"float32"), 3, )

[Pass] paddle.Tensor.flatten(Tensor([222823, 10, 8, 64, 2],"float32"), 3, )
2025-03-15 20:56:38.066936 test begin: paddle.Tensor.flatten(Tensor([22369622, 102, 1],"int32"), )

[paddle error] paddle.Tensor.flatten(Tensor([22369622, 102, 1],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265852], shape[0] = -2013265852.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265852 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 20:56:51.415094 test begin: paddle.Tensor.flatten(Tensor([225, 300, 184, 184],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([225, 300, 184, 184],"float32"), -2, )
2025-03-15 21:00:14.709173 test begin: paddle.Tensor.flatten(Tensor([22591103, 101, 1],"int32"), )

[paddle error] paddle.Tensor.flatten(Tensor([22591103, 101, 1],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265893], shape[0] = -2013265893.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265893 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 21:00:27.672775 test begin: paddle.Tensor.flatten(Tensor([2277148, 1002],"int32"), )

[paddle error] paddle.Tensor.flatten(Tensor([2277148, 1002],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265000], shape[0] = -2013265000.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265000 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 21:00:38.703459 test begin: paddle.Tensor.flatten(Tensor([2277148, 1002],"int64"), )

[paddle error] paddle.Tensor.flatten(Tensor([2277148, 1002],"int64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265000], shape[0] = -2013265000.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265000 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 21:01:04.659140 test begin: paddle.Tensor.flatten(Tensor([2279422, 1001],"int32"), )

[paddle error] paddle.Tensor.flatten(Tensor([2279422, 1001],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265874], shape[0] = -2013265874.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265874 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 21:01:17.524541 test begin: paddle.Tensor.flatten(Tensor([2279422, 1001],"int64"), )

[paddle error] paddle.Tensor.flatten(Tensor([2279422, 1001],"int64"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265874], shape[0] = -2013265874.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265874 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 21:01:38.533370 test begin: paddle.Tensor.flatten(Tensor([2281701379, 1, 1],"int32"), )

[paddle error] paddle.Tensor.flatten(Tensor([2281701379, 1, 1],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265917], shape[0] = -2013265917.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265917 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 21:01:49.091778 test begin: paddle.Tensor.flatten(Tensor([2281701379],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([2281701379],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265917], shape[0] = -2013265917.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265917 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 21:02:12.612635 test begin: paddle.Tensor.flatten(Tensor([2281701379],"int32"), )

[paddle error] paddle.Tensor.flatten(Tensor([2281701379],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265917], shape[0] = -2013265917.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265917 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 21:02:29.333978 test begin: paddle.Tensor.flatten(Tensor([228170138, 10, 1],"int32"), )

[paddle error] paddle.Tensor.flatten(Tensor([228170138, 10, 1],"int32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265916], shape[0] = -2013265916.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265916 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 21:02:40.494855 test begin: paddle.Tensor.flatten(Tensor([228170138, 10],"float32"), 0, 1, )

[paddle error] paddle.Tensor.flatten(Tensor([228170138, 10],"float32"), 0, 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265916], shape[0] = -2013265916.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265916 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 21:03:06.651680 test begin: paddle.Tensor.flatten(Tensor([22817014, 100],"float32"), 0, 1, )

[paddle error] paddle.Tensor.flatten(Tensor([22817014, 100],"float32"), 0, 1, ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265896], shape[0] = -2013265896.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265896 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 21:03:26.827800 test begin: paddle.Tensor.flatten(Tensor([22818, 10, 100, 100],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([22818, 10, 100, 100],"float32"), 2, )
2025-03-15 21:07:03.518183 test begin: paddle.Tensor.flatten(Tensor([23, 33, 1, 3006195],"float32"), 1, )

[Pass] paddle.Tensor.flatten(Tensor([23, 33, 1, 3006195],"float32"), 1, )
2025-03-15 21:10:29.448325 test begin: paddle.Tensor.flatten(Tensor([23, 33, 3006195, 1],"float32"), 1, )

[Pass] paddle.Tensor.flatten(Tensor([23, 33, 3006195, 1],"float32"), 1, )
2025-03-15 21:13:48.476563 test begin: paddle.Tensor.flatten(Tensor([23, 99204408, 1, 1],"float32"), 1, )

[Pass] paddle.Tensor.flatten(Tensor([23, 99204408, 1, 1],"float32"), 1, )
2025-03-15 21:16:29.746427 test begin: paddle.Tensor.flatten(Tensor([23767723, 6, 4, 4],"float32"), start_axis=1, stop_axis=3, )

[Pass] paddle.Tensor.flatten(Tensor([23767723, 6, 4, 4],"float32"), start_axis=1, stop_axis=3, )
2025-03-15 21:20:00.859818 test begin: paddle.Tensor.flatten(Tensor([246, 300, 176, 176],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([246, 300, 176, 176],"float32"), -2, )
2025-03-15 21:23:36.937384 test begin: paddle.Tensor.flatten(Tensor([247581, 18, 4, 64, 2],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([247581, 18, 4, 64, 2],"float32"), 2, )
2025-03-15 21:26:27.049431 test begin: paddle.Tensor.flatten(Tensor([2546542, 7, 64, 2],"float32"), -2, -1, )

[Pass] paddle.Tensor.flatten(Tensor([2546542, 7, 64, 2],"float32"), -2, -1, )
2025-03-15 21:30:31.245333 test begin: paddle.Tensor.flatten(Tensor([26, 1567103, 7, 4, 2],"float32"), -2, -1, )

[Pass] paddle.Tensor.flatten(Tensor([26, 1567103, 7, 4, 2],"float32"), -2, -1, )
2025-03-15 21:34:02.995367 test begin: paddle.Tensor.flatten(Tensor([26, 4, 2742430, 4, 2],"float32"), -2, -1, )

[Pass] paddle.Tensor.flatten(Tensor([26, 4, 2742430, 4, 2],"float32"), -2, -1, )
2025-03-15 21:37:25.716351 test begin: paddle.Tensor.flatten(Tensor([26, 4, 7, 1567103, 2],"float32"), -2, -1, )

[Pass] paddle.Tensor.flatten(Tensor([26, 4, 7, 1567103, 2],"float32"), -2, -1, )
2025-03-15 21:40:40.943614 test begin: paddle.Tensor.flatten(Tensor([26, 4, 7, 4, 783552],"float32"), -2, -1, )

[Pass] paddle.Tensor.flatten(Tensor([26, 4, 7, 4, 783552],"float32"), -2, -1, )
2025-03-15 21:43:58.402448 test begin: paddle.Tensor.flatten(Tensor([285212673, 1, 2, 2, 2],"float32"), 3, )

[Pass] paddle.Tensor.flatten(Tensor([285212673, 1, 2, 2, 2],"float32"), 3, )
2025-03-15 21:47:12.381372 test begin: paddle.Tensor.flatten(Tensor([28521268, 10, 8],"float32"), 1, )

[Pass] paddle.Tensor.flatten(Tensor([28521268, 10, 8],"float32"), 1, )
2025-03-15 21:51:17.740310 test begin: paddle.Tensor.flatten(Tensor([2970966, 1, 12, 64],"float32"), start_axis=2, )

[Pass] paddle.Tensor.flatten(Tensor([2970966, 1, 12, 64],"float32"), start_axis=2, )
2025-03-15 21:54:15.445259 test begin: paddle.Tensor.flatten(Tensor([32, 33, 1, 2160703],"float32"), 1, )

[Pass] paddle.Tensor.flatten(Tensor([32, 33, 1, 2160703],"float32"), 1, )
2025-03-15 21:58:04.762604 test begin: paddle.Tensor.flatten(Tensor([32, 33, 2160703, 1],"float32"), 1, )

[Pass] paddle.Tensor.flatten(Tensor([32, 33, 2160703, 1],"float32"), 1, )
2025-03-15 22:01:16.310125 test begin: paddle.Tensor.flatten(Tensor([32, 71303169, 1, 1],"float32"), 1, )

[Pass] paddle.Tensor.flatten(Tensor([32, 71303169, 1, 1],"float32"), 1, )
2025-03-15 22:05:08.220819 test begin: paddle.Tensor.flatten(Tensor([3241054, 8, 11, 4, 2],"float32"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([3241054, 8, 11, 4, 2],"float32"), 0, 1, )
2025-03-15 22:08:55.404036 test begin: paddle.Tensor.flatten(Tensor([32411, 8, 1100, 4, 2],"float32"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([32411, 8, 1100, 4, 2],"float32"), 0, 1, )
2025-03-15 22:12:13.601542 test begin: paddle.Tensor.flatten(Tensor([3532046, 19, 34, 1],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([3532046, 19, 34, 1],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265580], shape[0] = -2013265580.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265580 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-15 22:12:35.530821 test begin: paddle.Tensor.flatten(Tensor([3802836, 300, 2],"float32"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([3802836, 300, 2],"float32"), 0, 1, )
2025-03-15 22:15:49.572176 test begin: paddle.Tensor.flatten(Tensor([4, 217, 25, 376, 280],"float32"), start_axis=1, stop_axis=2, )

[Pass] paddle.Tensor.flatten(Tensor([4, 217, 25, 376, 280],"float32"), start_axis=1, stop_axis=2, )
2025-03-15 22:19:16.309457 test begin: paddle.Tensor.flatten(Tensor([4, 25, 29, 786794],"float32"), 1, 2, )

[Pass] paddle.Tensor.flatten(Tensor([4, 25, 29, 786794],"float32"), 1, 2, )
2025-03-15 22:23:08.172640 test begin: paddle.Tensor.flatten(Tensor([4, 25, 89129, 256],"float32"), 1, 2, )

[Pass] paddle.Tensor.flatten(Tensor([4, 25, 89129, 256],"float32"), 1, 2, )
2025-03-15 22:26:57.971665 test begin: paddle.Tensor.flatten(Tensor([4, 33, 1, 17285617],"float32"), 1, )

[Pass] paddle.Tensor.flatten(Tensor([4, 33, 1, 17285617],"float32"), 1, )
2025-03-15 22:30:34.002653 test begin: paddle.Tensor.flatten(Tensor([4, 33, 17285617, 1],"float32"), 1, )

[Pass] paddle.Tensor.flatten(Tensor([4, 33, 17285617, 1],"float32"), 1, )
2025-03-15 22:34:53.148370 test begin: paddle.Tensor.flatten(Tensor([4, 570425345, 1, 1],"float32"), 1, )

[Pass] paddle.Tensor.flatten(Tensor([4, 570425345, 1, 1],"float32"), 1, )
2025-03-15 22:39:15.004795 test begin: paddle.Tensor.flatten(Tensor([4, 64, 25, 1274, 280],"float32"), start_axis=1, stop_axis=2, )

[Pass] paddle.Tensor.flatten(Tensor([4, 64, 25, 1274, 280],"float32"), start_axis=1, stop_axis=2, )
2025-03-15 22:42:55.897838 test begin: paddle.Tensor.flatten(Tensor([4, 64, 25, 376, 949],"float32"), start_axis=1, stop_axis=2, )

[Pass] paddle.Tensor.flatten(Tensor([4, 64, 25, 376, 949],"float32"), start_axis=1, stop_axis=2, )
2025-03-15 22:46:07.345357 test begin: paddle.Tensor.flatten(Tensor([4, 64, 85, 376, 280],"float32"), start_axis=1, stop_axis=2, )

[Pass] paddle.Tensor.flatten(Tensor([4, 64, 85, 376, 280],"float32"), start_axis=1, stop_axis=2, )
2025-03-15 22:49:44.037487 test begin: paddle.Tensor.flatten(Tensor([4, 76836, 29, 256],"float32"), 1, 2, )

[Pass] paddle.Tensor.flatten(Tensor([4, 76836, 29, 256],"float32"), 1, 2, )
2025-03-15 22:53:14.771441 test begin: paddle.Tensor.flatten(Tensor([4044, 38, 58, 256],"float32"), 1, 2, )

[Pass] paddle.Tensor.flatten(Tensor([4044, 38, 58, 256],"float32"), 1, 2, )
2025-03-15 22:57:07.531255 test begin: paddle.Tensor.flatten(Tensor([405132, 11, 4, 64, 2],"float32"), 2, )

[Pass] paddle.Tensor.flatten(Tensor([405132, 11, 4, 64, 2],"float32"), 2, )
2025-03-15 22:59:51.181823 test begin: paddle.Tensor.flatten(Tensor([412, 300, 136, 136],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([412, 300, 136, 136],"float32"), -2, )
2025-03-15 23:02:54.331415 test begin: paddle.Tensor.flatten(Tensor([4294967297],"bfloat16"), )

[paddle error] paddle.Tensor.flatten(Tensor([4294967297],"bfloat16"), ) 
 (InvalidArgument) The 'shape' in ReshapeOp is invalid. The input tensor X'size must be equal to the capacity of 'shape'. But received X's shape = [4294967297], X's size = 4294967297, 'shape' is [1], the capacity of 'shape' is 1.
  [Hint: Expected capacity == in_size, but received capacity:1 != in_size:4294967297.] (at ../paddle/phi/infermeta/unary.cc:2258)

2025-03-15 23:03:33.533254 test begin: paddle.Tensor.flatten(Tensor([4294968, 1000, 1, 1],"float16"), 1, )

[Pass] paddle.Tensor.flatten(Tensor([4294968, 1000, 1, 1],"float16"), 1, )
2025-03-15 23:20:29.930878 test begin: paddle.Tensor.flatten(Tensor([4456449, 1, 8, 64],"float32"), start_axis=2, )

[Pass] paddle.Tensor.flatten(Tensor([4456449, 1, 8, 64],"float32"), start_axis=2, )
2025-03-15 23:24:18.005298 test begin: paddle.Tensor.flatten(Tensor([44739243, 6, 4, 4],"float16"), start_axis=1, stop_axis=3, )

[Pass] paddle.Tensor.flatten(Tensor([44739243, 6, 4, 4],"float16"), start_axis=1, stop_axis=3, )
2025-03-15 23:41:32.664916 test begin: paddle.Tensor.flatten(Tensor([47535446, 3, 4, 2, 2],"float32"), -2, )

[Pass] paddle.Tensor.flatten(Tensor([47535446, 3, 4, 2, 2],"float32"), -2, )
2025-03-15 23:45:45.264433 test begin: paddle.Tensor.flatten(Tensor([5070448, 18, 25],"float32"), 1, )

[Pass] paddle.Tensor.flatten(Tensor([5070448, 18, 25],"float32"), 1, )
2025-03-15 23:49:32.132546 test begin: paddle.Tensor.flatten(Tensor([52, 342804, 64, 2],"float32"), -2, -1, )

[Pass] paddle.Tensor.flatten(Tensor([52, 342804, 64, 2],"float32"), -2, -1, )
2025-03-15 23:54:11.062115 test begin: paddle.Tensor.flatten(Tensor([52, 7, 3134206, 2],"float32"), -2, -1, )

[Pass] paddle.Tensor.flatten(Tensor([52, 7, 3134206, 2],"float32"), -2, -1, )
2025-03-15 23:57:35.319778 test begin: paddle.Tensor.flatten(Tensor([52, 7, 64, 97944],"float32"), -2, -1, )

[Pass] paddle.Tensor.flatten(Tensor([52, 7, 64, 97944],"float32"), -2, -1, )
2025-03-16 00:01:40.547625 test begin: paddle.Tensor.flatten(Tensor([5281717, 3, 6, 6, 4],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([5281717, 3, 6, 6, 4],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013265552], shape[0] = -2013265552.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013265552 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-16 00:02:04.083583 test begin: paddle.Tensor.flatten(Tensor([55189, 4, 76, 136],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([55189, 4, 76, 136],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013233280], shape[0] = -2013233280.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013233280 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-16 00:02:29.118184 test begin: paddle.Tensor.flatten(Tensor([59417, 1, 38402],"float32"), stop_axis=-2, )

[Pass] paddle.Tensor.flatten(Tensor([59417, 1, 38402],"float32"), stop_axis=-2, )
2025-03-16 00:05:26.735004 test begin: paddle.Tensor.flatten(Tensor([5941931, 1, 6, 64],"float32"), start_axis=2, )

[Pass] paddle.Tensor.flatten(Tensor([5941931, 1, 6, 64],"float32"), start_axis=2, )
2025-03-16 00:09:29.371182 test begin: paddle.Tensor.flatten(Tensor([59420, 10, 15, 256],"float32"), 1, 2, )

[Pass] paddle.Tensor.flatten(Tensor([59420, 10, 15, 256],"float32"), 1, 2, )
2025-03-16 00:12:28.878328 test begin: paddle.Tensor.flatten(Tensor([607, 300, 12544],"float32"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([607, 300, 12544],"float32"), 0, 1, )
2025-03-16 00:15:56.017290 test begin: paddle.Tensor.flatten(Tensor([619, 144, 200, 64, 2],"float32"), 3, )

[Pass] paddle.Tensor.flatten(Tensor([619, 144, 200, 64, 2],"float32"), 3, )
2025-03-16 00:19:16.739274 test begin: paddle.Tensor.flatten(Tensor([61906, 1, 36858],"float32"), stop_axis=-2, )

[Pass] paddle.Tensor.flatten(Tensor([61906, 1, 36858],"float32"), stop_axis=-2, )
2025-03-16 00:23:00.171980 test begin: paddle.Tensor.flatten(Tensor([63380594, 1, 36],"int64"), stop_axis=-2, )

[Pass] paddle.Tensor.flatten(Tensor([63380594, 1, 36],"int64"), stop_axis=-2, )
2025-03-16 00:26:00.514579 test begin: paddle.Tensor.flatten(Tensor([6338060, 6, 10, 3, 2],"float32"), -2, -1, )

[Pass] paddle.Tensor.flatten(Tensor([6338060, 6, 10, 3, 2],"float32"), -2, -1, )
2025-03-16 00:28:40.474882 test begin: paddle.Tensor.flatten(Tensor([634159, 1, 3598],"float32"), stop_axis=-2, )

[Pass] paddle.Tensor.flatten(Tensor([634159, 1, 3598],"float32"), stop_axis=-2, )
2025-03-16 00:32:13.030649 test begin: paddle.Tensor.flatten(Tensor([64, 1000, 1, 67109],"float16"), 1, )

[Pass] paddle.Tensor.flatten(Tensor([64, 1000, 1, 67109],"float16"), 1, )
2025-03-16 00:49:22.493640 test begin: paddle.Tensor.flatten(Tensor([64, 1000, 67109, 1],"float16"), 1, )

[Pass] paddle.Tensor.flatten(Tensor([64, 1000, 67109, 1],"float16"), 1, )
2025-03-16 01:06:07.418771 test begin: paddle.Tensor.flatten(Tensor([64, 67108865, 1, 1],"float16"), 1, )

[Pass] paddle.Tensor.flatten(Tensor([64, 67108865, 1, 1],"float16"), 1, )
2025-03-16 01:22:30.468964 test begin: paddle.Tensor.flatten(Tensor([69142467, 33, 1, 1],"float32"), 1, )

[Pass] paddle.Tensor.flatten(Tensor([69142467, 33, 1, 1],"float32"), 1, )
2025-03-16 01:25:12.368703 test begin: paddle.Tensor.flatten(Tensor([691425, 300, 11],"float32"), 0, 1, )

[Pass] paddle.Tensor.flatten(Tensor([691425, 300, 11],"float32"), 0, 1, )
2025-03-16 01:28:17.790385 test begin: paddle.Tensor.flatten(Tensor([79226, 144, 200],"float32"), 1, )

[Pass] paddle.Tensor.flatten(Tensor([79226, 144, 200],"float32"), 1, )
2025-03-16 01:30:58.974955 test begin: paddle.Tensor.flatten(Tensor([82527, 192, 144],"float32"), 1, )

[Pass] paddle.Tensor.flatten(Tensor([82527, 192, 144],"float32"), 1, )
2025-03-16 01:34:06.174404 test begin: paddle.Tensor.flatten(Tensor([883012, 38, 68, 1],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([883012, 38, 68, 1],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013264288], shape[0] = -2013264288.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013264288 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-16 01:34:29.679454 test begin: paddle.Tensor.flatten(Tensor([883012, 4, 19, 34],"float32"), )

[paddle error] paddle.Tensor.flatten(Tensor([883012, 4, 19, 34],"float32"), ) 
 (InvalidArgument) Each dimension value of 'shape' in ReshapeOp must not be negative except one unknown dimension. But received  shape = [-2013264288], shape[0] = -2013264288.
  [Hint: Expected shape[i] > 0, but received shape[i]:-2013264288 <= 0:0.] (at ../paddle/phi/infermeta/unary.cc:2199)

2025-03-16 01:34:53.956195 test begin: paddle.Tensor.flatten(Tensor([8912897, 16, 16],"int64"), start_axis=1, )

[Pass] paddle.Tensor.flatten(Tensor([8912897, 16, 16],"int64"), start_axis=1, )
2025-03-16 01:37:55.965291 test begin: paddle.Tensor.flatten(Tensor([9054371, 6, 7, 3, 2],"float32"), -2, -1, )

[Pass] paddle.Tensor.flatten(Tensor([9054371, 6, 7, 3, 2],"float32"), -2, -1, )
2025-03-16 01:40:32.581687 test begin: paddle.Tensor.flip(Tensor([1140850690, 2],"int64"), list[1,], )

[Pass] paddle.Tensor.flip(Tensor([1140850690, 2],"int64"), list[1,], )
2025-03-16 01:43:46.976897 test begin: paddle.Tensor.flip(Tensor([13, 175515491],"int32"), list[-1,], )

[Pass] paddle.Tensor.flip(Tensor([13, 175515491],"int32"), list[-1,], )
2025-03-16 01:45:37.942939 test begin: paddle.Tensor.flip(Tensor([162978670, 14],"int32"), list[-1,], )

[Pass] paddle.Tensor.flip(Tensor([162978670, 14],"int32"), list[-1,], )
2025-03-16 01:47:41.926901 test begin: paddle.Tensor.flip(Tensor([19015, 400, 300],"float32"), axis=list[-1,], )

[Pass] paddle.Tensor.flip(Tensor([19015, 400, 300],"float32"), axis=list[-1,], )
2025-03-16 01:50:27.258902 test begin: paddle.Tensor.flip(Tensor([19015, 400, 300],"float32"), axis=list[-2,], )

[Pass] paddle.Tensor.flip(Tensor([19015, 400, 300],"float32"), axis=list[-2,], )
2025-03-16 01:53:12.164282 test begin: paddle.Tensor.flip(Tensor([23283, 280, 350],"float32"), axis=list[-1,], )

[Pass] paddle.Tensor.flip(Tensor([23283, 280, 350],"float32"), axis=list[-1,], )
2025-03-16 01:56:33.670227 test begin: paddle.Tensor.flip(Tensor([23283, 280, 350],"float32"), axis=list[-2,], )

[Pass] paddle.Tensor.flip(Tensor([23283, 280, 350],"float32"), axis=list[-2,], )
2025-03-16 01:59:47.819073 test begin: paddle.Tensor.flip(Tensor([3, 11883862, 64],"float32"), axis=list[-1,], )

[Pass] paddle.Tensor.flip(Tensor([3, 11883862, 64],"float32"), axis=list[-1,], )
2025-03-16 02:02:45.550188 test begin: paddle.Tensor.flip(Tensor([3, 11883862, 64],"float32"), axis=list[-2,], )

[Pass] paddle.Tensor.flip(Tensor([3, 11883862, 64],"float32"), axis=list[-2,], )
2025-03-16 02:05:26.959593 test begin: paddle.Tensor.flip(Tensor([3, 2173049, 350],"float32"), axis=list[-1,], )

[Pass] paddle.Tensor.flip(Tensor([3, 2173049, 350],"float32"), axis=list[-1,], )
2025-03-16 02:08:28.010160 test begin: paddle.Tensor.flip(Tensor([3, 2173049, 350],"float32"), axis=list[-2,], )

[Pass] paddle.Tensor.flip(Tensor([3, 2173049, 350],"float32"), axis=list[-2,], )
2025-03-16 02:11:27.497408 test begin: paddle.Tensor.flip(Tensor([3, 224, 3395389],"float32"), axis=list[-1,], )

[Pass] paddle.Tensor.flip(Tensor([3, 224, 3395389],"float32"), axis=list[-1,], )
2025-03-16 02:14:12.170195 test begin: paddle.Tensor.flip(Tensor([3, 2535224, 300],"float32"), axis=list[-1,], )

[Pass] paddle.Tensor.flip(Tensor([3, 2535224, 300],"float32"), axis=list[-1,], )
2025-03-16 02:16:55.344411 test begin: paddle.Tensor.flip(Tensor([3, 2535224, 300],"float32"), axis=list[-2,], )

[Pass] paddle.Tensor.flip(Tensor([3, 2535224, 300],"float32"), axis=list[-2,], )
2025-03-16 02:19:35.845991 test begin: paddle.Tensor.flip(Tensor([3, 280, 2716312],"float32"), axis=list[-1,], )

[Pass] paddle.Tensor.flip(Tensor([3, 280, 2716312],"float32"), axis=list[-1,], )
2025-03-16 02:22:17.488542 test begin: paddle.Tensor.flip(Tensor([3, 280, 2716312],"float32"), axis=list[-2,], )

[Pass] paddle.Tensor.flip(Tensor([3, 280, 2716312],"float32"), axis=list[-2,], )
2025-03-16 02:25:15.435034 test begin: paddle.Tensor.flip(Tensor([3, 3395389, 224],"float32"), axis=list[-1,], )

[Pass] paddle.Tensor.flip(Tensor([3, 3395389, 224],"float32"), axis=list[-1,], )
2025-03-16 02:28:14.538017 test begin: paddle.Tensor.flip(Tensor([3, 400, 1901418],"float32"), axis=list[-1,], )

[Pass] paddle.Tensor.flip(Tensor([3, 400, 1901418],"float32"), axis=list[-1,], )
2025-03-16 02:31:16.496869 test begin: paddle.Tensor.flip(Tensor([3, 400, 1901418],"float32"), axis=list[-2,], )

[Pass] paddle.Tensor.flip(Tensor([3, 400, 1901418],"float32"), axis=list[-2,], )
2025-03-16 02:34:32.575735 test begin: paddle.Tensor.flip(Tensor([3, 64, 11883862],"float32"), axis=list[-1,], )

[Pass] paddle.Tensor.flip(Tensor([3, 64, 11883862],"float32"), axis=list[-1,], )
2025-03-16 02:37:22.208008 test begin: paddle.Tensor.flip(Tensor([3, 64, 11883862],"float32"), axis=list[-2,], )

[Pass] paddle.Tensor.flip(Tensor([3, 64, 11883862],"float32"), axis=list[-2,], )
2025-03-16 02:40:04.753582 test begin: paddle.Tensor.flip(Tensor([4, 570425345],"int64"), list[1,], )

[Pass] paddle.Tensor.flip(Tensor([4, 570425345],"int64"), list[1,], )
2025-03-16 02:43:30.037053 test begin: paddle.Tensor.flip(Tensor([45474, 224, 224],"float32"), axis=list[-1,], )

[Pass] paddle.Tensor.flip(Tensor([45474, 224, 224],"float32"), axis=list[-1,], )
2025-03-16 02:47:00.721238 test begin: paddle.Tensor.flip(Tensor([5, 456340276],"int64"), list[1,], )

[Pass] paddle.Tensor.flip(Tensor([5, 456340276],"int64"), list[1,], )
2025-03-16 02:50:27.292260 test begin: paddle.Tensor.flip(Tensor([557057, 64, 64],"float32"), axis=list[-1,], )

[Pass] paddle.Tensor.flip(Tensor([557057, 64, 64],"float32"), axis=list[-1,], )
2025-03-16 02:53:12.472486 test begin: paddle.Tensor.flip(Tensor([557057, 64, 64],"float32"), axis=list[-2,], )

[Pass] paddle.Tensor.flip(Tensor([557057, 64, 64],"float32"), axis=list[-2,], )
2025-03-16 02:55:55.171756 test begin: paddle.Tensor.gcd(x=Tensor([114085069, 1, 4, 5],"int32"), y=Tensor([2, 1, 5],"int32"), )

2025-03-16 02:55:59.730072 test begin: paddle.Tensor.gcd(x=Tensor([114085069, 4, 5],"int32"), y=Tensor([114085069, 4, 5],"int32"), )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.gcd(x=Tensor([114085069, 4, 5],"int32"), y=Tensor([114085069, 4, 5],"int32"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_abs(_object*, _object*, _object*)
1   abs_ad_func(paddle::Tensor const&)
2   paddle::experimental::abs(paddle::Tensor const&)
3   void phi::AbsKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*)
4   int* phi::DeviceContext::Alloc<int>(phi::TensorBase*, unsigned long, bool) const
5   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 8.500000GB memory on GPU 0, 78.104431GB memory has been allocated and available memory is only 1.080444GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-16 02:56:21.846275 test begin: paddle.Tensor.gcd(x=Tensor([114085069, 4, 5],"int32"), y=Tensor([2, 4, 5],"int32"), )

[torch error] paddle.Tensor.gcd(x=Tensor([114085069, 4, 5],"int32"), y=Tensor([2, 4, 5],"int32"), ) 
 The size of tensor a (114085069) must match the size of tensor b (2) at non-singleton dimension 0
2025-03-16 02:56:24.644943 test begin: paddle.Tensor.gcd(x=Tensor([1],"int32"), y=Tensor([2281701379],"int32"), )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.gcd(x=Tensor([1],"int32"), y=Tensor([2281701379],"int32"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_where(_object*, _object*, _object*)
1   where_ad_func(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&)
2   paddle::experimental::where(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&)
3   void phi::WhereKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*)
4   int* phi::DeviceContext::Alloc<int>(phi::TensorBase*, unsigned long, bool) const
5   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 8.500000GB memory on GPU 0, 73.860291GB memory has been allocated and available memory is only 5.324585GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-16 02:56:41.517255 test begin: paddle.Tensor.gcd(x=Tensor([1],"int64"), y=Tensor([2281701379],"int64"), )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.gcd(x=Tensor([1],"int64"), y=Tensor([2281701379],"int64"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_expand(_object*, _object*, _object*)
1   expand_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>)
2   paddle::experimental::expand(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&)
3   void phi::ExpandKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
4   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
5   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.600525GB memory has been allocated and available memory is only 9.584351GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-16 02:57:15.432684 test begin: paddle.Tensor.gcd(x=Tensor([2, 228170138, 5],"int32"), y=Tensor([2, 228170138, 5],"int32"), )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.gcd(x=Tensor([2, 228170138, 5],"int32"), y=Tensor([2, 228170138, 5],"int32"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   not_equal_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::not_equal(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::NotEqualKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*)
3   void phi::NotEqualRawKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
4   bool* phi::DeviceContext::Alloc<bool>(phi::TensorBase*, unsigned long, bool) const
5   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 2.125000GB memory on GPU 0, 78.106384GB memory has been allocated and available memory is only 1.078491GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-16 02:57:34.299053 test begin: paddle.Tensor.gcd(x=Tensor([2, 228170138, 5],"int32"), y=Tensor([2, 4, 5],"int32"), )

[torch error] paddle.Tensor.gcd(x=Tensor([2, 228170138, 5],"int32"), y=Tensor([2, 4, 5],"int32"), ) 
 The size of tensor a (228170138) must match the size of tensor b (4) at non-singleton dimension 1
2025-03-16 02:57:37.085928 test begin: paddle.Tensor.gcd(x=Tensor([2, 4, 285212673],"int32"), y=Tensor([2, 4, 285212673],"int32"), )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.gcd(x=Tensor([2, 4, 285212673],"int32"), y=Tensor([2, 4, 285212673],"int32"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   not_equal_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::not_equal(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::NotEqualKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*)
3   void phi::NotEqualRawKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
4   bool* phi::DeviceContext::Alloc<bool>(phi::TensorBase*, unsigned long, bool) const
5   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 2.125000GB memory on GPU 0, 78.106384GB memory has been allocated and available memory is only 1.078491GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-16 02:57:57.081539 test begin: paddle.Tensor.gcd(x=Tensor([2, 4, 285212673],"int32"), y=Tensor([2, 4, 5],"int32"), )

[torch error] paddle.Tensor.gcd(x=Tensor([2, 4, 285212673],"int32"), y=Tensor([2, 4, 5],"int32"), ) 
 The size of tensor a (285212673) must match the size of tensor b (5) at non-singleton dimension 2
2025-03-16 02:58:00.334966 test begin: paddle.Tensor.gcd(x=Tensor([2, 4, 5],"int32"), y=Tensor([114085069, 4, 5],"int32"), )

[torch error] paddle.Tensor.gcd(x=Tensor([2, 4, 5],"int32"), y=Tensor([114085069, 4, 5],"int32"), ) 
 The size of tensor a (2) must match the size of tensor b (114085069) at non-singleton dimension 0
2025-03-16 02:58:02.227383 test begin: paddle.Tensor.gcd(x=Tensor([2, 4, 5],"int32"), y=Tensor([2, 228170138, 5],"int32"), )

[torch error] paddle.Tensor.gcd(x=Tensor([2, 4, 5],"int32"), y=Tensor([2, 228170138, 5],"int32"), ) 
 The size of tensor a (4) must match the size of tensor b (228170138) at non-singleton dimension 1
2025-03-16 02:58:04.085421 test begin: paddle.Tensor.gcd(x=Tensor([2, 4, 5],"int32"), y=Tensor([2, 4, 285212673],"int32"), )

[torch error] paddle.Tensor.gcd(x=Tensor([2, 4, 5],"int32"), y=Tensor([2, 4, 285212673],"int32"), ) 
 The size of tensor a (5) must match the size of tensor b (285212673) at non-singleton dimension 2
2025-03-16 02:58:06.601704 test begin: paddle.Tensor.gcd(x=Tensor([2281701379],"int32"), y=Tensor([1],"int32"), )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.gcd(x=Tensor([2281701379],"int32"), y=Tensor([1],"int32"), ) 
 (InvalidArgument) The [0] th of Inputs(X) and Inputs(Y) should be same. But received X's shape is [1], Y's shape is [2281701379]
  [Hint: Expected x_dims[i] == y_dims[i], but received x_dims[i]:1 != y_dims[i]:2281701379.] (at ../paddle/phi/infermeta/multiary.cc:5628)

2025-03-16 02:58:19.110672 test begin: paddle.Tensor.gcd(x=Tensor([2281701379],"int32"), y=Tensor([2281701379],"int32"), )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.gcd(x=Tensor([2281701379],"int32"), y=Tensor([2281701379],"int32"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   not_equal_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::not_equal(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::NotEqualKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*)
3   void phi::NotEqualRawKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
4   bool* phi::DeviceContext::Alloc<bool>(phi::TensorBase*, unsigned long, bool) const
5   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 2.125000GB memory on GPU 0, 78.106384GB memory has been allocated and available memory is only 1.078491GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-16 02:58:36.682738 test begin: paddle.Tensor.gcd(x=Tensor([2281701379],"int64"), y=Tensor([1],"int64"), )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.gcd(x=Tensor([2281701379],"int64"), y=Tensor([1],"int64"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_expand(_object*, _object*, _object*)
1   expand_ad_func(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor>)
2   paddle::experimental::expand(paddle::Tensor const&, paddle::experimental::IntArrayBase<paddle::Tensor> const&)
3   void phi::ExpandKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::IntArrayBase<phi::DenseTensor> const&, phi::DenseTensor*)
4   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
5   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.600525GB memory has been allocated and available memory is only 9.584351GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-16 02:59:08.647889 test begin: paddle.Tensor.gcd(x=Tensor([2281701379],"int64"), y=Tensor([2281701379],"int64"), )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.gcd(x=Tensor([2281701379],"int64"), y=Tensor([2281701379],"int64"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   paddle::memory::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 69.596619GB memory has been allocated and available memory is only 9.588257GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-16 02:59:33.461555 test begin: paddle.Tensor.gcd(x=Tensor([4, 570425345],"int32"), y=Tensor([4, 570425345],"int32"), )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.gcd(x=Tensor([4, 570425345],"int32"), y=Tensor([4, 570425345],"int32"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   not_equal_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::not_equal(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::NotEqualKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*)
3   void phi::NotEqualRawKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
4   bool* phi::DeviceContext::Alloc<bool>(phi::TensorBase*, unsigned long, bool) const
5   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 2.125000GB memory on GPU 0, 78.104431GB memory has been allocated and available memory is only 1.080444GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-16 02:59:53.775498 test begin: paddle.Tensor.gcd(x=Tensor([4, 570425345],"int32"), y=Tensor([4, 5],"int32"), )

[torch error] paddle.Tensor.gcd(x=Tensor([4, 570425345],"int32"), y=Tensor([4, 5],"int32"), ) 
 The size of tensor a (570425345) must match the size of tensor b (5) at non-singleton dimension 1
2025-03-16 02:59:56.736705 test begin: paddle.Tensor.gcd(x=Tensor([4, 5],"int32"), y=Tensor([4, 570425345],"int32"), )

[torch error] paddle.Tensor.gcd(x=Tensor([4, 5],"int32"), y=Tensor([4, 570425345],"int32"), ) 
 The size of tensor a (5) must match the size of tensor b (570425345) at non-singleton dimension 1
2025-03-16 02:59:58.596793 test begin: paddle.Tensor.gcd(x=Tensor([4, 5],"int32"), y=Tensor([456340276, 5],"int32"), )

[torch error] paddle.Tensor.gcd(x=Tensor([4, 5],"int32"), y=Tensor([456340276, 5],"int32"), ) 
 The size of tensor a (4) must match the size of tensor b (456340276) at non-singleton dimension 0
2025-03-16 03:00:00.487405 test begin: paddle.Tensor.gcd(x=Tensor([456340276, 5],"int32"), y=Tensor([4, 5],"int32"), )

[torch error] paddle.Tensor.gcd(x=Tensor([456340276, 5],"int32"), y=Tensor([4, 5],"int32"), ) 
 The size of tensor a (456340276) must match the size of tensor b (4) at non-singleton dimension 0
2025-03-16 03:00:02.343712 test begin: paddle.Tensor.gcd(x=Tensor([456340276, 5],"int32"), y=Tensor([456340276, 5],"int32"), )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.gcd(x=Tensor([456340276, 5],"int32"), y=Tensor([456340276, 5],"int32"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   not_equal_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::not_equal(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::NotEqualKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*)
3   void phi::NotEqualRawKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
4   bool* phi::DeviceContext::Alloc<bool>(phi::TensorBase*, unsigned long, bool) const
5   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 2.125000GB memory on GPU 0, 78.104431GB memory has been allocated and available memory is only 1.080444GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-16 03:00:19.731641 test begin: paddle.Tensor.gcd(x=Tensor([57042535, 2, 4, 5],"int32"), y=Tensor([57042535, 2, 4, 5],"int32"), )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.gcd(x=Tensor([57042535, 2, 4, 5],"int32"), y=Tensor([57042535, 2, 4, 5],"int32"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   not_equal_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::not_equal(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::NotEqualKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*)
3   void phi::NotEqualRawKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
4   bool* phi::DeviceContext::Alloc<bool>(phi::TensorBase*, unsigned long, bool) const
5   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 2.125000GB memory on GPU 0, 78.104431GB memory has been allocated and available memory is only 1.080444GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-16 03:00:39.221781 test begin: paddle.Tensor.gcd(x=Tensor([57042535, 2, 4, 5],"int32"), y=Tensor([6, 2, 4, 5],"int32"), )

[torch error] paddle.Tensor.gcd(x=Tensor([57042535, 2, 4, 5],"int32"), y=Tensor([6, 2, 4, 5],"int32"), ) 
 The size of tensor a (57042535) must match the size of tensor b (6) at non-singleton dimension 0
2025-03-16 03:00:41.791625 test begin: paddle.Tensor.gcd(x=Tensor([6, 1, 4, 5],"int32"), y=Tensor([2, 1, 1140850690],"int32"), )

[torch error] paddle.Tensor.gcd(x=Tensor([6, 1, 4, 5],"int32"), y=Tensor([2, 1, 1140850690],"int32"), ) 
 The size of tensor a (5) must match the size of tensor b (1140850690) at non-singleton dimension 3
2025-03-16 03:00:44.001986 test begin: paddle.Tensor.gcd(x=Tensor([6, 1, 4, 5],"int32"), y=Tensor([2, 228170138, 5],"int32"), )

[torch error] paddle.Tensor.gcd(x=Tensor([6, 1, 4, 5],"int32"), y=Tensor([2, 228170138, 5],"int32"), ) 
 The size of tensor a (4) must match the size of tensor b (228170138) at non-singleton dimension 2
2025-03-16 03:00:46.365286 test begin: paddle.Tensor.gcd(x=Tensor([6, 1, 4, 5],"int32"), y=Tensor([456340276, 1, 5],"int32"), )

[torch error] paddle.Tensor.gcd(x=Tensor([6, 1, 4, 5],"int32"), y=Tensor([456340276, 1, 5],"int32"), ) 
 CUDA out of memory. Tried to allocate 204.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 60.59 GiB is free. Process 109713 has 18.59 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 03:00:48.248065 test begin: paddle.Tensor.gcd(x=Tensor([6, 1, 4, 95070891],"int32"), y=Tensor([2, 1, 5],"int32"), )

[torch error] paddle.Tensor.gcd(x=Tensor([6, 1, 4, 95070891],"int32"), y=Tensor([2, 1, 5],"int32"), ) 
 The size of tensor a (95070891) must match the size of tensor b (5) at non-singleton dimension 3
2025-03-16 03:00:50.103685 test begin: paddle.Tensor.gcd(x=Tensor([6, 1, 76056713, 5],"int32"), y=Tensor([2, 1, 5],"int32"), )

2025-03-16 03:00:52.826054 test begin: paddle.Tensor.gcd(x=Tensor([6, 19014179, 4, 5],"int32"), y=Tensor([2, 1, 5],"int32"), )

[torch error] paddle.Tensor.gcd(x=Tensor([6, 19014179, 4, 5],"int32"), y=Tensor([2, 1, 5],"int32"), ) 
 The size of tensor a (19014179) must match the size of tensor b (2) at non-singleton dimension 1
2025-03-16 03:00:54.722486 test begin: paddle.Tensor.gcd(x=Tensor([6, 19014179, 4, 5],"int32"), y=Tensor([6, 19014179, 4, 5],"int32"), )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.gcd(x=Tensor([6, 19014179, 4, 5],"int32"), y=Tensor([6, 19014179, 4, 5],"int32"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_abs(_object*, _object*, _object*)
1   abs_ad_func(paddle::Tensor const&)
2   paddle::experimental::abs(paddle::Tensor const&)
3   void phi::AbsKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*)
4   int* phi::DeviceContext::Alloc<int>(phi::TensorBase*, unsigned long, bool) const
5   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 8.500000GB memory on GPU 0, 78.104431GB memory has been allocated and available memory is only 1.080444GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-16 03:01:12.288086 test begin: paddle.Tensor.gcd(x=Tensor([6, 19014179, 4, 5],"int32"), y=Tensor([6, 2, 4, 5],"int32"), )

[torch error] paddle.Tensor.gcd(x=Tensor([6, 19014179, 4, 5],"int32"), y=Tensor([6, 2, 4, 5],"int32"), ) 
 The size of tensor a (19014179) must match the size of tensor b (2) at non-singleton dimension 1
2025-03-16 03:01:15.030158 test begin: paddle.Tensor.gcd(x=Tensor([6, 2, 38028357, 5],"int32"), y=Tensor([6, 2, 38028357, 5],"int32"), )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.gcd(x=Tensor([6, 2, 38028357, 5],"int32"), y=Tensor([6, 2, 38028357, 5],"int32"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   not_equal_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::not_equal(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::NotEqualKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*)
3   void phi::NotEqualRawKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
4   bool* phi::DeviceContext::Alloc<bool>(phi::TensorBase*, unsigned long, bool) const
5   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 2.125000GB memory on GPU 0, 78.104431GB memory has been allocated and available memory is only 1.080444GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-16 03:01:32.542708 test begin: paddle.Tensor.gcd(x=Tensor([6, 2, 38028357, 5],"int32"), y=Tensor([6, 2, 4, 5],"int32"), )

[torch error] paddle.Tensor.gcd(x=Tensor([6, 2, 38028357, 5],"int32"), y=Tensor([6, 2, 4, 5],"int32"), ) 
 The size of tensor a (38028357) must match the size of tensor b (4) at non-singleton dimension 2
2025-03-16 03:01:35.120170 test begin: paddle.Tensor.gcd(x=Tensor([6, 2, 4, 47535446],"int32"), y=Tensor([6, 2, 4, 47535446],"int32"), )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.gcd(x=Tensor([6, 2, 4, 47535446],"int32"), y=Tensor([6, 2, 4, 47535446],"int32"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   not_equal_ad_func(paddle::Tensor const&, paddle::Tensor const&)
1   paddle::experimental::not_equal(paddle::Tensor const&, paddle::Tensor const&)
2   void phi::NotEqualKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*)
3   void phi::NotEqualRawKernel<int, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*)
4   bool* phi::DeviceContext::Alloc<bool>(phi::TensorBase*, unsigned long, bool) const
5   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 2.125000GB memory on GPU 0, 78.104431GB memory has been allocated and available memory is only 1.080444GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-16 03:01:52.224382 test begin: paddle.Tensor.gcd(x=Tensor([6, 2, 4, 47535446],"int32"), y=Tensor([6, 2, 4, 5],"int32"), )

[torch error] paddle.Tensor.gcd(x=Tensor([6, 2, 4, 47535446],"int32"), y=Tensor([6, 2, 4, 5],"int32"), ) 
 The size of tensor a (47535446) must match the size of tensor b (5) at non-singleton dimension 3
2025-03-16 03:01:54.809488 test begin: paddle.Tensor.gcd(x=Tensor([6, 2, 4, 5],"int32"), y=Tensor([57042535, 2, 4, 5],"int32"), )

[torch error] paddle.Tensor.gcd(x=Tensor([6, 2, 4, 5],"int32"), y=Tensor([57042535, 2, 4, 5],"int32"), ) 
 The size of tensor a (6) must match the size of tensor b (57042535) at non-singleton dimension 0
2025-03-16 03:01:56.767500 test begin: paddle.Tensor.gcd(x=Tensor([6, 2, 4, 5],"int32"), y=Tensor([6, 19014179, 4, 5],"int32"), )

[torch error] paddle.Tensor.gcd(x=Tensor([6, 2, 4, 5],"int32"), y=Tensor([6, 19014179, 4, 5],"int32"), ) 
 The size of tensor a (2) must match the size of tensor b (19014179) at non-singleton dimension 1
2025-03-16 03:01:59.267957 test begin: paddle.Tensor.gcd(x=Tensor([6, 2, 4, 5],"int32"), y=Tensor([6, 2, 38028357, 5],"int32"), )

[torch error] paddle.Tensor.gcd(x=Tensor([6, 2, 4, 5],"int32"), y=Tensor([6, 2, 38028357, 5],"int32"), ) 
 The size of tensor a (4) must match the size of tensor b (38028357) at non-singleton dimension 2
2025-03-16 03:02:01.147238 test begin: paddle.Tensor.gcd(x=Tensor([6, 2, 4, 5],"int32"), y=Tensor([6, 2, 4, 47535446],"int32"), )

[torch error] paddle.Tensor.gcd(x=Tensor([6, 2, 4, 5],"int32"), y=Tensor([6, 2, 4, 47535446],"int32"), ) 
 The size of tensor a (5) must match the size of tensor b (47535446) at non-singleton dimension 3
2025-03-16 03:02:02.995631 test begin: paddle.Tensor.inner(x=Tensor([1073741825, 4],"float16"), y=Tensor([1073741825, 4],"float16"), )

[torch error] paddle.Tensor.inner(x=Tensor([1073741825, 4],"float16"), y=Tensor([1073741825, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate more than 1EB memory.
2025-03-16 03:02:08.547762 test begin: paddle.Tensor.inner(x=Tensor([1073741825, 4],"float16"), y=Tensor([3, 2, 4],"float16"), )

2025-03-16 03:02:11.828085 test begin: paddle.Tensor.inner(x=Tensor([1073741825, 4],"float16"), y=Tensor([3, 2, 5, 4],"float16"), )

[torch error] paddle.Tensor.inner(x=Tensor([1073741825, 4],"float16"), y=Tensor([3, 2, 5, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 60.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 48.37 GiB is free. Process 109713 has 30.81 GiB memory in use. Of the allocated memory 28.01 GiB is allocated by PyTorch, and 1021.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 03:02:13.639705 test begin: paddle.Tensor.inner(x=Tensor([1073741825, 4],"float16"), y=Tensor([4, 4],"float16"), )

[Pass] paddle.Tensor.inner(x=Tensor([1073741825, 4],"float16"), y=Tensor([4, 4],"float16"), )
2025-03-16 03:19:28.792684 test begin: paddle.Tensor.inner(x=Tensor([1073741825, 4],"float16"), y=Tensor([5, 4],"float16"), )

2025-03-16 03:19:32.704254 test begin: paddle.Tensor.inner(x=Tensor([2, 178956971, 3, 4],"float16"), y=Tensor([3, 178956971, 5, 4],"float16"), )

[torch error] paddle.Tensor.inner(x=Tensor([2, 178956971, 3, 4],"float16"), y=Tensor([3, 178956971, 5, 4],"float16"), ) 
 cannot reshape array of size 4300000000 into shape (3,178956971,5,4)
2025-03-16 03:19:33.953967 test begin: paddle.Tensor.inner(x=Tensor([2, 178956971, 3, 4],"float16"), y=Tensor([3, 2, 5, 4],"float16"), )

[torch error] paddle.Tensor.inner(x=Tensor([2, 178956971, 3, 4],"float16"), y=Tensor([3, 2, 5, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 60.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 50.00 GiB is free. Process 109713 has 29.19 GiB memory in use. Of the allocated memory 26.02 GiB is allocated by PyTorch, and 1013.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 03:19:35.722138 test begin: paddle.Tensor.inner(x=Tensor([2, 5, 107374183, 4],"float16"), y=Tensor([3, 2, 107374183, 4],"float16"), )

[torch error] paddle.Tensor.inner(x=Tensor([2, 5, 107374183, 4],"float16"), y=Tensor([3, 2, 107374183, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate more than 1EB memory.
2025-03-16 03:19:39.461499 test begin: paddle.Tensor.inner(x=Tensor([2, 5, 107374183, 4],"float16"), y=Tensor([3, 2, 5, 4],"float16"), )

[torch error] paddle.Tensor.inner(x=Tensor([2, 5, 107374183, 4],"float16"), y=Tensor([3, 2, 5, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 60.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 50.00 GiB is free. Process 109713 has 29.19 GiB memory in use. Of the allocated memory 26.02 GiB is allocated by PyTorch, and 1013.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 03:19:41.117529 test begin: paddle.Tensor.inner(x=Tensor([2, 5, 3, 143165577],"float16"), y=Tensor([3, 2, 5, 143165577],"float16"), )

[accuracy error] paddle.Tensor.inner(x=Tensor([2, 5, 3, 143165577],"float16"), y=Tensor([3, 2, 5, 143165577],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 92 / 900 (10.2%)
Max absolute difference: 13.
Max relative difference: 1.26
 x: array([[[[[[        inf,  1.9580e+03,  1.1850e+03,  1.0890e+03,
             1.4660e+03],
           [-5.4700e+02,  1.0300e+03,  4.2850e+02,  4.2406e+01,...
 y: array([[[[[[        inf,  1.9570e+03,  1.1840e+03,  1.0840e+03,
             1.4700e+03],
           [-5.4750e+02,  1.0290e+03,  4.2875e+02,  4.1625e+01,...
2025-03-16 03:20:00.169944 test begin: paddle.Tensor.inner(x=Tensor([2, 5, 3, 143165577],"float16"), y=Tensor([3, 2, 5, 4],"float16"), )

[torch error] paddle.Tensor.inner(x=Tensor([2, 5, 3, 143165577],"float16"), y=Tensor([3, 2, 5, 4],"float16"), ) 
 inner() the last dimension must match on both input tensors but got shapes [2, 5, 3, 143165577] and [3, 2, 5, 4]
2025-03-16 03:20:02.439110 test begin: paddle.Tensor.inner(x=Tensor([2, 5, 3, 4],"float16"), y=Tensor([107374183, 2, 5, 4],"float16"), )

[torch error] paddle.Tensor.inner(x=Tensor([2, 5, 3, 4],"float16"), y=Tensor([107374183, 2, 5, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 60.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 60.00 GiB is free. Process 109713 has 19.18 GiB memory in use. Of the allocated memory 8.02 GiB is allocated by PyTorch, and 8.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 03:20:03.974390 test begin: paddle.Tensor.inner(x=Tensor([2, 5, 3, 4],"float16"), y=Tensor([3, 2, 178956971, 4],"float16"), )

[torch error] paddle.Tensor.inner(x=Tensor([2, 5, 3, 4],"float16"), y=Tensor([3, 2, 178956971, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 60.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 60.00 GiB is free. Process 109713 has 19.18 GiB memory in use. Of the allocated memory 8.02 GiB is allocated by PyTorch, and 8.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 03:20:05.272791 test begin: paddle.Tensor.inner(x=Tensor([2, 5, 3, 4],"float16"), y=Tensor([3, 2, 5, 143165577],"float16"), )

[torch error] paddle.Tensor.inner(x=Tensor([2, 5, 3, 4],"float16"), y=Tensor([3, 2, 5, 143165577],"float16"), ) 
 inner() the last dimension must match on both input tensors but got shapes [2, 5, 3, 4] and [3, 2, 5, 143165577]
2025-03-16 03:20:06.753944 test begin: paddle.Tensor.inner(x=Tensor([2, 5, 3, 4],"float16"), y=Tensor([3, 71582789, 5, 4],"float16"), )

[torch error] paddle.Tensor.inner(x=Tensor([2, 5, 3, 4],"float16"), y=Tensor([3, 71582789, 5, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 60.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 60.00 GiB is free. Process 109713 has 19.18 GiB memory in use. Of the allocated memory 8.02 GiB is allocated by PyTorch, and 8.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 03:20:07.731187 test begin: paddle.Tensor.inner(x=Tensor([3, 1431655766],"float16"), y=Tensor([3, 2, 4],"float16"), )

[torch error] paddle.Tensor.inner(x=Tensor([3, 1431655766],"float16"), y=Tensor([3, 2, 4],"float16"), ) 
 inner() the last dimension must match on both input tensors but got shapes [3, 1431655766] and [3, 2, 4]
2025-03-16 03:20:08.975616 test begin: paddle.Tensor.inner(x=Tensor([3, 1431655766],"float16"), y=Tensor([3, 2, 5, 4],"float16"), )

[torch error] paddle.Tensor.inner(x=Tensor([3, 1431655766],"float16"), y=Tensor([3, 2, 5, 4],"float16"), ) 
 inner() the last dimension must match on both input tensors but got shapes [3, 1431655766] and [3, 2, 5, 4]
2025-03-16 03:20:10.726034 test begin: paddle.Tensor.inner(x=Tensor([3, 1431655766],"float16"), y=Tensor([5, 1431655766],"float16"), )

[torch error] paddle.Tensor.inner(x=Tensor([3, 1431655766],"float16"), y=Tensor([5, 1431655766],"float16"), ) 
 cannot reshape array of size 4300000000 into shape (5,1431655766)
2025-03-16 03:20:13.073721 test begin: paddle.Tensor.inner(x=Tensor([3, 1431655766],"float16"), y=Tensor([5, 4],"float16"), )

[torch error] paddle.Tensor.inner(x=Tensor([3, 1431655766],"float16"), y=Tensor([5, 4],"float16"), ) 
 inner() the last dimension must match on both input tensors but got shapes [3, 1431655766] and [5, 4]
2025-03-16 03:20:14.831023 test begin: paddle.Tensor.inner(x=Tensor([3, 4],"float16"), y=Tensor([1073741825, 4],"float16"), )

[Pass] paddle.Tensor.inner(x=Tensor([3, 4],"float16"), y=Tensor([1073741825, 4],"float16"), )
2025-03-16 03:34:45.603460 test begin: paddle.Tensor.inner(x=Tensor([3, 4],"float16"), y=Tensor([107374183, 2, 5, 4],"float16"), )

[Pass] paddle.Tensor.inner(x=Tensor([3, 4],"float16"), y=Tensor([107374183, 2, 5, 4],"float16"), )
2025-03-16 03:49:40.577274 test begin: paddle.Tensor.inner(x=Tensor([3, 4],"float16"), y=Tensor([3, 2, 178956971, 4],"float16"), )

[Pass] paddle.Tensor.inner(x=Tensor([3, 4],"float16"), y=Tensor([3, 2, 178956971, 4],"float16"), )
2025-03-16 04:04:14.716654 test begin: paddle.Tensor.inner(x=Tensor([3, 4],"float16"), y=Tensor([3, 2, 5, 143165577],"float16"), )

[torch error] paddle.Tensor.inner(x=Tensor([3, 4],"float16"), y=Tensor([3, 2, 5, 143165577],"float16"), ) 
 inner() the last dimension must match on both input tensors but got shapes [3, 4] and [3, 2, 5, 143165577]
2025-03-16 04:04:18.896428 test begin: paddle.Tensor.inner(x=Tensor([3, 4],"float16"), y=Tensor([3, 2, 715827883],"float16"), )

[torch error] paddle.Tensor.inner(x=Tensor([3, 4],"float16"), y=Tensor([3, 2, 715827883],"float16"), ) 
 inner() the last dimension must match on both input tensors but got shapes [3, 4] and [3, 2, 715827883]
2025-03-16 04:04:20.416811 test begin: paddle.Tensor.inner(x=Tensor([3, 4],"float16"), y=Tensor([3, 357913942, 4],"float16"), )

[Pass] paddle.Tensor.inner(x=Tensor([3, 4],"float16"), y=Tensor([3, 357913942, 4],"float16"), )
2025-03-16 04:18:58.100828 test begin: paddle.Tensor.inner(x=Tensor([3, 4],"float16"), y=Tensor([3, 71582789, 5, 4],"float16"), )

[Pass] paddle.Tensor.inner(x=Tensor([3, 4],"float16"), y=Tensor([3, 71582789, 5, 4],"float16"), )
2025-03-16 04:33:22.480228 test begin: paddle.Tensor.inner(x=Tensor([3, 4],"float16"), y=Tensor([5, 858993460],"float16"), )

[torch error] paddle.Tensor.inner(x=Tensor([3, 4],"float16"), y=Tensor([5, 858993460],"float16"), ) 
 inner() the last dimension must match on both input tensors but got shapes [3, 4] and [5, 858993460]
2025-03-16 04:33:26.891010 test begin: paddle.Tensor.inner(x=Tensor([3, 4],"float16"), y=Tensor([536870913, 2, 4],"float16"), )

[Pass] paddle.Tensor.inner(x=Tensor([3, 4],"float16"), y=Tensor([536870913, 2, 4],"float16"), )
2025-03-16 04:47:47.014936 test begin: paddle.Tensor.inner(x=Tensor([357913942, 3, 4],"float16"), y=Tensor([2, 5, 4],"float16"), )

2025-03-16 04:47:51.687120 test begin: paddle.Tensor.inner(x=Tensor([357913942, 3, 4],"float16"), y=Tensor([357913942, 5, 4],"float16"), )

[torch error] paddle.Tensor.inner(x=Tensor([357913942, 3, 4],"float16"), y=Tensor([357913942, 5, 4],"float16"), ) 
 cannot reshape array of size 4300000000 into shape (357913942,5,4)
2025-03-16 04:47:53.359810 test begin: paddle.Tensor.inner(x=Tensor([4, 1073741825],"float16"), y=Tensor([4, 1073741825],"float16"), )

[Pass] paddle.Tensor.inner(x=Tensor([4, 1073741825],"float16"), y=Tensor([4, 1073741825],"float16"), )
2025-03-16 05:05:38.729725 test begin: paddle.Tensor.inner(x=Tensor([4, 1073741825],"float16"), y=Tensor([4, 4],"float16"), )

[torch error] paddle.Tensor.inner(x=Tensor([4, 1073741825],"float16"), y=Tensor([4, 4],"float16"), ) 
 inner() the last dimension must match on both input tensors but got shapes [4, 1073741825] and [4, 4]
2025-03-16 05:05:43.215315 test begin: paddle.Tensor.inner(x=Tensor([4, 4],"float16"), y=Tensor([1073741825, 4],"float16"), )

[Pass] paddle.Tensor.inner(x=Tensor([4, 4],"float16"), y=Tensor([1073741825, 4],"float16"), )
2025-03-16 05:22:37.912668 test begin: paddle.Tensor.inner(x=Tensor([4, 4],"float16"), y=Tensor([4, 1073741825],"float16"), )

[torch error] paddle.Tensor.inner(x=Tensor([4, 4],"float16"), y=Tensor([4, 1073741825],"float16"), ) 
 inner() the last dimension must match on both input tensors but got shapes [4, 4] and [4, 1073741825]
2025-03-16 05:22:41.999464 test begin: paddle.Tensor.inner(x=Tensor([4, 4],"float32"), y=Tensor([4, 570425345],"float32"), )

[torch error] paddle.Tensor.inner(x=Tensor([4, 4],"float32"), y=Tensor([4, 570425345],"float32"), ) 
 inner() the last dimension must match on both input tensors but got shapes [4, 4] and [4, 570425345]
2025-03-16 05:22:46.156182 test begin: paddle.Tensor.inner(x=Tensor([4, 4],"float32"), y=Tensor([570425345, 4],"float32"), )

[Pass] paddle.Tensor.inner(x=Tensor([4, 4],"float32"), y=Tensor([570425345, 4],"float32"), )
2025-03-16 05:25:46.810201 test begin: paddle.Tensor.inner(x=Tensor([4, 570425345],"float32"), y=Tensor([4, 4],"float32"), )

[torch error] paddle.Tensor.inner(x=Tensor([4, 570425345],"float32"), y=Tensor([4, 4],"float32"), ) 
 inner() the last dimension must match on both input tensors but got shapes [4, 570425345] and [4, 4]
2025-03-16 05:25:50.821582 test begin: paddle.Tensor.inner(x=Tensor([4, 570425345],"float32"), y=Tensor([4, 570425345],"float32"), )

[Pass] paddle.Tensor.inner(x=Tensor([4, 570425345],"float32"), y=Tensor([4, 570425345],"float32"), )
2025-03-16 05:28:55.765292 test begin: paddle.Tensor.inner(x=Tensor([4294967297],"float16"), y=Tensor([4294967297],"float16"), )

[torch error] paddle.Tensor.inner(x=Tensor([4294967297],"float16"), y=Tensor([4294967297],"float16"), ) 
 at::cuda::blas::gemm<at::Half> argument k must be non-negative and less than 2147483647 but got 4294967297
2025-03-16 05:29:00.782299 test begin: paddle.Tensor.inner(x=Tensor([4294967297],"float16"), y=Tensor([4],"float16"), )

[torch error] paddle.Tensor.inner(x=Tensor([4294967297],"float16"), y=Tensor([4],"float16"), ) 
 inner() the last dimension must match on both input tensors but got shapes [4294967297] and [4]
2025-03-16 05:29:01.974411 test begin: paddle.Tensor.inner(x=Tensor([4],"float16"), y=Tensor([4294967297],"float16"), )

[torch error] paddle.Tensor.inner(x=Tensor([4],"float16"), y=Tensor([4294967297],"float16"), ) 
 inner() the last dimension must match on both input tensors but got shapes [4] and [4294967297]
2025-03-16 05:29:02.940188 test begin: paddle.Tensor.inner(x=Tensor([5, 214748365, 4],"float16"), y=Tensor([2, 214748365, 4],"float16"), )

[torch error] paddle.Tensor.inner(x=Tensor([5, 214748365, 4],"float16"), y=Tensor([2, 214748365, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 858993460.80 GiB. GPU 0 has a total capacity of 79.18 GiB of which 59.97 GiB is free. Process 109713 has 19.21 GiB memory in use. Of the allocated memory 11.22 GiB is allocated by PyTorch, and 5.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 05:29:05.281587 test begin: paddle.Tensor.inner(x=Tensor([5, 214748365, 4],"float16"), y=Tensor([2, 5, 4],"float16"), )

2025-03-16 05:29:06.541039 test begin: paddle.Tensor.inner(x=Tensor([5, 3, 286331154],"float16"), y=Tensor([2, 5, 286331154],"float16"), )

[Pass] paddle.Tensor.inner(x=Tensor([5, 3, 286331154],"float16"), y=Tensor([2, 5, 286331154],"float16"), )
2025-03-16 05:42:26.854933 test begin: paddle.Tensor.inner(x=Tensor([5, 3, 286331154],"float16"), y=Tensor([2, 5, 4],"float16"), )

[torch error] paddle.Tensor.inner(x=Tensor([5, 3, 286331154],"float16"), y=Tensor([2, 5, 4],"float16"), ) 
 inner() the last dimension must match on both input tensors but got shapes [5, 3, 286331154] and [2, 5, 4]
2025-03-16 05:42:30.983314 test begin: paddle.Tensor.inner(x=Tensor([5, 3, 4],"float16"), y=Tensor([2, 5, 429496730],"float16"), )

[torch error] paddle.Tensor.inner(x=Tensor([5, 3, 4],"float16"), y=Tensor([2, 5, 429496730],"float16"), ) 
 inner() the last dimension must match on both input tensors but got shapes [5, 3, 4] and [2, 5, 429496730]
2025-03-16 05:42:31.981147 test begin: paddle.Tensor.inner(x=Tensor([5, 3, 4],"float16"), y=Tensor([2, 536870913, 4],"float16"), )

2025-03-16 05:42:33.050858 test begin: paddle.Tensor.inner(x=Tensor([5, 3, 4],"float16"), y=Tensor([214748365, 5, 4],"float16"), )

[torch error] paddle.Tensor.inner(x=Tensor([5, 3, 4],"float16"), y=Tensor([214748365, 5, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 30.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 29.97 GiB is free. Process 109713 has 49.21 GiB memory in use. Of the allocated memory 46.02 GiB is allocated by PyTorch, and 1013.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 05:42:34.405417 test begin: paddle.Tensor.inner(x=Tensor([570425345, 4],"float32"), y=Tensor([4, 4],"float32"), )

CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 4.47 GiB is free. Process 109713 has 74.72 GiB memory in use. Of the allocated memory 63.52 GiB is allocated by PyTorch, and 9.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 05:42:52.790565 test begin: paddle.Tensor.inner(x=Tensor([570425345, 4],"float32"), y=Tensor([570425345, 4],"float32"), )

W0316 05:44:04.533963 157210 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 05:44:04.535192 157210 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.Tensor.inner(x=Tensor([570425345, 4],"float32"), y=Tensor([570425345, 4],"float32"), ) 
 CUDA out of memory. Tried to allocate more than 1EB memory.
2025-03-16 05:44:06.464577 test begin: paddle.Tensor.inner(x=Tensor([71582789, 5, 3, 4],"float16"), y=Tensor([3, 2, 5, 4],"float16"), )

2025-03-16 05:45:29.267810 test begin: paddle.Tensor.inner(x=Tensor([71582789, 5, 3, 4],"float16"), y=Tensor([71582789, 2, 5, 4],"float16"), )

[torch error] paddle.Tensor.inner(x=Tensor([71582789, 5, 3, 4],"float16"), y=Tensor([71582789, 2, 5, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 5.33 GiB. GPU 0 has a total capacity of 79.18 GiB of which 890.38 MiB is free. Process 162195 has 78.31 GiB memory in use. Of the allocated memory 76.01 GiB is allocated by PyTorch, and 511.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 05:45:31.921310 test begin: paddle.Tensor.isclose(x=Tensor([214748365, 4, 5],"float16"), y=Tensor([214748365, 4, 5],"float16"), )

[torch error] paddle.Tensor.isclose(x=Tensor([214748365, 4, 5],"float16"), y=Tensor([214748365, 4, 5],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 890.38 MiB is free. Process 162195 has 78.31 GiB memory in use. Of the allocated memory 76.01 GiB is allocated by PyTorch, and 511.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 05:45:33.798279 test begin: paddle.Tensor.isclose(x=Tensor([214748365, 4, 5],"float16"), y=Tensor([3, 4, 5],"float16"), )

[torch error] paddle.Tensor.isclose(x=Tensor([214748365, 4, 5],"float16"), y=Tensor([3, 4, 5],"float16"), ) 
 The size of tensor a (214748365) must match the size of tensor b (3) at non-singleton dimension 0
2025-03-16 05:45:35.957107 test begin: paddle.Tensor.isclose(x=Tensor([2281701379],"float32"), y=Tensor([2281701379],"float32"), )

[torch error] paddle.Tensor.isclose(x=Tensor([2281701379],"float32"), y=Tensor([2281701379],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 378.38 MiB is free. Process 162195 has 78.81 GiB memory in use. Of the allocated memory 76.51 GiB is allocated by PyTorch, and 511.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 05:45:40.988205 test begin: paddle.Tensor.isclose(x=Tensor([2281701379],"float32"), y=Tensor([2],"float32"), )

[torch error] paddle.Tensor.isclose(x=Tensor([2281701379],"float32"), y=Tensor([2],"float32"), ) 
 The size of tensor a (2281701379) must match the size of tensor b (2) at non-singleton dimension 0
2025-03-16 05:45:42.385270 test begin: paddle.Tensor.isclose(x=Tensor([2],"float16"), y=Tensor([4294967297],"float16"), )

[torch error] paddle.Tensor.isclose(x=Tensor([2],"float16"), y=Tensor([4294967297],"float16"), ) 
 The size of tensor a (2) must match the size of tensor b (4294967297) at non-singleton dimension 0
2025-03-16 05:45:45.778203 test begin: paddle.Tensor.isclose(x=Tensor([2],"float32"), y=Tensor([2281701379],"float32"), )

[torch error] paddle.Tensor.isclose(x=Tensor([2],"float32"), y=Tensor([2281701379],"float32"), ) 
 The size of tensor a (2) must match the size of tensor b (2281701379) at non-singleton dimension 0
2025-03-16 05:45:48.158274 test begin: paddle.Tensor.isclose(x=Tensor([3, 286331154, 5],"float16"), y=Tensor([3, 286331154, 5],"float16"), )

[torch error] paddle.Tensor.isclose(x=Tensor([3, 286331154, 5],"float16"), y=Tensor([3, 286331154, 5],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 378.38 MiB is free. Process 162195 has 78.81 GiB memory in use. Of the allocated memory 76.01 GiB is allocated by PyTorch, and 1023.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 05:45:50.612584 test begin: paddle.Tensor.isclose(x=Tensor([3, 286331154, 5],"float16"), y=Tensor([3, 4, 5],"float16"), )

[torch error] paddle.Tensor.isclose(x=Tensor([3, 286331154, 5],"float16"), y=Tensor([3, 4, 5],"float16"), ) 
 The size of tensor a (286331154) must match the size of tensor b (4) at non-singleton dimension 1
2025-03-16 05:45:52.735172 test begin: paddle.Tensor.isclose(x=Tensor([3, 4, 357913942],"float16"), y=Tensor([3, 4, 357913942],"float16"), )

[torch error] paddle.Tensor.isclose(x=Tensor([3, 4, 357913942],"float16"), y=Tensor([3, 4, 357913942],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 378.38 MiB is free. Process 162195 has 78.81 GiB memory in use. Of the allocated memory 76.01 GiB is allocated by PyTorch, and 1023.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 05:45:54.859096 test begin: paddle.Tensor.isclose(x=Tensor([3, 4, 357913942],"float16"), y=Tensor([3, 4, 5],"float16"), )

[torch error] paddle.Tensor.isclose(x=Tensor([3, 4, 357913942],"float16"), y=Tensor([3, 4, 5],"float16"), ) 
 The size of tensor a (357913942) must match the size of tensor b (5) at non-singleton dimension 2
2025-03-16 05:45:56.959648 test begin: paddle.Tensor.isclose(x=Tensor([3, 4, 5],"float16"), y=Tensor([214748365, 4, 5],"float16"), )

[torch error] paddle.Tensor.isclose(x=Tensor([3, 4, 5],"float16"), y=Tensor([214748365, 4, 5],"float16"), ) 
 The size of tensor a (3) must match the size of tensor b (214748365) at non-singleton dimension 0
2025-03-16 05:45:59.072275 test begin: paddle.Tensor.isclose(x=Tensor([3, 4, 5],"float16"), y=Tensor([3, 286331154, 5],"float16"), )

[torch error] paddle.Tensor.isclose(x=Tensor([3, 4, 5],"float16"), y=Tensor([3, 286331154, 5],"float16"), ) 
 The size of tensor a (4) must match the size of tensor b (286331154) at non-singleton dimension 1
2025-03-16 05:46:00.494563 test begin: paddle.Tensor.isclose(x=Tensor([3, 4, 5],"float16"), y=Tensor([3, 4, 357913942],"float16"), )

[torch error] paddle.Tensor.isclose(x=Tensor([3, 4, 5],"float16"), y=Tensor([3, 4, 357913942],"float16"), ) 
 The size of tensor a (5) must match the size of tensor b (357913942) at non-singleton dimension 2
2025-03-16 05:46:02.084140 test begin: paddle.Tensor.isclose(x=Tensor([4294967297],"float16"), y=Tensor([2],"float16"), )

[torch error] paddle.Tensor.isclose(x=Tensor([4294967297],"float16"), y=Tensor([2],"float16"), ) 
 The size of tensor a (4294967297) must match the size of tensor b (2) at non-singleton dimension 0
2025-03-16 05:46:04.259396 test begin: paddle.Tensor.isclose(x=Tensor([4294967297],"float16"), y=Tensor([4294967297],"float16"), )

[torch error] paddle.Tensor.isclose(x=Tensor([4294967297],"float16"), y=Tensor([4294967297],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 378.38 MiB is free. Process 162195 has 78.81 GiB memory in use. Of the allocated memory 76.01 GiB is allocated by PyTorch, and 1023.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 05:46:05.707440 test begin: paddle.Tensor.isclose(x=Tensor([4294967297],"float16"), y=Tensor([6],"float16"), )

[torch error] paddle.Tensor.isclose(x=Tensor([4294967297],"float16"), y=Tensor([6],"float16"), ) 
 The size of tensor a (4294967297) must match the size of tensor b (6) at non-singleton dimension 0
2025-03-16 05:46:07.909299 test begin: paddle.Tensor.isclose(x=Tensor([6],"float16"), y=Tensor([4294967297],"float16"), )

[torch error] paddle.Tensor.isclose(x=Tensor([6],"float16"), y=Tensor([4294967297],"float16"), ) 
 The size of tensor a (6) must match the size of tensor b (4294967297) at non-singleton dimension 0
2025-03-16 05:46:09.536828 test begin: paddle.Tensor.kthvalue(Tensor([1140851, 200, 10],"float32"), k=200, axis=1, )

element 1 of tensors does not require grad and does not have a grad_fn
[accuracy error] paddle.Tensor.kthvalue(Tensor([1140851, 200, 10],"float32"), k=200, axis=1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 12 / 11408510 (0.000105%)
Max absolute difference: 143
Max relative difference: 2.60869565
 x: array([[196, 122,  45, ..., 194,  51,  70],
       [ 24,  10, 167, ...,  73, 186,  59],
       [ 46,  87, 192, ..., 171, 153, 147],...
 y: array([[196, 122,  45, ..., 194,  51,  70],
       [ 24,  10, 167, ...,  73, 186,  59],
       [ 46,  87, 192, ..., 171, 153, 147],...
2025-03-16 05:47:12.577244 test begin: paddle.Tensor.kthvalue(Tensor([2, 114085069, 10],"float32"), k=200, axis=1, )

element 1 of tensors does not require grad and does not have a grad_fn
[accuracy error] paddle.Tensor.kthvalue(Tensor([2, 114085069, 10],"float32"), k=200, axis=1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 6 / 20 (30%)
Max absolute difference: 56818773
Max relative difference: 311.23853372
 x: array([[ 41869364,  69104719,  93010017,  45871158,  63208692,   6167015,
         18820122,  11438208, 104324653,  57001330],
       [ 16423285,  17869880,  43231496,  99593488,  76775249,  89546910,
           252058,  19988483,  91302243,  66003440]])
 y: array([[ 84203602,  69104719,  93010017,  90798097,  63208692,   6167015,
         18820122,  11438208,  53185049,    182557],
       [ 16423285,  17869880,  43231496,  99593488,  76775249,  89546910,
           252058,  19988483, 103395485,  85025156]])
2025-03-16 05:47:20.667563 test begin: paddle.Tensor.kthvalue(Tensor([2, 200, 5704254],"float32"), k=200, axis=1, )

element 1 of tensors does not require grad and does not have a grad_fn
[accuracy error] paddle.Tensor.kthvalue(Tensor([2, 200, 5704254],"float32"), k=200, axis=1, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 14 / 11408508 (0.000123%)
Max absolute difference: 180
Max relative difference: 60.
 x: array([[165, 151, 196, ..., 115,  56, 186],
       [174, 151, 197, ...,  38,  88,  22]])
 y: array([[165, 151, 196, ..., 115,  56, 186],
       [174, 151, 197, ...,  38,  88,  22]])
2025-03-16 05:47:27.133254 test begin: paddle.Tensor.lerp(x=Tensor([214748365, 5, 4],"float16"), y=Tensor([214748365, 5, 4],"float16"), weight=0.5, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([214748365, 5, 4],"float16"), y=Tensor([214748365, 5, 4],"float16"), weight=0.5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4121368039 / 4294967300 (96%)
Max absolute difference: 0.25
Max relative difference: 1.
 x: array([[[-0.1406, -0.1407, -0.0886,  0.1873],
        [ 0.    ,  0.    ,  0.    ,  0.    ],
        [ 0.    ,  0.    ,  0.    ,  0.    ],...
 y: array([[[-1.4062e-01, -1.4075e-01, -8.8623e-02,  1.8726e-01],
        [-9.1400e-03, -5.0735e-03, -1.3098e-01,  1.3464e-01],
        [-1.1737e-01,  1.0547e-01,  2.7023e-02, -1.9617e-01],...
2025-03-16 06:08:30.290231 test begin: paddle.Tensor.lerp(x=Tensor([214748365, 5, 4],"float16"), y=Tensor([4, 5, 4],"float16"), weight=0.5, )

[torch error] paddle.Tensor.lerp(x=Tensor([214748365, 5, 4],"float16"), y=Tensor([4, 5, 4],"float16"), weight=0.5, ) 
 The size of tensor a (214748365) must match the size of tensor b (4) at non-singleton dimension 0
2025-03-16 06:08:34.437783 test begin: paddle.Tensor.lerp(x=Tensor([2281701379],"float32"), y=Tensor([2281701379],"float32"), weight=0.5, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([2281701379],"float32"), y=Tensor([2281701379],"float32"), weight=0.5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2189515941 / 2281701379 (96%)
Max absolute difference: 0.25
Max relative difference: 1.
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)
 y: array([-0.120022, -0.22697 ,  0.131221, ..., -0.055671,  0.11484 ,
        0.205664], dtype=float32)
2025-03-16 06:12:38.707618 test begin: paddle.Tensor.lerp(x=Tensor([2281701379],"float32"), y=Tensor([4],"float32"), weight=0.5, )

[torch error] paddle.Tensor.lerp(x=Tensor([2281701379],"float32"), y=Tensor([4],"float32"), weight=0.5, ) 
 The size of tensor a (2281701379) must match the size of tensor b (4) at non-singleton dimension 0
2025-03-16 06:12:43.012506 test begin: paddle.Tensor.lerp(x=Tensor([4, 1073741825],"float16"), y=Tensor([1],"float16"), weight=0.2, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([4, 1073741825],"float16"), y=Tensor([1],"float16"), weight=0.2, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4186472402 / 4294967300 (97.5%)
Max absolute difference: 0.4
Max relative difference: 1.
 x: array([[-0.225 , -0.2252, -0.1418, ...,  0.    ,  0.    ,  0.    ],
       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],
       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],...
 y: array([[-0.225  , -0.2252 , -0.1418 , ...,  0.209  ,  0.1667 , -0.3003 ],
       [-0.2727 ,  0.0503 , -0.2527 , ...,  0.3247 , -0.218  , -0.2974 ],
       [ 0.06177,  0.2703 , -0.2233 , ..., -0.32   , -0.2832 ,  0.3135 ],...
2025-03-16 06:38:45.772998 test begin: paddle.Tensor.lerp(x=Tensor([4, 1073741825],"float16"), y=Tensor([4, 1073741825],"float16"), weight=0.5, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([4, 1073741825],"float16"), y=Tensor([4, 1073741825],"float16"), weight=0.5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4121368039 / 4294967300 (96%)
Max absolute difference: 0.25
Max relative difference: 1.
 x: array([[-0.1406, -0.1407, -0.0886, ...,  0.    ,  0.    ,  0.    ],
       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],
       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],...
 y: array([[-0.1406 , -0.1407 , -0.0886 , ...,  0.1306 ,  0.10425, -0.1877 ],
       [-0.1704 ,  0.03143, -0.158  , ...,  0.2029 , -0.1362 , -0.1858 ],
       [ 0.0386 ,  0.169  , -0.1395 , ..., -0.2001 , -0.177  ,  0.1959 ],...
2025-03-16 07:00:02.639295 test begin: paddle.Tensor.lerp(x=Tensor([4, 1073741825],"float16"), y=Tensor([4, 5],"float16"), weight=0.5, )

[torch error] paddle.Tensor.lerp(x=Tensor([4, 1073741825],"float16"), y=Tensor([4, 5],"float16"), weight=0.5, ) 
 The size of tensor a (1073741825) must match the size of tensor b (5) at non-singleton dimension 1
2025-03-16 07:00:06.896862 test begin: paddle.Tensor.lerp(x=Tensor([4, 268435457, 4],"float16"), y=Tensor([4, 268435457, 4],"float16"), weight=0.5, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([4, 268435457, 4],"float16"), y=Tensor([4, 268435457, 4],"float16"), weight=0.5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4121368039 / 4294967312 (96%)
Max absolute difference: 0.25
Max relative difference: 1.
 x: array([[[-0.004704,  0.2462  ,  0.10144 ,  0.1119  ],
        [ 0.2319  , -0.04688 ,  0.2393  ,  0.07623 ],
        [-0.06052 ,  0.04044 , -0.2026  , -0.2222  ],...
 y: array([[[-0.004704,  0.2462  ,  0.10144 ,  0.1119  ],
        [ 0.2319  , -0.04688 ,  0.2393  ,  0.07623 ],
        [-0.06052 ,  0.04044 , -0.2026  , -0.2222  ],...
2025-03-16 07:21:12.259149 test begin: paddle.Tensor.lerp(x=Tensor([4, 268435457, 4],"float16"), y=Tensor([4, 5, 4],"float16"), weight=0.5, )

[torch error] paddle.Tensor.lerp(x=Tensor([4, 268435457, 4],"float16"), y=Tensor([4, 5, 4],"float16"), weight=0.5, ) 
 The size of tensor a (268435457) must match the size of tensor b (5) at non-singleton dimension 1
2025-03-16 07:21:16.464235 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 214748365],"float16"), y=Tensor([4, 5, 214748365],"float16"), weight=0.5, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([4, 5, 214748365],"float16"), y=Tensor([4, 5, 214748365],"float16"), weight=0.5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4121368039 / 4294967300 (96%)
Max absolute difference: 0.25
Max relative difference: 1.
 x: array([[[-0.1406, -0.1407, -0.0886, ...,  0.    ,  0.    ,  0.    ],
        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],
        [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],...
 y: array([[[-0.1406  , -0.1407  , -0.0886  , ..., -0.10126 ,  0.05148 ,
         -0.1293  ],
        [ 0.2445  ,  0.1747  , -0.2218  , ...,  0.00207 , -0.03552 ,...
2025-03-16 07:42:07.204791 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 214748365],"float16"), y=Tensor([4, 5, 4],"float16"), weight=0.5, )

[torch error] paddle.Tensor.lerp(x=Tensor([4, 5, 214748365],"float16"), y=Tensor([4, 5, 4],"float16"), weight=0.5, ) 
 The size of tensor a (214748365) must match the size of tensor b (4) at non-singleton dimension 2
2025-03-16 07:42:11.552270 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 3],"float16"), y=Tensor([4, 5, 4, 53687092],"float16"), weight=0.0, )

[torch error] paddle.Tensor.lerp(x=Tensor([4, 5, 4, 3],"float16"), y=Tensor([4, 5, 4, 53687092],"float16"), weight=0.0, ) 
 The size of tensor a (3) must match the size of tensor b (53687092) at non-singleton dimension 3
2025-03-16 07:42:13.924506 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 3],"float16"), y=Tensor([4, 5, 4, 53687092],"float16"), weight=0.5, )

[torch error] paddle.Tensor.lerp(x=Tensor([4, 5, 4, 3],"float16"), y=Tensor([4, 5, 4, 53687092],"float16"), weight=0.5, ) 
 The size of tensor a (3) must match the size of tensor b (53687092) at non-singleton dimension 3
2025-03-16 07:42:16.295426 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 3],"float16"), y=Tensor([4, 5, 4, 53687092],"float16"), weight=1.0, )

[torch error] paddle.Tensor.lerp(x=Tensor([4, 5, 4, 3],"float16"), y=Tensor([4, 5, 4, 53687092],"float16"), weight=1.0, ) 
 The size of tensor a (3) must match the size of tensor b (53687092) at non-singleton dimension 3
2025-03-16 07:42:18.641224 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 3],"float16"), y=Tensor([4, 5, 71582789, 3],"float16"), weight=0.0, )

[torch error] paddle.Tensor.lerp(x=Tensor([4, 5, 4, 3],"float16"), y=Tensor([4, 5, 71582789, 3],"float16"), weight=0.0, ) 
 The size of tensor a (4) must match the size of tensor b (71582789) at non-singleton dimension 2
2025-03-16 07:42:20.395869 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 3],"float16"), y=Tensor([4, 5, 71582789, 3],"float16"), weight=0.5, )

[torch error] paddle.Tensor.lerp(x=Tensor([4, 5, 4, 3],"float16"), y=Tensor([4, 5, 71582789, 3],"float16"), weight=0.5, ) 
 The size of tensor a (4) must match the size of tensor b (71582789) at non-singleton dimension 2
2025-03-16 07:42:22.057451 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 3],"float16"), y=Tensor([4, 5, 71582789, 3],"float16"), weight=1.0, )

[torch error] paddle.Tensor.lerp(x=Tensor([4, 5, 4, 3],"float16"), y=Tensor([4, 5, 71582789, 3],"float16"), weight=1.0, ) 
 The size of tensor a (4) must match the size of tensor b (71582789) at non-singleton dimension 2
2025-03-16 07:42:23.448265 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 3],"float16"), y=Tensor([4, 89478486, 4, 3],"float16"), weight=0.0, )

[torch error] paddle.Tensor.lerp(x=Tensor([4, 5, 4, 3],"float16"), y=Tensor([4, 89478486, 4, 3],"float16"), weight=0.0, ) 
 The size of tensor a (5) must match the size of tensor b (89478486) at non-singleton dimension 1
2025-03-16 07:42:24.894038 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 3],"float16"), y=Tensor([4, 89478486, 4, 3],"float16"), weight=0.5, )

[torch error] paddle.Tensor.lerp(x=Tensor([4, 5, 4, 3],"float16"), y=Tensor([4, 89478486, 4, 3],"float16"), weight=0.5, ) 
 The size of tensor a (5) must match the size of tensor b (89478486) at non-singleton dimension 1
2025-03-16 07:42:26.394877 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 3],"float16"), y=Tensor([4, 89478486, 4, 3],"float16"), weight=1.0, )

[torch error] paddle.Tensor.lerp(x=Tensor([4, 5, 4, 3],"float16"), y=Tensor([4, 89478486, 4, 3],"float16"), weight=1.0, ) 
 The size of tensor a (5) must match the size of tensor b (89478486) at non-singleton dimension 1
2025-03-16 07:42:28.023781 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 3],"float16"), y=Tensor([71582789, 5, 4, 3],"float16"), weight=0.0, )

[torch error] paddle.Tensor.lerp(x=Tensor([4, 5, 4, 3],"float16"), y=Tensor([71582789, 5, 4, 3],"float16"), weight=0.0, ) 
 The size of tensor a (4) must match the size of tensor b (71582789) at non-singleton dimension 0
2025-03-16 07:42:29.737775 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 3],"float16"), y=Tensor([71582789, 5, 4, 3],"float16"), weight=0.5, )

[torch error] paddle.Tensor.lerp(x=Tensor([4, 5, 4, 3],"float16"), y=Tensor([71582789, 5, 4, 3],"float16"), weight=0.5, ) 
 The size of tensor a (4) must match the size of tensor b (71582789) at non-singleton dimension 0
2025-03-16 07:42:31.985607 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 3],"float16"), y=Tensor([71582789, 5, 4, 3],"float16"), weight=1.0, )

[torch error] paddle.Tensor.lerp(x=Tensor([4, 5, 4, 3],"float16"), y=Tensor([71582789, 5, 4, 3],"float16"), weight=1.0, ) 
 The size of tensor a (4) must match the size of tensor b (71582789) at non-singleton dimension 0
2025-03-16 07:42:33.745933 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 53687092],"float16"), y=Tensor([4, 5, 4, 3],"float16"), weight=0.0, )

[torch error] paddle.Tensor.lerp(x=Tensor([4, 5, 4, 53687092],"float16"), y=Tensor([4, 5, 4, 3],"float16"), weight=0.0, ) 
 The size of tensor a (53687092) must match the size of tensor b (3) at non-singleton dimension 3
2025-03-16 07:42:35.192555 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 53687092],"float16"), y=Tensor([4, 5, 4, 3],"float16"), weight=0.5, )

[torch error] paddle.Tensor.lerp(x=Tensor([4, 5, 4, 53687092],"float16"), y=Tensor([4, 5, 4, 3],"float16"), weight=0.5, ) 
 The size of tensor a (53687092) must match the size of tensor b (3) at non-singleton dimension 3
2025-03-16 07:42:36.700286 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 53687092],"float16"), y=Tensor([4, 5, 4, 3],"float16"), weight=1.0, )

[torch error] paddle.Tensor.lerp(x=Tensor([4, 5, 4, 53687092],"float16"), y=Tensor([4, 5, 4, 3],"float16"), weight=1.0, ) 
 The size of tensor a (53687092) must match the size of tensor b (3) at non-singleton dimension 3
2025-03-16 07:42:38.215315 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 53687092],"float16"), y=Tensor([4, 5, 4, 53687092],"float16"), weight=0.0, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([4, 5, 4, 53687092],"float16"), y=Tensor([4, 5, 4, 53687092],"float16"), weight=0.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208163550 / 4294967360 (98%)
Max absolute difference: 0.5
Max relative difference: 1.
 x: array([[[[ 0.011116,  0.0988  , -0.2803  , ...,  0.      ,  0.      ,
           0.      ],
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,...
 y: array([[[[ 1.1116e-02,  9.8816e-02, -2.8027e-01, ..., -4.2542e-02,
          -1.8860e-01, -2.9028e-01],
         [-4.4482e-01,  8.4351e-02,  3.8794e-01, ..., -1.0181e-01,...
2025-03-16 08:04:21.413833 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 53687092],"float16"), y=Tensor([4, 5, 4, 53687092],"float16"), weight=0.5, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([4, 5, 4, 53687092],"float16"), y=Tensor([4, 5, 4, 53687092],"float16"), weight=0.5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4121368039 / 4294967360 (96%)
Max absolute difference: 0.25
Max relative difference: 1.
 x: array([[[[ 0.005558,  0.0494  , -0.1401  , ...,  0.      ,  0.      ,
           0.      ],
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,...
 y: array([[[[ 5.5580e-03,  4.9408e-02, -1.4014e-01, ..., -2.1271e-02,
          -9.4299e-02, -1.4514e-01],
         [-2.2241e-01,  4.2175e-02,  1.9397e-01, ..., -5.0903e-02,...
2025-03-16 08:26:09.917150 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4, 53687092],"float16"), y=Tensor([4, 5, 4, 53687092],"float16"), weight=1.0, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([4, 5, 4, 53687092],"float16"), y=Tensor([4, 5, 4, 53687092],"float16"), weight=1.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208163550 / 4294967360 (98%)
Max absolute difference: 0.5
Max relative difference: 1.
 x: array([[[[ 0.011116,  0.0988  , -0.2803  , ...,  0.      ,  0.      ,
           0.      ],
         [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,...
 y: array([[[[ 1.1116e-02,  9.8816e-02, -2.8027e-01, ..., -4.2542e-02,
          -1.8860e-01, -2.9028e-01],
         [-4.4482e-01,  8.4351e-02,  3.8794e-01, ..., -1.0181e-01,...
2025-03-16 08:55:27.843804 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4],"float16"), y=Tensor([214748365, 5, 4],"float16"), weight=0.5, )

[torch error] paddle.Tensor.lerp(x=Tensor([4, 5, 4],"float16"), y=Tensor([214748365, 5, 4],"float16"), weight=0.5, ) 
 The size of tensor a (4) must match the size of tensor b (214748365) at non-singleton dimension 0
2025-03-16 08:55:31.959717 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4],"float16"), y=Tensor([4, 268435457, 4],"float16"), weight=0.5, )

[torch error] paddle.Tensor.lerp(x=Tensor([4, 5, 4],"float16"), y=Tensor([4, 268435457, 4],"float16"), weight=0.5, ) 
 The size of tensor a (5) must match the size of tensor b (268435457) at non-singleton dimension 1
2025-03-16 08:55:34.446381 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 4],"float16"), y=Tensor([4, 5, 214748365],"float16"), weight=0.5, )

[torch error] paddle.Tensor.lerp(x=Tensor([4, 5, 4],"float16"), y=Tensor([4, 5, 214748365],"float16"), weight=0.5, ) 
 The size of tensor a (4) must match the size of tensor b (214748365) at non-singleton dimension 2
2025-03-16 08:55:36.356971 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 71582789, 3],"float16"), y=Tensor([4, 5, 4, 3],"float16"), weight=0.0, )

[torch error] paddle.Tensor.lerp(x=Tensor([4, 5, 71582789, 3],"float16"), y=Tensor([4, 5, 4, 3],"float16"), weight=0.0, ) 
 The size of tensor a (71582789) must match the size of tensor b (4) at non-singleton dimension 2
2025-03-16 08:55:38.206861 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 71582789, 3],"float16"), y=Tensor([4, 5, 4, 3],"float16"), weight=0.5, )

[torch error] paddle.Tensor.lerp(x=Tensor([4, 5, 71582789, 3],"float16"), y=Tensor([4, 5, 4, 3],"float16"), weight=0.5, ) 
 The size of tensor a (71582789) must match the size of tensor b (4) at non-singleton dimension 2
2025-03-16 08:55:40.056203 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 71582789, 3],"float16"), y=Tensor([4, 5, 4, 3],"float16"), weight=1.0, )

[torch error] paddle.Tensor.lerp(x=Tensor([4, 5, 71582789, 3],"float16"), y=Tensor([4, 5, 4, 3],"float16"), weight=1.0, ) 
 The size of tensor a (71582789) must match the size of tensor b (4) at non-singleton dimension 2
2025-03-16 08:55:41.646957 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 71582789, 3],"float16"), y=Tensor([4, 5, 71582789, 3],"float16"), weight=0.0, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([4, 5, 71582789, 3],"float16"), y=Tensor([4, 5, 71582789, 3],"float16"), weight=0.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208163550 / 4294967340 (98%)
Max absolute difference: 0.5
Max relative difference: 1.
 x: array([[[[ 0.2335 ,  0.448  ,  0.4126 ],
         [-0.0714 ,  0.3672 , -0.1488 ],
         [ 0.3037 ,  0.01222, -0.4143 ],...
 y: array([[[[ 0.2335  ,  0.448   ,  0.4126  ],
         [-0.0714  ,  0.3672  , -0.1488  ],
         [ 0.3037  ,  0.01222 , -0.4143  ],...
2025-03-16 09:16:54.745015 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 71582789, 3],"float16"), y=Tensor([4, 5, 71582789, 3],"float16"), weight=0.5, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([4, 5, 71582789, 3],"float16"), y=Tensor([4, 5, 71582789, 3],"float16"), weight=0.5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4121368039 / 4294967340 (96%)
Max absolute difference: 0.25
Max relative difference: 1.
 x: array([[[[ 0.11676,  0.224  ,  0.2063 ],
         [-0.0357 ,  0.1836 , -0.0744 ],
         [ 0.1519 ,  0.00611, -0.2072 ],...
 y: array([[[[ 0.11676 ,  0.224   ,  0.2063  ],
         [-0.0357  ,  0.1836  , -0.0744  ],
         [ 0.1519  ,  0.00611 , -0.2072  ],...
2025-03-16 09:37:54.202494 test begin: paddle.Tensor.lerp(x=Tensor([4, 5, 71582789, 3],"float16"), y=Tensor([4, 5, 71582789, 3],"float16"), weight=1.0, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([4, 5, 71582789, 3],"float16"), y=Tensor([4, 5, 71582789, 3],"float16"), weight=1.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208163550 / 4294967340 (98%)
Max absolute difference: 0.5
Max relative difference: 1.
 x: array([[[[ 0.2335 ,  0.448  ,  0.4126 ],
         [-0.0714 ,  0.3672 , -0.1488 ],
         [ 0.3037 ,  0.01222, -0.4143 ],...
 y: array([[[[ 0.2335  ,  0.448   ,  0.4126  ],
         [-0.0714  ,  0.3672  , -0.1488  ],
         [ 0.3037  ,  0.01222 , -0.4143  ],...
2025-03-16 10:06:48.132373 test begin: paddle.Tensor.lerp(x=Tensor([4, 5],"float16"), y=Tensor([4, 1073741825],"float16"), weight=0.5, )

[torch error] paddle.Tensor.lerp(x=Tensor([4, 5],"float16"), y=Tensor([4, 1073741825],"float16"), weight=0.5, ) 
 The size of tensor a (5) must match the size of tensor b (1073741825) at non-singleton dimension 1
2025-03-16 10:06:52.633128 test begin: paddle.Tensor.lerp(x=Tensor([4, 5],"float16"), y=Tensor([4294967297],"float16"), weight=0.2, )

[torch error] paddle.Tensor.lerp(x=Tensor([4, 5],"float16"), y=Tensor([4294967297],"float16"), weight=0.2, ) 
 The size of tensor a (5) must match the size of tensor b (4294967297) at non-singleton dimension 1
2025-03-16 10:06:54.414388 test begin: paddle.Tensor.lerp(x=Tensor([4, 5],"float16"), y=Tensor([858993460, 5],"float16"), weight=0.5, )

[torch error] paddle.Tensor.lerp(x=Tensor([4, 5],"float16"), y=Tensor([858993460, 5],"float16"), weight=0.5, ) 
 The size of tensor a (4) must match the size of tensor b (858993460) at non-singleton dimension 0
2025-03-16 10:06:56.767002 test begin: paddle.Tensor.lerp(x=Tensor([4, 89478486, 4, 3],"float16"), y=Tensor([4, 5, 4, 3],"float16"), weight=0.0, )

[torch error] paddle.Tensor.lerp(x=Tensor([4, 89478486, 4, 3],"float16"), y=Tensor([4, 5, 4, 3],"float16"), weight=0.0, ) 
 The size of tensor a (89478486) must match the size of tensor b (5) at non-singleton dimension 1
2025-03-16 10:06:58.521676 test begin: paddle.Tensor.lerp(x=Tensor([4, 89478486, 4, 3],"float16"), y=Tensor([4, 5, 4, 3],"float16"), weight=0.5, )

[torch error] paddle.Tensor.lerp(x=Tensor([4, 89478486, 4, 3],"float16"), y=Tensor([4, 5, 4, 3],"float16"), weight=0.5, ) 
 The size of tensor a (89478486) must match the size of tensor b (5) at non-singleton dimension 1
2025-03-16 10:07:00.915206 test begin: paddle.Tensor.lerp(x=Tensor([4, 89478486, 4, 3],"float16"), y=Tensor([4, 5, 4, 3],"float16"), weight=1.0, )

[torch error] paddle.Tensor.lerp(x=Tensor([4, 89478486, 4, 3],"float16"), y=Tensor([4, 5, 4, 3],"float16"), weight=1.0, ) 
 The size of tensor a (89478486) must match the size of tensor b (5) at non-singleton dimension 1
2025-03-16 10:07:02.667269 test begin: paddle.Tensor.lerp(x=Tensor([4, 89478486, 4, 3],"float16"), y=Tensor([4, 89478486, 4, 3],"float16"), weight=0.0, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([4, 89478486, 4, 3],"float16"), y=Tensor([4, 89478486, 4, 3],"float16"), weight=0.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208163550 / 4294967328 (98%)
Max absolute difference: 0.5
Max relative difference: 1.
 x: array([[[[-0.0429 , -0.1788 ,  0.2207 ],
         [ 0.06445,  0.4875 ,  0.1791 ],
         [-0.1886 ,  0.3738 , -0.3123 ],...
 y: array([[[[-0.0429  , -0.1788  ,  0.2207  ],
         [ 0.06445 ,  0.4875  ,  0.1791  ],
         [-0.1886  ,  0.3738  , -0.3123  ],...
2025-03-16 10:28:27.519634 test begin: paddle.Tensor.lerp(x=Tensor([4, 89478486, 4, 3],"float16"), y=Tensor([4, 89478486, 4, 3],"float16"), weight=0.5, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([4, 89478486, 4, 3],"float16"), y=Tensor([4, 89478486, 4, 3],"float16"), weight=0.5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4121368039 / 4294967328 (96%)
Max absolute difference: 0.25
Max relative difference: 1.
 x: array([[[[-0.02145 , -0.0894  ,  0.11035 ],
         [ 0.03223 ,  0.2438  ,  0.08954 ],
         [-0.0943  ,  0.1869  , -0.1561  ],...
 y: array([[[[-0.02145 , -0.0894  ,  0.11035 ],
         [ 0.03223 ,  0.2438  ,  0.08954 ],
         [-0.0943  ,  0.1869  , -0.1561  ],...
2025-03-16 10:50:23.510187 test begin: paddle.Tensor.lerp(x=Tensor([4, 89478486, 4, 3],"float16"), y=Tensor([4, 89478486, 4, 3],"float16"), weight=1.0, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([4, 89478486, 4, 3],"float16"), y=Tensor([4, 89478486, 4, 3],"float16"), weight=1.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208163550 / 4294967328 (98%)
Max absolute difference: 0.5
Max relative difference: 1.
 x: array([[[[-0.0429 , -0.1788 ,  0.2207 ],
         [ 0.06445,  0.4875 ,  0.1791 ],
         [-0.1886 ,  0.3738 , -0.3123 ],...
 y: array([[[[-0.0429  , -0.1788  ,  0.2207  ],
         [ 0.06445 ,  0.4875  ,  0.1791  ],
         [-0.1886  ,  0.3738  , -0.3123  ],...
2025-03-16 11:19:27.517705 test begin: paddle.Tensor.lerp(x=Tensor([4294967297],"float16"), y=Tensor([4294967297],"float16"), weight=0.5, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([4294967297],"float16"), y=Tensor([4294967297],"float16"), weight=0.5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4121368039 / 4294967297 (96%)
Max absolute difference: 0.25
Max relative difference: 1.
 x: array([0.1873, 0.    , 0.    , ..., 0.    , 0.    , 0.    ], dtype=float16)
 y: array([ 0.1873  , -0.00914 , -0.005074, ...,  0.0749  ,  0.1417  ,
        0.1156  ], dtype=float16)
2025-03-16 11:40:27.188963 test begin: paddle.Tensor.lerp(x=Tensor([4294967297],"float16"), y=Tensor([4],"float16"), weight=0.5, )

[torch error] paddle.Tensor.lerp(x=Tensor([4294967297],"float16"), y=Tensor([4],"float16"), weight=0.5, ) 
 The size of tensor a (4294967297) must match the size of tensor b (4) at non-singleton dimension 0
2025-03-16 11:40:31.246195 test begin: paddle.Tensor.lerp(x=Tensor([4],"float16"), y=Tensor([4294967297],"float16"), weight=0.5, )

[torch error] paddle.Tensor.lerp(x=Tensor([4],"float16"), y=Tensor([4294967297],"float16"), weight=0.5, ) 
 The size of tensor a (4) must match the size of tensor b (4294967297) at non-singleton dimension 0
2025-03-16 11:40:32.588782 test begin: paddle.Tensor.lerp(x=Tensor([4],"float32"), y=Tensor([2281701379],"float32"), weight=0.5, )

[torch error] paddle.Tensor.lerp(x=Tensor([4],"float32"), y=Tensor([2281701379],"float32"), weight=0.5, ) 
 The size of tensor a (4) must match the size of tensor b (2281701379) at non-singleton dimension 0
2025-03-16 11:40:37.362040 test begin: paddle.Tensor.lerp(x=Tensor([71582789, 5, 4, 3],"float16"), y=Tensor([4, 5, 4, 3],"float16"), weight=0.0, )

[torch error] paddle.Tensor.lerp(x=Tensor([71582789, 5, 4, 3],"float16"), y=Tensor([4, 5, 4, 3],"float16"), weight=0.0, ) 
 The size of tensor a (71582789) must match the size of tensor b (4) at non-singleton dimension 0
2025-03-16 11:40:39.728200 test begin: paddle.Tensor.lerp(x=Tensor([71582789, 5, 4, 3],"float16"), y=Tensor([4, 5, 4, 3],"float16"), weight=0.5, )

[torch error] paddle.Tensor.lerp(x=Tensor([71582789, 5, 4, 3],"float16"), y=Tensor([4, 5, 4, 3],"float16"), weight=0.5, ) 
 The size of tensor a (71582789) must match the size of tensor b (4) at non-singleton dimension 0
2025-03-16 11:40:42.259702 test begin: paddle.Tensor.lerp(x=Tensor([71582789, 5, 4, 3],"float16"), y=Tensor([4, 5, 4, 3],"float16"), weight=1.0, )

[torch error] paddle.Tensor.lerp(x=Tensor([71582789, 5, 4, 3],"float16"), y=Tensor([4, 5, 4, 3],"float16"), weight=1.0, ) 
 The size of tensor a (71582789) must match the size of tensor b (4) at non-singleton dimension 0
2025-03-16 11:40:44.616889 test begin: paddle.Tensor.lerp(x=Tensor([71582789, 5, 4, 3],"float16"), y=Tensor([71582789, 5, 4, 3],"float16"), weight=0.0, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([71582789, 5, 4, 3],"float16"), y=Tensor([71582789, 5, 4, 3],"float16"), weight=0.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208163550 / 4294967340 (98%)
Max absolute difference: 0.5
Max relative difference: 1.
 x: array([[[[ 0.2335 ,  0.448  ,  0.4126 ],
         [-0.0714 ,  0.3672 , -0.1488 ],
         [ 0.3037 ,  0.01222, -0.4143 ],...
 y: array([[[[ 2.3352e-01,  4.4800e-01,  4.1260e-01],
         [-7.1411e-02,  3.6719e-01, -1.4880e-01],
         [ 3.0371e-01,  1.2222e-02, -4.1431e-01],...
2025-03-16 12:01:42.896084 test begin: paddle.Tensor.lerp(x=Tensor([71582789, 5, 4, 3],"float16"), y=Tensor([71582789, 5, 4, 3],"float16"), weight=0.5, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([71582789, 5, 4, 3],"float16"), y=Tensor([71582789, 5, 4, 3],"float16"), weight=0.5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4121368039 / 4294967340 (96%)
Max absolute difference: 0.25
Max relative difference: 1.
 x: array([[[[ 0.11676 ,  0.224   ,  0.2063  ],
         [-0.0357  ,  0.1836  , -0.0744  ],
         [ 0.1519  ,  0.00611 , -0.2072  ],...
 y: array([[[[ 1.1676e-01,  2.2400e-01,  2.0630e-01],
         [-3.5706e-02,  1.8359e-01, -7.4402e-02],
         [ 1.5186e-01,  6.1111e-03, -2.0715e-01],...
2025-03-16 12:23:01.852686 test begin: paddle.Tensor.lerp(x=Tensor([71582789, 5, 4, 3],"float16"), y=Tensor([71582789, 5, 4, 3],"float16"), weight=1.0, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([71582789, 5, 4, 3],"float16"), y=Tensor([71582789, 5, 4, 3],"float16"), weight=1.0, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4208163550 / 4294967340 (98%)
Max absolute difference: 0.5
Max relative difference: 1.
 x: array([[[[ 0.2335 ,  0.448  ,  0.4126 ],
         [-0.0714 ,  0.3672 , -0.1488 ],
         [ 0.3037 ,  0.01222, -0.4143 ],...
 y: array([[[[ 2.3352e-01,  4.4800e-01,  4.1260e-01],
         [-7.1411e-02,  3.6719e-01, -1.4880e-01],
         [ 3.0371e-01,  1.2222e-02, -4.1431e-01],...
2025-03-16 12:52:29.840197 test begin: paddle.Tensor.lerp(x=Tensor([858993460, 5],"float16"), y=Tensor([1],"float16"), weight=0.2, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([858993460, 5],"float16"), y=Tensor([1],"float16"), weight=0.2, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4186472402 / 4294967300 (97.5%)
Max absolute difference: 0.4
Max relative difference: 1.
 x: array([[-0.225 , -0.2252, -0.1418,  0.2996,  0.    ],
       [ 0.    ,  0.    ,  0.    ,  0.    ,  0.    ],
       [ 0.    ,  0.    ,  0.    ,  0.    ,  0.    ],...
 y: array([[-0.225   , -0.2252  , -0.1418  ,  0.2996  , -0.014626],
       [-0.00812 , -0.2096  ,  0.2155  , -0.1877  ,  0.1687  ],
       [ 0.04324 , -0.314   , -0.2242  , -0.1985  , -0.0949  ],...
2025-03-16 13:16:34.075045 test begin: paddle.Tensor.lerp(x=Tensor([858993460, 5],"float16"), y=Tensor([4, 5],"float16"), weight=0.5, )

[torch error] paddle.Tensor.lerp(x=Tensor([858993460, 5],"float16"), y=Tensor([4, 5],"float16"), weight=0.5, ) 
 The size of tensor a (858993460) must match the size of tensor b (4) at non-singleton dimension 0
2025-03-16 13:16:38.312840 test begin: paddle.Tensor.lerp(x=Tensor([858993460, 5],"float16"), y=Tensor([858993460, 5],"float16"), weight=0.5, )

[accuracy error] backward  paddle.Tensor.lerp(x=Tensor([858993460, 5],"float16"), y=Tensor([858993460, 5],"float16"), weight=0.5, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4121368039 / 4294967300 (96%)
Max absolute difference: 0.25
Max relative difference: 1.
 x: array([[-0.1406, -0.1407, -0.0886,  0.1873,  0.    ],
       [ 0.    ,  0.    ,  0.    ,  0.    ,  0.    ],
       [ 0.    ,  0.    ,  0.    ,  0.    ,  0.    ],...
 y: array([[-0.1406  , -0.1407  , -0.0886  ,  0.1873  , -0.00914 ],
       [-0.005074, -0.131   ,  0.1346  , -0.1174  ,  0.10547 ],
       [ 0.02702 , -0.1962  , -0.1401  , -0.124   , -0.05933 ],...
2025-03-16 13:37:40.831924 test begin: paddle.Tensor.logical_and(Tensor([1038],"bool"), Tensor([2281701379],"bool"), )

[torch error] paddle.Tensor.logical_and(Tensor([1038],"bool"), Tensor([2281701379],"bool"), ) 
 The size of tensor a (1038) must match the size of tensor b (2281701379) at non-singleton dimension 0
2025-03-16 13:38:42.304492 test begin: paddle.Tensor.logical_and(Tensor([1697],"bool"), Tensor([2281701379],"bool"), )

[torch error] paddle.Tensor.logical_and(Tensor([1697],"bool"), Tensor([2281701379],"bool"), ) 
 The size of tensor a (1697) must match the size of tensor b (2281701379) at non-singleton dimension 0
2025-03-16 13:38:42.853353 test begin: paddle.Tensor.logical_and(Tensor([2, 1140850690],"bool"), Tensor([2, 1140850690],"bool"), )

[Pass] paddle.Tensor.logical_and(Tensor([2, 1140850690],"bool"), Tensor([2, 1140850690],"bool"), )
2025-03-16 13:39:16.327163 test begin: paddle.Tensor.logical_and(Tensor([2, 1140850690],"bool"), Tensor([2, 3],"bool"), )

[torch error] paddle.Tensor.logical_and(Tensor([2, 1140850690],"bool"), Tensor([2, 3],"bool"), ) 
 The size of tensor a (1140850690) must match the size of tensor b (3) at non-singleton dimension 1
2025-03-16 13:39:17.353565 test begin: paddle.Tensor.logical_and(Tensor([2, 3],"bool"), Tensor([2, 1140850690],"bool"), )

[torch error] paddle.Tensor.logical_and(Tensor([2, 3],"bool"), Tensor([2, 1140850690],"bool"), ) 
 The size of tensor a (3) must match the size of tensor b (1140850690) at non-singleton dimension 1
2025-03-16 13:39:17.821907 test begin: paddle.Tensor.logical_and(Tensor([2, 3],"bool"), Tensor([760567127, 3],"bool"), )

[torch error] paddle.Tensor.logical_and(Tensor([2, 3],"bool"), Tensor([760567127, 3],"bool"), ) 
 The size of tensor a (2) must match the size of tensor b (760567127) at non-singleton dimension 0
2025-03-16 13:39:18.447396 test begin: paddle.Tensor.logical_and(Tensor([2281701379],"bool"), Tensor([1038],"bool"), )

[torch error] paddle.Tensor.logical_and(Tensor([2281701379],"bool"), Tensor([1038],"bool"), ) 
 The size of tensor a (2281701379) must match the size of tensor b (1038) at non-singleton dimension 0
2025-03-16 13:39:19.080912 test begin: paddle.Tensor.logical_and(Tensor([2281701379],"bool"), Tensor([1697],"bool"), )

[torch error] paddle.Tensor.logical_and(Tensor([2281701379],"bool"), Tensor([1697],"bool"), ) 
 The size of tensor a (2281701379) must match the size of tensor b (1697) at non-singleton dimension 0
2025-03-16 13:39:19.710587 test begin: paddle.Tensor.logical_and(Tensor([2281701379],"bool"), Tensor([2281701379],"bool"), )

[Pass] paddle.Tensor.logical_and(Tensor([2281701379],"bool"), Tensor([2281701379],"bool"), )
2025-03-16 13:39:54.626090 test begin: paddle.Tensor.logical_and(Tensor([2281701379],"bool"), Tensor([28913],"bool"), )

[torch error] paddle.Tensor.logical_and(Tensor([2281701379],"bool"), Tensor([28913],"bool"), ) 
 The size of tensor a (2281701379) must match the size of tensor b (28913) at non-singleton dimension 0
2025-03-16 13:39:55.236080 test begin: paddle.Tensor.logical_and(Tensor([2281701379],"bool"), Tensor([393448],"bool"), )

[torch error] paddle.Tensor.logical_and(Tensor([2281701379],"bool"), Tensor([393448],"bool"), ) 
 The size of tensor a (2281701379) must match the size of tensor b (393448) at non-singleton dimension 0
2025-03-16 13:39:55.867613 test begin: paddle.Tensor.logical_and(Tensor([2281701379],"bool"), Tensor([402297],"bool"), )

[torch error] paddle.Tensor.logical_and(Tensor([2281701379],"bool"), Tensor([402297],"bool"), ) 
 The size of tensor a (2281701379) must match the size of tensor b (402297) at non-singleton dimension 0
2025-03-16 13:39:56.501077 test begin: paddle.Tensor.logical_and(Tensor([28913],"bool"), Tensor([2281701379],"bool"), )

[torch error] paddle.Tensor.logical_and(Tensor([28913],"bool"), Tensor([2281701379],"bool"), ) 
 The size of tensor a (28913) must match the size of tensor b (2281701379) at non-singleton dimension 0
2025-03-16 13:39:57.129684 test begin: paddle.Tensor.logical_and(Tensor([393448],"bool"), Tensor([2281701379],"bool"), )

[torch error] paddle.Tensor.logical_and(Tensor([393448],"bool"), Tensor([2281701379],"bool"), ) 
 The size of tensor a (393448) must match the size of tensor b (2281701379) at non-singleton dimension 0
2025-03-16 13:39:57.598477 test begin: paddle.Tensor.logical_and(Tensor([402297],"bool"), Tensor([2281701379],"bool"), )

[torch error] paddle.Tensor.logical_and(Tensor([402297],"bool"), Tensor([2281701379],"bool"), ) 
 The size of tensor a (402297) must match the size of tensor b (2281701379) at non-singleton dimension 0
2025-03-16 13:39:58.062806 test begin: paddle.Tensor.logical_and(Tensor([760567127, 3],"bool"), Tensor([2, 3],"bool"), )

[torch error] paddle.Tensor.logical_and(Tensor([760567127, 3],"bool"), Tensor([2, 3],"bool"), ) 
 The size of tensor a (760567127) must match the size of tensor b (2) at non-singleton dimension 0
2025-03-16 13:39:58.457975 test begin: paddle.Tensor.logical_and(Tensor([760567127, 3],"bool"), Tensor([760567127, 3],"bool"), )

[Pass] paddle.Tensor.logical_and(Tensor([760567127, 3],"bool"), Tensor([760567127, 3],"bool"), )
2025-03-16 13:40:33.529920 test begin: paddle.Tensor.logical_or(Tensor([1038],"bool"), Tensor([2281701379],"bool"), )

[torch error] paddle.Tensor.logical_or(Tensor([1038],"bool"), Tensor([2281701379],"bool"), ) 
 The size of tensor a (1038) must match the size of tensor b (2281701379) at non-singleton dimension 0
2025-03-16 13:40:34.432590 test begin: paddle.Tensor.logical_or(Tensor([1697],"bool"), Tensor([2281701379],"bool"), )

[torch error] paddle.Tensor.logical_or(Tensor([1697],"bool"), Tensor([2281701379],"bool"), ) 
 The size of tensor a (1697) must match the size of tensor b (2281701379) at non-singleton dimension 0
2025-03-16 13:40:35.065286 test begin: paddle.Tensor.logical_or(Tensor([2281701379],"bool"), Tensor([1038],"bool"), )

[torch error] paddle.Tensor.logical_or(Tensor([2281701379],"bool"), Tensor([1038],"bool"), ) 
 The size of tensor a (2281701379) must match the size of tensor b (1038) at non-singleton dimension 0
2025-03-16 13:40:35.536145 test begin: paddle.Tensor.logical_or(Tensor([2281701379],"bool"), Tensor([1697],"bool"), )

[torch error] paddle.Tensor.logical_or(Tensor([2281701379],"bool"), Tensor([1697],"bool"), ) 
 The size of tensor a (2281701379) must match the size of tensor b (1697) at non-singleton dimension 0
2025-03-16 13:40:35.930936 test begin: paddle.Tensor.logical_or(Tensor([2281701379],"bool"), Tensor([2281701379],"bool"), )

[Pass] paddle.Tensor.logical_or(Tensor([2281701379],"bool"), Tensor([2281701379],"bool"), )
2025-03-16 13:41:10.054082 test begin: paddle.Tensor.logical_or(Tensor([2281701379],"bool"), Tensor([28913],"bool"), )

[torch error] paddle.Tensor.logical_or(Tensor([2281701379],"bool"), Tensor([28913],"bool"), ) 
 The size of tensor a (2281701379) must match the size of tensor b (28913) at non-singleton dimension 0
2025-03-16 13:41:10.587937 test begin: paddle.Tensor.logical_or(Tensor([2281701379],"bool"), Tensor([393448],"bool"), )

[torch error] paddle.Tensor.logical_or(Tensor([2281701379],"bool"), Tensor([393448],"bool"), ) 
 The size of tensor a (2281701379) must match the size of tensor b (393448) at non-singleton dimension 0
2025-03-16 13:41:10.842667 test begin: paddle.Tensor.logical_or(Tensor([2281701379],"bool"), Tensor([402297],"bool"), )

[torch error] paddle.Tensor.logical_or(Tensor([2281701379],"bool"), Tensor([402297],"bool"), ) 
 The size of tensor a (2281701379) must match the size of tensor b (402297) at non-singleton dimension 0
2025-03-16 13:41:11.386415 test begin: paddle.Tensor.logical_or(Tensor([28913],"bool"), Tensor([2281701379],"bool"), )

[torch error] paddle.Tensor.logical_or(Tensor([28913],"bool"), Tensor([2281701379],"bool"), ) 
 The size of tensor a (28913) must match the size of tensor b (2281701379) at non-singleton dimension 0
2025-03-16 13:41:11.943862 test begin: paddle.Tensor.logical_or(Tensor([393448],"bool"), Tensor([2281701379],"bool"), )

[torch error] paddle.Tensor.logical_or(Tensor([393448],"bool"), Tensor([2281701379],"bool"), ) 
 The size of tensor a (393448) must match the size of tensor b (2281701379) at non-singleton dimension 0
2025-03-16 13:41:12.302232 test begin: paddle.Tensor.logical_or(Tensor([402297],"bool"), Tensor([2281701379],"bool"), )

[torch error] paddle.Tensor.logical_or(Tensor([402297],"bool"), Tensor([2281701379],"bool"), ) 
 The size of tensor a (402297) must match the size of tensor b (2281701379) at non-singleton dimension 0
2025-03-16 13:41:12.557182 test begin: paddle.Tensor.logit(x=Tensor([143165577, 3, 2, 5],"float16"), )

[accuracy error] backward  paddle.Tensor.logit(x=Tensor([143165577, 3, 2, 5],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[[  0.857  ,   0.     ,   0.     ,  -0.379  ,   6.473  ],
         [  0.     ,   0.     ,   0.396  ,  -2.078  ,   0.     ]],
...
 y: array([[[[  0.857  ,       nan,       nan,  -0.379  ,   6.473  ],
         [      nan,       nan,   0.396  ,  -2.078  ,       nan]],
...
2025-03-16 13:49:50.558175 test begin: paddle.Tensor.logit(x=Tensor([143165577, 3, 2, 5],"float16"), eps=0.2, )

[Pass] paddle.Tensor.logit(x=Tensor([143165577, 3, 2, 5],"float16"), eps=0.2, )
2025-03-16 14:06:19.033260 test begin: paddle.Tensor.logit(x=Tensor([2147483649, 2],"float16"), )

[accuracy error] backward  paddle.Tensor.logit(x=Tensor([2147483649, 2],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[-0.7485 ,  0.     ],
       [ 0.     , -0.04102],
       [-3.543  ,  0.     ],...
 y: array([[-0.7485 ,      nan],
       [     nan, -0.04102],
       [-3.543  ,      nan],...
2025-03-16 14:15:06.745969 test begin: paddle.Tensor.logit(x=Tensor([2281701379],"float32"), )

[accuracy error] backward  paddle.Tensor.logit(x=Tensor([2281701379],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([-1.606522,  0.      ,  1.569377, ...,  0.      ,  0.      ,
        1.764064], dtype=float32)
 y: array([-1.606522,       nan,  1.569377, ...,       nan,       nan,
        1.764064], dtype=float32)
2025-03-16 14:17:13.668876 test begin: paddle.Tensor.logit(x=Tensor([4, 1073741825],"float16"), )

[accuracy error] backward  paddle.Tensor.logit(x=Tensor([4, 1073741825],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[-1.1875,  0.    ,  0.    , ...,  0.    ,  1.85  ,  0.    ],
       [ 0.    ,  0.    , -1.424 , ...,  2.262 ,  0.    ,  0.    ],
       [ 0.3545,  0.    , -2.734 , ..., -1.88  ,  0.    ,  0.    ],...
 y: array([[-1.1875,     nan,     nan, ...,     nan,  1.85  ,     nan],
       [    nan,     nan, -1.424 , ...,  2.262 ,     nan,     nan],
       [ 0.3545,     nan, -2.734 , ..., -1.88  ,     nan,     nan],...
2025-03-16 14:25:50.295233 test begin: paddle.Tensor.logit(x=Tensor([4, 107374183, 2, 5],"float16"), )

[accuracy error] backward  paddle.Tensor.logit(x=Tensor([4, 107374183, 2, 5],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[[-1.3184e+00,  0.0000e+00,  0.0000e+00, -6.2842e-01,
           2.9668e+00],
         [ 0.0000e+00,  0.0000e+00,  1.3857e+00, -4.8248e-02,...
 y: array([[[[-1.3184e+00,         nan,         nan, -6.2842e-01,
           2.9668e+00],
         [        nan,         nan,  1.3857e+00, -4.8248e-02,...
2025-03-16 14:34:26.812832 test begin: paddle.Tensor.logit(x=Tensor([4, 107374183, 2, 5],"float16"), eps=0.2, )

[Pass] paddle.Tensor.logit(x=Tensor([4, 107374183, 2, 5],"float16"), eps=0.2, )
2025-03-16 14:51:13.323330 test begin: paddle.Tensor.logit(x=Tensor([4, 3, 2, 178956971],"float16"), )

[accuracy error] backward  paddle.Tensor.logit(x=Tensor([4, 3, 2, 178956971],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[[-5.1123e-01,  0.0000e+00,  0.0000e+00, ..., -8.8184e-01,
           2.1875e+00,  4.0625e-01],
         [ 0.0000e+00,  1.5840e+00, -4.1250e+00, ...,  9.8359e+00,...
 y: array([[[[-5.1123e-01,         nan,         nan, ..., -8.8184e-01,
           2.1875e+00,  4.0625e-01],
         [        nan,  1.5840e+00, -4.1250e+00, ...,  9.8359e+00,...
2025-03-16 14:59:51.735275 test begin: paddle.Tensor.logit(x=Tensor([4, 3, 2, 178956971],"float16"), eps=0.2, )

[Pass] paddle.Tensor.logit(x=Tensor([4, 3, 2, 178956971],"float16"), eps=0.2, )
2025-03-16 15:17:04.349481 test begin: paddle.Tensor.logit(x=Tensor([4, 3, 357913942],"float16"), )

[accuracy error] backward  paddle.Tensor.logit(x=Tensor([4, 3, 357913942],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[ -0.511  ,   0.     ,   0.     , ...,   9.836  ,   1.006  ,
           1.953  ],
        [  0.     ,  -3.26   ,   5.895  , ...,  -1.549  ,   0.     ,...
 y: array([[[ -0.511  ,       nan,       nan, ...,   9.836  ,   1.006  ,
           1.953  ],
        [      nan,  -3.26   ,   5.895  , ...,  -1.549  ,       nan,...
2025-03-16 15:26:09.033756 test begin: paddle.Tensor.logit(x=Tensor([4, 3, 71582789, 5],"float16"), )

[accuracy error] backward  paddle.Tensor.logit(x=Tensor([4, 3, 71582789, 5],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[[ 9.8633e-01,  0.0000e+00,  0.0000e+00, -2.8857e-01,
           4.9688e+00],
         [ 0.0000e+00,  0.0000e+00,  5.9845e-02, -2.1250e+00,...
 y: array([[[[ 9.8633e-01,         nan,         nan, -2.8857e-01,
           4.9688e+00],
         [        nan,         nan,  5.9845e-02, -2.1250e+00,...
2025-03-16 15:34:49.836324 test begin: paddle.Tensor.logit(x=Tensor([4, 3, 71582789, 5],"float16"), eps=0.2, )

[Pass] paddle.Tensor.logit(x=Tensor([4, 3, 71582789, 5],"float16"), eps=0.2, )
2025-03-16 15:52:02.048815 test begin: paddle.Tensor.logit(x=Tensor([4, 536870913, 2],"float16"), )

[accuracy error] backward  paddle.Tensor.logit(x=Tensor([4, 536870913, 2],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[-0.511 ,  0.    ],
        [ 0.    , -1.796 ],
        [-3.805 ,  0.    ],...
 y: array([[[-0.511 ,     nan],
        [    nan, -1.796 ],
        [-3.805 ,     nan],...
2025-03-16 16:01:10.087628 test begin: paddle.Tensor.logit(x=Tensor([4294967297],"float16"), )

[accuracy error] backward  paddle.Tensor.logit(x=Tensor([4294967297],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([1.582, 0.   , 0.   , ..., 0.616, 1.317, 0.   ], dtype=float16)
 y: array([1.582,   nan,   nan, ..., 0.616, 1.317,   nan], dtype=float16)
2025-03-16 16:10:03.284522 test begin: paddle.Tensor.logit(x=Tensor([715827883, 3, 2],"float16"), )

[accuracy error] backward  paddle.Tensor.logit(x=Tensor([715827883, 3, 2],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[-0.7485 ,  0.     ],
        [ 0.     , -0.04102],
        [-3.543  ,  0.     ]],...
 y: array([[[-0.7485 ,      nan],
        [     nan, -0.04102],
        [-3.543  ,      nan]],...
2025-03-16 16:19:00.496707 test begin: paddle.Tensor.lu(Tensor([1431655766, 3],"float16"), )

/usr/local/lib/python3.9/dist-packages/torch/_tensor.py:857: UserWarning: torch.lu is deprecated in favor of torch.linalg.lu_factor / torch.linalg.lu_factor_ex and will be removed in a future PyTorch release.
LU, pivots = torch.lu(A, compute_pivots)
should be replaced with
LU, pivots = torch.linalg.lu_factor(A, compute_pivots)
and
LU, pivots, info = torch.lu(A, compute_pivots, get_infos=True)
should be replaced with
LU, pivots, info = torch.linalg.lu_factor_ex(A, compute_pivots) (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:1990.)
  LU, pivots, infos = torch._lu_with_info(
[torch error] paddle.Tensor.lu(Tensor([1431655766, 3],"float16"), ) 
 "lu_factor_magma_batched" not implemented for 'Half'
2025-03-16 16:19:04.757572 test begin: paddle.Tensor.lu(Tensor([3, 1431655766],"float16"), )

[torch error] paddle.Tensor.lu(Tensor([3, 1431655766],"float16"), ) 
 "lu_factor_magma_batched" not implemented for 'Half'
2025-03-16 16:19:06.501283 test begin: paddle.Tensor.lu(Tensor([3, 3, 477218589],"float16"), )

[torch error] paddle.Tensor.lu(Tensor([3, 3, 477218589],"float16"), ) 
 "lu_factor_magma_batched" not implemented for 'Half'
2025-03-16 16:19:08.754676 test begin: paddle.Tensor.lu(Tensor([3, 477218589, 3],"float16"), )

[torch error] paddle.Tensor.lu(Tensor([3, 477218589, 3],"float16"), ) 
 "lu_factor_magma_batched" not implemented for 'Half'
2025-03-16 16:19:10.730530 test begin: paddle.Tensor.lu(Tensor([3, 760567127],"float32"), )

=========================================================================================
   WARNING batched routines are designed for small sizes. It might be better to use the
   Native/Hybrid classical routines if you want good performance.
=========================================================================================
element 1 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.lu(Tensor([3, 760567127],"float32"), ) 
 (External) CUSOLVER error(3). 
  [Hint: 'CUSOLVER_STATUS_INVALID_VALUE'. An unsupported value or parameter was passed to the function (a negative vector size, for example).To correct: ensure that all the parameters being passed have valid values.] (at ../paddle/phi/kernels/gpu/lu_kernel.cu:53)

2025-03-16 16:20:20.243303 test begin: paddle.Tensor.lu(Tensor([357913942, 3, 2, 2],"float16"), )

[torch error] paddle.Tensor.lu(Tensor([357913942, 3, 2, 2],"float16"), ) 
 "lu_factor_cublas" not implemented for 'Half'
2025-03-16 16:20:24.439315 test begin: paddle.Tensor.lu(Tensor([4, 268435457, 2, 2],"float16"), )

[torch error] paddle.Tensor.lu(Tensor([4, 268435457, 2, 2],"float16"), ) 
 "lu_factor_cublas" not implemented for 'Half'
2025-03-16 16:20:26.200298 test begin: paddle.Tensor.lu(Tensor([4, 3, 178956971, 2],"float16"), )

[torch error] paddle.Tensor.lu(Tensor([4, 3, 178956971, 2],"float16"), ) 
 "lu_factor_magma_batched" not implemented for 'Half'
2025-03-16 16:20:28.013744 test begin: paddle.Tensor.lu(Tensor([4, 3, 2, 178956971],"float16"), )

[torch error] paddle.Tensor.lu(Tensor([4, 3, 2, 178956971],"float16"), ) 
 "lu_factor_magma_batched" not implemented for 'Half'
2025-03-16 16:20:30.549601 test begin: paddle.Tensor.lu(Tensor([477218589, 3, 3],"float16"), )

[torch error] paddle.Tensor.lu(Tensor([477218589, 3, 3],"float16"), ) 
 "lu_factor_cublas" not implemented for 'Half'
2025-03-16 16:20:32.551335 test begin: paddle.Tensor.lu(Tensor([760567127, 3],"float32"), )

=========================================================================================
   WARNING batched routines are designed for small sizes. It might be better to use the
   Native/Hybrid classical routines if you want good performance.
=========================================================================================
element 1 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.Tensor.lu(Tensor([760567127, 3],"float32"), ) 
 (External) CUSOLVER error(3). 
  [Hint: 'CUSOLVER_STATUS_INVALID_VALUE'. An unsupported value or parameter was passed to the function (a negative vector size, for example).To correct: ensure that all the parameters being passed have valid values.] (at ../paddle/phi/kernels/gpu/lu_kernel.cu:77)

2025-03-16 16:20:52.324495 test begin: paddle.Tensor.masked_fill(Tensor([1, 36828, 256],"float32"), Tensor([1, 2281701379, 1],"bool"), 0.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([1, 36828, 256],"float32"), Tensor([1, 2281701379, 1],"bool"), 0.0, ) 
 The size of tensor a (2281701379) must match the size of tensor b (36828) at non-singleton dimension 1
2025-03-16 16:20:53.555326 test begin: paddle.Tensor.masked_fill(Tensor([1, 36828, 256],"float32"), Tensor([1, 36828, 61956],"bool"), 0.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([1, 36828, 256],"float32"), Tensor([1, 36828, 61956],"bool"), 0.0, ) 
 The size of tensor a (61956) must match the size of tensor b (256) at non-singleton dimension 2
2025-03-16 16:20:54.208807 test begin: paddle.Tensor.masked_fill(Tensor([1, 36828, 256],"float32"), Tensor([61956, 36828, 1],"bool"), 0.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([1, 36828, 256],"float32"), Tensor([61956, 36828, 1],"bool"), 0.0, ) 
 CUDA out of memory. Tried to allocate 2176.01 GiB. GPU 0 has a total capacity of 79.18 GiB of which 68.33 GiB is free. Process 162195 has 10.85 GiB memory in use. Of the allocated memory 2.17 GiB is allocated by PyTorch, and 6.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 16:20:54.714114 test begin: paddle.Tensor.masked_fill(Tensor([1, 36828, 4],"float32"), Tensor([1, 2281701379, 1],"bool"), math.inf, )

[torch error] paddle.Tensor.masked_fill(Tensor([1, 36828, 4],"float32"), Tensor([1, 2281701379, 1],"bool"), math.inf, ) 
 The size of tensor a (2281701379) must match the size of tensor b (36828) at non-singleton dimension 1
2025-03-16 16:20:55.182528 test begin: paddle.Tensor.masked_fill(Tensor([1, 36828, 4],"float32"), Tensor([1, 36828, 61956],"bool"), math.inf, )

[torch error] paddle.Tensor.masked_fill(Tensor([1, 36828, 4],"float32"), Tensor([1, 36828, 61956],"bool"), math.inf, ) 
 The size of tensor a (61956) must match the size of tensor b (4) at non-singleton dimension 2
2025-03-16 16:20:55.650791 test begin: paddle.Tensor.masked_fill(Tensor([1, 36828, 4],"float32"), Tensor([61956, 36828, 1],"bool"), math.inf, )

2025-03-16 16:20:56.396565 test begin: paddle.Tensor.masked_fill(Tensor([1, 36828, 61956],"float32"), Tensor([1, 36828, 1],"bool"), 0.0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([1, 36828, 61956],"float32"), Tensor([1, 36828, 1],"bool"), 0.0, )
2025-03-16 16:22:21.697418 test begin: paddle.Tensor.masked_fill(Tensor([1, 36828, 61956],"float32"), Tensor([1, 36828, 1],"bool"), math.inf, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([1, 36828, 61956],"float32"), Tensor([1, 36828, 1],"bool"), math.inf, )
2025-03-16 16:23:02.322186 test begin: paddle.Tensor.masked_fill(Tensor([1, 36828, 61956],"float32"), Tensor([1, 36828, 61956],"bool"), 0.0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([1, 36828, 61956],"float32"), Tensor([1, 36828, 61956],"bool"), 0.0, )
2025-03-16 16:24:26.044066 test begin: paddle.Tensor.masked_fill(Tensor([1, 36828, 61956],"float32"), Tensor([1, 36828, 61956],"bool"), math.inf, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([1, 36828, 61956],"float32"), Tensor([1, 36828, 61956],"bool"), math.inf, )
2025-03-16 16:25:21.880540 test begin: paddle.Tensor.masked_fill(Tensor([1, 38367, 256],"float32"), Tensor([1, 2281701379, 1],"bool"), 0.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([1, 38367, 256],"float32"), Tensor([1, 2281701379, 1],"bool"), 0.0, ) 
 The size of tensor a (2281701379) must match the size of tensor b (38367) at non-singleton dimension 1
2025-03-16 16:25:23.256832 test begin: paddle.Tensor.masked_fill(Tensor([1, 38367, 256],"float32"), Tensor([1, 38367, 59471],"bool"), 0.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([1, 38367, 256],"float32"), Tensor([1, 38367, 59471],"bool"), 0.0, ) 
 The size of tensor a (59471) must match the size of tensor b (256) at non-singleton dimension 2
2025-03-16 16:25:24.036701 test begin: paddle.Tensor.masked_fill(Tensor([1, 38367, 256],"float32"), Tensor([59471, 38367, 1],"bool"), 0.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([1, 38367, 256],"float32"), Tensor([59471, 38367, 1],"bool"), 0.0, ) 
 CUDA out of memory. Tried to allocate 2176.02 GiB. GPU 0 has a total capacity of 79.18 GiB of which 68.33 GiB is free. Process 162195 has 10.85 GiB memory in use. Of the allocated memory 2.17 GiB is allocated by PyTorch, and 6.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 16:25:24.710088 test begin: paddle.Tensor.masked_fill(Tensor([1, 38367, 4],"float32"), Tensor([1, 2281701379, 1],"bool"), math.inf, )

[torch error] paddle.Tensor.masked_fill(Tensor([1, 38367, 4],"float32"), Tensor([1, 2281701379, 1],"bool"), math.inf, ) 
 The size of tensor a (2281701379) must match the size of tensor b (38367) at non-singleton dimension 1
2025-03-16 16:25:25.336053 test begin: paddle.Tensor.masked_fill(Tensor([1, 38367, 4],"float32"), Tensor([1, 38367, 59471],"bool"), math.inf, )

[torch error] paddle.Tensor.masked_fill(Tensor([1, 38367, 4],"float32"), Tensor([1, 38367, 59471],"bool"), math.inf, ) 
 The size of tensor a (59471) must match the size of tensor b (4) at non-singleton dimension 2
2025-03-16 16:25:25.737241 test begin: paddle.Tensor.masked_fill(Tensor([1, 38367, 4],"float32"), Tensor([59471, 38367, 1],"bool"), math.inf, )

2025-03-16 16:25:26.409719 test begin: paddle.Tensor.masked_fill(Tensor([1, 38367, 59471],"float32"), Tensor([1, 38367, 1],"bool"), 0.0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([1, 38367, 59471],"float32"), Tensor([1, 38367, 1],"bool"), 0.0, )
2025-03-16 16:27:15.377402 test begin: paddle.Tensor.masked_fill(Tensor([1, 38367, 59471],"float32"), Tensor([1, 38367, 1],"bool"), math.inf, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([1, 38367, 59471],"float32"), Tensor([1, 38367, 1],"bool"), math.inf, )
2025-03-16 16:28:09.458616 test begin: paddle.Tensor.masked_fill(Tensor([1, 38367, 59471],"float32"), Tensor([1, 38367, 59471],"bool"), 0.0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([1, 38367, 59471],"float32"), Tensor([1, 38367, 59471],"bool"), 0.0, )
2025-03-16 16:29:47.163834 test begin: paddle.Tensor.masked_fill(Tensor([1, 38367, 59471],"float32"), Tensor([1, 38367, 59471],"bool"), math.inf, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([1, 38367, 59471],"float32"), Tensor([1, 38367, 59471],"bool"), math.inf, )
2025-03-16 16:30:42.646701 test begin: paddle.Tensor.masked_fill(Tensor([1, 570425345, 4],"float32"), Tensor([1, 36828, 1],"bool"), math.inf, )

[torch error] paddle.Tensor.masked_fill(Tensor([1, 570425345, 4],"float32"), Tensor([1, 36828, 1],"bool"), math.inf, ) 
 The size of tensor a (36828) must match the size of tensor b (570425345) at non-singleton dimension 1
2025-03-16 16:30:47.461349 test begin: paddle.Tensor.masked_fill(Tensor([1, 570425345, 4],"float32"), Tensor([1, 38367, 1],"bool"), math.inf, )

[torch error] paddle.Tensor.masked_fill(Tensor([1, 570425345, 4],"float32"), Tensor([1, 38367, 1],"bool"), math.inf, ) 
 The size of tensor a (38367) must match the size of tensor b (570425345) at non-singleton dimension 1
2025-03-16 16:30:49.288214 test begin: paddle.Tensor.masked_fill(Tensor([1, 570425345, 4],"float32"), Tensor([1, 570425345, 1],"bool"), math.inf, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([1, 570425345, 4],"float32"), Tensor([1, 570425345, 1],"bool"), math.inf, )
2025-03-16 16:31:40.104924 test begin: paddle.Tensor.masked_fill(Tensor([1, 570425345, 4],"float32"), Tensor([1, 6380, 1],"bool"), math.inf, )

[torch error] paddle.Tensor.masked_fill(Tensor([1, 570425345, 4],"float32"), Tensor([1, 6380, 1],"bool"), math.inf, ) 
 The size of tensor a (6380) must match the size of tensor b (570425345) at non-singleton dimension 1
2025-03-16 16:31:44.891939 test begin: paddle.Tensor.masked_fill(Tensor([1, 570425345, 4],"float32"), Tensor([1, 8550, 1],"bool"), math.inf, )

[torch error] paddle.Tensor.masked_fill(Tensor([1, 570425345, 4],"float32"), Tensor([1, 8550, 1],"bool"), math.inf, ) 
 The size of tensor a (8550) must match the size of tensor b (570425345) at non-singleton dimension 1
2025-03-16 16:31:47.223850 test begin: paddle.Tensor.masked_fill(Tensor([1, 6380, 256],"float32"), Tensor([1, 2281701379, 1],"bool"), 0.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([1, 6380, 256],"float32"), Tensor([1, 2281701379, 1],"bool"), 0.0, ) 
 The size of tensor a (2281701379) must match the size of tensor b (6380) at non-singleton dimension 1
2025-03-16 16:31:48.214654 test begin: paddle.Tensor.masked_fill(Tensor([1, 6380, 256],"float32"), Tensor([1, 6380, 357634],"bool"), 0.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([1, 6380, 256],"float32"), Tensor([1, 6380, 357634],"bool"), 0.0, ) 
 The size of tensor a (357634) must match the size of tensor b (256) at non-singleton dimension 2
2025-03-16 16:31:48.935463 test begin: paddle.Tensor.masked_fill(Tensor([1, 6380, 256],"float32"), Tensor([357634, 6380, 1],"bool"), 0.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([1, 6380, 256],"float32"), Tensor([357634, 6380, 1],"bool"), 0.0, ) 
 CUDA out of memory. Tried to allocate 2176.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 68.33 GiB is free. Process 162195 has 10.85 GiB memory in use. Of the allocated memory 2.14 GiB is allocated by PyTorch, and 6.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 16:31:49.366979 test begin: paddle.Tensor.masked_fill(Tensor([1, 6380, 357634],"float32"), Tensor([1, 6380, 1],"bool"), 0.0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([1, 6380, 357634],"float32"), Tensor([1, 6380, 1],"bool"), 0.0, )
2025-03-16 16:33:24.620467 test begin: paddle.Tensor.masked_fill(Tensor([1, 6380, 357634],"float32"), Tensor([1, 6380, 1],"bool"), math.inf, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([1, 6380, 357634],"float32"), Tensor([1, 6380, 1],"bool"), math.inf, )
2025-03-16 16:34:10.790496 test begin: paddle.Tensor.masked_fill(Tensor([1, 6380, 357634],"float32"), Tensor([1, 6380, 357634],"bool"), 0.0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([1, 6380, 357634],"float32"), Tensor([1, 6380, 357634],"bool"), 0.0, )
2025-03-16 16:35:49.543789 test begin: paddle.Tensor.masked_fill(Tensor([1, 6380, 357634],"float32"), Tensor([1, 6380, 357634],"bool"), math.inf, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([1, 6380, 357634],"float32"), Tensor([1, 6380, 357634],"bool"), math.inf, )
2025-03-16 16:36:34.392303 test begin: paddle.Tensor.masked_fill(Tensor([1, 6380, 4],"float32"), Tensor([1, 2281701379, 1],"bool"), math.inf, )

[torch error] paddle.Tensor.masked_fill(Tensor([1, 6380, 4],"float32"), Tensor([1, 2281701379, 1],"bool"), math.inf, ) 
 The size of tensor a (2281701379) must match the size of tensor b (6380) at non-singleton dimension 1
2025-03-16 16:36:35.690501 test begin: paddle.Tensor.masked_fill(Tensor([1, 6380, 4],"float32"), Tensor([1, 6380, 357634],"bool"), math.inf, )

[torch error] paddle.Tensor.masked_fill(Tensor([1, 6380, 4],"float32"), Tensor([1, 6380, 357634],"bool"), math.inf, ) 
 The size of tensor a (357634) must match the size of tensor b (4) at non-singleton dimension 2
2025-03-16 16:36:36.460796 test begin: paddle.Tensor.masked_fill(Tensor([1, 6380, 4],"float32"), Tensor([357634, 6380, 1],"bool"), math.inf, )

2025-03-16 16:36:37.213289 test begin: paddle.Tensor.masked_fill(Tensor([1, 8550, 256],"float32"), Tensor([1, 2281701379, 1],"bool"), 0.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([1, 8550, 256],"float32"), Tensor([1, 2281701379, 1],"bool"), 0.0, ) 
 The size of tensor a (2281701379) must match the size of tensor b (8550) at non-singleton dimension 1
2025-03-16 16:36:37.847687 test begin: paddle.Tensor.masked_fill(Tensor([1, 8550, 256],"float32"), Tensor([1, 8550, 266866],"bool"), 0.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([1, 8550, 256],"float32"), Tensor([1, 8550, 266866],"bool"), 0.0, ) 
 The size of tensor a (266866) must match the size of tensor b (256) at non-singleton dimension 2
2025-03-16 16:36:38.481157 test begin: paddle.Tensor.masked_fill(Tensor([1, 8550, 256],"float32"), Tensor([266866, 8550, 1],"bool"), 0.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([1, 8550, 256],"float32"), Tensor([266866, 8550, 1],"bool"), 0.0, ) 
 CUDA out of memory. Tried to allocate 2176.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 34.33 GiB is free. Process 162195 has 44.86 GiB memory in use. Of the allocated memory 38.27 GiB is allocated by PyTorch, and 4.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 16:36:39.124684 test begin: paddle.Tensor.masked_fill(Tensor([1, 8550, 266866],"float32"), Tensor([1, 8550, 1],"bool"), 0.0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([1, 8550, 266866],"float32"), Tensor([1, 8550, 1],"bool"), 0.0, )
2025-03-16 16:37:59.467306 test begin: paddle.Tensor.masked_fill(Tensor([1, 8550, 266866],"float32"), Tensor([1, 8550, 1],"bool"), math.inf, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([1, 8550, 266866],"float32"), Tensor([1, 8550, 1],"bool"), math.inf, )
2025-03-16 16:38:46.757669 test begin: paddle.Tensor.masked_fill(Tensor([1, 8550, 266866],"float32"), Tensor([1, 8550, 266866],"bool"), 0.0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([1, 8550, 266866],"float32"), Tensor([1, 8550, 266866],"bool"), 0.0, )
2025-03-16 16:40:29.814375 test begin: paddle.Tensor.masked_fill(Tensor([1, 8550, 266866],"float32"), Tensor([1, 8550, 266866],"bool"), math.inf, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([1, 8550, 266866],"float32"), Tensor([1, 8550, 266866],"bool"), math.inf, )
2025-03-16 16:41:24.912261 test begin: paddle.Tensor.masked_fill(Tensor([1, 8550, 4],"float32"), Tensor([1, 2281701379, 1],"bool"), math.inf, )

[torch error] paddle.Tensor.masked_fill(Tensor([1, 8550, 4],"float32"), Tensor([1, 2281701379, 1],"bool"), math.inf, ) 
 The size of tensor a (2281701379) must match the size of tensor b (8550) at non-singleton dimension 1
2025-03-16 16:41:26.256002 test begin: paddle.Tensor.masked_fill(Tensor([1, 8550, 4],"float32"), Tensor([1, 8550, 266866],"bool"), math.inf, )

[torch error] paddle.Tensor.masked_fill(Tensor([1, 8550, 4],"float32"), Tensor([1, 8550, 266866],"bool"), math.inf, ) 
 The size of tensor a (266866) must match the size of tensor b (4) at non-singleton dimension 2
2025-03-16 16:41:26.941733 test begin: paddle.Tensor.masked_fill(Tensor([1, 8550, 4],"float32"), Tensor([266866, 8550, 1],"bool"), math.inf, )

2025-03-16 16:41:27.634552 test begin: paddle.Tensor.masked_fill(Tensor([1, 8912897, 256],"float32"), Tensor([1, 36828, 1],"bool"), 0.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([1, 8912897, 256],"float32"), Tensor([1, 36828, 1],"bool"), 0.0, ) 
 The size of tensor a (36828) must match the size of tensor b (8912897) at non-singleton dimension 1
2025-03-16 16:41:31.726293 test begin: paddle.Tensor.masked_fill(Tensor([1, 8912897, 256],"float32"), Tensor([1, 38367, 1],"bool"), 0.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([1, 8912897, 256],"float32"), Tensor([1, 38367, 1],"bool"), 0.0, ) 
 The size of tensor a (38367) must match the size of tensor b (8912897) at non-singleton dimension 1
2025-03-16 16:41:33.886943 test begin: paddle.Tensor.masked_fill(Tensor([1, 8912897, 256],"float32"), Tensor([1, 6380, 1],"bool"), 0.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([1, 8912897, 256],"float32"), Tensor([1, 6380, 1],"bool"), 0.0, ) 
 The size of tensor a (6380) must match the size of tensor b (8912897) at non-singleton dimension 1
2025-03-16 16:41:35.310624 test begin: paddle.Tensor.masked_fill(Tensor([1, 8912897, 256],"float32"), Tensor([1, 8550, 1],"bool"), 0.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([1, 8912897, 256],"float32"), Tensor([1, 8550, 1],"bool"), 0.0, ) 
 The size of tensor a (8550) must match the size of tensor b (8912897) at non-singleton dimension 1
2025-03-16 16:41:37.074428 test begin: paddle.Tensor.masked_fill(Tensor([1, 8912897, 256],"float32"), Tensor([1, 8912897, 1],"bool"), 0.0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([1, 8912897, 256],"float32"), Tensor([1, 8912897, 1],"bool"), 0.0, )
2025-03-16 16:43:08.058767 test begin: paddle.Tensor.masked_fill(Tensor([10, 25, 25],"float32"), Tensor([10, 25, 9126806],"bool"), -100.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([10, 25, 25],"float32"), Tensor([10, 25, 9126806],"bool"), -100.0, ) 
 The size of tensor a (9126806) must match the size of tensor b (25) at non-singleton dimension 2
2025-03-16 16:43:09.286576 test begin: paddle.Tensor.masked_fill(Tensor([10, 25, 25],"float32"), Tensor([10, 25, 9126806],"bool"), 0.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([10, 25, 25],"float32"), Tensor([10, 25, 9126806],"bool"), 0.0, ) 
 The size of tensor a (9126806) must match the size of tensor b (25) at non-singleton dimension 2
2025-03-16 16:43:10.023170 test begin: paddle.Tensor.masked_fill(Tensor([10, 25, 25],"float32"), Tensor([10, 9126806, 25],"bool"), -100.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([10, 25, 25],"float32"), Tensor([10, 9126806, 25],"bool"), -100.0, ) 
 The size of tensor a (9126806) must match the size of tensor b (25) at non-singleton dimension 1
2025-03-16 16:43:10.528829 test begin: paddle.Tensor.masked_fill(Tensor([10, 25, 25],"float32"), Tensor([10, 9126806, 25],"bool"), 0.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([10, 25, 25],"float32"), Tensor([10, 9126806, 25],"bool"), 0.0, ) 
 The size of tensor a (9126806) must match the size of tensor b (25) at non-singleton dimension 1
2025-03-16 16:43:10.955259 test begin: paddle.Tensor.masked_fill(Tensor([10, 25, 25],"float32"), Tensor([3650723, 25, 25],"bool"), -100.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([10, 25, 25],"float32"), Tensor([3650723, 25, 25],"bool"), -100.0, ) 
 The size of tensor a (3650723) must match the size of tensor b (10) at non-singleton dimension 0
2025-03-16 16:43:11.536417 test begin: paddle.Tensor.masked_fill(Tensor([10, 25, 25],"float32"), Tensor([3650723, 25, 25],"bool"), 0.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([10, 25, 25],"float32"), Tensor([3650723, 25, 25],"bool"), 0.0, ) 
 The size of tensor a (3650723) must match the size of tensor b (10) at non-singleton dimension 0
2025-03-16 16:43:11.887139 test begin: paddle.Tensor.masked_fill(Tensor([10, 25, 9126806],"float32"), Tensor([10, 25, 25],"bool"), -100.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([10, 25, 9126806],"float32"), Tensor([10, 25, 25],"bool"), -100.0, ) 
 The size of tensor a (25) must match the size of tensor b (9126806) at non-singleton dimension 2
2025-03-16 16:43:15.973404 test begin: paddle.Tensor.masked_fill(Tensor([10, 25, 9126806],"float32"), Tensor([10, 25, 25],"bool"), 0.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([10, 25, 9126806],"float32"), Tensor([10, 25, 25],"bool"), 0.0, ) 
 The size of tensor a (25) must match the size of tensor b (9126806) at non-singleton dimension 2
2025-03-16 16:43:17.586770 test begin: paddle.Tensor.masked_fill(Tensor([10, 25, 9126806],"float32"), Tensor([10, 25, 9126806],"bool"), -100.0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([10, 25, 9126806],"float32"), Tensor([10, 25, 9126806],"bool"), -100.0, )
2025-03-16 16:44:54.052741 test begin: paddle.Tensor.masked_fill(Tensor([10, 25, 9126806],"float32"), Tensor([10, 25, 9126806],"bool"), 0.0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([10, 25, 9126806],"float32"), Tensor([10, 25, 9126806],"bool"), 0.0, )
2025-03-16 16:46:38.055273 test begin: paddle.Tensor.masked_fill(Tensor([10, 9126806, 25],"float32"), Tensor([10, 25, 25],"bool"), -100.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([10, 9126806, 25],"float32"), Tensor([10, 25, 25],"bool"), -100.0, ) 
 The size of tensor a (25) must match the size of tensor b (9126806) at non-singleton dimension 1
2025-03-16 16:46:42.620270 test begin: paddle.Tensor.masked_fill(Tensor([10, 9126806, 25],"float32"), Tensor([10, 25, 25],"bool"), 0.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([10, 9126806, 25],"float32"), Tensor([10, 25, 25],"bool"), 0.0, ) 
 The size of tensor a (25) must match the size of tensor b (9126806) at non-singleton dimension 1
2025-03-16 16:46:44.085231 test begin: paddle.Tensor.masked_fill(Tensor([10, 9126806, 25],"float32"), Tensor([10, 9126806, 25],"bool"), -100.0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([10, 9126806, 25],"float32"), Tensor([10, 9126806, 25],"bool"), -100.0, )
2025-03-16 16:48:31.913422 test begin: paddle.Tensor.masked_fill(Tensor([10, 9126806, 25],"float32"), Tensor([10, 9126806, 25],"bool"), 0.0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([10, 9126806, 25],"float32"), Tensor([10, 9126806, 25],"bool"), 0.0, )
2025-03-16 16:50:01.726662 test begin: paddle.Tensor.masked_fill(Tensor([10186167, 7, 32],"float32"), Tensor([10186167, 7, 32],"bool"), 0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([10186167, 7, 32],"float32"), Tensor([10186167, 7, 32],"bool"), 0, )
2025-03-16 16:51:28.549614 test begin: paddle.Tensor.masked_fill(Tensor([10186167, 7, 32],"float32"), Tensor([13, 7, 32],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([10186167, 7, 32],"float32"), Tensor([13, 7, 32],"bool"), 0, ) 
 The size of tensor a (13) must match the size of tensor b (10186167) at non-singleton dimension 0
2025-03-16 16:51:31.502562 test begin: paddle.Tensor.masked_fill(Tensor([10186167, 7, 32],"float32"), Tensor([52, 7, 32],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([10186167, 7, 32],"float32"), Tensor([52, 7, 32],"bool"), 0, ) 
 The size of tensor a (52) must match the size of tensor b (10186167) at non-singleton dimension 0
2025-03-16 16:51:33.809912 test begin: paddle.Tensor.masked_fill(Tensor([1043, 8550, 256],"float32"), Tensor([1, 8550, 1],"bool"), 0.0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([1043, 8550, 256],"float32"), Tensor([1, 8550, 1],"bool"), 0.0, )
2025-03-16 16:52:50.301572 test begin: paddle.Tensor.masked_fill(Tensor([1043, 8550, 256],"float32"), Tensor([1043, 8550, 1],"bool"), 0.0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([1043, 8550, 256],"float32"), Tensor([1043, 8550, 1],"bool"), 0.0, )
2025-03-16 16:54:39.855855 test begin: paddle.Tensor.masked_fill(Tensor([11641334, 4, 7, 7],"float32"), Tensor([11641334, 4, 7, 7],"bool"), 0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([11641334, 4, 7, 7],"float32"), Tensor([11641334, 4, 7, 7],"bool"), 0, )
2025-03-16 16:56:08.124124 test begin: paddle.Tensor.masked_fill(Tensor([11641334, 4, 7, 7],"float32"), Tensor([13, 4, 7, 7],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([11641334, 4, 7, 7],"float32"), Tensor([13, 4, 7, 7],"bool"), 0, ) 
 The size of tensor a (13) must match the size of tensor b (11641334) at non-singleton dimension 0
2025-03-16 16:56:11.085036 test begin: paddle.Tensor.masked_fill(Tensor([11641334, 4, 7, 7],"float32"), Tensor([52, 4, 7, 7],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([11641334, 4, 7, 7],"float32"), Tensor([52, 4, 7, 7],"bool"), 0, ) 
 The size of tensor a (52) must match the size of tensor b (11641334) at non-singleton dimension 0
2025-03-16 16:56:12.777928 test begin: paddle.Tensor.masked_fill(Tensor([13, 175515491],"float32"), Tensor([13, 175515491],"bool"), 0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([13, 175515491],"float32"), Tensor([13, 175515491],"bool"), 0, )
2025-03-16 16:57:36.852823 test begin: paddle.Tensor.masked_fill(Tensor([13, 175515491],"float32"), Tensor([13, 32],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([13, 175515491],"float32"), Tensor([13, 32],"bool"), 0, ) 
 The size of tensor a (32) must match the size of tensor b (175515491) at non-singleton dimension 1
2025-03-16 16:57:40.947884 test begin: paddle.Tensor.masked_fill(Tensor([13, 32],"float32"), Tensor([13, 175515491],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([13, 32],"float32"), Tensor([13, 175515491],"bool"), 0, ) 
 The size of tensor a (175515491) must match the size of tensor b (32) at non-singleton dimension 1
2025-03-16 16:57:41.311548 test begin: paddle.Tensor.masked_fill(Tensor([13, 32],"float32"), Tensor([71303169, 32],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([13, 32],"float32"), Tensor([71303169, 32],"bool"), 0, ) 
 The size of tensor a (71303169) must match the size of tensor b (13) at non-singleton dimension 0
2025-03-16 16:57:41.974066 test begin: paddle.Tensor.masked_fill(Tensor([13, 3581949, 7, 7],"float32"), Tensor([13, 3581949, 7, 7],"bool"), 0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([13, 3581949, 7, 7],"float32"), Tensor([13, 3581949, 7, 7],"bool"), 0, )
2025-03-16 16:59:24.511932 test begin: paddle.Tensor.masked_fill(Tensor([13, 3581949, 7, 7],"float32"), Tensor([13, 4, 7, 7],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([13, 3581949, 7, 7],"float32"), Tensor([13, 4, 7, 7],"bool"), 0, ) 
 The size of tensor a (4) must match the size of tensor b (3581949) at non-singleton dimension 1
2025-03-16 16:59:29.197264 test begin: paddle.Tensor.masked_fill(Tensor([13, 4, 6268411, 7],"float32"), Tensor([13, 4, 6268411, 7],"bool"), 0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([13, 4, 6268411, 7],"float32"), Tensor([13, 4, 6268411, 7],"bool"), 0, )
2025-03-16 17:01:28.130104 test begin: paddle.Tensor.masked_fill(Tensor([13, 4, 6268411, 7],"float32"), Tensor([13, 4, 7, 7],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([13, 4, 6268411, 7],"float32"), Tensor([13, 4, 7, 7],"bool"), 0, ) 
 The size of tensor a (7) must match the size of tensor b (6268411) at non-singleton dimension 2
2025-03-16 17:01:32.586346 test begin: paddle.Tensor.masked_fill(Tensor([13, 4, 7, 6268411],"float32"), Tensor([13, 4, 7, 6268411],"bool"), 0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([13, 4, 7, 6268411],"float32"), Tensor([13, 4, 7, 6268411],"bool"), 0, )
2025-03-16 17:03:10.963518 test begin: paddle.Tensor.masked_fill(Tensor([13, 4, 7, 6268411],"float32"), Tensor([13, 4, 7, 7],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([13, 4, 7, 6268411],"float32"), Tensor([13, 4, 7, 7],"bool"), 0, ) 
 The size of tensor a (7) must match the size of tensor b (6268411) at non-singleton dimension 3
2025-03-16 17:03:15.477742 test begin: paddle.Tensor.masked_fill(Tensor([13, 4, 7, 7],"float32"), Tensor([11641334, 4, 7, 7],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([13, 4, 7, 7],"float32"), Tensor([11641334, 4, 7, 7],"bool"), 0, ) 
 The size of tensor a (11641334) must match the size of tensor b (13) at non-singleton dimension 0
2025-03-16 17:03:16.626691 test begin: paddle.Tensor.masked_fill(Tensor([13, 4, 7, 7],"float32"), Tensor([13, 3581949, 7, 7],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([13, 4, 7, 7],"float32"), Tensor([13, 3581949, 7, 7],"bool"), 0, ) 
 The size of tensor a (3581949) must match the size of tensor b (4) at non-singleton dimension 1
2025-03-16 17:03:17.377415 test begin: paddle.Tensor.masked_fill(Tensor([13, 4, 7, 7],"float32"), Tensor([13, 4, 6268411, 7],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([13, 4, 7, 7],"float32"), Tensor([13, 4, 6268411, 7],"bool"), 0, ) 
 The size of tensor a (6268411) must match the size of tensor b (7) at non-singleton dimension 2
2025-03-16 17:03:17.739959 test begin: paddle.Tensor.masked_fill(Tensor([13, 4, 7, 7],"float32"), Tensor([13, 4, 7, 6268411],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([13, 4, 7, 7],"float32"), Tensor([13, 4, 7, 6268411],"bool"), 0, ) 
 The size of tensor a (6268411) must match the size of tensor b (7) at non-singleton dimension 3
2025-03-16 17:03:18.334260 test begin: paddle.Tensor.masked_fill(Tensor([13, 5484860, 32],"float32"), Tensor([13, 5484860, 32],"bool"), 0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([13, 5484860, 32],"float32"), Tensor([13, 5484860, 32],"bool"), 0, )
2025-03-16 17:04:40.404707 test begin: paddle.Tensor.masked_fill(Tensor([13, 5484860, 32],"float32"), Tensor([13, 7, 32],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([13, 5484860, 32],"float32"), Tensor([13, 7, 32],"bool"), 0, ) 
 The size of tensor a (7) must match the size of tensor b (5484860) at non-singleton dimension 1
2025-03-16 17:04:43.437748 test begin: paddle.Tensor.masked_fill(Tensor([13, 7, 25073642],"float32"), Tensor([13, 7, 25073642],"bool"), 0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([13, 7, 25073642],"float32"), Tensor([13, 7, 25073642],"bool"), 0, )
2025-03-16 17:06:05.789228 test begin: paddle.Tensor.masked_fill(Tensor([13, 7, 25073642],"float32"), Tensor([13, 7, 32],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([13, 7, 25073642],"float32"), Tensor([13, 7, 32],"bool"), 0, ) 
 The size of tensor a (32) must match the size of tensor b (25073642) at non-singleton dimension 2
2025-03-16 17:06:10.005716 test begin: paddle.Tensor.masked_fill(Tensor([13, 7, 32],"float32"), Tensor([10186167, 7, 32],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([13, 7, 32],"float32"), Tensor([10186167, 7, 32],"bool"), 0, ) 
 The size of tensor a (10186167) must match the size of tensor b (13) at non-singleton dimension 0
2025-03-16 17:06:10.593512 test begin: paddle.Tensor.masked_fill(Tensor([13, 7, 32],"float32"), Tensor([13, 5484860, 32],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([13, 7, 32],"float32"), Tensor([13, 5484860, 32],"bool"), 0, ) 
 The size of tensor a (5484860) must match the size of tensor b (7) at non-singleton dimension 1
2025-03-16 17:06:11.282977 test begin: paddle.Tensor.masked_fill(Tensor([13, 7, 32],"float32"), Tensor([13, 7, 25073642],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([13, 7, 32],"float32"), Tensor([13, 7, 25073642],"bool"), 0, ) 
 The size of tensor a (25073642) must match the size of tensor b (32) at non-singleton dimension 2
2025-03-16 17:06:11.630512 test begin: paddle.Tensor.masked_fill(Tensor([1398, 6380, 256],"float32"), Tensor([1, 6380, 1],"bool"), 0.0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([1398, 6380, 256],"float32"), Tensor([1, 6380, 1],"bool"), 0.0, )
2025-03-16 17:07:45.305248 test begin: paddle.Tensor.masked_fill(Tensor([1398, 6380, 256],"float32"), Tensor([1398, 6380, 1],"bool"), 0.0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([1398, 6380, 256],"float32"), Tensor([1398, 6380, 1],"bool"), 0.0, )
2025-03-16 17:09:28.463682 test begin: paddle.Tensor.masked_fill(Tensor([14868, 38367, 4],"float32"), Tensor([1, 38367, 1],"bool"), math.inf, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([14868, 38367, 4],"float32"), Tensor([1, 38367, 1],"bool"), math.inf, )
2025-03-16 17:10:29.492252 test begin: paddle.Tensor.masked_fill(Tensor([14868, 38367, 4],"float32"), Tensor([14868, 38367, 1],"bool"), math.inf, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([14868, 38367, 4],"float32"), Tensor([14868, 38367, 1],"bool"), math.inf, )
2025-03-16 17:11:23.398371 test begin: paddle.Tensor.masked_fill(Tensor([15489, 36828, 4],"float32"), Tensor([1, 36828, 1],"bool"), math.inf, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([15489, 36828, 4],"float32"), Tensor([1, 36828, 1],"bool"), math.inf, )
2025-03-16 17:12:20.175055 test begin: paddle.Tensor.masked_fill(Tensor([15489, 36828, 4],"float32"), Tensor([15489, 36828, 1],"bool"), math.inf, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([15489, 36828, 4],"float32"), Tensor([15489, 36828, 1],"bool"), math.inf, )
2025-03-16 17:13:06.844816 test begin: paddle.Tensor.masked_fill(Tensor([233, 38367, 256],"float32"), Tensor([1, 38367, 1],"bool"), 0.0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([233, 38367, 256],"float32"), Tensor([1, 38367, 1],"bool"), 0.0, )
2025-03-16 17:14:45.259957 test begin: paddle.Tensor.masked_fill(Tensor([233, 38367, 256],"float32"), Tensor([233, 38367, 1],"bool"), 0.0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([233, 38367, 256],"float32"), Tensor([233, 38367, 1],"bool"), 0.0, )
2025-03-16 17:16:29.548503 test begin: paddle.Tensor.masked_fill(Tensor([243, 36828, 256],"float32"), Tensor([1, 36828, 1],"bool"), 0.0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([243, 36828, 256],"float32"), Tensor([1, 36828, 1],"bool"), 0.0, )
2025-03-16 17:17:54.928695 test begin: paddle.Tensor.masked_fill(Tensor([243, 36828, 256],"float32"), Tensor([243, 36828, 1],"bool"), 0.0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([243, 36828, 256],"float32"), Tensor([243, 36828, 1],"bool"), 0.0, )
2025-03-16 17:19:18.249126 test begin: paddle.Tensor.masked_fill(Tensor([27, 25, 25],"float32"), Tensor([27, 25, 3380299],"bool"), -100.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([27, 25, 25],"float32"), Tensor([27, 25, 3380299],"bool"), -100.0, ) 
 The size of tensor a (3380299) must match the size of tensor b (25) at non-singleton dimension 2
2025-03-16 17:19:19.595154 test begin: paddle.Tensor.masked_fill(Tensor([27, 25, 25],"float32"), Tensor([27, 3380299, 25],"bool"), -100.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([27, 25, 25],"float32"), Tensor([27, 3380299, 25],"bool"), -100.0, ) 
 The size of tensor a (3380299) must match the size of tensor b (25) at non-singleton dimension 1
2025-03-16 17:19:20.255980 test begin: paddle.Tensor.masked_fill(Tensor([27, 25, 25],"float32"), Tensor([3650723, 25, 25],"bool"), -100.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([27, 25, 25],"float32"), Tensor([3650723, 25, 25],"bool"), -100.0, ) 
 The size of tensor a (3650723) must match the size of tensor b (27) at non-singleton dimension 0
2025-03-16 17:19:20.742458 test begin: paddle.Tensor.masked_fill(Tensor([27, 25, 3380299],"float32"), Tensor([27, 25, 25],"bool"), -100.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([27, 25, 3380299],"float32"), Tensor([27, 25, 25],"bool"), -100.0, ) 
 The size of tensor a (25) must match the size of tensor b (3380299) at non-singleton dimension 2
2025-03-16 17:19:24.265748 test begin: paddle.Tensor.masked_fill(Tensor([27, 25, 3380299],"float32"), Tensor([27, 25, 3380299],"bool"), -100.0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([27, 25, 3380299],"float32"), Tensor([27, 25, 3380299],"bool"), -100.0, )
2025-03-16 17:21:03.633424 test begin: paddle.Tensor.masked_fill(Tensor([27, 3380299, 25],"float32"), Tensor([27, 25, 25],"bool"), -100.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([27, 3380299, 25],"float32"), Tensor([27, 25, 25],"bool"), -100.0, ) 
 The size of tensor a (25) must match the size of tensor b (3380299) at non-singleton dimension 1
2025-03-16 17:21:08.165353 test begin: paddle.Tensor.masked_fill(Tensor([27, 3380299, 25],"float32"), Tensor([27, 3380299, 25],"bool"), -100.0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([27, 3380299, 25],"float32"), Tensor([27, 3380299, 25],"bool"), -100.0, )
2025-03-16 17:23:06.062964 test begin: paddle.Tensor.masked_fill(Tensor([340, 25, 25],"float32"), Tensor([340, 25, 268436],"bool"), -100.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([340, 25, 25],"float32"), Tensor([340, 25, 268436],"bool"), -100.0, ) 
 The size of tensor a (268436) must match the size of tensor b (25) at non-singleton dimension 2
2025-03-16 17:23:07.403053 test begin: paddle.Tensor.masked_fill(Tensor([340, 25, 25],"float32"), Tensor([340, 268436, 25],"bool"), -100.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([340, 25, 25],"float32"), Tensor([340, 268436, 25],"bool"), -100.0, ) 
 The size of tensor a (268436) must match the size of tensor b (25) at non-singleton dimension 1
2025-03-16 17:23:08.057829 test begin: paddle.Tensor.masked_fill(Tensor([340, 25, 25],"float32"), Tensor([3650723, 25, 25],"bool"), -100.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([340, 25, 25],"float32"), Tensor([3650723, 25, 25],"bool"), -100.0, ) 
 The size of tensor a (3650723) must match the size of tensor b (340) at non-singleton dimension 0
2025-03-16 17:23:08.644357 test begin: paddle.Tensor.masked_fill(Tensor([340, 25, 268436],"float32"), Tensor([340, 25, 25],"bool"), -100.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([340, 25, 268436],"float32"), Tensor([340, 25, 25],"bool"), -100.0, ) 
 The size of tensor a (25) must match the size of tensor b (268436) at non-singleton dimension 2
2025-03-16 17:23:12.808065 test begin: paddle.Tensor.masked_fill(Tensor([340, 25, 268436],"float32"), Tensor([340, 25, 268436],"bool"), -100.0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([340, 25, 268436],"float32"), Tensor([340, 25, 268436],"bool"), -100.0, )
2025-03-16 17:24:43.160393 test begin: paddle.Tensor.masked_fill(Tensor([340, 268436, 25],"float32"), Tensor([340, 25, 25],"bool"), -100.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([340, 268436, 25],"float32"), Tensor([340, 25, 25],"bool"), -100.0, ) 
 The size of tensor a (25) must match the size of tensor b (268436) at non-singleton dimension 1
2025-03-16 17:24:47.545219 test begin: paddle.Tensor.masked_fill(Tensor([340, 268436, 25],"float32"), Tensor([340, 268436, 25],"bool"), -100.0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([340, 268436, 25],"float32"), Tensor([340, 268436, 25],"bool"), -100.0, )
2025-03-16 17:26:21.968730 test begin: paddle.Tensor.masked_fill(Tensor([3650723, 25, 25],"float32"), Tensor([10, 25, 25],"bool"), -100.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([3650723, 25, 25],"float32"), Tensor([10, 25, 25],"bool"), -100.0, ) 
 The size of tensor a (10) must match the size of tensor b (3650723) at non-singleton dimension 0
2025-03-16 17:26:26.365735 test begin: paddle.Tensor.masked_fill(Tensor([3650723, 25, 25],"float32"), Tensor([10, 25, 25],"bool"), 0.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([3650723, 25, 25],"float32"), Tensor([10, 25, 25],"bool"), 0.0, ) 
 The size of tensor a (10) must match the size of tensor b (3650723) at non-singleton dimension 0
2025-03-16 17:26:28.135118 test begin: paddle.Tensor.masked_fill(Tensor([3650723, 25, 25],"float32"), Tensor([27, 25, 25],"bool"), -100.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([3650723, 25, 25],"float32"), Tensor([27, 25, 25],"bool"), -100.0, ) 
 The size of tensor a (27) must match the size of tensor b (3650723) at non-singleton dimension 0
2025-03-16 17:26:30.001299 test begin: paddle.Tensor.masked_fill(Tensor([3650723, 25, 25],"float32"), Tensor([340, 25, 25],"bool"), -100.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([3650723, 25, 25],"float32"), Tensor([340, 25, 25],"bool"), -100.0, ) 
 The size of tensor a (340) must match the size of tensor b (3650723) at non-singleton dimension 0
2025-03-16 17:26:32.307408 test begin: paddle.Tensor.masked_fill(Tensor([3650723, 25, 25],"float32"), Tensor([3650723, 25, 25],"bool"), -100.0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([3650723, 25, 25],"float32"), Tensor([3650723, 25, 25],"bool"), -100.0, )
2025-03-16 17:27:53.712331 test begin: paddle.Tensor.masked_fill(Tensor([3650723, 25, 25],"float32"), Tensor([3650723, 25, 25],"bool"), 0.0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([3650723, 25, 25],"float32"), Tensor([3650723, 25, 25],"bool"), 0.0, )
2025-03-16 17:29:25.904831 test begin: paddle.Tensor.masked_fill(Tensor([3650723, 25, 25],"float32"), Tensor([85, 25, 25],"bool"), -100.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([3650723, 25, 25],"float32"), Tensor([85, 25, 25],"bool"), -100.0, ) 
 The size of tensor a (85) must match the size of tensor b (3650723) at non-singleton dimension 0
2025-03-16 17:29:30.320340 test begin: paddle.Tensor.masked_fill(Tensor([52, 1371215, 32],"float32"), Tensor([52, 1371215, 32],"bool"), 0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([52, 1371215, 32],"float32"), Tensor([52, 1371215, 32],"bool"), 0, )
2025-03-16 17:30:57.906260 test begin: paddle.Tensor.masked_fill(Tensor([52, 1371215, 32],"float32"), Tensor([52, 7, 32],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([52, 1371215, 32],"float32"), Tensor([52, 7, 32],"bool"), 0, ) 
 The size of tensor a (7) must match the size of tensor b (1371215) at non-singleton dimension 1
2025-03-16 17:31:00.530092 test begin: paddle.Tensor.masked_fill(Tensor([52, 32],"float32"), Tensor([52, 43878873],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([52, 32],"float32"), Tensor([52, 43878873],"bool"), 0, ) 
 The size of tensor a (43878873) must match the size of tensor b (32) at non-singleton dimension 1
2025-03-16 17:31:01.014109 test begin: paddle.Tensor.masked_fill(Tensor([52, 32],"float32"), Tensor([71303169, 32],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([52, 32],"float32"), Tensor([71303169, 32],"bool"), 0, ) 
 The size of tensor a (71303169) must match the size of tensor b (52) at non-singleton dimension 0
2025-03-16 17:31:01.748696 test begin: paddle.Tensor.masked_fill(Tensor([52, 4, 1567103, 7],"float32"), Tensor([52, 4, 1567103, 7],"bool"), 0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([52, 4, 1567103, 7],"float32"), Tensor([52, 4, 1567103, 7],"bool"), 0, )
2025-03-16 17:32:22.428299 test begin: paddle.Tensor.masked_fill(Tensor([52, 4, 1567103, 7],"float32"), Tensor([52, 4, 7, 7],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([52, 4, 1567103, 7],"float32"), Tensor([52, 4, 7, 7],"bool"), 0, ) 
 The size of tensor a (7) must match the size of tensor b (1567103) at non-singleton dimension 2
2025-03-16 17:32:25.440500 test begin: paddle.Tensor.masked_fill(Tensor([52, 4, 7, 1567103],"float32"), Tensor([52, 4, 7, 1567103],"bool"), 0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([52, 4, 7, 1567103],"float32"), Tensor([52, 4, 7, 1567103],"bool"), 0, )
2025-03-16 17:33:44.559421 test begin: paddle.Tensor.masked_fill(Tensor([52, 4, 7, 1567103],"float32"), Tensor([52, 4, 7, 7],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([52, 4, 7, 1567103],"float32"), Tensor([52, 4, 7, 7],"bool"), 0, ) 
 The size of tensor a (7) must match the size of tensor b (1567103) at non-singleton dimension 3
2025-03-16 17:33:48.677891 test begin: paddle.Tensor.masked_fill(Tensor([52, 4, 7, 7],"float32"), Tensor([11641334, 4, 7, 7],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([52, 4, 7, 7],"float32"), Tensor([11641334, 4, 7, 7],"bool"), 0, ) 
 The size of tensor a (11641334) must match the size of tensor b (52) at non-singleton dimension 0
2025-03-16 17:33:49.271905 test begin: paddle.Tensor.masked_fill(Tensor([52, 4, 7, 7],"float32"), Tensor([52, 4, 1567103, 7],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([52, 4, 7, 7],"float32"), Tensor([52, 4, 1567103, 7],"bool"), 0, ) 
 The size of tensor a (1567103) must match the size of tensor b (7) at non-singleton dimension 2
2025-03-16 17:33:49.955099 test begin: paddle.Tensor.masked_fill(Tensor([52, 4, 7, 7],"float32"), Tensor([52, 4, 7, 1567103],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([52, 4, 7, 7],"float32"), Tensor([52, 4, 7, 1567103],"bool"), 0, ) 
 The size of tensor a (1567103) must match the size of tensor b (7) at non-singleton dimension 3
2025-03-16 17:33:50.303867 test begin: paddle.Tensor.masked_fill(Tensor([52, 4, 7, 7],"float32"), Tensor([52, 895488, 7, 7],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([52, 4, 7, 7],"float32"), Tensor([52, 895488, 7, 7],"bool"), 0, ) 
 The size of tensor a (895488) must match the size of tensor b (4) at non-singleton dimension 1
2025-03-16 17:33:50.881944 test begin: paddle.Tensor.masked_fill(Tensor([52, 43878873],"float32"), Tensor([52, 32],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([52, 43878873],"float32"), Tensor([52, 32],"bool"), 0, ) 
 The size of tensor a (32) must match the size of tensor b (43878873) at non-singleton dimension 1
2025-03-16 17:33:52.940017 test begin: paddle.Tensor.masked_fill(Tensor([52, 43878873],"float32"), Tensor([52, 43878873],"bool"), 0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([52, 43878873],"float32"), Tensor([52, 43878873],"bool"), 0, )
2025-03-16 17:35:46.982844 test begin: paddle.Tensor.masked_fill(Tensor([52, 7, 32],"float32"), Tensor([10186167, 7, 32],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([52, 7, 32],"float32"), Tensor([10186167, 7, 32],"bool"), 0, ) 
 The size of tensor a (10186167) must match the size of tensor b (52) at non-singleton dimension 0
2025-03-16 17:35:48.327439 test begin: paddle.Tensor.masked_fill(Tensor([52, 7, 32],"float32"), Tensor([52, 1371215, 32],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([52, 7, 32],"float32"), Tensor([52, 1371215, 32],"bool"), 0, ) 
 The size of tensor a (1371215) must match the size of tensor b (7) at non-singleton dimension 1
2025-03-16 17:35:49.058380 test begin: paddle.Tensor.masked_fill(Tensor([52, 7, 32],"float32"), Tensor([52, 7, 6268411],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([52, 7, 32],"float32"), Tensor([52, 7, 6268411],"bool"), 0, ) 
 The size of tensor a (6268411) must match the size of tensor b (32) at non-singleton dimension 2
2025-03-16 17:35:49.404919 test begin: paddle.Tensor.masked_fill(Tensor([52, 7, 6268411],"float32"), Tensor([52, 7, 32],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([52, 7, 6268411],"float32"), Tensor([52, 7, 32],"bool"), 0, ) 
 The size of tensor a (32) must match the size of tensor b (6268411) at non-singleton dimension 2
2025-03-16 17:35:53.585341 test begin: paddle.Tensor.masked_fill(Tensor([52, 7, 6268411],"float32"), Tensor([52, 7, 6268411],"bool"), 0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([52, 7, 6268411],"float32"), Tensor([52, 7, 6268411],"bool"), 0, )
2025-03-16 17:37:39.374425 test begin: paddle.Tensor.masked_fill(Tensor([52, 895488, 7, 7],"float32"), Tensor([52, 4, 7, 7],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([52, 895488, 7, 7],"float32"), Tensor([52, 4, 7, 7],"bool"), 0, ) 
 The size of tensor a (4) must match the size of tensor b (895488) at non-singleton dimension 1
2025-03-16 17:37:43.936263 test begin: paddle.Tensor.masked_fill(Tensor([52, 895488, 7, 7],"float32"), Tensor([52, 895488, 7, 7],"bool"), 0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([52, 895488, 7, 7],"float32"), Tensor([52, 895488, 7, 7],"bool"), 0, )
2025-03-16 17:39:21.035727 test begin: paddle.Tensor.masked_fill(Tensor([66717, 8550, 4],"float32"), Tensor([1, 8550, 1],"bool"), math.inf, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([66717, 8550, 4],"float32"), Tensor([1, 8550, 1],"bool"), math.inf, )
2025-03-16 17:40:08.800462 test begin: paddle.Tensor.masked_fill(Tensor([66717, 8550, 4],"float32"), Tensor([66717, 8550, 1],"bool"), math.inf, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([66717, 8550, 4],"float32"), Tensor([66717, 8550, 1],"bool"), math.inf, )
2025-03-16 17:40:47.626179 test begin: paddle.Tensor.masked_fill(Tensor([71303169, 32],"float32"), Tensor([13, 32],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([71303169, 32],"float32"), Tensor([13, 32],"bool"), 0, ) 
 The size of tensor a (13) must match the size of tensor b (71303169) at non-singleton dimension 0
2025-03-16 17:40:52.007224 test begin: paddle.Tensor.masked_fill(Tensor([71303169, 32],"float32"), Tensor([52, 32],"bool"), 0, )

[torch error] paddle.Tensor.masked_fill(Tensor([71303169, 32],"float32"), Tensor([52, 32],"bool"), 0, ) 
 The size of tensor a (52) must match the size of tensor b (71303169) at non-singleton dimension 0
2025-03-16 17:40:53.650564 test begin: paddle.Tensor.masked_fill(Tensor([71303169, 32],"float32"), Tensor([71303169, 32],"bool"), 0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([71303169, 32],"float32"), Tensor([71303169, 32],"bool"), 0, )
2025-03-16 17:42:33.298384 test begin: paddle.Tensor.masked_fill(Tensor([85, 1073742, 25],"float32"), Tensor([85, 1073742, 25],"bool"), -100.0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([85, 1073742, 25],"float32"), Tensor([85, 1073742, 25],"bool"), -100.0, )
2025-03-16 17:44:23.466116 test begin: paddle.Tensor.masked_fill(Tensor([85, 1073742, 25],"float32"), Tensor([85, 25, 25],"bool"), -100.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([85, 1073742, 25],"float32"), Tensor([85, 25, 25],"bool"), -100.0, ) 
 The size of tensor a (25) must match the size of tensor b (1073742) at non-singleton dimension 1
2025-03-16 17:44:28.009003 test begin: paddle.Tensor.masked_fill(Tensor([85, 25, 1073742],"float32"), Tensor([85, 25, 1073742],"bool"), -100.0, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([85, 25, 1073742],"float32"), Tensor([85, 25, 1073742],"bool"), -100.0, )
2025-03-16 17:45:59.703149 test begin: paddle.Tensor.masked_fill(Tensor([85, 25, 1073742],"float32"), Tensor([85, 25, 25],"bool"), -100.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([85, 25, 1073742],"float32"), Tensor([85, 25, 25],"bool"), -100.0, ) 
 The size of tensor a (25) must match the size of tensor b (1073742) at non-singleton dimension 2
2025-03-16 17:46:04.151948 test begin: paddle.Tensor.masked_fill(Tensor([85, 25, 25],"float32"), Tensor([3650723, 25, 25],"bool"), -100.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([85, 25, 25],"float32"), Tensor([3650723, 25, 25],"bool"), -100.0, ) 
 The size of tensor a (3650723) must match the size of tensor b (85) at non-singleton dimension 0
2025-03-16 17:46:05.110883 test begin: paddle.Tensor.masked_fill(Tensor([85, 25, 25],"float32"), Tensor([85, 1073742, 25],"bool"), -100.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([85, 25, 25],"float32"), Tensor([85, 1073742, 25],"bool"), -100.0, ) 
 The size of tensor a (1073742) must match the size of tensor b (25) at non-singleton dimension 1
2025-03-16 17:46:05.773838 test begin: paddle.Tensor.masked_fill(Tensor([85, 25, 25],"float32"), Tensor([85, 25, 1073742],"bool"), -100.0, )

[torch error] paddle.Tensor.masked_fill(Tensor([85, 25, 25],"float32"), Tensor([85, 25, 1073742],"bool"), -100.0, ) 
 The size of tensor a (1073742) must match the size of tensor b (25) at non-singleton dimension 2
2025-03-16 17:46:06.284613 test begin: paddle.Tensor.masked_fill(Tensor([89409, 6380, 4],"float32"), Tensor([1, 6380, 1],"bool"), math.inf, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([89409, 6380, 4],"float32"), Tensor([1, 6380, 1],"bool"), math.inf, )
2025-03-16 17:46:48.651290 test begin: paddle.Tensor.masked_fill(Tensor([89409, 6380, 4],"float32"), Tensor([89409, 6380, 1],"bool"), math.inf, )

One of the differentiated Tensors does not require grad
[Pass] paddle.Tensor.masked_fill(Tensor([89409, 6380, 4],"float32"), Tensor([89409, 6380, 1],"bool"), math.inf, )
2025-03-16 17:47:30.092307 test begin: paddle.Tensor.masked_select(Tensor([1, 142606337, 16],"float32"), Tensor([1, 142606337, 16],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([1, 142606337, 16],"float32"), Tensor([1, 142606337, 16],"bool"), ) 
 nonzero is not supported for tensors with more than INT_MAX elements,   See https://github.com/pytorch/pytorch/issues/51871
2025-03-16 17:47:35.154242 test begin: paddle.Tensor.masked_select(Tensor([1, 142606337, 16],"float32"), Tensor([1, 22, 16],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([1, 142606337, 16],"float32"), Tensor([1, 22, 16],"bool"), ) 
 The size of tensor a (22) must match the size of tensor b (142606337) at non-singleton dimension 1
2025-03-16 17:47:36.698462 test begin: paddle.Tensor.masked_select(Tensor([1, 1],"float32"), Tensor([1, 2281701379],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([1, 1],"float32"), Tensor([1, 2281701379],"bool"), ) 
 nonzero is not supported for tensors with more than INT_MAX elements,   See https://github.com/pytorch/pytorch/issues/51871
2025-03-16 17:47:37.212302 test begin: paddle.Tensor.masked_select(Tensor([1, 1],"float32"), Tensor([2281701379, 1],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([1, 1],"float32"), Tensor([2281701379, 1],"bool"), ) 
 nonzero is not supported for tensors with more than INT_MAX elements,   See https://github.com/pytorch/pytorch/issues/51871
2025-03-16 17:47:37.894549 test begin: paddle.Tensor.masked_select(Tensor([1, 22, 103713700],"float32"), Tensor([1, 22, 103713700],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([1, 22, 103713700],"float32"), Tensor([1, 22, 103713700],"bool"), ) 
 nonzero is not supported for tensors with more than INT_MAX elements,   See https://github.com/pytorch/pytorch/issues/51871
2025-03-16 17:47:40.819356 test begin: paddle.Tensor.masked_select(Tensor([1, 22, 103713700],"float32"), Tensor([1, 22, 16],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([1, 22, 103713700],"float32"), Tensor([1, 22, 16],"bool"), ) 
 The size of tensor a (16) must match the size of tensor b (103713700) at non-singleton dimension 2
2025-03-16 17:47:42.998538 test begin: paddle.Tensor.masked_select(Tensor([1, 22, 16],"float32"), Tensor([1, 142606337, 16],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([1, 22, 16],"float32"), Tensor([1, 142606337, 16],"bool"), ) 
 The size of tensor a (142606337) must match the size of tensor b (22) at non-singleton dimension 1
2025-03-16 17:47:43.350520 test begin: paddle.Tensor.masked_select(Tensor([1, 22, 16],"float32"), Tensor([1, 22, 103713700],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([1, 22, 16],"float32"), Tensor([1, 22, 103713700],"bool"), ) 
 The size of tensor a (103713700) must match the size of tensor b (16) at non-singleton dimension 2
2025-03-16 17:47:44.089278 test begin: paddle.Tensor.masked_select(Tensor([1, 22, 16],"float32"), Tensor([6482107, 22, 16],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([1, 22, 16],"float32"), Tensor([6482107, 22, 16],"bool"), ) 
 nonzero is not supported for tensors with more than INT_MAX elements,   See https://github.com/pytorch/pytorch/issues/51871
2025-03-16 17:47:44.438079 test begin: paddle.Tensor.masked_select(Tensor([1, 2281701379],"float32"), Tensor([1, 1],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([1, 2281701379],"float32"), Tensor([1, 1],"bool"), ) 
 nonzero is not supported for tensors with more than INT_MAX elements,   See https://github.com/pytorch/pytorch/issues/51871
2025-03-16 17:47:46.596918 test begin: paddle.Tensor.masked_select(Tensor([1, 2281701379],"float32"), Tensor([1, 2281701379],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([1, 2281701379],"float32"), Tensor([1, 2281701379],"bool"), ) 
 nonzero is not supported for tensors with more than INT_MAX elements,   See https://github.com/pytorch/pytorch/issues/51871
2025-03-16 17:47:49.350686 test begin: paddle.Tensor.masked_select(Tensor([10, 10, 10, 10, 10],"float64"), Tensor([10, 10, 10, 10, 228171],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([10, 10, 10, 10, 10],"float64"), Tensor([10, 10, 10, 10, 228171],"bool"), ) 
 The size of tensor a (228171) must match the size of tensor b (10) at non-singleton dimension 4
2025-03-16 17:49:11.961344 test begin: paddle.Tensor.masked_select(Tensor([10, 10, 10, 10, 10],"float64"), Tensor([10, 10, 10, 228171, 10],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([10, 10, 10, 10, 10],"float64"), Tensor([10, 10, 10, 228171, 10],"bool"), ) 
 The size of tensor a (228171) must match the size of tensor b (10) at non-singleton dimension 3
2025-03-16 17:49:12.518237 test begin: paddle.Tensor.masked_select(Tensor([10, 10, 10, 10, 10],"float64"), Tensor([10, 10, 228171, 10, 10],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([10, 10, 10, 10, 10],"float64"), Tensor([10, 10, 228171, 10, 10],"bool"), ) 
 The size of tensor a (228171) must match the size of tensor b (10) at non-singleton dimension 2
2025-03-16 17:49:12.998790 test begin: paddle.Tensor.masked_select(Tensor([10, 10, 10, 10, 10],"float64"), Tensor([10, 228171, 10, 10, 10],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([10, 10, 10, 10, 10],"float64"), Tensor([10, 228171, 10, 10, 10],"bool"), ) 
 The size of tensor a (228171) must match the size of tensor b (10) at non-singleton dimension 1
2025-03-16 17:49:13.585196 test begin: paddle.Tensor.masked_select(Tensor([10, 10, 10, 10, 10],"float64"), Tensor([228171, 10, 10, 10, 10],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([10, 10, 10, 10, 10],"float64"), Tensor([228171, 10, 10, 10, 10],"bool"), ) 
 The size of tensor a (228171) must match the size of tensor b (10) at non-singleton dimension 0
2025-03-16 17:49:13.932031 test begin: paddle.Tensor.masked_select(Tensor([10, 10, 10, 10, 429497],"float16"), Tensor([10, 10, 10, 10, 10],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([10, 10, 10, 10, 429497],"float16"), Tensor([10, 10, 10, 10, 10],"bool"), ) 
 The size of tensor a (10) must match the size of tensor b (429497) at non-singleton dimension 4
2025-03-16 17:49:17.889794 test begin: paddle.Tensor.masked_select(Tensor([10, 10, 10, 10, 429497],"float16"), Tensor([10, 10, 10, 10, 228171],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([10, 10, 10, 10, 429497],"float16"), Tensor([10, 10, 10, 10, 228171],"bool"), ) 
 The size of tensor a (228171) must match the size of tensor b (429497) at non-singleton dimension 4
2025-03-16 17:49:20.695891 test begin: paddle.Tensor.masked_select(Tensor([10, 10, 10, 429497, 10],"float16"), Tensor([10, 10, 10, 10, 10],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([10, 10, 10, 429497, 10],"float16"), Tensor([10, 10, 10, 10, 10],"bool"), ) 
 The size of tensor a (10) must match the size of tensor b (429497) at non-singleton dimension 3
2025-03-16 17:49:22.636974 test begin: paddle.Tensor.masked_select(Tensor([10, 10, 10, 429497, 10],"float16"), Tensor([10, 10, 10, 228171, 10],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([10, 10, 10, 429497, 10],"float16"), Tensor([10, 10, 10, 228171, 10],"bool"), ) 
 The size of tensor a (228171) must match the size of tensor b (429497) at non-singleton dimension 3
2025-03-16 17:49:25.574681 test begin: paddle.Tensor.masked_select(Tensor([10, 10, 429497, 10, 10],"float16"), Tensor([10, 10, 10, 10, 10],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([10, 10, 429497, 10, 10],"float16"), Tensor([10, 10, 10, 10, 10],"bool"), ) 
 The size of tensor a (10) must match the size of tensor b (429497) at non-singleton dimension 2
2025-03-16 17:49:28.046159 test begin: paddle.Tensor.masked_select(Tensor([10, 10, 429497, 10, 10],"float16"), Tensor([10, 10, 228171, 10, 10],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([10, 10, 429497, 10, 10],"float16"), Tensor([10, 10, 228171, 10, 10],"bool"), ) 
 The size of tensor a (228171) must match the size of tensor b (429497) at non-singleton dimension 2
2025-03-16 17:49:30.935906 test begin: paddle.Tensor.masked_select(Tensor([10, 429497, 10, 10, 10],"float16"), Tensor([10, 10, 10, 10, 10],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([10, 429497, 10, 10, 10],"float16"), Tensor([10, 10, 10, 10, 10],"bool"), ) 
 The size of tensor a (10) must match the size of tensor b (429497) at non-singleton dimension 1
2025-03-16 17:49:32.860095 test begin: paddle.Tensor.masked_select(Tensor([10, 429497, 10, 10, 10],"float16"), Tensor([10, 228171, 10, 10, 10],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([10, 429497, 10, 10, 10],"float16"), Tensor([10, 228171, 10, 10, 10],"bool"), ) 
 The size of tensor a (228171) must match the size of tensor b (429497) at non-singleton dimension 1
2025-03-16 17:49:35.875866 test begin: paddle.Tensor.masked_select(Tensor([10],"int32"), Tensor([2281701379],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([10],"int32"), Tensor([2281701379],"bool"), ) 
 The size of tensor a (2281701379) must match the size of tensor b (10) at non-singleton dimension 0
2025-03-16 17:49:36.386716 test begin: paddle.Tensor.masked_select(Tensor([1140850690, 2],"float32"), Tensor([1140850690, 2],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([1140850690, 2],"float32"), Tensor([1140850690, 2],"bool"), ) 
 nonzero is not supported for tensors with more than INT_MAX elements,   See https://github.com/pytorch/pytorch/issues/51871
2025-03-16 17:49:42.665202 test begin: paddle.Tensor.masked_select(Tensor([1140850690, 2],"float32"), Tensor([15000, 2],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([1140850690, 2],"float32"), Tensor([15000, 2],"bool"), ) 
 The size of tensor a (15000) must match the size of tensor b (1140850690) at non-singleton dimension 0
2025-03-16 17:49:44.256165 test begin: paddle.Tensor.masked_select(Tensor([1140850690, 2],"float32"), Tensor([3750, 2],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([1140850690, 2],"float32"), Tensor([3750, 2],"bool"), ) 
 The size of tensor a (3750) must match the size of tensor b (1140850690) at non-singleton dimension 0
2025-03-16 17:49:46.181933 test begin: paddle.Tensor.masked_select(Tensor([1140850690, 2],"float32"), Tensor([60000, 2],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([1140850690, 2],"float32"), Tensor([60000, 2],"bool"), ) 
 The size of tensor a (60000) must match the size of tensor b (1140850690) at non-singleton dimension 0
2025-03-16 17:49:48.504232 test begin: paddle.Tensor.masked_select(Tensor([128],"int64"), Tensor([2281701379],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([128],"int64"), Tensor([2281701379],"bool"), ) 
 The size of tensor a (2281701379) must match the size of tensor b (128) at non-singleton dimension 0
2025-03-16 17:49:48.860578 test begin: paddle.Tensor.masked_select(Tensor([13, 10969719, 16],"float32"), Tensor([13, 10969719, 16],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([13, 10969719, 16],"float32"), Tensor([13, 10969719, 16],"bool"), ) 
 nonzero is not supported for tensors with more than INT_MAX elements,   See https://github.com/pytorch/pytorch/issues/51871
2025-03-16 17:49:50.924336 test begin: paddle.Tensor.masked_select(Tensor([13, 10969719, 16],"float32"), Tensor([13, 7, 16],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([13, 10969719, 16],"float32"), Tensor([13, 7, 16],"bool"), ) 
 The size of tensor a (7) must match the size of tensor b (10969719) at non-singleton dimension 1
2025-03-16 17:49:53.260969 test begin: paddle.Tensor.masked_select(Tensor([13, 7, 16],"float32"), Tensor([13, 10969719, 16],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([13, 7, 16],"float32"), Tensor([13, 10969719, 16],"bool"), ) 
 The size of tensor a (10969719) must match the size of tensor b (7) at non-singleton dimension 1
2025-03-16 17:49:53.849270 test begin: paddle.Tensor.masked_select(Tensor([13, 7, 16],"float32"), Tensor([13, 7, 25073642],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([13, 7, 16],"float32"), Tensor([13, 7, 25073642],"bool"), ) 
 The size of tensor a (25073642) must match the size of tensor b (16) at non-singleton dimension 2
2025-03-16 17:49:54.529842 test begin: paddle.Tensor.masked_select(Tensor([13, 7, 16],"float32"), Tensor([20372334, 7, 16],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([13, 7, 16],"float32"), Tensor([20372334, 7, 16],"bool"), ) 
 The size of tensor a (20372334) must match the size of tensor b (13) at non-singleton dimension 0
2025-03-16 17:49:55.034269 test begin: paddle.Tensor.masked_select(Tensor([13, 7, 24],"float32"), Tensor([13, 7, 25073642],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([13, 7, 24],"float32"), Tensor([13, 7, 25073642],"bool"), ) 
 The size of tensor a (25073642) must match the size of tensor b (24) at non-singleton dimension 2
2025-03-16 17:49:55.377975 test begin: paddle.Tensor.masked_select(Tensor([13, 7, 24],"float32"), Tensor([13, 7313146, 24],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([13, 7, 24],"float32"), Tensor([13, 7313146, 24],"bool"), ) 
 The size of tensor a (7313146) must match the size of tensor b (7) at non-singleton dimension 1
2025-03-16 17:49:55.957148 test begin: paddle.Tensor.masked_select(Tensor([13, 7, 24],"float32"), Tensor([13581556, 7, 24],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([13, 7, 24],"float32"), Tensor([13581556, 7, 24],"bool"), ) 
 The size of tensor a (13581556) must match the size of tensor b (13) at non-singleton dimension 0
2025-03-16 17:49:56.301579 test begin: paddle.Tensor.masked_select(Tensor([13, 7, 25073642],"float32"), Tensor([13, 7, 16],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([13, 7, 25073642],"float32"), Tensor([13, 7, 16],"bool"), ) 
 The size of tensor a (16) must match the size of tensor b (25073642) at non-singleton dimension 2
2025-03-16 17:49:58.326719 test begin: paddle.Tensor.masked_select(Tensor([13, 7, 25073642],"float32"), Tensor([13, 7, 24],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([13, 7, 25073642],"float32"), Tensor([13, 7, 24],"bool"), ) 
 The size of tensor a (24) must match the size of tensor b (25073642) at non-singleton dimension 2
2025-03-16 17:49:59.718819 test begin: paddle.Tensor.masked_select(Tensor([13, 7, 25073642],"float32"), Tensor([13, 7, 25073642],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([13, 7, 25073642],"float32"), Tensor([13, 7, 25073642],"bool"), ) 
 nonzero is not supported for tensors with more than INT_MAX elements,   See https://github.com/pytorch/pytorch/issues/51871
2025-03-16 17:50:02.623861 test begin: paddle.Tensor.masked_select(Tensor([13, 7313146, 24],"float32"), Tensor([13, 7, 24],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([13, 7313146, 24],"float32"), Tensor([13, 7, 24],"bool"), ) 
 The size of tensor a (7) must match the size of tensor b (7313146) at non-singleton dimension 1
2025-03-16 17:50:04.663330 test begin: paddle.Tensor.masked_select(Tensor([13, 7313146, 24],"float32"), Tensor([13, 7313146, 24],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([13, 7313146, 24],"float32"), Tensor([13, 7313146, 24],"bool"), ) 
 nonzero is not supported for tensors with more than INT_MAX elements,   See https://github.com/pytorch/pytorch/issues/51871
2025-03-16 17:50:07.219860 test begin: paddle.Tensor.masked_select(Tensor([13581556, 7, 24],"float32"), Tensor([13, 7, 24],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([13581556, 7, 24],"float32"), Tensor([13, 7, 24],"bool"), ) 
 The size of tensor a (13) must match the size of tensor b (13581556) at non-singleton dimension 0
2025-03-16 17:50:09.249471 test begin: paddle.Tensor.masked_select(Tensor([13581556, 7, 24],"float32"), Tensor([13581556, 7, 24],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([13581556, 7, 24],"float32"), Tensor([13581556, 7, 24],"bool"), ) 
 nonzero is not supported for tensors with more than INT_MAX elements,   See https://github.com/pytorch/pytorch/issues/51871
2025-03-16 17:50:11.804998 test begin: paddle.Tensor.masked_select(Tensor([15000, 152114],"float32"), Tensor([15000, 152114],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([15000, 152114],"float32"), Tensor([15000, 152114],"bool"), ) 
 nonzero is not supported for tensors with more than INT_MAX elements,   See https://github.com/pytorch/pytorch/issues/51871
2025-03-16 17:50:13.512056 test begin: paddle.Tensor.masked_select(Tensor([15000, 152114],"float32"), Tensor([15000, 2],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([15000, 152114],"float32"), Tensor([15000, 2],"bool"), ) 
 The size of tensor a (2) must match the size of tensor b (152114) at non-singleton dimension 1
2025-03-16 17:50:15.890229 test begin: paddle.Tensor.masked_select(Tensor([15000, 152114],"float32"), Tensor([15000, 50],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([15000, 152114],"float32"), Tensor([15000, 50],"bool"), ) 
 The size of tensor a (50) must match the size of tensor b (152114) at non-singleton dimension 1
2025-03-16 17:50:17.257425 test begin: paddle.Tensor.masked_select(Tensor([15000, 2],"float32"), Tensor([1140850690, 2],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([15000, 2],"float32"), Tensor([1140850690, 2],"bool"), ) 
 The size of tensor a (1140850690) must match the size of tensor b (15000) at non-singleton dimension 0
2025-03-16 17:50:17.761652 test begin: paddle.Tensor.masked_select(Tensor([15000, 2],"float32"), Tensor([15000, 152114],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([15000, 2],"float32"), Tensor([15000, 152114],"bool"), ) 
 The size of tensor a (152114) must match the size of tensor b (2) at non-singleton dimension 1
2025-03-16 17:50:18.504424 test begin: paddle.Tensor.masked_select(Tensor([15000, 50],"float32"), Tensor([15000, 152114],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([15000, 50],"float32"), Tensor([15000, 152114],"bool"), ) 
 The size of tensor a (152114) must match the size of tensor b (50) at non-singleton dimension 1
2025-03-16 17:50:18.872333 test begin: paddle.Tensor.masked_select(Tensor([15000, 50],"float32"), Tensor([45634028, 50],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([15000, 50],"float32"), Tensor([45634028, 50],"bool"), ) 
 The size of tensor a (45634028) must match the size of tensor b (15000) at non-singleton dimension 0
2025-03-16 17:50:19.360248 test begin: paddle.Tensor.masked_select(Tensor([15000],"float32"), Tensor([2281701379],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([15000],"float32"), Tensor([2281701379],"bool"), ) 
 The size of tensor a (2281701379) must match the size of tensor b (15000) at non-singleton dimension 0
2025-03-16 17:50:19.947064 test begin: paddle.Tensor.masked_select(Tensor([20372334, 7, 16],"float32"), Tensor([13, 7, 16],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([20372334, 7, 16],"float32"), Tensor([13, 7, 16],"bool"), ) 
 The size of tensor a (13) must match the size of tensor b (20372334) at non-singleton dimension 0
2025-03-16 17:50:21.901304 test begin: paddle.Tensor.masked_select(Tensor([20372334, 7, 16],"float32"), Tensor([20372334, 7, 16],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([20372334, 7, 16],"float32"), Tensor([20372334, 7, 16],"bool"), ) 
 nonzero is not supported for tensors with more than INT_MAX elements,   See https://github.com/pytorch/pytorch/issues/51871
2025-03-16 17:50:24.760843 test begin: paddle.Tensor.masked_select(Tensor([20],"float32"), Tensor([2281701379],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([20],"float32"), Tensor([2281701379],"bool"), ) 
 The size of tensor a (2281701379) must match the size of tensor b (20) at non-singleton dimension 0
2025-03-16 17:50:25.284355 test begin: paddle.Tensor.masked_select(Tensor([2112],"float32"), Tensor([2281701379],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([2112],"float32"), Tensor([2281701379],"bool"), ) 
 The size of tensor a (2281701379) must match the size of tensor b (2112) at non-singleton dimension 0
2025-03-16 17:50:25.849200 test begin: paddle.Tensor.masked_select(Tensor([2281701379, 1],"float32"), Tensor([1, 1],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([2281701379, 1],"float32"), Tensor([1, 1],"bool"), ) 
 nonzero is not supported for tensors with more than INT_MAX elements,   See https://github.com/pytorch/pytorch/issues/51871
2025-03-16 17:50:27.274627 test begin: paddle.Tensor.masked_select(Tensor([2281701379, 1],"float32"), Tensor([2281701379, 1],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([2281701379, 1],"float32"), Tensor([2281701379, 1],"bool"), ) 
 nonzero is not supported for tensors with more than INT_MAX elements,   See https://github.com/pytorch/pytorch/issues/51871
2025-03-16 17:50:30.196406 test begin: paddle.Tensor.masked_select(Tensor([2281701379],"float32"), Tensor([15000],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([2281701379],"float32"), Tensor([15000],"bool"), ) 
 The size of tensor a (15000) must match the size of tensor b (2281701379) at non-singleton dimension 0
2025-03-16 17:50:31.648678 test begin: paddle.Tensor.masked_select(Tensor([2281701379],"float32"), Tensor([20],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([2281701379],"float32"), Tensor([20],"bool"), ) 
 The size of tensor a (20) must match the size of tensor b (2281701379) at non-singleton dimension 0
2025-03-16 17:50:34.010648 test begin: paddle.Tensor.masked_select(Tensor([2281701379],"float32"), Tensor([2112],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([2281701379],"float32"), Tensor([2112],"bool"), ) 
 The size of tensor a (2112) must match the size of tensor b (2281701379) at non-singleton dimension 0
2025-03-16 17:50:36.079419 test begin: paddle.Tensor.masked_select(Tensor([2281701379],"float32"), Tensor([2281701379],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([2281701379],"float32"), Tensor([2281701379],"bool"), ) 
 nonzero is not supported for tensors with more than INT_MAX elements,   See https://github.com/pytorch/pytorch/issues/51871
2025-03-16 17:50:38.622381 test begin: paddle.Tensor.masked_select(Tensor([2281701379],"int32"), Tensor([10],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([2281701379],"int32"), Tensor([10],"bool"), ) 
 The size of tensor a (10) must match the size of tensor b (2281701379) at non-singleton dimension 0
2025-03-16 17:50:42.466543 test begin: paddle.Tensor.masked_select(Tensor([2281701379],"int32"), Tensor([2281701379],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([2281701379],"int32"), Tensor([2281701379],"bool"), ) 
 nonzero is not supported for tensors with more than INT_MAX elements,   See https://github.com/pytorch/pytorch/issues/51871
2025-03-16 17:50:45.010462 test begin: paddle.Tensor.masked_select(Tensor([2281701379],"int64"), Tensor([128],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([2281701379],"int64"), Tensor([128],"bool"), ) 
 The size of tensor a (128) must match the size of tensor b (2281701379) at non-singleton dimension 0
2025-03-16 17:51:03.990463 test begin: paddle.Tensor.masked_select(Tensor([2281701379],"int64"), Tensor([2281701379],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([2281701379],"int64"), Tensor([2281701379],"bool"), ) 
 nonzero is not supported for tensors with more than INT_MAX elements,   See https://github.com/pytorch/pytorch/issues/51871
2025-03-16 17:51:08.886606 test begin: paddle.Tensor.masked_select(Tensor([3750, 2],"float32"), Tensor([1140850690, 2],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([3750, 2],"float32"), Tensor([1140850690, 2],"bool"), ) 
 The size of tensor a (1140850690) must match the size of tensor b (3750) at non-singleton dimension 0
2025-03-16 17:51:09.252893 test begin: paddle.Tensor.masked_select(Tensor([3750, 2],"float32"), Tensor([3750, 608454],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([3750, 2],"float32"), Tensor([3750, 608454],"bool"), ) 
 The size of tensor a (608454) must match the size of tensor b (2) at non-singleton dimension 1
2025-03-16 17:51:10.097305 test begin: paddle.Tensor.masked_select(Tensor([3750, 608454],"float32"), Tensor([3750, 2],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([3750, 608454],"float32"), Tensor([3750, 2],"bool"), ) 
 The size of tensor a (2) must match the size of tensor b (608454) at non-singleton dimension 1
2025-03-16 17:51:11.491854 test begin: paddle.Tensor.masked_select(Tensor([3750, 608454],"float32"), Tensor([3750, 608454],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([3750, 608454],"float32"), Tensor([3750, 608454],"bool"), ) 
 nonzero is not supported for tensors with more than INT_MAX elements,   See https://github.com/pytorch/pytorch/issues/51871
2025-03-16 17:51:14.409965 test begin: paddle.Tensor.masked_select(Tensor([429497, 10, 10, 10, 10],"float16"), Tensor([10, 10, 10, 10, 10],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([429497, 10, 10, 10, 10],"float16"), Tensor([10, 10, 10, 10, 10],"bool"), ) 
 The size of tensor a (10) must match the size of tensor b (429497) at non-singleton dimension 0
2025-03-16 17:51:16.294935 test begin: paddle.Tensor.masked_select(Tensor([429497, 10, 10, 10, 10],"float16"), Tensor([228171, 10, 10, 10, 10],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([429497, 10, 10, 10, 10],"float16"), Tensor([228171, 10, 10, 10, 10],"bool"), ) 
 The size of tensor a (228171) must match the size of tensor b (429497) at non-singleton dimension 0
2025-03-16 17:51:18.398183 test begin: paddle.Tensor.masked_select(Tensor([45634028, 50],"float32"), Tensor([15000, 50],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([45634028, 50],"float32"), Tensor([15000, 50],"bool"), ) 
 The size of tensor a (15000) must match the size of tensor b (45634028) at non-singleton dimension 0
2025-03-16 17:51:19.772323 test begin: paddle.Tensor.masked_select(Tensor([45634028, 50],"float32"), Tensor([45634028, 50],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([45634028, 50],"float32"), Tensor([45634028, 50],"bool"), ) 
 nonzero is not supported for tensors with more than INT_MAX elements,   See https://github.com/pytorch/pytorch/issues/51871
2025-03-16 17:51:22.219331 test begin: paddle.Tensor.masked_select(Tensor([60000, 2],"float32"), Tensor([1140850690, 2],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([60000, 2],"float32"), Tensor([1140850690, 2],"bool"), ) 
 The size of tensor a (1140850690) must match the size of tensor b (60000) at non-singleton dimension 0
2025-03-16 17:51:22.717492 test begin: paddle.Tensor.masked_select(Tensor([60000, 2],"float32"), Tensor([60000, 38029],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([60000, 2],"float32"), Tensor([60000, 38029],"bool"), ) 
 The size of tensor a (38029) must match the size of tensor b (2) at non-singleton dimension 1
2025-03-16 17:51:23.475407 test begin: paddle.Tensor.masked_select(Tensor([60000, 38029],"float32"), Tensor([60000, 2],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([60000, 38029],"float32"), Tensor([60000, 2],"bool"), ) 
 The size of tensor a (2) must match the size of tensor b (38029) at non-singleton dimension 1
2025-03-16 17:51:25.849234 test begin: paddle.Tensor.masked_select(Tensor([60000, 38029],"float32"), Tensor([60000, 38029],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([60000, 38029],"float32"), Tensor([60000, 38029],"bool"), ) 
 nonzero is not supported for tensors with more than INT_MAX elements,   See https://github.com/pytorch/pytorch/issues/51871
2025-03-16 17:51:28.389136 test begin: paddle.Tensor.masked_select(Tensor([6482107, 22, 16],"float32"), Tensor([1, 22, 16],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([6482107, 22, 16],"float32"), Tensor([1, 22, 16],"bool"), ) 
 nonzero is not supported for tensors with more than INT_MAX elements,   See https://github.com/pytorch/pytorch/issues/51871
2025-03-16 17:51:30.430303 test begin: paddle.Tensor.masked_select(Tensor([6482107, 22, 16],"float32"), Tensor([6482107, 22, 16],"bool"), )

[torch error] paddle.Tensor.masked_select(Tensor([6482107, 22, 16],"float32"), Tensor([6482107, 22, 16],"bool"), ) 
 nonzero is not supported for tensors with more than INT_MAX elements,   See https://github.com/pytorch/pytorch/issues/51871
2025-03-16 17:51:32.959288 test begin: paddle.Tensor.matmul(Tensor([1, 100, 12],"float16"), Tensor([1, 107374183, 40],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 100, 12],"float16"), Tensor([1, 107374183, 40],"float16"), ) 
 Expected size for first two dimensions of batch2 tensor to be: [1, 12] but got: [1, 107374183].
2025-03-16 17:51:35.307527 test begin: paddle.Tensor.matmul(Tensor([1, 100, 12],"float16"), Tensor([1, 12, 357913942],"float16"), )

2025-03-16 17:51:38.087734 test begin: paddle.Tensor.matmul(Tensor([1, 100, 12],"float16"), Tensor([1, 357913942, 12],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 100, 12],"float16"), Tensor([1, 357913942, 12],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.098537 test begin: paddle.Tensor.matmul(Tensor([1, 100, 12],"float16"), Tensor([29826162, 12, 12],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 100, 12],"float16"), Tensor([29826162, 12, 12],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.109927 test begin: paddle.Tensor.matmul(Tensor([1, 100, 12],"float16"), Tensor([8947849, 12, 40],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 100, 12],"float16"), Tensor([8947849, 12, 40],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.119859 test begin: paddle.Tensor.matmul(Tensor([1, 100, 1],"float16"), Tensor([1, 1, 4294967297],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 100, 1],"float16"), Tensor([1, 1, 4294967297],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.129859 test begin: paddle.Tensor.matmul(Tensor([1, 100, 1],"float16"), Tensor([1, 1073741825, 4],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 100, 1],"float16"), Tensor([1, 1073741825, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.139758 test begin: paddle.Tensor.matmul(Tensor([1, 100, 1],"float16"), Tensor([1, 107374183, 40],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 100, 1],"float16"), Tensor([1, 107374183, 40],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.149610 test begin: paddle.Tensor.matmul(Tensor([1, 100, 1],"float16"), Tensor([1073741825, 1, 4],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 100, 1],"float16"), Tensor([1073741825, 1, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.160174 test begin: paddle.Tensor.matmul(Tensor([1, 100, 1],"float16"), Tensor([107374183, 1, 40],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 100, 1],"float16"), Tensor([107374183, 1, 40],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.170342 test begin: paddle.Tensor.matmul(Tensor([1, 100, 2],"float16"), Tensor([1, 2, 2147483649],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 100, 2],"float16"), Tensor([1, 2, 2147483649],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.180170 test begin: paddle.Tensor.matmul(Tensor([1, 100, 2],"float16"), Tensor([1, 2147483649, 2],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 100, 2],"float16"), Tensor([1, 2147483649, 2],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.189939 test begin: paddle.Tensor.matmul(Tensor([1, 100, 2],"float16"), Tensor([1073741825, 2, 2],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 100, 2],"float16"), Tensor([1073741825, 2, 2],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.199652 test begin: paddle.Tensor.matmul(Tensor([1, 100, 42949673],"float16"), Tensor([1, 1, 40],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 100, 42949673],"float16"), Tensor([1, 1, 40],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.209281 test begin: paddle.Tensor.matmul(Tensor([1, 100, 42949673],"float16"), Tensor([1, 1, 42949673],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 100, 42949673],"float16"), Tensor([1, 1, 42949673],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.226138 test begin: paddle.Tensor.matmul(Tensor([1, 100, 42949673],"float16"), Tensor([1, 1, 4],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 100, 42949673],"float16"), Tensor([1, 1, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.235690 test begin: paddle.Tensor.matmul(Tensor([1, 100, 42949673],"float16"), Tensor([1, 12, 12],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 100, 42949673],"float16"), Tensor([1, 12, 12],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.245984 test begin: paddle.Tensor.matmul(Tensor([1, 100, 42949673],"float16"), Tensor([1, 12, 40],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 100, 42949673],"float16"), Tensor([1, 12, 40],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.255778 test begin: paddle.Tensor.matmul(Tensor([1, 100, 42949673],"float16"), Tensor([1, 12, 42949673],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 100, 42949673],"float16"), Tensor([1, 12, 42949673],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.265648 test begin: paddle.Tensor.matmul(Tensor([1, 100, 42949673],"float16"), Tensor([1, 2, 2],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 100, 42949673],"float16"), Tensor([1, 2, 2],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.275608 test begin: paddle.Tensor.matmul(Tensor([1, 100, 42949673],"float16"), Tensor([1, 2, 42949673],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 100, 42949673],"float16"), Tensor([1, 2, 42949673],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.285808 test begin: paddle.Tensor.matmul(Tensor([1, 10],"float16"), Tensor([10, 429496730],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 10],"float16"), Tensor([10, 429496730],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.295679 test begin: paddle.Tensor.matmul(Tensor([1, 10],"float16"), Tensor([4294967297, 1],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 10],"float16"), Tensor([4294967297, 1],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.305437 test begin: paddle.Tensor.matmul(Tensor([1, 125],"float16"), Tensor([125, 34359739],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 125],"float16"), Tensor([125, 34359739],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.315048 test begin: paddle.Tensor.matmul(Tensor([1, 125],"float16"), Tensor([4294967297, 1],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 125],"float16"), Tensor([4294967297, 1],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.324567 test begin: paddle.Tensor.matmul(Tensor([1, 1],"float16"), Tensor([1, 4294967297],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 1],"float16"), Tensor([1, 4294967297],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.334226 test begin: paddle.Tensor.matmul(Tensor([1, 1],"float16"), Tensor([4294967297, 1],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 1],"float16"), Tensor([4294967297, 1],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.343814 test begin: paddle.Tensor.matmul(Tensor([1, 1],"float16"), Tensor([42949673, 100],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 1],"float16"), Tensor([42949673, 100],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.353370 test begin: paddle.Tensor.matmul(Tensor([1, 1],"float16"), Tensor([858993460, 5],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 1],"float16"), Tensor([858993460, 5],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.363288 test begin: paddle.Tensor.matmul(Tensor([1, 2147483649, 2],"float16"), Tensor([1, 2, 2],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 2147483649, 2],"float16"), Tensor([1, 2, 2],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.372867 test begin: paddle.Tensor.matmul(Tensor([1, 2147483649, 2],"float16"), Tensor([1, 2147483649, 2],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 2147483649, 2],"float16"), Tensor([1, 2147483649, 2],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.382308 test begin: paddle.Tensor.matmul(Tensor([1, 21504, 1, 106106],"float32"), Tensor([91],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 21504, 1, 106106],"float32"), Tensor([91],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.392167 test begin: paddle.Tensor.matmul(Tensor([1, 21504, 1, 91],"float32"), Tensor([2281701379],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 21504, 1, 91],"float32"), Tensor([2281701379],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.68 GiB is allocated by PyTorch, and 501.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.403670 test begin: paddle.Tensor.matmul(Tensor([1, 21504, 1166, 91],"float32"), Tensor([91],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 21504, 1166, 91],"float32"), Tensor([91],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.413253 test begin: paddle.Tensor.matmul(Tensor([1, 25073642, 1, 91],"float32"), Tensor([91],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 25073642, 1, 91],"float32"), Tensor([91],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.422861 test begin: paddle.Tensor.matmul(Tensor([1, 357913942, 12],"float16"), Tensor([1, 12, 12],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 357913942, 12],"float16"), Tensor([1, 12, 12],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.432309 test begin: paddle.Tensor.matmul(Tensor([1, 357913942, 12],"float16"), Tensor([1, 12, 40],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 357913942, 12],"float16"), Tensor([1, 12, 40],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.441694 test begin: paddle.Tensor.matmul(Tensor([1, 357913942, 12],"float16"), Tensor([1, 357913942, 12],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 357913942, 12],"float16"), Tensor([1, 357913942, 12],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.451096 test begin: paddle.Tensor.matmul(Tensor([1, 357913942, 12],"float16"), Tensor([1, 357913942, 40],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 357913942, 12],"float16"), Tensor([1, 357913942, 40],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.460705 test begin: paddle.Tensor.matmul(Tensor([1, 4294967297, 1],"float16"), Tensor([1, 1, 40],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 4294967297, 1],"float16"), Tensor([1, 1, 40],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.471849 test begin: paddle.Tensor.matmul(Tensor([1, 4294967297, 1],"float16"), Tensor([1, 1, 4],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 4294967297, 1],"float16"), Tensor([1, 1, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.482547 test begin: paddle.Tensor.matmul(Tensor([1, 4294967297, 1],"float16"), Tensor([1, 4294967297, 40],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 4294967297, 1],"float16"), Tensor([1, 4294967297, 40],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.493191 test begin: paddle.Tensor.matmul(Tensor([1, 4294967297, 1],"float16"), Tensor([1, 4294967297, 4],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 4294967297, 1],"float16"), Tensor([1, 4294967297, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.503725 test begin: paddle.Tensor.matmul(Tensor([1, 4294967297],"float16"), Tensor([1, 100],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 4294967297],"float16"), Tensor([1, 100],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.513817 test begin: paddle.Tensor.matmul(Tensor([1, 4294967297],"float16"), Tensor([1, 1],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 4294967297],"float16"), Tensor([1, 1],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.523939 test begin: paddle.Tensor.matmul(Tensor([1, 4294967297],"float16"), Tensor([1, 4294967297],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 4294967297],"float16"), Tensor([1, 4294967297],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.534084 test begin: paddle.Tensor.matmul(Tensor([1, 4294967297],"float16"), Tensor([1, 5],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 4294967297],"float16"), Tensor([1, 5],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.544100 test begin: paddle.Tensor.matmul(Tensor([1, 4294967297],"float16"), Tensor([10, 1],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 4294967297],"float16"), Tensor([10, 1],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.554263 test begin: paddle.Tensor.matmul(Tensor([1, 4294967297],"float16"), Tensor([10, 4294967297],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 4294967297],"float16"), Tensor([10, 4294967297],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.563715 test begin: paddle.Tensor.matmul(Tensor([1, 4294967297],"float16"), Tensor([125, 1],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 4294967297],"float16"), Tensor([125, 1],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.575840 test begin: paddle.Tensor.matmul(Tensor([1, 4294967297],"float16"), Tensor([125, 4294967297],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([1, 4294967297],"float16"), Tensor([125, 4294967297],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.586147 test begin: paddle.Tensor.matmul(Tensor([112, 12, 16222, 197],"float16"), Tensor([112, 12, 16222, 64],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 12, 16222, 197],"float16"), Tensor([112, 12, 16222, 64],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.596363 test begin: paddle.Tensor.matmul(Tensor([112, 12, 16222, 197],"float16"), Tensor([112, 12, 197, 64],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 12, 16222, 197],"float16"), Tensor([112, 12, 197, 64],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.606430 test begin: paddle.Tensor.matmul(Tensor([112, 12, 197, 16222],"float16"), Tensor([112, 12, 197, 16222],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 12, 197, 16222],"float16"), Tensor([112, 12, 197, 16222],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.616278 test begin: paddle.Tensor.matmul(Tensor([112, 12, 197, 16222],"float16"), Tensor([112, 12, 197, 64],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 12, 197, 16222],"float16"), Tensor([112, 12, 197, 64],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.628087 test begin: paddle.Tensor.matmul(Tensor([112, 12, 197, 16222],"float16"), Tensor([112, 12, 64, 16222],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 12, 197, 16222],"float16"), Tensor([112, 12, 64, 16222],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.639683 test begin: paddle.Tensor.matmul(Tensor([112, 12, 197, 16222],"float16"), Tensor([112, 12, 64, 197],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 12, 197, 16222],"float16"), Tensor([112, 12, 64, 197],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.652765 test begin: paddle.Tensor.matmul(Tensor([112, 12, 197, 197],"float16"), Tensor([112, 12, 197, 16222],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 12, 197, 197],"float16"), Tensor([112, 12, 197, 16222],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.77 GiB is allocated by PyTorch, and 409.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.691014 test begin: paddle.Tensor.matmul(Tensor([112, 12, 197, 197],"float16"), Tensor([112, 12, 49933, 64],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 12, 197, 197],"float16"), Tensor([112, 12, 49933, 64],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.77 GiB is allocated by PyTorch, and 409.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.728897 test begin: paddle.Tensor.matmul(Tensor([112, 12, 197, 197],"float16"), Tensor([112, 3042, 197, 64],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 12, 197, 197],"float16"), Tensor([112, 3042, 197, 64],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.77 GiB is allocated by PyTorch, and 409.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.756928 test begin: paddle.Tensor.matmul(Tensor([112, 12, 197, 197],"float16"), Tensor([28388, 12, 197, 64],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 12, 197, 197],"float16"), Tensor([28388, 12, 197, 64],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.77 GiB is allocated by PyTorch, and 409.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.778576 test begin: paddle.Tensor.matmul(Tensor([112, 12, 197, 197],"float32"), Tensor([112, 12, 197, 8618],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 12, 197, 197],"float32"), Tensor([112, 12, 197, 8618],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.87 GiB is allocated by PyTorch, and 310.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.819323 test begin: paddle.Tensor.matmul(Tensor([112, 12, 197, 197],"float32"), Tensor([112, 12, 26527, 64],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 12, 197, 197],"float32"), Tensor([112, 12, 26527, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.87 GiB is allocated by PyTorch, and 310.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.874127 test begin: paddle.Tensor.matmul(Tensor([112, 12, 197, 197],"float32"), Tensor([112, 1616, 197, 64],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 12, 197, 197],"float32"), Tensor([112, 1616, 197, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.87 GiB is allocated by PyTorch, and 310.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.928283 test begin: paddle.Tensor.matmul(Tensor([112, 12, 197, 197],"float32"), Tensor([15082, 12, 197, 64],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 12, 197, 197],"float32"), Tensor([15082, 12, 197, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.87 GiB is allocated by PyTorch, and 310.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.982062 test begin: paddle.Tensor.matmul(Tensor([112, 12, 197, 64],"float16"), Tensor([112, 12, 16222, 197],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 12, 197, 64],"float16"), Tensor([112, 12, 16222, 197],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.71 GiB is allocated by PyTorch, and 476.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:38.995526 test begin: paddle.Tensor.matmul(Tensor([112, 12, 197, 64],"float16"), Tensor([112, 12, 64, 49933],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 12, 197, 64],"float16"), Tensor([112, 12, 64, 49933],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.71 GiB is allocated by PyTorch, and 476.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.008926 test begin: paddle.Tensor.matmul(Tensor([112, 12, 197, 64],"float16"), Tensor([112, 3042, 64, 197],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 12, 197, 64],"float16"), Tensor([112, 3042, 64, 197],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.71 GiB is allocated by PyTorch, and 476.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.027002 test begin: paddle.Tensor.matmul(Tensor([112, 12, 197, 64],"float16"), Tensor([28388, 12, 64, 197],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 12, 197, 64],"float16"), Tensor([28388, 12, 64, 197],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.71 GiB is allocated by PyTorch, and 476.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.046021 test begin: paddle.Tensor.matmul(Tensor([112, 12, 197, 64],"float32"), Tensor([112, 12, 64, 26527],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 12, 197, 64],"float32"), Tensor([112, 12, 64, 26527],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.74 GiB is allocated by PyTorch, and 444.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.073226 test begin: paddle.Tensor.matmul(Tensor([112, 12, 197, 64],"float32"), Tensor([112, 12, 8618, 197],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 12, 197, 64],"float32"), Tensor([112, 12, 8618, 197],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.74 GiB is allocated by PyTorch, and 444.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.098018 test begin: paddle.Tensor.matmul(Tensor([112, 12, 197, 64],"float32"), Tensor([112, 1616, 64, 197],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 12, 197, 64],"float32"), Tensor([112, 1616, 64, 197],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.74 GiB is allocated by PyTorch, and 444.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.122835 test begin: paddle.Tensor.matmul(Tensor([112, 12, 197, 64],"float32"), Tensor([15082, 12, 64, 197],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 12, 197, 64],"float32"), Tensor([15082, 12, 64, 197],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.74 GiB is allocated by PyTorch, and 444.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.147308 test begin: paddle.Tensor.matmul(Tensor([112, 12, 197, 8618],"float32"), Tensor([112, 12, 197, 64],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 12, 197, 8618],"float32"), Tensor([112, 12, 197, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.156716 test begin: paddle.Tensor.matmul(Tensor([112, 12, 197, 8618],"float32"), Tensor([112, 12, 197, 8618],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 12, 197, 8618],"float32"), Tensor([112, 12, 197, 8618],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.166068 test begin: paddle.Tensor.matmul(Tensor([112, 12, 197, 8618],"float32"), Tensor([112, 12, 64, 197],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 12, 197, 8618],"float32"), Tensor([112, 12, 64, 197],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.175408 test begin: paddle.Tensor.matmul(Tensor([112, 12, 197, 8618],"float32"), Tensor([112, 12, 64, 8618],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 12, 197, 8618],"float32"), Tensor([112, 12, 64, 8618],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.184713 test begin: paddle.Tensor.matmul(Tensor([112, 12, 26527, 64],"float32"), Tensor([112, 12, 26527, 197],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 12, 26527, 64],"float32"), Tensor([112, 12, 26527, 197],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.193988 test begin: paddle.Tensor.matmul(Tensor([112, 12, 26527, 64],"float32"), Tensor([112, 12, 64, 197],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 12, 26527, 64],"float32"), Tensor([112, 12, 64, 197],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.203267 test begin: paddle.Tensor.matmul(Tensor([112, 12, 49933, 64],"float16"), Tensor([112, 12, 49933, 197],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 12, 49933, 64],"float16"), Tensor([112, 12, 49933, 197],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.212527 test begin: paddle.Tensor.matmul(Tensor([112, 12, 49933, 64],"float16"), Tensor([112, 12, 64, 197],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 12, 49933, 64],"float16"), Tensor([112, 12, 64, 197],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.221981 test begin: paddle.Tensor.matmul(Tensor([112, 12, 8618, 197],"float32"), Tensor([112, 12, 197, 64],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 12, 8618, 197],"float32"), Tensor([112, 12, 197, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.231481 test begin: paddle.Tensor.matmul(Tensor([112, 12, 8618, 197],"float32"), Tensor([112, 12, 8618, 64],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 12, 8618, 197],"float32"), Tensor([112, 12, 8618, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.241004 test begin: paddle.Tensor.matmul(Tensor([112, 1616, 197, 64],"float32"), Tensor([112, 12, 64, 197],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 1616, 197, 64],"float32"), Tensor([112, 12, 64, 197],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.250456 test begin: paddle.Tensor.matmul(Tensor([112, 1616, 197, 64],"float32"), Tensor([112, 1616, 64, 197],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 1616, 197, 64],"float32"), Tensor([112, 1616, 64, 197],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.259868 test begin: paddle.Tensor.matmul(Tensor([112, 3042, 197, 64],"float16"), Tensor([112, 12, 64, 197],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 3042, 197, 64],"float16"), Tensor([112, 12, 64, 197],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.269308 test begin: paddle.Tensor.matmul(Tensor([112, 3042, 197, 64],"float16"), Tensor([112, 3042, 64, 197],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 3042, 197, 64],"float16"), Tensor([112, 3042, 64, 197],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.278846 test begin: paddle.Tensor.matmul(Tensor([112, 525, 197, 197],"float32"), Tensor([112, 12, 197, 64],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 525, 197, 197],"float32"), Tensor([112, 12, 197, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.289145 test begin: paddle.Tensor.matmul(Tensor([112, 525, 197, 197],"float32"), Tensor([112, 525, 197, 64],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 525, 197, 197],"float32"), Tensor([112, 525, 197, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.298911 test begin: paddle.Tensor.matmul(Tensor([112, 989, 197, 197],"float16"), Tensor([112, 12, 197, 64],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 989, 197, 197],"float16"), Tensor([112, 12, 197, 64],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.01 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.308590 test begin: paddle.Tensor.matmul(Tensor([112, 989, 197, 197],"float16"), Tensor([112, 989, 197, 64],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([112, 989, 197, 197],"float16"), Tensor([112, 989, 197, 64],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.01 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.318495 test begin: paddle.Tensor.matmul(Tensor([1166, 21504, 1, 91],"float32"), Tensor([91],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([1166, 21504, 1, 91],"float32"), Tensor([91],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.328096 test begin: paddle.Tensor.matmul(Tensor([124, 16, 257, 257],"float32"), Tensor([124, 1119, 257, 64],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([124, 16, 257, 257],"float32"), Tensor([124, 1119, 257, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 75.16 GiB is allocated by PyTorch, and 9.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.460191 test begin: paddle.Tensor.matmul(Tensor([124, 16, 257, 257],"float32"), Tensor([124, 16, 17970, 64],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([124, 16, 257, 257],"float32"), Tensor([124, 16, 17970, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 75.16 GiB is allocated by PyTorch, and 9.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.587041 test begin: paddle.Tensor.matmul(Tensor([124, 16, 257, 257],"float32"), Tensor([124, 16, 257, 4475],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([124, 16, 257, 257],"float32"), Tensor([124, 16, 257, 4475],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 75.16 GiB is allocated by PyTorch, and 9.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.714655 test begin: paddle.Tensor.matmul(Tensor([124, 16, 257, 257],"float32"), Tensor([8671, 16, 257, 64],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([124, 16, 257, 257],"float32"), Tensor([8671, 16, 257, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 75.16 GiB is allocated by PyTorch, and 9.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.803218 test begin: paddle.Tensor.matmul(Tensor([124, 16, 257, 4475],"float32"), Tensor([124, 16, 257, 4475],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([124, 16, 257, 4475],"float32"), Tensor([124, 16, 257, 4475],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.813055 test begin: paddle.Tensor.matmul(Tensor([124, 16, 257, 4475],"float32"), Tensor([124, 16, 257, 64],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([124, 16, 257, 4475],"float32"), Tensor([124, 16, 257, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.822584 test begin: paddle.Tensor.matmul(Tensor([124, 16, 4475, 257],"float32"), Tensor([124, 16, 257, 64],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([124, 16, 4475, 257],"float32"), Tensor([124, 16, 257, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.831969 test begin: paddle.Tensor.matmul(Tensor([124, 16, 4475, 257],"float32"), Tensor([124, 16, 4475, 64],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([124, 16, 4475, 257],"float32"), Tensor([124, 16, 4475, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.841608 test begin: paddle.Tensor.matmul(Tensor([124, 279, 257, 257],"float32"), Tensor([124, 16, 257, 64],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([124, 279, 257, 257],"float32"), Tensor([124, 16, 257, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.851105 test begin: paddle.Tensor.matmul(Tensor([124, 279, 257, 257],"float32"), Tensor([124, 279, 257, 64],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([124, 279, 257, 257],"float32"), Tensor([124, 279, 257, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.861267 test begin: paddle.Tensor.matmul(Tensor([15082, 12, 197, 64],"float32"), Tensor([112, 12, 64, 197],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([15082, 12, 197, 64],"float32"), Tensor([112, 12, 64, 197],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.871013 test begin: paddle.Tensor.matmul(Tensor([15082, 12, 197, 64],"float32"), Tensor([15082, 12, 64, 197],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([15082, 12, 197, 64],"float32"), Tensor([15082, 12, 64, 197],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.880733 test begin: paddle.Tensor.matmul(Tensor([21474837, 100, 2],"float16"), Tensor([1, 2, 2],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([21474837, 100, 2],"float16"), Tensor([1, 2, 2],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.890101 test begin: paddle.Tensor.matmul(Tensor([21474837, 100, 2],"float16"), Tensor([21474837, 2, 2],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([21474837, 100, 2],"float16"), Tensor([21474837, 2, 2],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.899432 test begin: paddle.Tensor.matmul(Tensor([2160, 16, 257, 257],"float32"), Tensor([124, 16, 257, 64],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([2160, 16, 257, 257],"float32"), Tensor([124, 16, 257, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.908880 test begin: paddle.Tensor.matmul(Tensor([2160, 16, 257, 257],"float32"), Tensor([2160, 16, 257, 64],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([2160, 16, 257, 257],"float32"), Tensor([2160, 16, 257, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.918817 test begin: paddle.Tensor.matmul(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.928218 test begin: paddle.Tensor.matmul(Tensor([2281701379],"float32"), Tensor([4],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([2281701379],"float32"), Tensor([4],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.938343 test begin: paddle.Tensor.matmul(Tensor([28388, 12, 197, 64],"float16"), Tensor([112, 12, 64, 197],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([28388, 12, 197, 64],"float16"), Tensor([112, 12, 64, 197],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.947844 test begin: paddle.Tensor.matmul(Tensor([28388, 12, 197, 64],"float16"), Tensor([28388, 12, 64, 197],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([28388, 12, 197, 64],"float16"), Tensor([28388, 12, 64, 197],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.957415 test begin: paddle.Tensor.matmul(Tensor([34359739, 125],"float16"), Tensor([125, 1],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([34359739, 125],"float16"), Tensor([125, 1],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.967007 test begin: paddle.Tensor.matmul(Tensor([34359739, 125],"float16"), Tensor([34359739, 1],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([34359739, 125],"float16"), Tensor([34359739, 1],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.976514 test begin: paddle.Tensor.matmul(Tensor([3579140, 100, 12],"float16"), Tensor([1, 12, 12],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([3579140, 100, 12],"float16"), Tensor([1, 12, 12],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.985866 test begin: paddle.Tensor.matmul(Tensor([3579140, 100, 12],"float16"), Tensor([1, 12, 40],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([3579140, 100, 12],"float16"), Tensor([1, 12, 40],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:39.995224 test begin: paddle.Tensor.matmul(Tensor([3579140, 100, 12],"float16"), Tensor([3579140, 12, 12],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([3579140, 100, 12],"float16"), Tensor([3579140, 12, 12],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.004987 test begin: paddle.Tensor.matmul(Tensor([3579140, 100, 12],"float16"), Tensor([3579140, 12, 40],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([3579140, 100, 12],"float16"), Tensor([3579140, 12, 40],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.014867 test begin: paddle.Tensor.matmul(Tensor([4294967297, 1],"float16"), Tensor([1, 100],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([4294967297, 1],"float16"), Tensor([1, 100],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.024422 test begin: paddle.Tensor.matmul(Tensor([4294967297, 1],"float16"), Tensor([1, 1],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([4294967297, 1],"float16"), Tensor([1, 1],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.033849 test begin: paddle.Tensor.matmul(Tensor([4294967297, 1],"float16"), Tensor([1, 5],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([4294967297, 1],"float16"), Tensor([1, 5],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.043258 test begin: paddle.Tensor.matmul(Tensor([4294967297, 1],"float16"), Tensor([4294967297, 100],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([4294967297, 1],"float16"), Tensor([4294967297, 100],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.052637 test begin: paddle.Tensor.matmul(Tensor([4294967297, 1],"float16"), Tensor([4294967297, 1],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([4294967297, 1],"float16"), Tensor([4294967297, 1],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.061964 test begin: paddle.Tensor.matmul(Tensor([4294967297, 1],"float16"), Tensor([4294967297, 5],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([4294967297, 1],"float16"), Tensor([4294967297, 5],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.071354 test begin: paddle.Tensor.matmul(Tensor([42949673, 100, 1],"float16"), Tensor([1, 1, 40],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([42949673, 100, 1],"float16"), Tensor([1, 1, 40],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.080653 test begin: paddle.Tensor.matmul(Tensor([42949673, 100, 1],"float16"), Tensor([1, 1, 4],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([42949673, 100, 1],"float16"), Tensor([1, 1, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.090247 test begin: paddle.Tensor.matmul(Tensor([42949673, 100, 1],"float16"), Tensor([42949673, 1, 40],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([42949673, 100, 1],"float16"), Tensor([42949673, 1, 40],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.099727 test begin: paddle.Tensor.matmul(Tensor([42949673, 100, 1],"float16"), Tensor([42949673, 1, 4],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([42949673, 100, 1],"float16"), Tensor([42949673, 1, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.109207 test begin: paddle.Tensor.matmul(Tensor([429496730, 10],"float16"), Tensor([10, 1],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([429496730, 10],"float16"), Tensor([10, 1],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.118541 test begin: paddle.Tensor.matmul(Tensor([429496730, 10],"float16"), Tensor([429496730, 1],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([429496730, 10],"float16"), Tensor([429496730, 1],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.127822 test begin: paddle.Tensor.matmul(Tensor([4900, 12, 197, 197],"float32"), Tensor([112, 12, 197, 64],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([4900, 12, 197, 197],"float32"), Tensor([112, 12, 197, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.137112 test begin: paddle.Tensor.matmul(Tensor([4900, 12, 197, 197],"float32"), Tensor([4900, 12, 197, 64],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([4900, 12, 197, 197],"float32"), Tensor([4900, 12, 197, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.146876 test begin: paddle.Tensor.matmul(Tensor([4],"float32"), Tensor([2281701379],"float32"), )

[torch error] paddle.Tensor.matmul(Tensor([4],"float32"), Tensor([2281701379],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.156590 test begin: paddle.Tensor.matmul(Tensor([9223, 12, 197, 197],"float16"), Tensor([112, 12, 197, 64],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([9223, 12, 197, 197],"float16"), Tensor([112, 12, 197, 64],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.166110 test begin: paddle.Tensor.matmul(Tensor([9223, 12, 197, 197],"float16"), Tensor([9223, 12, 197, 64],"float16"), )

[torch error] paddle.Tensor.matmul(Tensor([9223, 12, 197, 197],"float16"), Tensor([9223, 12, 197, 64],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.175522 test begin: paddle.Tensor.mean(Tensor([1, 1, 1, 12, 1, 190141782],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1, 1, 1, 12, 1, 190141782],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.185084 test begin: paddle.Tensor.mean(Tensor([1, 1, 1, 12, 2970966, 64],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1, 1, 1, 12, 2970966, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.194488 test begin: paddle.Tensor.mean(Tensor([1, 1, 1, 35651585, 1, 64],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1, 1, 1, 35651585, 1, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.204178 test begin: paddle.Tensor.mean(Tensor([1, 1, 2281701379],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1, 1, 2281701379],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.213608 test begin: paddle.Tensor.mean(Tensor([1, 1, 2281701379],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 1, 2281701379],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.223188 test begin: paddle.Tensor.mean(Tensor([1, 1, 2281701379],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 1, 2281701379],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.232498 test begin: paddle.Tensor.mean(Tensor([1, 1, 2970966, 12, 1, 64],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1, 1, 2970966, 12, 1, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.242003 test begin: paddle.Tensor.mean(Tensor([1, 1, 45634028, 50],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1, 1, 45634028, 50],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.251424 test begin: paddle.Tensor.mean(Tensor([1, 1, 50, 45634028],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1, 1, 50, 45634028],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.260916 test begin: paddle.Tensor.mean(Tensor([1, 10, 228170138],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1, 10, 228170138],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.270389 test begin: paddle.Tensor.mean(Tensor([1, 10, 228170138],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 10, 228170138],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.279898 test begin: paddle.Tensor.mean(Tensor([1, 100, 22817014],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1, 100, 22817014],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.289314 test begin: paddle.Tensor.mean(Tensor([1, 100, 22817014],"float32"), 1, )

[torch error] paddle.Tensor.mean(Tensor([1, 100, 22817014],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.298645 test begin: paddle.Tensor.mean(Tensor([1, 100, 42949673],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 100, 42949673],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.307970 test begin: paddle.Tensor.mean(Tensor([1, 1024, 2228225],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 1024, 2228225],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.317299 test begin: paddle.Tensor.mean(Tensor([1, 1073741825, 4],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 1073741825, 4],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.326646 test begin: paddle.Tensor.mean(Tensor([1, 107374183, 40],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 107374183, 40],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.336069 test begin: paddle.Tensor.mean(Tensor([1, 110036, 144, 144],"float32"), tuple(2,3,), keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 110036, 144, 144],"float32"), tuple(2,3,), keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.345457 test begin: paddle.Tensor.mean(Tensor([1, 1140850690, 2],"float32"), 1, )

[torch error] paddle.Tensor.mean(Tensor([1, 1140850690, 2],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.354735 test begin: paddle.Tensor.mean(Tensor([1, 11883862, 192],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1, 11883862, 192],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.364296 test begin: paddle.Tensor.mean(Tensor([1, 12, 13581556, 14],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1, 12, 13581556, 14],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.373860 test begin: paddle.Tensor.mean(Tensor([1, 12, 8, 23767723],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1, 12, 8, 23767723],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.383315 test begin: paddle.Tensor.mean(Tensor([1, 123362, 136, 136],"float32"), tuple(2,3,), keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 123362, 136, 136],"float32"), tuple(2,3,), keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.392896 test begin: paddle.Tensor.mean(Tensor([1, 139265, 128, 128],"float32"), tuple(2,3,), keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 139265, 128, 128],"float32"), tuple(2,3,), keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.402495 test begin: paddle.Tensor.mean(Tensor([1, 15, 2376773, 64],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1, 15, 2376773, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.411983 test begin: paddle.Tensor.mean(Tensor([1, 15, 64, 2376773],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1, 15, 64, 2376773],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.421396 test begin: paddle.Tensor.mean(Tensor([1, 15018, 151936],"float32"), -1, )

[torch error] paddle.Tensor.mean(Tensor([1, 15018, 151936],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.430874 test begin: paddle.Tensor.mean(Tensor([1, 17, 252645136],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 17, 252645136],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.440241 test begin: paddle.Tensor.mean(Tensor([1, 17825793, 128],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 17825793, 128],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.449634 test begin: paddle.Tensor.mean(Tensor([1, 192, 11883862],"float32"), 1, )

[torch error] paddle.Tensor.mean(Tensor([1, 192, 11883862],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.458974 test begin: paddle.Tensor.mean(Tensor([1, 192, 128, 92843],"float32"), tuple(2,3,), keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 192, 128, 92843],"float32"), tuple(2,3,), keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.468360 test begin: paddle.Tensor.mean(Tensor([1, 192, 136, 87382],"float32"), tuple(2,3,), keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 192, 136, 87382],"float32"), tuple(2,3,), keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.477973 test begin: paddle.Tensor.mean(Tensor([1, 192, 144, 82527],"float32"), tuple(2,3,), keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 192, 144, 82527],"float32"), tuple(2,3,), keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.487502 test begin: paddle.Tensor.mean(Tensor([1, 192, 152, 78184],"float32"), tuple(2,3,), keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 192, 152, 78184],"float32"), tuple(2,3,), keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.496948 test begin: paddle.Tensor.mean(Tensor([1, 192, 160, 74275],"float32"), tuple(2,3,), keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 192, 160, 74275],"float32"), tuple(2,3,), keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.506458 test begin: paddle.Tensor.mean(Tensor([1, 192, 74275, 160],"float32"), tuple(2,3,), keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 192, 74275, 160],"float32"), tuple(2,3,), keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.515828 test begin: paddle.Tensor.mean(Tensor([1, 192, 78184, 152],"float32"), tuple(2,3,), keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 192, 78184, 152],"float32"), tuple(2,3,), keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.525193 test begin: paddle.Tensor.mean(Tensor([1, 192, 82527, 144],"float32"), tuple(2,3,), keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 192, 82527, 144],"float32"), tuple(2,3,), keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.534511 test begin: paddle.Tensor.mean(Tensor([1, 192, 87382, 136],"float32"), tuple(2,3,), keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 192, 87382, 136],"float32"), tuple(2,3,), keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.544008 test begin: paddle.Tensor.mean(Tensor([1, 192, 92843, 128],"float32"), tuple(2,3,), keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 192, 92843, 128],"float32"), tuple(2,3,), keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.553515 test begin: paddle.Tensor.mean(Tensor([1, 196, 11641334],"float32"), 1, )

[torch error] paddle.Tensor.mean(Tensor([1, 196, 11641334],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.562983 test begin: paddle.Tensor.mean(Tensor([1, 2, 190141782, 6],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1, 2, 190141782, 6],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.572300 test begin: paddle.Tensor.mean(Tensor([1, 2, 2, 2, 285212673],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1, 2, 2, 2, 285212673],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.581582 test begin: paddle.Tensor.mean(Tensor([1, 2, 2, 285212673, 2],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1, 2, 2, 285212673, 2],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.590848 test begin: paddle.Tensor.mean(Tensor([1, 2, 2, 570425345],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1, 2, 2, 570425345],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.600136 test begin: paddle.Tensor.mean(Tensor([1, 2, 285212673, 2, 2],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1, 2, 285212673, 2, 2],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.609377 test begin: paddle.Tensor.mean(Tensor([1, 2, 570425345, 2],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1, 2, 570425345, 2],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.618651 test begin: paddle.Tensor.mean(Tensor([1, 2, 6, 190141782],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1, 2, 6, 190141782],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.627964 test begin: paddle.Tensor.mean(Tensor([1, 200, 11408507],"float32"), 1, )

[torch error] paddle.Tensor.mean(Tensor([1, 200, 11408507],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.637279 test begin: paddle.Tensor.mean(Tensor([1, 20372334, 8, 14],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1, 20372334, 8, 14],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.647073 test begin: paddle.Tensor.mean(Tensor([1, 207427399, 11],"float32"), 1, )

[torch error] paddle.Tensor.mean(Tensor([1, 207427399, 11],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.656368 test begin: paddle.Tensor.mean(Tensor([1, 2281701379],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1, 2281701379],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.665609 test begin: paddle.Tensor.mean(Tensor([1, 2281701379],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 2281701379],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.674870 test begin: paddle.Tensor.mean(Tensor([1, 2281701379],"float32"), 1, )

[torch error] paddle.Tensor.mean(Tensor([1, 2281701379],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.684127 test begin: paddle.Tensor.mean(Tensor([1, 252645136, 17],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 252645136, 17],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.693532 test begin: paddle.Tensor.mean(Tensor([1, 285212673, 2, 2, 2],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1, 285212673, 2, 2, 2],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.702864 test begin: paddle.Tensor.mean(Tensor([1, 2970966, 1, 12, 1, 64],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1, 2970966, 1, 12, 1, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.712195 test begin: paddle.Tensor.mean(Tensor([1, 2970966, 768],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 2970966, 768],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.721579 test begin: paddle.Tensor.mean(Tensor([1, 300, 7605672],"float32"), tuple(0,1,), keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 300, 7605672],"float32"), tuple(0,1,), keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.730977 test begin: paddle.Tensor.mean(Tensor([1, 35651585, 64],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 35651585, 64],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.740371 test begin: paddle.Tensor.mean(Tensor([1, 4, 1073741825],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 4, 1073741825],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.749702 test begin: paddle.Tensor.mean(Tensor([1, 42253730, 54],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 42253730, 54],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.759045 test begin: paddle.Tensor.mean(Tensor([1, 4294967297],"float16"), )

[torch error] paddle.Tensor.mean(Tensor([1, 4294967297],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.768534 test begin: paddle.Tensor.mean(Tensor([1, 42949673, 100],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 42949673, 100],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.779562 test begin: paddle.Tensor.mean(Tensor([1, 50704476, 45],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 50704476, 45],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.790227 test begin: paddle.Tensor.mean(Tensor([1, 557057, 4096],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 557057, 4096],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.799931 test begin: paddle.Tensor.mean(Tensor([1, 557057, 4096],"float32"), tuple(0,1,), keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 557057, 4096],"float32"), tuple(0,1,), keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.809770 test begin: paddle.Tensor.mean(Tensor([1, 557057, 64, 64],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1, 557057, 64, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.820353 test begin: paddle.Tensor.mean(Tensor([1, 570425345, 2, 2],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1, 570425345, 2, 2],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.830887 test begin: paddle.Tensor.mean(Tensor([1, 570425345, 4],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1, 570425345, 4],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.841003 test begin: paddle.Tensor.mean(Tensor([1, 570425345, 4],"float32"), 1, )

[torch error] paddle.Tensor.mean(Tensor([1, 570425345, 4],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.851080 test begin: paddle.Tensor.mean(Tensor([1, 63380594, 36],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 63380594, 36],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.861285 test begin: paddle.Tensor.mean(Tensor([1, 63380594, 6, 6],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1, 63380594, 6, 6],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.871549 test begin: paddle.Tensor.mean(Tensor([1, 71303169, 32],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1, 71303169, 32],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.881091 test begin: paddle.Tensor.mean(Tensor([1, 71303169, 32],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 71303169, 32],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.893194 test begin: paddle.Tensor.mean(Tensor([1, 71303169, 32],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 71303169, 32],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.903358 test begin: paddle.Tensor.mean(Tensor([1, 8, 285212673],"float32"), -1, )

[torch error] paddle.Tensor.mean(Tensor([1, 8, 285212673],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.913389 test begin: paddle.Tensor.mean(Tensor([1, 8912897, 256],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1, 8912897, 256],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.923425 test begin: paddle.Tensor.mean(Tensor([1, 89129, 160, 160],"float32"), tuple(2,3,), keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 89129, 160, 160],"float32"), tuple(2,3,), keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.933406 test begin: paddle.Tensor.mean(Tensor([1, 912681, 50, 50],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1, 912681, 50, 50],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.943113 test begin: paddle.Tensor.mean(Tensor([1, 98758, 152, 152],"float32"), tuple(2,3,), keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1, 98758, 152, 152],"float32"), tuple(2,3,), keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.955049 test begin: paddle.Tensor.mean(Tensor([10, 228170138],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([10, 228170138],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.968553 test begin: paddle.Tensor.mean(Tensor([10, 228170138],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([10, 228170138],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.977941 test begin: paddle.Tensor.mean(Tensor([100, 22817014],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([100, 22817014],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.987335 test begin: paddle.Tensor.mean(Tensor([100, 22817014],"float32"), 1, )

[torch error] paddle.Tensor.mean(Tensor([100, 22817014],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:40.996971 test begin: paddle.Tensor.mean(Tensor([100, 42949673],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([100, 42949673],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.006551 test begin: paddle.Tensor.mean(Tensor([1000, 2281702],"float32"), axis=0, )

[torch error] paddle.Tensor.mean(Tensor([1000, 2281702],"float32"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.015955 test begin: paddle.Tensor.mean(Tensor([10000, 143166, 3],"float16"), axis=0, )

[torch error] paddle.Tensor.mean(Tensor([10000, 143166, 3],"float16"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.025546 test begin: paddle.Tensor.mean(Tensor([10000, 2, 114086],"float32"), axis=0, )

[torch error] paddle.Tensor.mean(Tensor([10000, 2, 114086],"float32"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.034945 test begin: paddle.Tensor.mean(Tensor([10000, 2, 214749],"float16"), axis=0, )

[torch error] paddle.Tensor.mean(Tensor([10000, 2, 214749],"float16"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.044407 test begin: paddle.Tensor.mean(Tensor([10000, 76057, 3],"float32"), axis=0, )

[torch error] paddle.Tensor.mean(Tensor([10000, 76057, 3],"float32"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.053860 test begin: paddle.Tensor.mean(Tensor([100000, 42950],"float16"), axis=0, )

[torch error] paddle.Tensor.mean(Tensor([100000, 42950],"float16"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.063365 test begin: paddle.Tensor.mean(Tensor([1000000, 4295],"float16"), axis=0, )

[torch error] paddle.Tensor.mean(Tensor([1000000, 4295],"float16"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.072813 test begin: paddle.Tensor.mean(Tensor([101, 22591103],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([101, 22591103],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.082216 test begin: paddle.Tensor.mean(Tensor([102, 22369622],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([102, 22369622],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.091789 test begin: paddle.Tensor.mean(Tensor([1037137, 200, 11],"float32"), 1, )

[torch error] paddle.Tensor.mean(Tensor([1037137, 200, 11],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.101196 test begin: paddle.Tensor.mean(Tensor([1058304, 196, 11],"float32"), 1, )

[torch error] paddle.Tensor.mean(Tensor([1058304, 196, 11],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.110572 test begin: paddle.Tensor.mean(Tensor([1073741825, 4],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1073741825, 4],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.120063 test begin: paddle.Tensor.mean(Tensor([1073741825, 4],"float16"), axis=0, )

[torch error] paddle.Tensor.mean(Tensor([1073741825, 4],"float16"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.129422 test begin: paddle.Tensor.mean(Tensor([107374183, 40],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([107374183, 40],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.138780 test begin: paddle.Tensor.mean(Tensor([10737419, 100, 4],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([10737419, 100, 4],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.148118 test begin: paddle.Tensor.mean(Tensor([10737419, 4, 100],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([10737419, 4, 100],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.157667 test begin: paddle.Tensor.mean(Tensor([1073742, 100, 40],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1073742, 100, 40],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.167079 test begin: paddle.Tensor.mean(Tensor([1080352, 192, 11],"float32"), 1, )

[torch error] paddle.Tensor.mean(Tensor([1080352, 192, 11],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.176376 test begin: paddle.Tensor.mean(Tensor([11, 207427399],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([11, 207427399],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.185714 test begin: paddle.Tensor.mean(Tensor([11369, 256, 28, 28],"float32"), 1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([11369, 256, 28, 28],"float32"), 1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.195082 test begin: paddle.Tensor.mean(Tensor([11408507, 100, 2],"float32"), 1, )

[torch error] paddle.Tensor.mean(Tensor([11408507, 100, 2],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.204402 test begin: paddle.Tensor.mean(Tensor([11883862, 1, 192],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([11883862, 1, 192],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.213686 test begin: paddle.Tensor.mean(Tensor([124, 1024, 12, 1498],"float32"), list[-2,-1,], )

[torch error] paddle.Tensor.mean(Tensor([124, 1024, 12, 1498],"float32"), list[-2,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.223197 test begin: paddle.Tensor.mean(Tensor([124, 1024, 1498, 12],"float32"), list[-2,-1,], )

[torch error] paddle.Tensor.mean(Tensor([124, 1024, 1498, 12],"float32"), list[-2,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.232477 test begin: paddle.Tensor.mean(Tensor([124, 1024, 2568, 7],"float32"), list[-2,-1,], )

[torch error] paddle.Tensor.mean(Tensor([124, 1024, 2568, 7],"float32"), list[-2,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.241769 test begin: paddle.Tensor.mean(Tensor([124, 1024, 7, 2568],"float32"), list[-2,-1,], )

[torch error] paddle.Tensor.mean(Tensor([124, 1024, 7, 2568],"float32"), list[-2,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.251126 test begin: paddle.Tensor.mean(Tensor([124, 127784, 12, 12],"float32"), list[-2,-1,], )

[torch error] paddle.Tensor.mean(Tensor([124, 127784, 12, 12],"float32"), list[-2,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.260410 test begin: paddle.Tensor.mean(Tensor([124, 128, 1498, 96],"float32"), 1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([124, 128, 1498, 96],"float32"), 1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.269631 test begin: paddle.Tensor.mean(Tensor([124, 128, 2568, 56],"float32"), 1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([124, 128, 2568, 56],"float32"), 1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.278988 test begin: paddle.Tensor.mean(Tensor([124, 128, 56, 2568],"float32"), 1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([124, 128, 56, 2568],"float32"), 1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.288605 test begin: paddle.Tensor.mean(Tensor([124, 128, 96, 1498],"float32"), 1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([124, 128, 96, 1498],"float32"), 1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.298787 test begin: paddle.Tensor.mean(Tensor([124, 1536, 1712, 7],"float32"), list[-2,-1,], )

[torch error] paddle.Tensor.mean(Tensor([124, 1536, 1712, 7],"float32"), list[-2,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.308421 test begin: paddle.Tensor.mean(Tensor([124, 1536, 7, 1712],"float32"), list[-2,-1,], )

[torch error] paddle.Tensor.mean(Tensor([124, 1536, 7, 1712],"float32"), list[-2,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.318063 test begin: paddle.Tensor.mean(Tensor([124, 192, 1712, 56],"float32"), 1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([124, 192, 1712, 56],"float32"), 1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.327499 test begin: paddle.Tensor.mean(Tensor([124, 192, 28, 3423],"float32"), 1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([124, 192, 28, 3423],"float32"), 1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.336905 test begin: paddle.Tensor.mean(Tensor([124, 192, 3423, 28],"float32"), 1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([124, 192, 3423, 28],"float32"), 1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.346284 test begin: paddle.Tensor.mean(Tensor([124, 192, 56, 1712],"float32"), 1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([124, 192, 56, 1712],"float32"), 1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.355632 test begin: paddle.Tensor.mean(Tensor([124, 1997, 96, 96],"float32"), 1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([124, 1997, 96, 96],"float32"), 1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.364993 test begin: paddle.Tensor.mean(Tensor([124, 23471, 28, 28],"float32"), 1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([124, 23471, 28, 28],"float32"), 1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.374360 test begin: paddle.Tensor.mean(Tensor([124, 256, 2568, 28],"float32"), 1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([124, 256, 2568, 28],"float32"), 1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.383656 test begin: paddle.Tensor.mean(Tensor([124, 256, 28, 2568],"float32"), 1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([124, 256, 28, 2568],"float32"), 1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.393279 test begin: paddle.Tensor.mean(Tensor([124, 375527, 7, 7],"float32"), list[-2,-1,], )

[torch error] paddle.Tensor.mean(Tensor([124, 375527, 7, 7],"float32"), list[-2,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.402789 test begin: paddle.Tensor.mean(Tensor([124, 5868, 56, 56],"float32"), 1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([124, 5868, 56, 56],"float32"), 1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.412280 test begin: paddle.Tensor.mean(Tensor([124, 768, 3423, 7],"float32"), list[-2,-1,], )

[torch error] paddle.Tensor.mean(Tensor([124, 768, 3423, 7],"float32"), list[-2,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.421757 test begin: paddle.Tensor.mean(Tensor([124, 768, 7, 3423],"float32"), list[-2,-1,], )

[torch error] paddle.Tensor.mean(Tensor([124, 768, 7, 3423],"float32"), list[-2,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.431318 test begin: paddle.Tensor.mean(Tensor([1273271, 256, 7],"float32"), -1, )

[torch error] paddle.Tensor.mean(Tensor([1273271, 256, 7],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.440773 test begin: paddle.Tensor.mean(Tensor([128, 1024, 12, 1451],"float32"), list[-2,-1,], )

[torch error] paddle.Tensor.mean(Tensor([128, 1024, 12, 1451],"float32"), list[-2,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.450386 test begin: paddle.Tensor.mean(Tensor([128, 1024, 1451, 12],"float32"), list[-2,-1,], )

[torch error] paddle.Tensor.mean(Tensor([128, 1024, 1451, 12],"float32"), list[-2,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.459817 test begin: paddle.Tensor.mean(Tensor([128, 123791, 12, 12],"float32"), list[-2,-1,], )

[torch error] paddle.Tensor.mean(Tensor([128, 123791, 12, 12],"float32"), list[-2,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.469449 test begin: paddle.Tensor.mean(Tensor([128, 17825793],"float32"), axis=-1, )

[torch error] paddle.Tensor.mean(Tensor([128, 17825793],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.478842 test begin: paddle.Tensor.mean(Tensor([128, 34817, 512],"float32"), axis=1, )

[torch error] paddle.Tensor.mean(Tensor([128, 34817, 512],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.488300 test begin: paddle.Tensor.mean(Tensor([128, 49, 363792],"float32"), axis=1, )

[torch error] paddle.Tensor.mean(Tensor([128, 49, 363792],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.497678 test begin: paddle.Tensor.mean(Tensor([128, 49, 684785],"float16"), axis=1, )

[torch error] paddle.Tensor.mean(Tensor([128, 49, 684785],"float16"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.509787 test begin: paddle.Tensor.mean(Tensor([128, 65537, 512],"float16"), axis=1, )

[torch error] paddle.Tensor.mean(Tensor([128, 65537, 512],"float16"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.519169 test begin: paddle.Tensor.mean(Tensor([13, 1096972, 160],"float32"), axis=tuple(1,2,), )

[torch error] paddle.Tensor.mean(Tensor([13, 1096972, 160],"float32"), axis=tuple(1,2,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.528469 test begin: paddle.Tensor.mean(Tensor([13, 160, 1096972],"float32"), axis=tuple(1,2,), )

[torch error] paddle.Tensor.mean(Tensor([13, 160, 1096972],"float32"), axis=tuple(1,2,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.537804 test begin: paddle.Tensor.mean(Tensor([13, 2742430, 2, 4, 8],"float32"), axis=-2, )

[torch error] paddle.Tensor.mean(Tensor([13, 2742430, 2, 4, 8],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.547171 test begin: paddle.Tensor.mean(Tensor([13, 4, 1371215, 4, 8],"float32"), axis=-2, )

[torch error] paddle.Tensor.mean(Tensor([13, 4, 1371215, 4, 8],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.556516 test begin: paddle.Tensor.mean(Tensor([13, 4, 2, 2742430, 8],"float32"), axis=-2, )

[torch error] paddle.Tensor.mean(Tensor([13, 4, 2, 2742430, 8],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.565831 test begin: paddle.Tensor.mean(Tensor([13, 4, 2, 4, 5484860],"float32"), axis=-2, )

[torch error] paddle.Tensor.mean(Tensor([13, 4, 2, 4, 5484860],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.575187 test begin: paddle.Tensor.mean(Tensor([13, 49, 3581949],"float32"), axis=1, )

[torch error] paddle.Tensor.mean(Tensor([13, 49, 3581949],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.584543 test begin: paddle.Tensor.mean(Tensor([13, 5484860, 32],"float32"), axis=1, )

[torch error] paddle.Tensor.mean(Tensor([13, 5484860, 32],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.593854 test begin: paddle.Tensor.mean(Tensor([13, 7313146, 24],"float32"), axis=1, )

[torch error] paddle.Tensor.mean(Tensor([13, 7313146, 24],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.603177 test begin: paddle.Tensor.mean(Tensor([131073, 128, 16, 16],"float16"), -1, )

[torch error] paddle.Tensor.mean(Tensor([131073, 128, 16, 16],"float16"), -1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.612469 test begin: paddle.Tensor.mean(Tensor([131073, 128, 16, 16],"float16"), -2, )

[torch error] paddle.Tensor.mean(Tensor([131073, 128, 16, 16],"float16"), -2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.621877 test begin: paddle.Tensor.mean(Tensor([142606337, 2, 2, 2, 2],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([142606337, 2, 2, 2, 2],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.631173 test begin: paddle.Tensor.mean(Tensor([144, 20632, 768],"float32"), axis=1, )

[torch error] paddle.Tensor.mean(Tensor([144, 20632, 768],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.640486 test begin: paddle.Tensor.mean(Tensor([144, 49, 323371],"float32"), axis=1, )

[torch error] paddle.Tensor.mean(Tensor([144, 49, 323371],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.649759 test begin: paddle.Tensor.mean(Tensor([1455167, 49, 32],"float32"), axis=1, )

[torch error] paddle.Tensor.mean(Tensor([1455167, 49, 32],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.659343 test begin: paddle.Tensor.mean(Tensor([149131, 2, 3, 10, 10, 12, 4],"float16"), )

[torch error] paddle.Tensor.mean(Tensor([149131, 2, 3, 10, 10, 12, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.668726 test begin: paddle.Tensor.mean(Tensor([15158, 192, 28, 28],"float32"), 1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([15158, 192, 28, 28],"float32"), 1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.678245 test begin: paddle.Tensor.mean(Tensor([15474, 1024, 12, 12],"float32"), list[-2,-1,], )

[torch error] paddle.Tensor.mean(Tensor([15474, 1024, 12, 12],"float32"), list[-2,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.687641 test begin: paddle.Tensor.mean(Tensor([162978670, 14],"float32"), 1, )

[torch error] paddle.Tensor.mean(Tensor([162978670, 14],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.697000 test begin: paddle.Tensor.mean(Tensor([1697695, 12, 8, 14],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([1697695, 12, 8, 14],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.706343 test begin: paddle.Tensor.mean(Tensor([17, 252645136],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([17, 252645136],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.715722 test begin: paddle.Tensor.mean(Tensor([171197, 49, 512],"float16"), axis=1, )

[torch error] paddle.Tensor.mean(Tensor([171197, 49, 512],"float16"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.725094 test begin: paddle.Tensor.mean(Tensor([17409, 1024, 128],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([17409, 1024, 128],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.734499 test begin: paddle.Tensor.mean(Tensor([178956971, 3, 2, 1, 4],"float16"), )

[torch error] paddle.Tensor.mean(Tensor([178956971, 3, 2, 1, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.743924 test begin: paddle.Tensor.mean(Tensor([17895698, 2, 3, 4, 10],"float16"), )

[torch error] paddle.Tensor.mean(Tensor([17895698, 2, 3, 4, 10],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.753287 test begin: paddle.Tensor.mean(Tensor([18, 126761188],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([18, 126761188],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.763126 test begin: paddle.Tensor.mean(Tensor([181896, 12544],"float32"), 1, )

[torch error] paddle.Tensor.mean(Tensor([181896, 12544],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.772544 test begin: paddle.Tensor.mean(Tensor([185686, 192, 8, 8],"float32"), -1, )

[torch error] paddle.Tensor.mean(Tensor([185686, 192, 8, 8],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.782075 test begin: paddle.Tensor.mean(Tensor([185686, 192, 8, 8],"float32"), -2, )

[torch error] paddle.Tensor.mean(Tensor([185686, 192, 8, 8],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.791528 test begin: paddle.Tensor.mean(Tensor([1857, 300, 4096],"float32"), tuple(0,1,), keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1857, 300, 4096],"float32"), tuple(0,1,), keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.801408 test begin: paddle.Tensor.mean(Tensor([1878, 8, 151936],"float32"), -1, )

[torch error] paddle.Tensor.mean(Tensor([1878, 8, 151936],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.810910 test begin: paddle.Tensor.mean(Tensor([1935, 128, 96, 96],"float32"), 1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([1935, 128, 96, 96],"float32"), 1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.820298 test begin: paddle.Tensor.mean(Tensor([1940223, 49, 24],"float32"), axis=1, )

[torch error] paddle.Tensor.mean(Tensor([1940223, 49, 24],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.829616 test begin: paddle.Tensor.mean(Tensor([2, 1, 512, 4, 557057],"float32"), list[2,3,4,], keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([2, 1, 512, 4, 557057],"float32"), list[2,3,4,], keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.839037 test begin: paddle.Tensor.mean(Tensor([2, 1, 512, 557057, 4],"float32"), list[2,3,4,], keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([2, 1, 512, 557057, 4],"float32"), list[2,3,4,], keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.848596 test begin: paddle.Tensor.mean(Tensor([2, 1, 71303169, 4, 4],"float32"), list[2,3,4,], keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([2, 1, 71303169, 4, 4],"float32"), list[2,3,4,], keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.858433 test begin: paddle.Tensor.mean(Tensor([2, 1140850690],"float32"), 1, )

[torch error] paddle.Tensor.mean(Tensor([2, 1140850690],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.867863 test begin: paddle.Tensor.mean(Tensor([2, 139265, 512, 4, 4],"float32"), list[2,3,4,], keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([2, 139265, 512, 4, 4],"float32"), list[2,3,4,], keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.877257 test begin: paddle.Tensor.mean(Tensor([2, 3, 100, 7158279],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([2, 3, 100, 7158279],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.886649 test begin: paddle.Tensor.mean(Tensor([2, 3, 17, 42107523],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([2, 3, 17, 42107523],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.896047 test begin: paddle.Tensor.mean(Tensor([2, 3, 178956971, 4],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([2, 3, 178956971, 4],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.905417 test begin: paddle.Tensor.mean(Tensor([2, 3, 17895698, 40],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([2, 3, 17895698, 40],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.915056 test begin: paddle.Tensor.mean(Tensor([2, 3, 4, 178956971],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([2, 3, 4, 178956971],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.924654 test begin: paddle.Tensor.mean(Tensor([2, 3, 42107523, 17],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([2, 3, 42107523, 17],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.934196 test begin: paddle.Tensor.mean(Tensor([2, 3, 7158279, 100],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([2, 3, 7158279, 100],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.943693 test begin: paddle.Tensor.mean(Tensor([2, 31580642, 17, 4],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([2, 31580642, 17, 4],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.953034 test begin: paddle.Tensor.mean(Tensor([2, 31580642, 4, 17],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([2, 31580642, 4, 17],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.962486 test begin: paddle.Tensor.mean(Tensor([2, 536871, 100, 40],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([2, 536871, 100, 40],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.971814 test begin: paddle.Tensor.mean(Tensor([2, 5368710, 100, 4],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([2, 5368710, 100, 4],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.982128 test begin: paddle.Tensor.mean(Tensor([2, 5368710, 4, 100],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([2, 5368710, 4, 100],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:41.993493 test begin: paddle.Tensor.mean(Tensor([200, 11408507],"float32"), 1, )

[torch error] paddle.Tensor.mean(Tensor([200, 11408507],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.005085 test begin: paddle.Tensor.mean(Tensor([200, 11408507],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([200, 11408507],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.016128 test begin: paddle.Tensor.mean(Tensor([21, 108652447],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([21, 108652447],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.026563 test begin: paddle.Tensor.mean(Tensor([21053762, 3, 17, 4],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([21053762, 3, 17, 4],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.036742 test begin: paddle.Tensor.mean(Tensor([21053762, 3, 4, 17],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([21053762, 3, 4, 17],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.047611 test begin: paddle.Tensor.mean(Tensor([21474837, 10, 20],"float16"), axis=0, )

[torch error] paddle.Tensor.mean(Tensor([21474837, 10, 20],"float16"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.057852 test begin: paddle.Tensor.mean(Tensor([22, 103713700],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([22, 103713700],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.068180 test begin: paddle.Tensor.mean(Tensor([2281701379, 1],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([2281701379, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.078585 test begin: paddle.Tensor.mean(Tensor([2281701379],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([2281701379],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.088182 test begin: paddle.Tensor.mean(Tensor([2281701379],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([2281701379],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.100398 test begin: paddle.Tensor.mean(Tensor([2281701379],"float32"), axis=0, )

[torch error] paddle.Tensor.mean(Tensor([2281701379],"float32"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.110652 test begin: paddle.Tensor.mean(Tensor([228170138, 10],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([228170138, 10],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.120707 test begin: paddle.Tensor.mean(Tensor([2281702, 1000],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([2281702, 1000],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.130787 test begin: paddle.Tensor.mean(Tensor([2281702, 1000],"float32"), axis=-1, )

[torch error] paddle.Tensor.mean(Tensor([2281702, 1000],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.140876 test begin: paddle.Tensor.mean(Tensor([252645136, 17],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([252645136, 17],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.150675 test begin: paddle.Tensor.mean(Tensor([278529, 1, 512, 4, 4],"float32"), list[2,3,4,], keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([278529, 1, 512, 4, 4],"float32"), list[2,3,4,], keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.162788 test begin: paddle.Tensor.mean(Tensor([285212673, 2, 2, 2],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([285212673, 2, 2, 2],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.176476 test begin: paddle.Tensor.mean(Tensor([2852127, 200, 4],"float32"), 1, )

[torch error] paddle.Tensor.mean(Tensor([2852127, 200, 4],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.186095 test begin: paddle.Tensor.mean(Tensor([2910334, 784],"float32"), axis=0, )

[torch error] paddle.Tensor.mean(Tensor([2910334, 784],"float32"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.195410 test begin: paddle.Tensor.mean(Tensor([2970966, 1, 1, 12, 1, 64],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([2970966, 1, 1, 12, 1, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.204658 test begin: paddle.Tensor.mean(Tensor([2970966, 1, 768],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([2970966, 1, 768],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.213965 test begin: paddle.Tensor.mean(Tensor([2970966, 768],"float32"), 0, )

[torch error] paddle.Tensor.mean(Tensor([2970966, 768],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.223452 test begin: paddle.Tensor.mean(Tensor([3, 11930465, 3, 4, 10],"float16"), )

[torch error] paddle.Tensor.mean(Tensor([3, 11930465, 3, 4, 10],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.232737 test begin: paddle.Tensor.mean(Tensor([3, 2, 149131, 10, 10, 12, 4],"float16"), )

[torch error] paddle.Tensor.mean(Tensor([3, 2, 149131, 10, 10, 12, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.242063 test begin: paddle.Tensor.mean(Tensor([3, 2, 17895698, 4, 10],"float16"), )

[torch error] paddle.Tensor.mean(Tensor([3, 2, 17895698, 4, 10],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.251335 test begin: paddle.Tensor.mean(Tensor([3, 2, 3, 10, 10, 12, 198842],"float16"), )

[torch error] paddle.Tensor.mean(Tensor([3, 2, 3, 10, 10, 12, 198842],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.260622 test begin: paddle.Tensor.mean(Tensor([3, 2, 3, 10, 10, 596524, 4],"float16"), )

[torch error] paddle.Tensor.mean(Tensor([3, 2, 3, 10, 10, 596524, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.270140 test begin: paddle.Tensor.mean(Tensor([3, 2, 3, 10, 497103, 12, 4],"float16"), )

[torch error] paddle.Tensor.mean(Tensor([3, 2, 3, 10, 497103, 12, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.279475 test begin: paddle.Tensor.mean(Tensor([3, 2, 3, 23860930, 10],"float16"), )

[torch error] paddle.Tensor.mean(Tensor([3, 2, 3, 23860930, 10],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.288934 test begin: paddle.Tensor.mean(Tensor([3, 2, 3, 4, 59652324],"float16"), )

[torch error] paddle.Tensor.mean(Tensor([3, 2, 3, 4, 59652324],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.298266 test begin: paddle.Tensor.mean(Tensor([3, 2, 3, 497103, 10, 12, 4],"float16"), )

[torch error] paddle.Tensor.mean(Tensor([3, 2, 3, 497103, 10, 12, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.307582 test begin: paddle.Tensor.mean(Tensor([3, 99421, 3, 10, 10, 12, 4],"float16"), )

[torch error] paddle.Tensor.mean(Tensor([3, 99421, 3, 10, 10, 12, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.316877 test begin: paddle.Tensor.mean(Tensor([30316, 1536, 7, 7],"float32"), list[-2,-1,], )

[torch error] paddle.Tensor.mean(Tensor([30316, 1536, 7, 7],"float32"), list[-2,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.326254 test begin: paddle.Tensor.mean(Tensor([31690297, 2, 6, 6],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([31690297, 2, 6, 6],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.335552 test begin: paddle.Tensor.mean(Tensor([349526, 192, 8, 8],"float16"), -1, )

[torch error] paddle.Tensor.mean(Tensor([349526, 192, 8, 8],"float16"), -1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.344927 test begin: paddle.Tensor.mean(Tensor([349526, 192, 8, 8],"float16"), -2, )

[torch error] paddle.Tensor.mean(Tensor([349526, 192, 8, 8],"float16"), -2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.354237 test begin: paddle.Tensor.mean(Tensor([35651585, 1, 64],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([35651585, 1, 64],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.363861 test begin: paddle.Tensor.mean(Tensor([357914, 3, 100, 40],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([357914, 3, 100, 40],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.373369 test begin: paddle.Tensor.mean(Tensor([3579140, 3, 100, 4],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([3579140, 3, 100, 4],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.382817 test begin: paddle.Tensor.mean(Tensor([3579140, 3, 4, 100],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([3579140, 3, 4, 100],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.392276 test begin: paddle.Tensor.mean(Tensor([37138, 15, 64, 64],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([37138, 15, 64, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.401661 test begin: paddle.Tensor.mean(Tensor([3790, 192, 56, 56],"float32"), 1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([3790, 192, 56, 56],"float32"), 1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.411120 test begin: paddle.Tensor.mean(Tensor([380283564, 2, 3],"float32"), axis=0, )

[torch error] paddle.Tensor.mean(Tensor([380283564, 2, 3],"float32"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.420531 test begin: paddle.Tensor.mean(Tensor([4, 1073741825],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([4, 1073741825],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.430128 test begin: paddle.Tensor.mean(Tensor([4, 128, 16, 278529],"float32"), -1, )

[torch error] paddle.Tensor.mean(Tensor([4, 128, 16, 278529],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.439464 test begin: paddle.Tensor.mean(Tensor([4, 128, 16, 278529],"float32"), -2, )

[torch error] paddle.Tensor.mean(Tensor([4, 128, 16, 278529],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.448959 test begin: paddle.Tensor.mean(Tensor([4, 128, 16, 524289],"float16"), -1, )

[torch error] paddle.Tensor.mean(Tensor([4, 128, 16, 524289],"float16"), -1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.458419 test begin: paddle.Tensor.mean(Tensor([4, 128, 16, 524289],"float16"), -2, )

[torch error] paddle.Tensor.mean(Tensor([4, 128, 16, 524289],"float16"), -2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.467780 test begin: paddle.Tensor.mean(Tensor([4, 128, 278529, 16],"float32"), -1, )

[torch error] paddle.Tensor.mean(Tensor([4, 128, 278529, 16],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.477173 test begin: paddle.Tensor.mean(Tensor([4, 128, 278529, 16],"float32"), -2, )

[torch error] paddle.Tensor.mean(Tensor([4, 128, 278529, 16],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.486500 test begin: paddle.Tensor.mean(Tensor([4, 128, 524289, 16],"float16"), -1, )

[torch error] paddle.Tensor.mean(Tensor([4, 128, 524289, 16],"float16"), -1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.495843 test begin: paddle.Tensor.mean(Tensor([4, 128, 524289, 16],"float16"), -2, )

[torch error] paddle.Tensor.mean(Tensor([4, 128, 524289, 16],"float16"), -2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.505359 test begin: paddle.Tensor.mean(Tensor([4, 16777217, 8, 8],"float16"), -1, )

[torch error] paddle.Tensor.mean(Tensor([4, 16777217, 8, 8],"float16"), -1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.514847 test begin: paddle.Tensor.mean(Tensor([4, 16777217, 8, 8],"float16"), -2, )

[torch error] paddle.Tensor.mean(Tensor([4, 16777217, 8, 8],"float16"), -2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.524319 test begin: paddle.Tensor.mean(Tensor([4, 192, 371371, 8],"float32"), -1, )

[torch error] paddle.Tensor.mean(Tensor([4, 192, 371371, 8],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.533721 test begin: paddle.Tensor.mean(Tensor([4, 192, 371371, 8],"float32"), -2, )

[torch error] paddle.Tensor.mean(Tensor([4, 192, 371371, 8],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.543073 test begin: paddle.Tensor.mean(Tensor([4, 192, 699051, 8],"float16"), -1, )

[torch error] paddle.Tensor.mean(Tensor([4, 192, 699051, 8],"float16"), -1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.552432 test begin: paddle.Tensor.mean(Tensor([4, 192, 699051, 8],"float16"), -2, )

[torch error] paddle.Tensor.mean(Tensor([4, 192, 699051, 8],"float16"), -2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.561748 test begin: paddle.Tensor.mean(Tensor([4, 192, 8, 371371],"float32"), -1, )

[torch error] paddle.Tensor.mean(Tensor([4, 192, 8, 371371],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.571041 test begin: paddle.Tensor.mean(Tensor([4, 192, 8, 371371],"float32"), -2, )

[torch error] paddle.Tensor.mean(Tensor([4, 192, 8, 371371],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.580344 test begin: paddle.Tensor.mean(Tensor([4, 192, 8, 699051],"float16"), -1, )

[torch error] paddle.Tensor.mean(Tensor([4, 192, 8, 699051],"float16"), -1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.589622 test begin: paddle.Tensor.mean(Tensor([4, 192, 8, 699051],"float16"), -2, )

[torch error] paddle.Tensor.mean(Tensor([4, 192, 8, 699051],"float16"), -2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.599163 test begin: paddle.Tensor.mean(Tensor([4, 2228225, 16, 16],"float32"), -1, )

[torch error] paddle.Tensor.mean(Tensor([4, 2228225, 16, 16],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.608581 test begin: paddle.Tensor.mean(Tensor([4, 2228225, 16, 16],"float32"), -2, )

[torch error] paddle.Tensor.mean(Tensor([4, 2228225, 16, 16],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.618000 test begin: paddle.Tensor.mean(Tensor([4, 256, 16, 262145],"float16"), -1, )

[torch error] paddle.Tensor.mean(Tensor([4, 256, 16, 262145],"float16"), -1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.627385 test begin: paddle.Tensor.mean(Tensor([4, 256, 262145, 16],"float16"), -1, )

[torch error] paddle.Tensor.mean(Tensor([4, 256, 262145, 16],"float16"), -1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.636690 test begin: paddle.Tensor.mean(Tensor([4, 4194305, 16, 16],"float16"), -1, )

[torch error] paddle.Tensor.mean(Tensor([4, 4194305, 16, 16],"float16"), -1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.646008 test begin: paddle.Tensor.mean(Tensor([4, 4194305, 16, 16],"float16"), -2, )

[torch error] paddle.Tensor.mean(Tensor([4, 4194305, 16, 16],"float16"), -2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.655355 test begin: paddle.Tensor.mean(Tensor([4, 570425345],"float32"), 1, )

[torch error] paddle.Tensor.mean(Tensor([4, 570425345],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.664676 test begin: paddle.Tensor.mean(Tensor([4, 8912897, 8, 8],"float32"), -1, )

[torch error] paddle.Tensor.mean(Tensor([4, 8912897, 8, 8],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.673972 test begin: paddle.Tensor.mean(Tensor([4, 8912897, 8, 8],"float32"), -2, )

[torch error] paddle.Tensor.mean(Tensor([4, 8912897, 8, 8],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.683247 test begin: paddle.Tensor.mean(Tensor([42253730, 1, 54],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([42253730, 1, 54],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.692533 test begin: paddle.Tensor.mean(Tensor([4294967297, 1],"float16"), )

[torch error] paddle.Tensor.mean(Tensor([4294967297, 1],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.702001 test begin: paddle.Tensor.mean(Tensor([4294967297, 1],"float16"), axis=0, )

[torch error] paddle.Tensor.mean(Tensor([4294967297, 1],"float16"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.711383 test begin: paddle.Tensor.mean(Tensor([4294967297],"float16"), )

[torch error] paddle.Tensor.mean(Tensor([4294967297],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.720699 test begin: paddle.Tensor.mean(Tensor([4294967297],"float16"), axis=0, )

[torch error] paddle.Tensor.mean(Tensor([4294967297],"float16"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.730012 test begin: paddle.Tensor.mean(Tensor([42949673, 100],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([42949673, 100],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.739883 test begin: paddle.Tensor.mean(Tensor([429496730, 10],"float16"), )

[torch error] paddle.Tensor.mean(Tensor([429496730, 10],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.750070 test begin: paddle.Tensor.mean(Tensor([45474, 1024, 7, 7],"float32"), list[-2,-1,], )

[torch error] paddle.Tensor.mean(Tensor([45474, 1024, 7, 7],"float32"), list[-2,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.759455 test begin: paddle.Tensor.mean(Tensor([465, 192, 160, 160],"float32"), tuple(2,3,), keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([465, 192, 160, 160],"float32"), tuple(2,3,), keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.52 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.768823 test begin: paddle.Tensor.mean(Tensor([5, 107374183, 2, 1, 4],"float16"), )

[torch error] paddle.Tensor.mean(Tensor([5, 107374183, 2, 1, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.778125 test begin: paddle.Tensor.mean(Tensor([5, 3, 2, 1, 143165577],"float16"), )

[torch error] paddle.Tensor.mean(Tensor([5, 3, 2, 1, 143165577],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.787406 test begin: paddle.Tensor.mean(Tensor([5, 3, 2, 35791395, 4],"float16"), )

[torch error] paddle.Tensor.mean(Tensor([5, 3, 2, 35791395, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.796899 test begin: paddle.Tensor.mean(Tensor([5, 3, 71582789, 1, 4],"float16"), )

[torch error] paddle.Tensor.mean(Tensor([5, 3, 71582789, 1, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.806267 test begin: paddle.Tensor.mean(Tensor([5, 858993460],"float16"), axis=-1, )

[torch error] paddle.Tensor.mean(Tensor([5, 858993460],"float16"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.815580 test begin: paddle.Tensor.mean(Tensor([5000, 10, 85900],"float16"), axis=0, )

[torch error] paddle.Tensor.mean(Tensor([5000, 10, 85900],"float16"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.824862 test begin: paddle.Tensor.mean(Tensor([5000, 42950, 20],"float16"), axis=0, )

[torch error] paddle.Tensor.mean(Tensor([5000, 42950, 20],"float16"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.834098 test begin: paddle.Tensor.mean(Tensor([5000, 858994],"float16"), axis=0, )

[torch error] paddle.Tensor.mean(Tensor([5000, 858994],"float16"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.843326 test begin: paddle.Tensor.mean(Tensor([50000, 2, 42950],"float16"), axis=0, )

[torch error] paddle.Tensor.mean(Tensor([50000, 2, 42950],"float16"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.852549 test begin: paddle.Tensor.mean(Tensor([50000, 28634, 3],"float16"), axis=0, )

[torch error] paddle.Tensor.mean(Tensor([50000, 28634, 3],"float16"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.861788 test begin: paddle.Tensor.mean(Tensor([50704476, 1, 45],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([50704476, 1, 45],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.871008 test begin: paddle.Tensor.mean(Tensor([512, 256, 17409],"float32"), -1, )

[torch error] paddle.Tensor.mean(Tensor([512, 256, 17409],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.880272 test begin: paddle.Tensor.mean(Tensor([512, 636636, 7],"float32"), -1, )

[torch error] paddle.Tensor.mean(Tensor([512, 636636, 7],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.889688 test begin: paddle.Tensor.mean(Tensor([515, 192, 152, 152],"float32"), tuple(2,3,), keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([515, 192, 152, 152],"float32"), tuple(2,3,), keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.899435 test begin: paddle.Tensor.mean(Tensor([545, 1024, 4096],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([545, 1024, 4096],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.52 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.908874 test begin: paddle.Tensor.mean(Tensor([5685, 128, 56, 56],"float32"), 1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([5685, 128, 56, 56],"float32"), 1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.918803 test begin: paddle.Tensor.mean(Tensor([570425345, 4],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([570425345, 4],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.928268 test begin: paddle.Tensor.mean(Tensor([570425345, 4],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([570425345, 4],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.937674 test begin: paddle.Tensor.mean(Tensor([5704254, 100, 4],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([5704254, 100, 4],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.947045 test begin: paddle.Tensor.mean(Tensor([574, 192, 144, 144],"float32"), tuple(2,3,), keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([574, 192, 144, 144],"float32"), tuple(2,3,), keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.956426 test begin: paddle.Tensor.mean(Tensor([60632, 49, 768],"float32"), axis=1, )

[torch error] paddle.Tensor.mean(Tensor([60632, 49, 768],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.965724 test begin: paddle.Tensor.mean(Tensor([60632, 768, 7, 7],"float32"), list[-2,-1,], )

[torch error] paddle.Tensor.mean(Tensor([60632, 768, 7, 7],"float32"), list[-2,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.975066 test begin: paddle.Tensor.mean(Tensor([63161284, 17, 4],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([63161284, 17, 4],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.984388 test begin: paddle.Tensor.mean(Tensor([63161284, 4, 17],"float16"), axis=-2, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([63161284, 4, 17],"float16"), axis=-2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:42.993657 test begin: paddle.Tensor.mean(Tensor([63380594, 1, 36],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([63380594, 1, 36],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.003384 test begin: paddle.Tensor.mean(Tensor([643, 192, 136, 136],"float32"), tuple(2,3,), keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([643, 192, 136, 136],"float32"), tuple(2,3,), keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.013213 test begin: paddle.Tensor.mean(Tensor([65537, 256, 16, 16],"float16"), -1, )

[torch error] paddle.Tensor.mean(Tensor([65537, 256, 16, 16],"float16"), -1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.022733 test begin: paddle.Tensor.mean(Tensor([69633, 128, 16, 16],"float32"), -1, )

[torch error] paddle.Tensor.mean(Tensor([69633, 128, 16, 16],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.032140 test begin: paddle.Tensor.mean(Tensor([69633, 128, 16, 16],"float32"), -2, )

[torch error] paddle.Tensor.mean(Tensor([69633, 128, 16, 16],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.041627 test begin: paddle.Tensor.mean(Tensor([71303169, 1, 32],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([71303169, 1, 32],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.051197 test begin: paddle.Tensor.mean(Tensor([71303169, 1, 32],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([71303169, 1, 32],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.060714 test begin: paddle.Tensor.mean(Tensor([71303169, 1, 32],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([71303169, 1, 32],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.070062 test begin: paddle.Tensor.mean(Tensor([7130317, 10, 32],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([7130317, 10, 32],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.079896 test begin: paddle.Tensor.mean(Tensor([7130317, 10, 32],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([7130317, 10, 32],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.089374 test begin: paddle.Tensor.mean(Tensor([715827883, 2, 3],"float16"), axis=0, )

[torch error] paddle.Tensor.mean(Tensor([715827883, 2, 3],"float16"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.098794 test begin: paddle.Tensor.mean(Tensor([726, 192, 128, 128],"float32"), tuple(2,3,), keepdim=True, )

[torch error] paddle.Tensor.mean(Tensor([726, 192, 128, 128],"float32"), tuple(2,3,), keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.108108 test begin: paddle.Tensor.mean(Tensor([80, 28521268],"float32"), 0, )

[torch error] paddle.Tensor.mean(Tensor([80, 28521268],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.117610 test begin: paddle.Tensor.mean(Tensor([80, 28521268],"float32"), axis=-1, )

[torch error] paddle.Tensor.mean(Tensor([80, 28521268],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.126981 test begin: paddle.Tensor.mean(Tensor([858993460, 5],"float16"), axis=-1, )

[torch error] paddle.Tensor.mean(Tensor([858993460, 5],"float16"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.136254 test begin: paddle.Tensor.mean(Tensor([8912897, 4, 2, 4, 8],"float32"), axis=-2, )

[torch error] paddle.Tensor.mean(Tensor([8912897, 4, 2, 4, 8],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.145558 test begin: paddle.Tensor.mean(Tensor([89129, 100, 256],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([89129, 100, 256],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.154899 test begin: paddle.Tensor.mean(Tensor([89129, 160, 160],"float32"), axis=tuple(1,2,), )

[torch error] paddle.Tensor.mean(Tensor([89129, 160, 160],"float32"), axis=tuple(1,2,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.164238 test begin: paddle.Tensor.mean(Tensor([90948, 49, 512],"float32"), axis=1, )

[torch error] paddle.Tensor.mean(Tensor([90948, 49, 512],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.173501 test begin: paddle.Tensor.mean(Tensor([912681, 1, 50, 50],"float32"), )

[torch error] paddle.Tensor.mean(Tensor([912681, 1, 50, 50],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.182731 test begin: paddle.Tensor.median(Tensor([1000, 2281702],"float32"), )

[torch error] paddle.Tensor.median(Tensor([1000, 2281702],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.192693 test begin: paddle.Tensor.median(Tensor([2910334, 784],"float32"), )

[torch error] paddle.Tensor.median(Tensor([2910334, 784],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.203745 test begin: paddle.Tensor.mm(Tensor([10, 10],"float32"), Tensor([10, 228170138],"float32"), )

[torch error] paddle.Tensor.mm(Tensor([10, 10],"float32"), Tensor([10, 228170138],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.215188 test begin: paddle.Tensor.mm(Tensor([10, 10],"float32"), Tensor([228170138, 10],"float32"), )

[torch error] paddle.Tensor.mm(Tensor([10, 10],"float32"), Tensor([228170138, 10],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.225682 test begin: paddle.Tensor.mm(Tensor([10, 228170138],"float32"), Tensor([10, 10],"float32"), )

[torch error] paddle.Tensor.mm(Tensor([10, 228170138],"float32"), Tensor([10, 10],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.235996 test begin: paddle.Tensor.mm(Tensor([10, 228170138],"float32"), Tensor([10, 228170138],"float32"), )

[torch error] paddle.Tensor.mm(Tensor([10, 228170138],"float32"), Tensor([10, 228170138],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.245933 test begin: paddle.Tensor.mm(Tensor([228170138, 10],"float32"), Tensor([10, 10],"float32"), )

[torch error] paddle.Tensor.mm(Tensor([228170138, 10],"float32"), Tensor([10, 10],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.255948 test begin: paddle.Tensor.mm(Tensor([228170138, 10],"float32"), Tensor([228170138, 10],"float32"), )

[torch error] paddle.Tensor.mm(Tensor([228170138, 10],"float32"), Tensor([228170138, 10],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.265922 test begin: paddle.Tensor.mode(Tensor([3, 2, 715827883],"float16"), )

[torch error] paddle.Tensor.mode(Tensor([3, 2, 715827883],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.275912 test begin: paddle.Tensor.mode(Tensor([3, 2, 715827883],"float16"), axis=1, keepdim=False, )

[torch error] paddle.Tensor.mode(Tensor([3, 2, 715827883],"float16"), axis=1, keepdim=False, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.287064 test begin: paddle.Tensor.mode(Tensor([3, 2, 715827883],"float16"), axis=2, keepdim=True, )

[torch error] paddle.Tensor.mode(Tensor([3, 2, 715827883],"float16"), axis=2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.297337 test begin: paddle.Tensor.mode(Tensor([3, 477218589, 3],"float16"), )

[torch error] paddle.Tensor.mode(Tensor([3, 477218589, 3],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.309019 test begin: paddle.Tensor.mode(Tensor([3, 477218589, 3],"float16"), axis=1, keepdim=False, )

[torch error] paddle.Tensor.mode(Tensor([3, 477218589, 3],"float16"), axis=1, keepdim=False, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.321183 test begin: paddle.Tensor.mode(Tensor([3, 477218589, 3],"float16"), axis=2, keepdim=True, )

[torch error] paddle.Tensor.mode(Tensor([3, 477218589, 3],"float16"), axis=2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.331207 test begin: paddle.Tensor.mode(Tensor([715827883, 2, 3],"float16"), )

[torch error] paddle.Tensor.mode(Tensor([715827883, 2, 3],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.340959 test begin: paddle.Tensor.mode(Tensor([715827883, 2, 3],"float16"), axis=1, keepdim=False, )

[torch error] paddle.Tensor.mode(Tensor([715827883, 2, 3],"float16"), axis=1, keepdim=False, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.352924 test begin: paddle.Tensor.mode(Tensor([715827883, 2, 3],"float16"), axis=2, keepdim=True, )

[torch error] paddle.Tensor.mode(Tensor([715827883, 2, 3],"float16"), axis=2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.365022 test begin: paddle.Tensor.moveaxis(Tensor([2, 3, 715827883],"float16"), -2, -1, )

[torch error] paddle.Tensor.moveaxis(Tensor([2, 3, 715827883],"float16"), -2, -1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.375432 test begin: paddle.Tensor.moveaxis(Tensor([2, 429496730, 5],"float16"), -2, -1, )

[torch error] paddle.Tensor.moveaxis(Tensor([2, 429496730, 5],"float16"), -2, -1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.384908 test begin: paddle.Tensor.moveaxis(Tensor([286331154, 3, 5],"float16"), -2, -1, )

[torch error] paddle.Tensor.moveaxis(Tensor([286331154, 3, 5],"float16"), -2, -1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.397772 test begin: paddle.Tensor.moveaxis(x=Tensor([1140850690, 2],"float32"), source=0, destination=1, )

[torch error] paddle.Tensor.moveaxis(x=Tensor([1140850690, 2],"float32"), source=0, destination=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.407121 test begin: paddle.Tensor.moveaxis(x=Tensor([1140850690, 2],"int32"), source=0, destination=1, )

[torch error] paddle.Tensor.moveaxis(x=Tensor([1140850690, 2],"int32"), source=0, destination=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.417032 test begin: paddle.Tensor.moveaxis(x=Tensor([143165577, 2, 3, 5],"float16"), source=0, destination=2, )

[torch error] paddle.Tensor.moveaxis(x=Tensor([143165577, 2, 3, 5],"float16"), source=0, destination=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.426380 test begin: paddle.Tensor.moveaxis(x=Tensor([20452226, 2, 3, 5, 7],"float16"), source=0, destination=2, )

[torch error] paddle.Tensor.moveaxis(x=Tensor([20452226, 2, 3, 5, 7],"float16"), source=0, destination=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.435677 test begin: paddle.Tensor.moveaxis(x=Tensor([20452226, 2, 3, 5, 7],"float16"), source=tuple(0,1,), destination=tuple(2,3,), )

[torch error] paddle.Tensor.moveaxis(x=Tensor([20452226, 2, 3, 5, 7],"float16"), source=tuple(0,1,), destination=tuple(2,3,), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.445058 test begin: paddle.Tensor.moveaxis(x=Tensor([2147483649, 2],"float16"), source=0, destination=1, )

[torch error] paddle.Tensor.moveaxis(x=Tensor([2147483649, 2],"float16"), source=0, destination=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.454418 test begin: paddle.Tensor.moveaxis(x=Tensor([4, 10226113, 3, 5, 7],"float16"), source=0, destination=2, )

[torch error] paddle.Tensor.moveaxis(x=Tensor([4, 10226113, 3, 5, 7],"float16"), source=0, destination=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.463997 test begin: paddle.Tensor.moveaxis(x=Tensor([4, 10226113, 3, 5, 7],"float16"), source=tuple(0,1,), destination=tuple(2,3,), )

[torch error] paddle.Tensor.moveaxis(x=Tensor([4, 10226113, 3, 5, 7],"float16"), source=tuple(0,1,), destination=tuple(2,3,), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.473507 test begin: paddle.Tensor.moveaxis(x=Tensor([4, 1073741825],"float16"), source=0, destination=1, )

[torch error] paddle.Tensor.moveaxis(x=Tensor([4, 1073741825],"float16"), source=0, destination=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.483087 test begin: paddle.Tensor.moveaxis(x=Tensor([4, 2, 107374183, 5],"float16"), source=0, destination=2, )

[torch error] paddle.Tensor.moveaxis(x=Tensor([4, 2, 107374183, 5],"float16"), source=0, destination=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.492497 test begin: paddle.Tensor.moveaxis(x=Tensor([4, 2, 15339169, 5, 7],"float16"), source=0, destination=2, )

[torch error] paddle.Tensor.moveaxis(x=Tensor([4, 2, 15339169, 5, 7],"float16"), source=0, destination=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.502089 test begin: paddle.Tensor.moveaxis(x=Tensor([4, 2, 15339169, 5, 7],"float16"), source=tuple(0,1,), destination=tuple(2,3,), )

[torch error] paddle.Tensor.moveaxis(x=Tensor([4, 2, 15339169, 5, 7],"float16"), source=tuple(0,1,), destination=tuple(2,3,), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.511588 test begin: paddle.Tensor.moveaxis(x=Tensor([4, 2, 3, 178956971],"float16"), source=0, destination=2, )

[torch error] paddle.Tensor.moveaxis(x=Tensor([4, 2, 3, 178956971],"float16"), source=0, destination=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.520987 test begin: paddle.Tensor.moveaxis(x=Tensor([4, 2, 3, 25565282, 7],"float16"), source=0, destination=2, )

[torch error] paddle.Tensor.moveaxis(x=Tensor([4, 2, 3, 25565282, 7],"float16"), source=0, destination=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.530408 test begin: paddle.Tensor.moveaxis(x=Tensor([4, 2, 3, 25565282, 7],"float16"), source=tuple(0,1,), destination=tuple(2,3,), )

[torch error] paddle.Tensor.moveaxis(x=Tensor([4, 2, 3, 25565282, 7],"float16"), source=tuple(0,1,), destination=tuple(2,3,), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.539962 test begin: paddle.Tensor.moveaxis(x=Tensor([4, 2, 3, 5, 35791395],"float16"), source=0, destination=2, )

[torch error] paddle.Tensor.moveaxis(x=Tensor([4, 2, 3, 5, 35791395],"float16"), source=0, destination=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.549378 test begin: paddle.Tensor.moveaxis(x=Tensor([4, 2, 3, 5, 35791395],"float16"), source=tuple(0,1,), destination=tuple(2,3,), )

[torch error] paddle.Tensor.moveaxis(x=Tensor([4, 2, 3, 5, 35791395],"float16"), source=tuple(0,1,), destination=tuple(2,3,), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.558744 test begin: paddle.Tensor.moveaxis(x=Tensor([4, 2, 536870913],"float16"), source=0, destination=2, )

[torch error] paddle.Tensor.moveaxis(x=Tensor([4, 2, 536870913],"float16"), source=0, destination=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.568227 test begin: paddle.Tensor.moveaxis(x=Tensor([4, 357913942, 3],"float16"), source=0, destination=2, )

[torch error] paddle.Tensor.moveaxis(x=Tensor([4, 357913942, 3],"float16"), source=0, destination=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.577648 test begin: paddle.Tensor.moveaxis(x=Tensor([4, 570425345],"float32"), source=0, destination=1, )

[torch error] paddle.Tensor.moveaxis(x=Tensor([4, 570425345],"float32"), source=0, destination=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.587005 test begin: paddle.Tensor.moveaxis(x=Tensor([4, 570425345],"int32"), source=0, destination=1, )

[torch error] paddle.Tensor.moveaxis(x=Tensor([4, 570425345],"int32"), source=0, destination=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.596489 test begin: paddle.Tensor.moveaxis(x=Tensor([4, 71582789, 3, 5],"float16"), source=0, destination=2, )

[torch error] paddle.Tensor.moveaxis(x=Tensor([4, 71582789, 3, 5],"float16"), source=0, destination=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.605855 test begin: paddle.Tensor.moveaxis(x=Tensor([715827883, 2, 3],"float16"), source=0, destination=2, )

[torch error] paddle.Tensor.moveaxis(x=Tensor([715827883, 2, 3],"float16"), source=0, destination=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.615202 test begin: paddle.Tensor.multigammaln(Tensor([2281701379],"float32"), 3, )

[torch error] paddle.Tensor.multigammaln(Tensor([2281701379],"float32"), 3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.624553 test begin: paddle.Tensor.nansum(Tensor([17895698, 2, 3, 4, 5, 1, 2],"float16"), axis=3, keepdim=True, )

[torch error] paddle.Tensor.nansum(Tensor([17895698, 2, 3, 4, 5, 1, 2],"float16"), axis=3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.633919 test begin: paddle.Tensor.nansum(Tensor([253522376, 3, 3],"float32"), )

[torch error] paddle.Tensor.nansum(Tensor([253522376, 3, 3],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.643174 test begin: paddle.Tensor.nansum(Tensor([3, 11930465, 3, 4, 5, 1, 2],"float16"), axis=3, keepdim=True, )

[torch error] paddle.Tensor.nansum(Tensor([3, 11930465, 3, 4, 5, 1, 2],"float16"), axis=3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.652673 test begin: paddle.Tensor.nansum(Tensor([3, 2, 17895698, 4, 5, 1, 2],"float16"), axis=3, keepdim=True, )

[torch error] paddle.Tensor.nansum(Tensor([3, 2, 17895698, 4, 5, 1, 2],"float16"), axis=3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.662086 test begin: paddle.Tensor.nansum(Tensor([3, 2, 3, 23860930, 5, 1, 2],"float16"), axis=3, keepdim=True, )

[torch error] paddle.Tensor.nansum(Tensor([3, 2, 3, 23860930, 5, 1, 2],"float16"), axis=3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.671600 test begin: paddle.Tensor.nansum(Tensor([3, 2, 3, 4, 29826162, 1, 2],"float16"), axis=3, keepdim=True, )

[torch error] paddle.Tensor.nansum(Tensor([3, 2, 3, 4, 29826162, 1, 2],"float16"), axis=3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.680955 test begin: paddle.Tensor.nansum(Tensor([3, 2, 3, 4, 5, 1, 11930465],"float16"), axis=3, keepdim=True, )

[torch error] paddle.Tensor.nansum(Tensor([3, 2, 3, 4, 5, 1, 11930465],"float16"), axis=3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.690261 test begin: paddle.Tensor.nansum(Tensor([3, 2, 3, 4, 5, 5965233, 2],"float16"), axis=3, keepdim=True, )

[torch error] paddle.Tensor.nansum(Tensor([3, 2, 3, 4, 5, 5965233, 2],"float16"), axis=3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.699556 test begin: paddle.Tensor.nansum(Tensor([3, 253522376, 3],"float32"), )

[torch error] paddle.Tensor.nansum(Tensor([3, 253522376, 3],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.708826 test begin: paddle.Tensor.nansum(Tensor([3, 3, 253522376],"float32"), )

[torch error] paddle.Tensor.nansum(Tensor([3, 3, 253522376],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.718100 test begin: paddle.Tensor.nansum(Tensor([3, 3, 477218589],"float16"), )

[torch error] paddle.Tensor.nansum(Tensor([3, 3, 477218589],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.727350 test begin: paddle.Tensor.nansum(Tensor([3, 3, 477218589],"float16"), axis=-1, )

[torch error] paddle.Tensor.nansum(Tensor([3, 3, 477218589],"float16"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.736602 test begin: paddle.Tensor.nansum(Tensor([3, 3, 477218589],"float16"), axis=0, keepdim=True, )

[torch error] paddle.Tensor.nansum(Tensor([3, 3, 477218589],"float16"), axis=0, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.745871 test begin: paddle.Tensor.nansum(Tensor([3, 477218589, 3],"float16"), )

[torch error] paddle.Tensor.nansum(Tensor([3, 477218589, 3],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.755203 test begin: paddle.Tensor.nansum(Tensor([3, 477218589, 3],"float16"), axis=-1, )

[torch error] paddle.Tensor.nansum(Tensor([3, 477218589, 3],"float16"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.764532 test begin: paddle.Tensor.nansum(Tensor([3, 477218589, 3],"float16"), axis=0, keepdim=True, )

[torch error] paddle.Tensor.nansum(Tensor([3, 477218589, 3],"float16"), axis=0, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.773821 test begin: paddle.Tensor.nansum(Tensor([477218589, 3, 3],"float16"), axis=-1, )

[torch error] paddle.Tensor.nansum(Tensor([477218589, 3, 3],"float16"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.783122 test begin: paddle.Tensor.outer(x=Tensor([4294967297],"float16"), y=Tensor([4],"float16"), )

[torch error] paddle.Tensor.outer(x=Tensor([4294967297],"float16"), y=Tensor([4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.792468 test begin: paddle.Tensor.outer(x=Tensor([4],"float16"), y=Tensor([4294967297],"float16"), )

[torch error] paddle.Tensor.outer(x=Tensor([4],"float16"), y=Tensor([4294967297],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.801896 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 1, 1140850690],"float32"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 1, 1140850690],"float32"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.811214 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 1, 1140850690],"float32"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 1, 1140850690],"float32"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.820502 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 10, 114085069],"int64"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 10, 114085069],"int64"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.829831 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 10, 114085069],"int64"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 10, 114085069],"int64"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.839165 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 10, 114085069],"int64"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 10, 114085069],"int64"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.848470 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 114085069, 10],"int64"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 114085069, 10],"int64"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.857794 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 114085069, 10],"int64"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 114085069, 10],"int64"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.867099 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 114085069, 10],"int64"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 114085069, 10],"int64"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.876403 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 3, 380283564],"int64"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 3, 380283564],"int64"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.885956 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 3, 380283564],"int64"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 3, 380283564],"int64"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.895383 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 3, 380283564],"int64"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 3, 380283564],"int64"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.904784 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 380283564, 3],"float32"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 380283564, 3],"float32"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.914167 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 380283564, 3],"float32"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 380283564, 3],"float32"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.923630 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 380283564, 3],"float32"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 380283564, 3],"float32"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.936266 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 380283564, 3],"int64"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 380283564, 3],"int64"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.945613 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 380283564, 3],"int64"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 380283564, 3],"int64"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.955013 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1, 380283564, 3],"int64"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1, 380283564, 3],"int64"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.964939 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1140850690],"int64"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1140850690],"int64"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.974366 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1140850690],"int64"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1140850690],"int64"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.983694 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 1140850690],"int64"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 1140850690],"int64"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:43.993057 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 11408507, 10, 10],"int64"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 11408507, 10, 10],"int64"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.003325 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 11408507, 10, 10],"int64"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 11408507, 10, 10],"int64"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.012853 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 11408507, 10, 10],"int64"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 11408507, 10, 10],"int64"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.022262 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 126761188, 3, 3],"int64"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 126761188, 3, 3],"int64"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.031702 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 126761188, 3, 3],"int64"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 126761188, 3, 3],"int64"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.041366 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 126761188, 3, 3],"int64"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 126761188, 3, 3],"int64"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.050880 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 3, 380283564],"float32"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 3, 380283564],"float32"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.060747 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 3, 380283564],"float32"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 3, 380283564],"float32"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.070220 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 3, 380283564],"float32"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 3, 380283564],"float32"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.079672 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 35651585, 32],"float32"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 35651585, 32],"float32"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.089031 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 35651585, 32],"float32"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 35651585, 32],"float32"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.099396 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 35651585, 32],"float32"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 35651585, 32],"float32"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.108930 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 380283564, 1, 3],"float32"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 380283564, 1, 3],"float32"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.118311 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 380283564, 1, 3],"float32"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 380283564, 1, 3],"float32"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.127900 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 380283564, 1, 3],"float32"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 380283564, 1, 3],"float32"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.137375 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 71303169, 16],"float32"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 71303169, 16],"float32"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.146756 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 71303169, 16],"float32"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 71303169, 16],"float32"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.156113 test begin: paddle.Tensor.repeat_interleave(Tensor([2, 71303169, 16],"float32"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2, 71303169, 16],"float32"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.165590 test begin: paddle.Tensor.repeat_interleave(Tensor([2281701379, 1],"int64"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2281701379, 1],"int64"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.174987 test begin: paddle.Tensor.repeat_interleave(Tensor([2281701379, 1],"int64"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2281701379, 1],"int64"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.184825 test begin: paddle.Tensor.repeat_interleave(Tensor([2281701379, 1],"int64"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([2281701379, 1],"int64"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.194186 test begin: paddle.Tensor.repeat_interleave(Tensor([228170138, 10],"int64"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([228170138, 10],"int64"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.203566 test begin: paddle.Tensor.repeat_interleave(Tensor([228170138, 10],"int64"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([228170138, 10],"int64"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.212913 test begin: paddle.Tensor.repeat_interleave(Tensor([228170138, 10],"int64"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([228170138, 10],"int64"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.223967 test begin: paddle.Tensor.repeat_interleave(Tensor([22817014, 1, 10, 10],"int64"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([22817014, 1, 10, 10],"int64"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.240618 test begin: paddle.Tensor.repeat_interleave(Tensor([22817014, 1, 10, 10],"int64"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([22817014, 1, 10, 10],"int64"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.250157 test begin: paddle.Tensor.repeat_interleave(Tensor([22817014, 1, 10, 10],"int64"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([22817014, 1, 10, 10],"int64"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.259964 test begin: paddle.Tensor.repeat_interleave(Tensor([23767723, 3, 32],"float32"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([23767723, 3, 32],"float32"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.269406 test begin: paddle.Tensor.repeat_interleave(Tensor([23767723, 3, 32],"float32"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([23767723, 3, 32],"float32"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.278788 test begin: paddle.Tensor.repeat_interleave(Tensor([23767723, 3, 32],"float32"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([23767723, 3, 32],"float32"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.289046 test begin: paddle.Tensor.repeat_interleave(Tensor([253522376, 1, 3, 3],"int64"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([253522376, 1, 3, 3],"int64"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.298875 test begin: paddle.Tensor.repeat_interleave(Tensor([253522376, 1, 3, 3],"int64"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([253522376, 1, 3, 3],"int64"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.308599 test begin: paddle.Tensor.repeat_interleave(Tensor([253522376, 1, 3, 3],"int64"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([253522376, 1, 3, 3],"int64"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.318071 test begin: paddle.Tensor.repeat_interleave(Tensor([456340276, 5],"int64"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([456340276, 5],"int64"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.327473 test begin: paddle.Tensor.repeat_interleave(Tensor([456340276, 5],"int64"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([456340276, 5],"int64"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.336828 test begin: paddle.Tensor.repeat_interleave(Tensor([456340276, 5],"int64"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([456340276, 5],"int64"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.346336 test begin: paddle.Tensor.repeat_interleave(Tensor([47535446, 3, 16],"float32"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([47535446, 3, 16],"float32"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.356117 test begin: paddle.Tensor.repeat_interleave(Tensor([47535446, 3, 16],"float32"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([47535446, 3, 16],"float32"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.365586 test begin: paddle.Tensor.repeat_interleave(Tensor([47535446, 3, 16],"float32"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([47535446, 3, 16],"float32"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.375051 test begin: paddle.Tensor.repeat_interleave(Tensor([760567127, 1, 1, 3],"float32"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([760567127, 1, 1, 3],"float32"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.384395 test begin: paddle.Tensor.repeat_interleave(Tensor([760567127, 1, 1, 3],"float32"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([760567127, 1, 1, 3],"float32"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.394081 test begin: paddle.Tensor.repeat_interleave(Tensor([760567127, 1, 1, 3],"float32"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([760567127, 1, 1, 3],"float32"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.404226 test begin: paddle.Tensor.repeat_interleave(Tensor([760567127, 3],"int64"), 1, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([760567127, 3],"int64"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.415476 test begin: paddle.Tensor.repeat_interleave(Tensor([760567127, 3],"int64"), 2, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([760567127, 3],"int64"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.426827 test begin: paddle.Tensor.repeat_interleave(Tensor([760567127, 3],"int64"), 3, axis=0, )

[torch error] paddle.Tensor.repeat_interleave(Tensor([760567127, 3],"int64"), 3, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.437700 test begin: paddle.Tensor.repeat_interleave(x=Tensor([107374183, 2, 4, 5],"float16"), repeats=2, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([107374183, 2, 4, 5],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.448137 test begin: paddle.Tensor.repeat_interleave(x=Tensor([14260634, 2, 4, 4, 5],"int32"), repeats=2, axis=3, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([14260634, 2, 4, 4, 5],"int32"), repeats=2, axis=3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.458771 test begin: paddle.Tensor.repeat_interleave(x=Tensor([2147483649, 2],"float16"), repeats=2, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([2147483649, 2],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.468899 test begin: paddle.Tensor.repeat_interleave(x=Tensor([2281701379],"float32"), repeats=3, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([2281701379],"float32"), repeats=3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.478868 test begin: paddle.Tensor.repeat_interleave(x=Tensor([26843546, 2, 4, 4, 5],"float16"), repeats=2, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([26843546, 2, 4, 4, 5],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.488891 test begin: paddle.Tensor.repeat_interleave(x=Tensor([26843546, 2, 4, 4, 5],"float16"), repeats=2, axis=1, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([26843546, 2, 4, 4, 5],"float16"), repeats=2, axis=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.498949 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 1073741825],"float16"), repeats=2, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 1073741825],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.508409 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 13421773, 4, 4, 5],"float16"), repeats=2, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 13421773, 4, 4, 5],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.521044 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 13421773, 4, 4, 5],"float16"), repeats=2, axis=1, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 13421773, 4, 4, 5],"float16"), repeats=2, axis=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.531215 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 107374183, 5],"float16"), repeats=2, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 107374183, 5],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.542214 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 14260634, 4, 5],"int32"), repeats=2, axis=3, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 14260634, 4, 5],"int32"), repeats=2, axis=3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.552774 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 26843546, 4, 5],"float16"), repeats=2, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 26843546, 4, 5],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.562691 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 26843546, 4, 5],"float16"), repeats=2, axis=1, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 26843546, 4, 5],"float16"), repeats=2, axis=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.572178 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 134217729],"float16"), repeats=2, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 134217729],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.585522 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 14260634, 5],"int32"), repeats=2, axis=3, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 14260634, 5],"int32"), repeats=2, axis=3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.595272 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 26843546, 5],"float16"), repeats=2, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 26843546, 5],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.604869 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 26843546, 5],"float16"), repeats=2, axis=1, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 26843546, 5],"float16"), repeats=2, axis=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.614887 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 4, 17825793],"int32"), repeats=2, axis=3, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 4, 17825793],"int32"), repeats=2, axis=3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.624437 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 4, 33554433],"float16"), repeats=2, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 4, 33554433],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.633804 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 4, 33554433],"float16"), repeats=2, axis=1, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 4, 4, 33554433],"float16"), repeats=2, axis=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.643531 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 536870913],"float16"), repeats=2, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 2, 536870913],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.653146 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 268435457, 4],"float16"), repeats=2, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 268435457, 4],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.662793 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 53687092, 4, 5],"float16"), repeats=2, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 53687092, 4, 5],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.672779 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4, 7130317, 4, 4, 5],"int32"), repeats=2, axis=3, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4, 7130317, 4, 4, 5],"int32"), repeats=2, axis=3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.682383 test begin: paddle.Tensor.repeat_interleave(x=Tensor([4294967297],"float16"), repeats=3, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([4294967297],"float16"), repeats=3, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.691797 test begin: paddle.Tensor.repeat_interleave(x=Tensor([536870913, 2, 4],"float16"), repeats=2, )

[torch error] paddle.Tensor.repeat_interleave(x=Tensor([536870913, 2, 4],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.702250 test begin: paddle.Tensor.rot90(Tensor([1140850690, 2],"float32"), 1, axes=list[0,1,], )

[torch error] paddle.Tensor.rot90(Tensor([1140850690, 2],"float32"), 1, axes=list[0,1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.711891 test begin: paddle.Tensor.rot90(Tensor([3, 760567127],"float32"), 1, axes=list[0,1,], )

[torch error] paddle.Tensor.rot90(Tensor([3, 760567127],"float32"), 1, axes=list[0,1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.721396 test begin: paddle.Tensor.rot90(x=Tensor([1073741825, 4],"float16"), )

[torch error] paddle.Tensor.rot90(x=Tensor([1073741825, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.732677 test begin: paddle.Tensor.rot90(x=Tensor([1073741825, 4],"float16"), k=-1, )

[torch error] paddle.Tensor.rot90(x=Tensor([1073741825, 4],"float16"), k=-1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.742611 test begin: paddle.Tensor.rot90(x=Tensor([1073741825, 4],"float16"), k=4, )

[torch error] paddle.Tensor.rot90(x=Tensor([1073741825, 4],"float16"), k=4, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.752154 test begin: paddle.Tensor.rot90(x=Tensor([2, 2147483649],"float16"), k=-4, )

[torch error] paddle.Tensor.rot90(x=Tensor([2, 2147483649],"float16"), k=-4, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.763266 test begin: paddle.Tensor.rot90(x=Tensor([2147483649, 2],"float16"), k=-4, )

[torch error] paddle.Tensor.rot90(x=Tensor([2147483649, 2],"float16"), k=-4, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.773118 test begin: paddle.Tensor.rot90(x=Tensor([268435457, 4, 4],"float16"), )

[torch error] paddle.Tensor.rot90(x=Tensor([268435457, 4, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.783130 test begin: paddle.Tensor.rot90(x=Tensor([3, 1431655766],"float16"), )

[torch error] paddle.Tensor.rot90(x=Tensor([3, 1431655766],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.792533 test begin: paddle.Tensor.rot90(x=Tensor([4, 1073741825],"float16"), )

[torch error] paddle.Tensor.rot90(x=Tensor([4, 1073741825],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.801813 test begin: paddle.Tensor.rot90(x=Tensor([4, 1073741825],"float16"), k=-1, )

[torch error] paddle.Tensor.rot90(x=Tensor([4, 1073741825],"float16"), k=-1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.811422 test begin: paddle.Tensor.rot90(x=Tensor([4, 1073741825],"float16"), k=4, )

[torch error] paddle.Tensor.rot90(x=Tensor([4, 1073741825],"float16"), k=4, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.820870 test begin: paddle.Tensor.rot90(x=Tensor([4, 268435457, 4],"float16"), )

[torch error] paddle.Tensor.rot90(x=Tensor([4, 268435457, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.830251 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 268435457],"float16"), )

[torch error] paddle.Tensor.rot90(x=Tensor([4, 4, 268435457],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.839566 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 4, 67108865],"float16"), )

[torch error] paddle.Tensor.rot90(x=Tensor([4, 4, 4, 67108865],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.849068 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 4, 67108865],"float16"), k=-1, axes=list[1,2,], )

[torch error] paddle.Tensor.rot90(x=Tensor([4, 4, 4, 67108865],"float16"), k=-1, axes=list[1,2,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.858543 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 4, 67108865],"float16"), k=-1, axes=tuple(2,3,), )

[torch error] paddle.Tensor.rot90(x=Tensor([4, 4, 4, 67108865],"float16"), k=-1, axes=tuple(2,3,), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.867982 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 67108865, 4],"float16"), )

[torch error] paddle.Tensor.rot90(x=Tensor([4, 4, 67108865, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.877416 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 67108865, 4],"float16"), k=-1, axes=list[1,2,], )

[torch error] paddle.Tensor.rot90(x=Tensor([4, 4, 67108865, 4],"float16"), k=-1, axes=list[1,2,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.886860 test begin: paddle.Tensor.rot90(x=Tensor([4, 4, 67108865, 4],"float16"), k=-1, axes=tuple(2,3,), )

[torch error] paddle.Tensor.rot90(x=Tensor([4, 4, 67108865, 4],"float16"), k=-1, axes=tuple(2,3,), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.896258 test begin: paddle.Tensor.rot90(x=Tensor([4, 570425345],"float32"), )

[torch error] paddle.Tensor.rot90(x=Tensor([4, 570425345],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.905516 test begin: paddle.Tensor.rot90(x=Tensor([4, 570425345],"int32"), )

[torch error] paddle.Tensor.rot90(x=Tensor([4, 570425345],"int32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.914766 test begin: paddle.Tensor.rot90(x=Tensor([4, 570425345],"int64"), )

[torch error] paddle.Tensor.rot90(x=Tensor([4, 570425345],"int64"), ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.924188 test begin: paddle.Tensor.rot90(x=Tensor([4, 67108865, 4, 4],"float16"), )

[torch error] paddle.Tensor.rot90(x=Tensor([4, 67108865, 4, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.933445 test begin: paddle.Tensor.rot90(x=Tensor([4, 67108865, 4, 4],"float16"), k=-1, axes=list[1,2,], )

[torch error] paddle.Tensor.rot90(x=Tensor([4, 67108865, 4, 4],"float16"), k=-1, axes=list[1,2,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.943340 test begin: paddle.Tensor.rot90(x=Tensor([4, 67108865, 4, 4],"float16"), k=-1, axes=tuple(2,3,), )

[torch error] paddle.Tensor.rot90(x=Tensor([4, 67108865, 4, 4],"float16"), k=-1, axes=tuple(2,3,), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.952856 test begin: paddle.Tensor.rot90(x=Tensor([570425345, 4],"float32"), )

[torch error] paddle.Tensor.rot90(x=Tensor([570425345, 4],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.962175 test begin: paddle.Tensor.rot90(x=Tensor([570425345, 4],"int32"), )

[torch error] paddle.Tensor.rot90(x=Tensor([570425345, 4],"int32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.971475 test begin: paddle.Tensor.rot90(x=Tensor([570425345, 4],"int64"), )

[torch error] paddle.Tensor.rot90(x=Tensor([570425345, 4],"int64"), ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.981570 test begin: paddle.Tensor.rot90(x=Tensor([67108865, 4, 4, 4],"float16"), )

[torch error] paddle.Tensor.rot90(x=Tensor([67108865, 4, 4, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:44.991086 test begin: paddle.Tensor.rot90(x=Tensor([67108865, 4, 4, 4],"float16"), k=-1, axes=list[1,2,], )

[torch error] paddle.Tensor.rot90(x=Tensor([67108865, 4, 4, 4],"float16"), k=-1, axes=list[1,2,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:45.000501 test begin: paddle.Tensor.rot90(x=Tensor([67108865, 4, 4, 4],"float16"), k=-1, axes=tuple(2,3,), )

[torch error] paddle.Tensor.rot90(x=Tensor([67108865, 4, 4, 4],"float16"), k=-1, axes=tuple(2,3,), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:45.010123 test begin: paddle.Tensor.signbit(Tensor([107374183, 20, 2],"float16"), )

[torch error] paddle.Tensor.signbit(Tensor([107374183, 20, 2],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:51:45.019539 test begin: paddle.Tensor.signbit(Tensor([107374183, 20, 2],"int16"), )

[torch error] paddle.Tensor.signbit(Tensor([107374183, 20, 2],"int16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:21.771389 test begin: paddle.Tensor.signbit(Tensor([12, 178956971, 2],"float16"), )

[torch error] paddle.Tensor.signbit(Tensor([12, 178956971, 2],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:21.781912 test begin: paddle.Tensor.signbit(Tensor([12, 178956971, 2],"int16"), )

[torch error] paddle.Tensor.signbit(Tensor([12, 178956971, 2],"int16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:21.791605 test begin: paddle.Tensor.signbit(Tensor([12, 20, 17895698],"float16"), )

[torch error] paddle.Tensor.signbit(Tensor([12, 20, 17895698],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:21.801207 test begin: paddle.Tensor.signbit(Tensor([12, 20, 17895698],"int16"), )

[torch error] paddle.Tensor.signbit(Tensor([12, 20, 17895698],"int16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:21.810564 test begin: paddle.Tensor.signbit(Tensor([12, 20, 9507090],"float32"), )

[torch error] paddle.Tensor.signbit(Tensor([12, 20, 9507090],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:21.819927 test begin: paddle.Tensor.signbit(Tensor([12, 20, 9507090],"int32"), )

[torch error] paddle.Tensor.signbit(Tensor([12, 20, 9507090],"int32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:21.829317 test begin: paddle.Tensor.signbit(Tensor([12, 20, 9507090],"int64"), )

[torch error] paddle.Tensor.signbit(Tensor([12, 20, 9507090],"int64"), ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:21.838617 test begin: paddle.Tensor.signbit(Tensor([12, 95070891, 2],"float32"), )

[torch error] paddle.Tensor.signbit(Tensor([12, 95070891, 2],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:21.848824 test begin: paddle.Tensor.signbit(Tensor([12, 95070891, 2],"int32"), )

[torch error] paddle.Tensor.signbit(Tensor([12, 95070891, 2],"int32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:21.858324 test begin: paddle.Tensor.signbit(Tensor([12, 95070891, 2],"int64"), )

[torch error] paddle.Tensor.signbit(Tensor([12, 95070891, 2],"int64"), ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:21.867818 test begin: paddle.Tensor.signbit(Tensor([57042535, 20, 2],"float32"), )

[torch error] paddle.Tensor.signbit(Tensor([57042535, 20, 2],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:21.877163 test begin: paddle.Tensor.signbit(Tensor([57042535, 20, 2],"int32"), )

[torch error] paddle.Tensor.signbit(Tensor([57042535, 20, 2],"int32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:21.886563 test begin: paddle.Tensor.signbit(Tensor([57042535, 20, 2],"int64"), )

[torch error] paddle.Tensor.signbit(Tensor([57042535, 20, 2],"int64"), ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:21.895909 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 1, 2281701379],"float32"), axis=list[0,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 1, 2281701379],"float32"), axis=list[0,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:21.905358 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 1, 2281701379],"float32"), list[1,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 1, 2281701379],"float32"), list[1,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:21.914642 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 1, 2281701379],"int64"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 1, 2281701379],"int64"), -2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:21.924063 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 10, 228170138],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 10, 228170138],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:21.933638 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 114085069, 20],"int64"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 114085069, 20],"int64"), -2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:21.943083 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 11408507, 200],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 11408507, 200],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:21.952535 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 12, 190141782],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 12, 190141782],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:21.962173 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 144, 15845149],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 144, 15845149],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:21.971590 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 15845149, 144],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 15845149, 144],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:21.981245 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 18, 126761188],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 18, 126761188],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:21.990869 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 181896, 12544],"float32"), list[1,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 181896, 12544],"float32"), list[1,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.000913 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 192, 11883862],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 192, 11883862],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.010506 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 21504, 106106],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 21504, 106106],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.019902 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 2281701379],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 2281701379],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.029476 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 2281701379],"int32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 2281701379],"int32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.039034 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 24276, 93991],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 24276, 93991],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.048910 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 253522376, 9],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 253522376, 9],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.058483 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 27216, 83837],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 27216, 83837],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.067861 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 285212673, 8],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 285212673, 8],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.077445 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 285212673, 8],"float32"), axis=list[0,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 285212673, 8],"float32"), axis=list[0,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.088595 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 30324, 75245],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 30324, 75245],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.099330 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 33600, 67908],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 33600, 67908],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.110296 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 570425345, 4],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 570425345, 4],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.120971 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 60632, 37632],"float32"), list[1,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 60632, 37632],"float32"), list[1,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.131318 test begin: paddle.Tensor.squeeze(Tensor([1, 1, 91268056, 25],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1, 91268056, 25],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.141385 test begin: paddle.Tensor.squeeze(Tensor([1, 10, 1, 228170138],"float32"), axis=list[0,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([1, 10, 1, 228170138],"float32"), axis=list[0,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.151471 test begin: paddle.Tensor.squeeze(Tensor([1, 10, 1, 429496730],"float16"), axis=list[0,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([1, 10, 1, 429496730],"float16"), axis=list[0,2,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.161857 test begin: paddle.Tensor.squeeze(Tensor([1, 10, 228170138],"int32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 10, 228170138],"int32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.172282 test begin: paddle.Tensor.squeeze(Tensor([1, 10, 28521268, 8],"float32"), axis=list[0,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([1, 10, 28521268, 8],"float32"), axis=list[0,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.182181 test begin: paddle.Tensor.squeeze(Tensor([1, 10, 4473925, 96],"float16"), axis=list[0,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([1, 10, 4473925, 96],"float16"), axis=list[0,2,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.194069 test begin: paddle.Tensor.squeeze(Tensor([1, 10, 53687092, 8],"float16"), axis=list[0,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([1, 10, 53687092, 8],"float16"), axis=list[0,2,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.206325 test begin: paddle.Tensor.squeeze(Tensor([1, 10, 891290, 256],"float32"), axis=list[0,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([1, 10, 891290, 256],"float32"), axis=list[0,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.218186 test begin: paddle.Tensor.squeeze(Tensor([1, 100, 22817014],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 100, 22817014],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.228815 test begin: paddle.Tensor.squeeze(Tensor([1, 101, 22591103],"int32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 101, 22591103],"int32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.240216 test begin: paddle.Tensor.squeeze(Tensor([1, 102, 22369622],"int32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 102, 22369622],"int32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.250292 test begin: paddle.Tensor.squeeze(Tensor([1, 103, 22152441],"int32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 103, 22152441],"int32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.264104 test begin: paddle.Tensor.squeeze(Tensor([1, 108, 21126865],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 108, 21126865],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.273603 test begin: paddle.Tensor.squeeze(Tensor([1, 114085069, 1, 20],"int64"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 114085069, 1, 20],"int64"), -2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.283021 test begin: paddle.Tensor.squeeze(Tensor([1, 117, 19501722],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 117, 19501722],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.292632 test begin: paddle.Tensor.squeeze(Tensor([1, 1200, 1901418],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1200, 1901418],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.302104 test begin: paddle.Tensor.squeeze(Tensor([1, 1273271, 1792],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 1273271, 1792],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.311529 test begin: paddle.Tensor.squeeze(Tensor([1, 139265, 128, 128],"float32"), )

[torch error] paddle.Tensor.squeeze(Tensor([1, 139265, 128, 128],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.320884 test begin: paddle.Tensor.squeeze(Tensor([1, 16977, 33600, 4],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 16977, 33600, 4],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.330410 test begin: paddle.Tensor.squeeze(Tensor([1, 17, 1048577, 128],"float32"), )

[torch error] paddle.Tensor.squeeze(Tensor([1, 17, 1048577, 128],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.339803 test begin: paddle.Tensor.squeeze(Tensor([1, 17, 128, 1048577],"float32"), )

[torch error] paddle.Tensor.squeeze(Tensor([1, 17, 128, 1048577],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.349155 test begin: paddle.Tensor.squeeze(Tensor([1, 17, 128, 1973791],"float16"), )

[torch error] paddle.Tensor.squeeze(Tensor([1, 17, 128, 1973791],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.358526 test begin: paddle.Tensor.squeeze(Tensor([1, 17, 1973791, 128],"float16"), )

[torch error] paddle.Tensor.squeeze(Tensor([1, 17, 1973791, 128],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.368042 test begin: paddle.Tensor.squeeze(Tensor([1, 181896, 1, 12544],"float32"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 181896, 1, 12544],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.377380 test begin: paddle.Tensor.squeeze(Tensor([1, 181896, 1, 12544],"float32"), list[1,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([1, 181896, 1, 12544],"float32"), list[1,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.386677 test begin: paddle.Tensor.squeeze(Tensor([1, 18812, 30324, 4],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 18812, 30324, 4],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.395964 test begin: paddle.Tensor.squeeze(Tensor([1, 2, 32, 32, 2097153],"float16"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 2, 32, 32, 2097153],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.405304 test begin: paddle.Tensor.squeeze(Tensor([1, 2, 32, 524289, 128],"float16"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 2, 32, 524289, 128],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.414657 test begin: paddle.Tensor.squeeze(Tensor([1, 2, 524289, 32, 128],"float16"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 2, 524289, 32, 128],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.423966 test begin: paddle.Tensor.squeeze(Tensor([1, 20, 114085069],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 20, 114085069],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.433238 test begin: paddle.Tensor.squeeze(Tensor([1, 2048, 1114113],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 2048, 1114113],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.442542 test begin: paddle.Tensor.squeeze(Tensor([1, 20960, 27216, 4],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 20960, 27216, 4],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.451827 test begin: paddle.Tensor.squeeze(Tensor([1, 21126865, 12, 9],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 21126865, 12, 9],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.461176 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379, 1],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 2281701379, 1],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.470499 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 2281701379, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.479782 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379, 1],"int32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 2281701379, 1],"int32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.489124 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379],"float32"), )

[torch error] paddle.Tensor.squeeze(Tensor([1, 2281701379],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.498470 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 2281701379],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.507820 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 2281701379],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.517172 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 2281701379],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.526906 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379],"int32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 2281701379],"int32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.536396 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379],"int32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 2281701379],"int32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.546051 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379],"int64"), )

[torch error] paddle.Tensor.squeeze(Tensor([1, 2281701379],"int64"), ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.555489 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379],"int64"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 2281701379],"int64"), -1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.564890 test begin: paddle.Tensor.squeeze(Tensor([1, 2281701379],"int64"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 2281701379],"int64"), 1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.574287 test begin: paddle.Tensor.squeeze(Tensor([1, 23498, 24276, 4],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 23498, 24276, 4],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.583660 test begin: paddle.Tensor.squeeze(Tensor([1, 262145, 128, 128],"float16"), )

[torch error] paddle.Tensor.squeeze(Tensor([1, 262145, 128, 128],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.593053 test begin: paddle.Tensor.squeeze(Tensor([1, 26527, 21504, 4],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 26527, 21504, 4],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.602567 test begin: paddle.Tensor.squeeze(Tensor([1, 285212673, 1, 8],"float32"), axis=list[0,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([1, 285212673, 1, 8],"float32"), axis=list[0,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.611912 test begin: paddle.Tensor.squeeze(Tensor([1, 28521268, 10, 8],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 28521268, 10, 8],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.621509 test begin: paddle.Tensor.squeeze(Tensor([1, 30, 17, 4473925],"int64"), )

[torch error] paddle.Tensor.squeeze(Tensor([1, 30, 17, 4473925],"int64"), ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.630879 test begin: paddle.Tensor.squeeze(Tensor([1, 30, 19014179, 4],"int64"), )

[torch error] paddle.Tensor.squeeze(Tensor([1, 30, 19014179, 4],"int64"), ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.640230 test begin: paddle.Tensor.squeeze(Tensor([1, 300, 1, 7605672],"float32"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 300, 1, 7605672],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.649550 test begin: paddle.Tensor.squeeze(Tensor([1, 300, 607, 12544],"float32"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 300, 607, 12544],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.658906 test begin: paddle.Tensor.squeeze(Tensor([1, 32769, 32, 32, 128],"float16"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 32769, 32, 32, 128],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.668263 test begin: paddle.Tensor.squeeze(Tensor([1, 33554433, 17, 4],"int64"), )

[torch error] paddle.Tensor.squeeze(Tensor([1, 33554433, 17, 4],"int64"), ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.677659 test begin: paddle.Tensor.squeeze(Tensor([1, 4294967297],"float16"), )

[torch error] paddle.Tensor.squeeze(Tensor([1, 4294967297],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.686970 test begin: paddle.Tensor.squeeze(Tensor([1, 44739243, 1, 96],"float16"), axis=list[0,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([1, 44739243, 1, 96],"float16"), axis=list[0,2,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.696283 test begin: paddle.Tensor.squeeze(Tensor([1, 5070448, 18, 25],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 5070448, 18, 25],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.705597 test begin: paddle.Tensor.squeeze(Tensor([1, 536870913, 1, 8],"float16"), axis=list[0,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([1, 536870913, 1, 8],"float16"), axis=list[0,2,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.714841 test begin: paddle.Tensor.squeeze(Tensor([1, 570425345, 4],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 570425345, 4],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.724103 test begin: paddle.Tensor.squeeze(Tensor([1, 60632, 1, 37632],"float32"), list[1,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([1, 60632, 1, 37632],"float32"), list[1,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.733349 test begin: paddle.Tensor.squeeze(Tensor([1, 79226, 144, 200],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 79226, 144, 200],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.742609 test begin: paddle.Tensor.squeeze(Tensor([1, 82527, 192, 144],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1, 82527, 192, 144],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.752056 test begin: paddle.Tensor.squeeze(Tensor([1, 8912897, 1, 256],"float32"), axis=list[0,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([1, 8912897, 1, 256],"float32"), axis=list[0,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.761410 test begin: paddle.Tensor.squeeze(Tensor([10, 228170138],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([10, 228170138],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.770703 test begin: paddle.Tensor.squeeze(Tensor([10, 228170138],"int32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([10, 228170138],"int32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.779971 test begin: paddle.Tensor.squeeze(Tensor([10, 228170138],"int64"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([10, 228170138],"int64"), 1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.789893 test begin: paddle.Tensor.squeeze(Tensor([100, 1, 1, 22817014],"float32"), list[1,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([100, 1, 1, 22817014],"float32"), list[1,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.799358 test begin: paddle.Tensor.squeeze(Tensor([100, 1, 1819, 12544],"float32"), list[1,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([100, 1, 1819, 12544],"float32"), list[1,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.808873 test begin: paddle.Tensor.squeeze(Tensor([100, 1, 607, 37632],"float32"), list[1,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([100, 1, 607, 37632],"float32"), list[1,2,], ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.818329 test begin: paddle.Tensor.squeeze(Tensor([100, 1819, 1, 12544],"float32"), list[1,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([100, 1819, 1, 12544],"float32"), list[1,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.827715 test begin: paddle.Tensor.squeeze(Tensor([100, 22817014],"int64"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([100, 22817014],"int64"), 1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.837201 test begin: paddle.Tensor.squeeze(Tensor([100, 607, 1, 37632],"float32"), list[1,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([100, 607, 1, 37632],"float32"), list[1,2,], ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.846595 test begin: paddle.Tensor.squeeze(Tensor([1000, 2281702],"int64"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([1000, 2281702],"int64"), 1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.855897 test begin: paddle.Tensor.squeeze(Tensor([1073741825, 4, 1],"float16"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([1073741825, 4, 1],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.865202 test begin: paddle.Tensor.squeeze(Tensor([11, 1, 207427399],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([11, 1, 207427399],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.874483 test begin: paddle.Tensor.squeeze(Tensor([11, 51856850, 4],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([11, 51856850, 4],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.883748 test begin: paddle.Tensor.squeeze(Tensor([1114113, 2048, 1],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([1114113, 2048, 1],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.893083 test begin: paddle.Tensor.squeeze(Tensor([111412, 512, 1, 40],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([111412, 512, 1, 40],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.902467 test begin: paddle.Tensor.squeeze(Tensor([1118482, 30, 17, 4],"int64"), )

[torch error] paddle.Tensor.squeeze(Tensor([1118482, 30, 17, 4],"int64"), ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.911777 test begin: paddle.Tensor.squeeze(Tensor([114085069, 1, 1, 20],"int64"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([114085069, 1, 1, 20],"int64"), -2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.921161 test begin: paddle.Tensor.squeeze(Tensor([114085069, 10, 2, 1],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([114085069, 10, 2, 1],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.930462 test begin: paddle.Tensor.squeeze(Tensor([114085069, 20, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([114085069, 20, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.939757 test begin: paddle.Tensor.squeeze(Tensor([114085069, 20],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([114085069, 20],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.949125 test begin: paddle.Tensor.squeeze(Tensor([1140850690, 1, 2],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([1140850690, 1, 2],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.958398 test begin: paddle.Tensor.squeeze(Tensor([1140850690, 2, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([1140850690, 2, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.967824 test begin: paddle.Tensor.squeeze(Tensor([12, 1, 30, 6338060],"float32"), )

[torch error] paddle.Tensor.squeeze(Tensor([12, 1, 30, 6338060],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.977689 test begin: paddle.Tensor.squeeze(Tensor([12, 1, 6338060, 30],"float32"), )

[torch error] paddle.Tensor.squeeze(Tensor([12, 1, 6338060, 30],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.987275 test begin: paddle.Tensor.squeeze(Tensor([12, 211269, 30, 30],"float32"), )

[torch error] paddle.Tensor.squeeze(Tensor([12, 211269, 30, 30],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:22.996814 test begin: paddle.Tensor.squeeze(Tensor([1273271, 1, 1792],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1273271, 1, 1792],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.006270 test begin: paddle.Tensor.squeeze(Tensor([1273271, 1, 1792],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([1273271, 1, 1792],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.016040 test begin: paddle.Tensor.squeeze(Tensor([128, 120, 1, 148549],"float32"), axis=2, )

[torch error] paddle.Tensor.squeeze(Tensor([128, 120, 1, 148549],"float32"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.025471 test begin: paddle.Tensor.squeeze(Tensor([128, 120, 1, 279621],"float16"), axis=2, )

[torch error] paddle.Tensor.squeeze(Tensor([128, 120, 1, 279621],"float16"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.034955 test begin: paddle.Tensor.squeeze(Tensor([128, 120, 3714, 40],"float32"), axis=2, )

[torch error] paddle.Tensor.squeeze(Tensor([128, 120, 3714, 40],"float32"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.044367 test begin: paddle.Tensor.squeeze(Tensor([128, 120, 6991, 40],"float16"), axis=2, )

[torch error] paddle.Tensor.squeeze(Tensor([128, 120, 6991, 40],"float16"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.053864 test begin: paddle.Tensor.squeeze(Tensor([128, 445645, 1, 40],"float32"), axis=2, )

[torch error] paddle.Tensor.squeeze(Tensor([128, 445645, 1, 40],"float32"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.063489 test begin: paddle.Tensor.squeeze(Tensor([128, 838861, 1, 40],"float16"), axis=2, )

[torch error] paddle.Tensor.squeeze(Tensor([128, 838861, 1, 40],"float16"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.072869 test begin: paddle.Tensor.squeeze(Tensor([12988, 1, 175678],"float32"), )

[torch error] paddle.Tensor.squeeze(Tensor([12988, 1, 175678],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.082135 test begin: paddle.Tensor.squeeze(Tensor([12988, 1, 330688],"float16"), )

[torch error] paddle.Tensor.squeeze(Tensor([12988, 1, 330688],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.091352 test begin: paddle.Tensor.squeeze(Tensor([12988, 2745, 64],"float32"), )

[torch error] paddle.Tensor.squeeze(Tensor([12988, 2745, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.100562 test begin: paddle.Tensor.squeeze(Tensor([12988, 5167, 64],"float16"), )

[torch error] paddle.Tensor.squeeze(Tensor([12988, 5167, 64],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.109909 test begin: paddle.Tensor.squeeze(Tensor([13, 1, 1, 175515491],"int32"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([13, 1, 1, 175515491],"int32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.119188 test begin: paddle.Tensor.squeeze(Tensor([13, 1, 1, 175515491],"int64"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([13, 1, 1, 175515491],"int64"), -2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.128697 test begin: paddle.Tensor.squeeze(Tensor([13, 1, 175515491, 1],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([13, 1, 175515491, 1],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.138122 test begin: paddle.Tensor.squeeze(Tensor([13, 1, 25073642, 7],"int32"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([13, 1, 25073642, 7],"int32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.147566 test begin: paddle.Tensor.squeeze(Tensor([13, 1, 25073642, 7],"int64"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([13, 1, 25073642, 7],"int64"), -2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.156918 test begin: paddle.Tensor.squeeze(Tensor([13, 1, 96, 1828287],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([13, 1, 96, 1828287],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.166270 test begin: paddle.Tensor.squeeze(Tensor([13, 175515491, 1],"float32"), )

[torch error] paddle.Tensor.squeeze(Tensor([13, 175515491, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.175606 test begin: paddle.Tensor.squeeze(Tensor([13, 175515491, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([13, 175515491, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.184970 test begin: paddle.Tensor.squeeze(Tensor([13, 175515491],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([13, 175515491],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.194388 test begin: paddle.Tensor.squeeze(Tensor([13, 1828287, 96, 1],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([13, 1828287, 96, 1],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.203730 test begin: paddle.Tensor.squeeze(Tensor([13, 2, 64, 1371215],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([13, 2, 64, 1371215],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.213163 test begin: paddle.Tensor.squeeze(Tensor([13, 2, 87757746, 1],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([13, 2, 87757746, 1],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.222539 test begin: paddle.Tensor.squeeze(Tensor([13, 25073642, 1, 7],"int32"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([13, 25073642, 1, 7],"int32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.231931 test begin: paddle.Tensor.squeeze(Tensor([13, 25073642, 1, 7],"int64"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([13, 25073642, 1, 7],"int64"), -2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.241425 test begin: paddle.Tensor.squeeze(Tensor([13, 2742430, 64, 1],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([13, 2742430, 64, 1],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.251064 test begin: paddle.Tensor.squeeze(Tensor([13, 7, 25073642],"float32"), )

[torch error] paddle.Tensor.squeeze(Tensor([13, 7, 25073642],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.260637 test begin: paddle.Tensor.squeeze(Tensor([13, 7, 25073642],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([13, 7, 25073642],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.270114 test begin: paddle.Tensor.squeeze(Tensor([134217729, 1, 32],"float16"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([134217729, 1, 32],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.280138 test begin: paddle.Tensor.squeeze(Tensor([15421, 17, 128, 128],"float16"), )

[torch error] paddle.Tensor.squeeze(Tensor([15421, 17, 128, 128],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.291468 test begin: paddle.Tensor.squeeze(Tensor([16, 1, 142606337, 1],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([16, 1, 142606337, 1],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.303011 test begin: paddle.Tensor.squeeze(Tensor([16, 1, 142606337],"float32"), )

[torch error] paddle.Tensor.squeeze(Tensor([16, 1, 142606337],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.313612 test begin: paddle.Tensor.squeeze(Tensor([16, 1, 96, 1485483],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([16, 1, 96, 1485483],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.324082 test begin: paddle.Tensor.squeeze(Tensor([16, 10, 111412, 64, 2, 1],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([16, 10, 111412, 64, 2, 1],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.334295 test begin: paddle.Tensor.squeeze(Tensor([16, 10, 14260634, 1],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([16, 10, 14260634, 1],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.344424 test begin: paddle.Tensor.squeeze(Tensor([16, 10, 2, 7130317],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([16, 10, 2, 7130317],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.354580 test begin: paddle.Tensor.squeeze(Tensor([16, 10, 64, 111412, 2, 1],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([16, 10, 64, 111412, 2, 1],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.364610 test begin: paddle.Tensor.squeeze(Tensor([16, 10, 64, 64, 2, 1741],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([16, 10, 64, 64, 2, 1741],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.374819 test begin: paddle.Tensor.squeeze(Tensor([16, 10, 64, 64, 3482, 1],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([16, 10, 64, 64, 3482, 1],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.384572 test begin: paddle.Tensor.squeeze(Tensor([16, 1485483, 96, 1],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([16, 1485483, 96, 1],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.397813 test begin: paddle.Tensor.squeeze(Tensor([16, 17409, 64, 64, 2, 1],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([16, 17409, 64, 64, 2, 1],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.407763 test begin: paddle.Tensor.squeeze(Tensor([16, 185686, 768],"float32"), )

[torch error] paddle.Tensor.squeeze(Tensor([16, 185686, 768],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.417944 test begin: paddle.Tensor.squeeze(Tensor([16, 46422, 3072],"float32"), )

[torch error] paddle.Tensor.squeeze(Tensor([16, 46422, 3072],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.427935 test begin: paddle.Tensor.squeeze(Tensor([16, 65536, 2, 1089],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([16, 65536, 2, 1089],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.437932 test begin: paddle.Tensor.squeeze(Tensor([16, 65536, 2177, 1],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([16, 65536, 2177, 1],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.447976 test begin: paddle.Tensor.squeeze(Tensor([16, 71303169, 2, 1],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([16, 71303169, 2, 1],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.457542 test begin: paddle.Tensor.squeeze(Tensor([16385, 2, 32, 32, 128],"float16"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([16385, 2, 32, 32, 128],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.470612 test begin: paddle.Tensor.squeeze(Tensor([16977, 1, 33600, 4],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([16977, 1, 33600, 4],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.480067 test begin: paddle.Tensor.squeeze(Tensor([17409, 65536, 2, 1],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([17409, 65536, 2, 1],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.489395 test begin: paddle.Tensor.squeeze(Tensor([17825793, 2, 64, 1],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([17825793, 2, 64, 1],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.498681 test begin: paddle.Tensor.squeeze(Tensor([181896, 1, 1, 12544],"float32"), list[1,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([181896, 1, 1, 12544],"float32"), list[1,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.508000 test begin: paddle.Tensor.squeeze(Tensor([18812, 1, 30324, 4],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([18812, 1, 30324, 4],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.517369 test begin: paddle.Tensor.squeeze(Tensor([1901418, 1200],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([1901418, 1200],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.527155 test begin: paddle.Tensor.squeeze(Tensor([190142, 480, 1, 25],"float32"), axis=2, )

[torch error] paddle.Tensor.squeeze(Tensor([190142, 480, 1, 25],"float32"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.536627 test begin: paddle.Tensor.squeeze(Tensor([192, 120, 1, 186414],"float16"), axis=2, )

[torch error] paddle.Tensor.squeeze(Tensor([192, 120, 1, 186414],"float16"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.546078 test begin: paddle.Tensor.squeeze(Tensor([192, 120, 4661, 40],"float16"), axis=2, )

[torch error] paddle.Tensor.squeeze(Tensor([192, 120, 4661, 40],"float16"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.555521 test begin: paddle.Tensor.squeeze(Tensor([192, 559241, 1, 40],"float16"), axis=2, )

[torch error] paddle.Tensor.squeeze(Tensor([192, 559241, 1, 40],"float16"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.565131 test begin: paddle.Tensor.squeeze(Tensor([19501722, 117],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([19501722, 117],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.574531 test begin: paddle.Tensor.squeeze(Tensor([19501722, 117],"int32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([19501722, 117],"int32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.583968 test begin: paddle.Tensor.squeeze(Tensor([2, 1, 1, 1, 1140850690],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([2, 1, 1, 1, 1140850690],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.593406 test begin: paddle.Tensor.squeeze(Tensor([2, 1, 1, 1140850690, 1],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([2, 1, 1, 1140850690, 1],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.602680 test begin: paddle.Tensor.squeeze(Tensor([2, 1, 1, 1140850690],"float32"), list[1,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([2, 1, 1, 1140850690],"float32"), list[1,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.612052 test begin: paddle.Tensor.squeeze(Tensor([2, 1, 1140850690, 1, 1],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([2, 1, 1140850690, 1, 1],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.621369 test begin: paddle.Tensor.squeeze(Tensor([2, 1, 1140850690],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([2, 1, 1140850690],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.630708 test begin: paddle.Tensor.squeeze(Tensor([2, 1, 1140850690],"int64"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([2, 1, 1140850690],"int64"), 1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.640032 test begin: paddle.Tensor.squeeze(Tensor([2, 1, 32, 35651585],"float32"), axis=1, )

[torch error] paddle.Tensor.squeeze(Tensor([2, 1, 32, 35651585],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.649601 test begin: paddle.Tensor.squeeze(Tensor([2, 1, 35651585, 32],"float32"), axis=1, )

[torch error] paddle.Tensor.squeeze(Tensor([2, 1, 35651585, 32],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.659051 test begin: paddle.Tensor.squeeze(Tensor([2, 1, 90948, 12544],"float32"), list[1,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([2, 1, 90948, 12544],"float32"), list[1,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.668450 test begin: paddle.Tensor.squeeze(Tensor([2, 1114113, 32, 32],"float32"), axis=1, )

[torch error] paddle.Tensor.squeeze(Tensor([2, 1114113, 32, 32],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.677988 test begin: paddle.Tensor.squeeze(Tensor([2, 1140850690, 1, 1, 1],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([2, 1140850690, 1, 1, 1],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.687473 test begin: paddle.Tensor.squeeze(Tensor([2, 1140850690, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([2, 1140850690, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.696943 test begin: paddle.Tensor.squeeze(Tensor([2, 1140850690, 1],"int64"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([2, 1140850690, 1],"int64"), 1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.706400 test begin: paddle.Tensor.squeeze(Tensor([2, 1140850690],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([2, 1140850690],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.715738 test begin: paddle.Tensor.squeeze(Tensor([2, 1140850690],"int64"), axis=1, )

[torch error] paddle.Tensor.squeeze(Tensor([2, 1140850690],"int64"), axis=1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.725069 test begin: paddle.Tensor.squeeze(Tensor([2, 2, 570425345],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([2, 2, 570425345],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.734406 test begin: paddle.Tensor.squeeze(Tensor([2, 300, 1, 3802836],"float32"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([2, 300, 1, 3802836],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.743693 test begin: paddle.Tensor.squeeze(Tensor([2, 300, 304, 12544],"float32"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([2, 300, 304, 12544],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.53 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.753012 test begin: paddle.Tensor.squeeze(Tensor([2, 570425345, 2],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([2, 570425345, 2],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.762418 test begin: paddle.Tensor.squeeze(Tensor([2, 636636, 1792],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([2, 636636, 1792],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.771738 test begin: paddle.Tensor.squeeze(Tensor([2, 90948, 1, 12544],"float32"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([2, 90948, 1, 12544],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.781107 test begin: paddle.Tensor.squeeze(Tensor([2, 90948, 1, 12544],"float32"), list[1,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([2, 90948, 1, 12544],"float32"), list[1,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.790693 test begin: paddle.Tensor.squeeze(Tensor([2049, 1, 2096129],"float16"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([2049, 1, 2096129],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.800200 test begin: paddle.Tensor.squeeze(Tensor([2049, 65505, 32],"float16"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([2049, 65505, 32],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.809497 test begin: paddle.Tensor.squeeze(Tensor([20960, 1, 27216, 4],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([20960, 1, 27216, 4],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.818788 test begin: paddle.Tensor.squeeze(Tensor([209716, 512, 1, 40],"float16"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([209716, 512, 1, 40],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.828045 test begin: paddle.Tensor.squeeze(Tensor([21126865, 1, 12, 9],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([21126865, 1, 12, 9],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.837416 test begin: paddle.Tensor.squeeze(Tensor([21126865, 108],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([21126865, 108],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.846670 test begin: paddle.Tensor.squeeze(Tensor([21126865, 108],"int32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([21126865, 108],"int32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.855907 test begin: paddle.Tensor.squeeze(Tensor([22152441, 103, 1],"int32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([22152441, 103, 1],"int32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.865163 test begin: paddle.Tensor.squeeze(Tensor([2228225, 1, 32, 32],"float32"), axis=1, )

[torch error] paddle.Tensor.squeeze(Tensor([2228225, 1, 32, 32],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.874406 test begin: paddle.Tensor.squeeze(Tensor([22369622, 102, 1],"int32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([22369622, 102, 1],"int32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.883804 test begin: paddle.Tensor.squeeze(Tensor([22591103, 101, 1],"int32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([22591103, 101, 1],"int32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.893240 test begin: paddle.Tensor.squeeze(Tensor([2281701379, 1, 1, 1, 1],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([2281701379, 1, 1, 1, 1],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.902952 test begin: paddle.Tensor.squeeze(Tensor([2281701379, 1, 1],"int32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([2281701379, 1, 1],"int32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.913013 test begin: paddle.Tensor.squeeze(Tensor([2281701379, 1, 1],"int64"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([2281701379, 1, 1],"int64"), 1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.922739 test begin: paddle.Tensor.squeeze(Tensor([2281701379, 1],"float32"), )

[torch error] paddle.Tensor.squeeze(Tensor([2281701379, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.932219 test begin: paddle.Tensor.squeeze(Tensor([2281701379, 1],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([2281701379, 1],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.941657 test begin: paddle.Tensor.squeeze(Tensor([2281701379, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([2281701379, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.950932 test begin: paddle.Tensor.squeeze(Tensor([2281701379, 1],"int32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([2281701379, 1],"int32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.960602 test begin: paddle.Tensor.squeeze(Tensor([2281701379, 1],"int64"), )

[torch error] paddle.Tensor.squeeze(Tensor([2281701379, 1],"int64"), ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.970058 test begin: paddle.Tensor.squeeze(Tensor([2281701379, 1],"int64"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([2281701379, 1],"int64"), -1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.979422 test begin: paddle.Tensor.squeeze(Tensor([2281701379, 1],"int64"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([2281701379, 1],"int64"), 1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.990297 test begin: paddle.Tensor.squeeze(Tensor([2281701379, 1],"int64"), axis=1, )

[torch error] paddle.Tensor.squeeze(Tensor([2281701379, 1],"int64"), axis=1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:23.999699 test begin: paddle.Tensor.squeeze(Tensor([2281701379],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([2281701379],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.015675 test begin: paddle.Tensor.squeeze(Tensor([228170138, 10, 1],"int32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([228170138, 10, 1],"int32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.027774 test begin: paddle.Tensor.squeeze(Tensor([2281702, 1000, 1, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([2281702, 1000, 1, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.037256 test begin: paddle.Tensor.squeeze(Tensor([2281702, 1000, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([2281702, 1000, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.053261 test begin: paddle.Tensor.squeeze(Tensor([23498, 1, 24276, 4],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([23498, 1, 24276, 4],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.062996 test begin: paddle.Tensor.squeeze(Tensor([23767723, 1, 96, 1],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([23767723, 1, 96, 1],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.072372 test begin: paddle.Tensor.squeeze(Tensor([2527, 7, 126, 1, 1024],"float32"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([2527, 7, 126, 1, 1024],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.081880 test begin: paddle.Tensor.squeeze(Tensor([2535224, 1, 30, 30],"float32"), )

[torch error] paddle.Tensor.squeeze(Tensor([2535224, 1, 30, 30],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.091290 test begin: paddle.Tensor.squeeze(Tensor([256, 356516, 1, 25],"float32"), axis=2, )

[torch error] paddle.Tensor.squeeze(Tensor([256, 356516, 1, 25],"float32"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.101732 test begin: paddle.Tensor.squeeze(Tensor([256, 480, 1, 18569],"float32"), axis=2, )

[torch error] paddle.Tensor.squeeze(Tensor([256, 480, 1, 18569],"float32"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.111234 test begin: paddle.Tensor.squeeze(Tensor([256, 480, 1, 34953],"float16"), axis=2, )

[torch error] paddle.Tensor.squeeze(Tensor([256, 480, 1, 34953],"float16"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.120575 test begin: paddle.Tensor.squeeze(Tensor([256, 480, 1399, 25],"float16"), axis=2, )

[torch error] paddle.Tensor.squeeze(Tensor([256, 480, 1399, 25],"float16"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.01 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.129883 test begin: paddle.Tensor.squeeze(Tensor([256, 480, 743, 25],"float32"), axis=2, )

[torch error] paddle.Tensor.squeeze(Tensor([256, 480, 743, 25],"float32"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.145039 test begin: paddle.Tensor.squeeze(Tensor([256, 671089, 1, 25],"float16"), axis=2, )

[torch error] paddle.Tensor.squeeze(Tensor([256, 671089, 1, 25],"float16"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.154454 test begin: paddle.Tensor.squeeze(Tensor([26527, 1, 21504, 4],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([26527, 1, 21504, 4],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.163956 test begin: paddle.Tensor.squeeze(Tensor([27853, 10, 64, 64, 2, 1],"float32"), -1, )

[torch error] paddle.Tensor.squeeze(Tensor([27853, 10, 64, 64, 2, 1],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.173314 test begin: paddle.Tensor.squeeze(Tensor([285212673, 1, 1, 8],"float32"), axis=list[0,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([285212673, 1, 1, 8],"float32"), axis=list[0,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.182970 test begin: paddle.Tensor.squeeze(Tensor([28521268, 1, 10, 8],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([28521268, 1, 10, 8],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.192788 test begin: paddle.Tensor.squeeze(Tensor([28521268, 10, 1, 8],"float32"), axis=list[0,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([28521268, 10, 1, 8],"float32"), axis=list[0,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.202048 test begin: paddle.Tensor.squeeze(Tensor([2970966, 1, 768],"float32"), )

[torch error] paddle.Tensor.squeeze(Tensor([2970966, 1, 768],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.211289 test begin: paddle.Tensor.squeeze(Tensor([3, 1431655766, 1],"float16"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([3, 1431655766, 1],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.220538 test begin: paddle.Tensor.squeeze(Tensor([3, 4, 190141782],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([3, 4, 190141782],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.231185 test begin: paddle.Tensor.squeeze(Tensor([3, 4, 190141782],"int32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([3, 4, 190141782],"int32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.240685 test begin: paddle.Tensor.squeeze(Tensor([3, 4, 190141782],"int64"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([3, 4, 190141782],"int64"), 2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.250601 test begin: paddle.Tensor.squeeze(Tensor([3, 4, 357913942],"float16"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([3, 4, 357913942],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.260233 test begin: paddle.Tensor.squeeze(Tensor([3, 5895, 126, 1, 1024],"float32"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([3, 5895, 126, 1, 1024],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.269619 test begin: paddle.Tensor.squeeze(Tensor([3, 7, 106106, 1, 1024],"float32"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([3, 7, 106106, 1, 1024],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.278891 test begin: paddle.Tensor.squeeze(Tensor([3, 7, 126, 1, 862322],"float32"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([3, 7, 126, 1, 862322],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.291283 test begin: paddle.Tensor.squeeze(Tensor([3, 7, 126, 843, 1024],"float32"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([3, 7, 126, 843, 1024],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.300963 test begin: paddle.Tensor.squeeze(Tensor([3, 760567127, 1],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([3, 760567127, 1],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.310471 test begin: paddle.Tensor.squeeze(Tensor([3, 760567127, 1],"int32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([3, 760567127, 1],"int32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.320159 test begin: paddle.Tensor.squeeze(Tensor([3, 760567127, 1],"int64"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([3, 760567127, 1],"int64"), 2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.329602 test begin: paddle.Tensor.squeeze(Tensor([325957340, 1, 1, 7],"int32"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([325957340, 1, 1, 7],"int32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.339046 test begin: paddle.Tensor.squeeze(Tensor([325957340, 1, 1, 7],"int64"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([325957340, 1, 1, 7],"int64"), -2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.348569 test begin: paddle.Tensor.squeeze(Tensor([325957340, 7, 1],"float32"), )

[torch error] paddle.Tensor.squeeze(Tensor([325957340, 7, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.357911 test begin: paddle.Tensor.squeeze(Tensor([325957340, 7, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([325957340, 7, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.367298 test begin: paddle.Tensor.squeeze(Tensor([325957340, 7],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([325957340, 7],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.376554 test begin: paddle.Tensor.squeeze(Tensor([35651585, 1, 64],"float32"), )

[torch error] paddle.Tensor.squeeze(Tensor([35651585, 1, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.385782 test begin: paddle.Tensor.squeeze(Tensor([357914, 480, 1, 25],"float16"), axis=2, )

[torch error] paddle.Tensor.squeeze(Tensor([357914, 480, 1, 25],"float16"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.395052 test begin: paddle.Tensor.squeeze(Tensor([4294967297, 1],"float16"), )

[torch error] paddle.Tensor.squeeze(Tensor([4294967297, 1],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.404896 test begin: paddle.Tensor.squeeze(Tensor([4294968, 1000, 1, 1],"float16"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([4294968, 1000, 1, 1],"float16"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.414301 test begin: paddle.Tensor.squeeze(Tensor([4294968, 1000, 1],"float16"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([4294968, 1000, 1],"float16"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.423647 test begin: paddle.Tensor.squeeze(Tensor([4473925, 10, 1, 96],"float16"), axis=list[0,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([4473925, 10, 1, 96],"float16"), axis=list[0,2,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.433226 test begin: paddle.Tensor.squeeze(Tensor([475355, 120, 1, 40],"float32"), axis=2, )

[torch error] paddle.Tensor.squeeze(Tensor([475355, 120, 1, 40],"float32"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.443818 test begin: paddle.Tensor.squeeze(Tensor([475355, 1200, 4],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([475355, 1200, 4],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.453276 test begin: paddle.Tensor.squeeze(Tensor([4875431, 117, 4],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([4875431, 117, 4],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.462567 test begin: paddle.Tensor.squeeze(Tensor([5070448, 1, 18, 25],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([5070448, 1, 18, 25],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.472163 test begin: paddle.Tensor.squeeze(Tensor([5281717, 108, 4],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([5281717, 108, 4],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.483468 test begin: paddle.Tensor.squeeze(Tensor([53687092, 10, 1, 8],"float16"), axis=list[0,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([53687092, 10, 1, 8],"float16"), axis=list[0,2,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.494000 test begin: paddle.Tensor.squeeze(Tensor([570425345, 1, 4],"float32"), 1, )

[torch error] paddle.Tensor.squeeze(Tensor([570425345, 1, 4],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.504806 test begin: paddle.Tensor.squeeze(Tensor([570425345, 4, 1],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([570425345, 4, 1],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.515376 test begin: paddle.Tensor.squeeze(Tensor([570425345, 4, 1],"int32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([570425345, 4, 1],"int32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.525587 test begin: paddle.Tensor.squeeze(Tensor([570425345, 4, 1],"int64"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([570425345, 4, 1],"int64"), 2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.536505 test begin: paddle.Tensor.squeeze(Tensor([5704254, 100, 4],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([5704254, 100, 4],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.546892 test begin: paddle.Tensor.squeeze(Tensor([60632, 1, 1, 37632],"float32"), list[1,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([60632, 1, 1, 37632],"float32"), list[1,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.557398 test begin: paddle.Tensor.squeeze(Tensor([607, 300, 1, 12544],"float32"), -2, )

[torch error] paddle.Tensor.squeeze(Tensor([607, 300, 1, 12544],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.567827 test begin: paddle.Tensor.squeeze(Tensor([61906, 36858],"float32"), )

[torch error] paddle.Tensor.squeeze(Tensor([61906, 36858],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.577617 test begin: paddle.Tensor.squeeze(Tensor([64, 1000, 1, 35652],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([64, 1000, 1, 35652],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.590038 test begin: paddle.Tensor.squeeze(Tensor([64, 1000, 1, 67109],"float16"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([64, 1000, 1, 67109],"float16"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.600157 test begin: paddle.Tensor.squeeze(Tensor([64, 1000, 35652, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([64, 1000, 35652, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.610294 test begin: paddle.Tensor.squeeze(Tensor([64, 1000, 35652],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([64, 1000, 35652],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.620460 test begin: paddle.Tensor.squeeze(Tensor([64, 1000, 67109, 1],"float16"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([64, 1000, 67109, 1],"float16"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.631370 test begin: paddle.Tensor.squeeze(Tensor([64, 1000, 67109],"float16"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([64, 1000, 67109],"float16"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.641353 test begin: paddle.Tensor.squeeze(Tensor([64, 1677722, 1, 40],"float16"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([64, 1677722, 1, 40],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.654323 test begin: paddle.Tensor.squeeze(Tensor([64, 35651585, 1, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([64, 35651585, 1, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.664725 test begin: paddle.Tensor.squeeze(Tensor([64, 35651585, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([64, 35651585, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.674232 test begin: paddle.Tensor.squeeze(Tensor([64, 512, 1, 131073],"float16"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([64, 512, 1, 131073],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.683532 test begin: paddle.Tensor.squeeze(Tensor([64, 512, 1, 69633],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([64, 512, 1, 69633],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.692828 test begin: paddle.Tensor.squeeze(Tensor([64, 512, 1741, 40],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([64, 512, 1741, 40],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.702134 test begin: paddle.Tensor.squeeze(Tensor([64, 512, 3277, 40],"float16"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([64, 512, 3277, 40],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.711493 test begin: paddle.Tensor.squeeze(Tensor([64, 67108865, 1, 1],"float16"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([64, 67108865, 1, 1],"float16"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.720846 test begin: paddle.Tensor.squeeze(Tensor([64, 67108865, 1],"float16"), axis=-1, )

[torch error] paddle.Tensor.squeeze(Tensor([64, 67108865, 1],"float16"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.730659 test begin: paddle.Tensor.squeeze(Tensor([64, 891290, 1, 40],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([64, 891290, 1, 40],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.740156 test begin: paddle.Tensor.squeeze(Tensor([67108865, 1, 64],"float16"), )

[torch error] paddle.Tensor.squeeze(Tensor([67108865, 1, 64],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.749689 test begin: paddle.Tensor.squeeze(Tensor([742742, 1, 3072],"float32"), )

[torch error] paddle.Tensor.squeeze(Tensor([742742, 1, 3072],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.759544 test begin: paddle.Tensor.squeeze(Tensor([79226, 1, 144, 200],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([79226, 1, 144, 200],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.768910 test begin: paddle.Tensor.squeeze(Tensor([8, 512, 1, 557057],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([8, 512, 1, 557057],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.778280 test begin: paddle.Tensor.squeeze(Tensor([8, 512, 13927, 40],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([8, 512, 13927, 40],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.787594 test begin: paddle.Tensor.squeeze(Tensor([8, 7130317, 1, 40],"float32"), 2, )

[torch error] paddle.Tensor.squeeze(Tensor([8, 7130317, 1, 40],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.796995 test begin: paddle.Tensor.squeeze(Tensor([8193, 17, 128, 128],"float32"), )

[torch error] paddle.Tensor.squeeze(Tensor([8193, 17, 128, 128],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.806677 test begin: paddle.Tensor.squeeze(Tensor([82527, 1, 192, 144],"float32"), 0, )

[torch error] paddle.Tensor.squeeze(Tensor([82527, 1, 192, 144],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.816044 test begin: paddle.Tensor.squeeze(Tensor([891290, 10, 1, 256],"float32"), axis=list[0,2,], )

[torch error] paddle.Tensor.squeeze(Tensor([891290, 10, 1, 256],"float32"), axis=list[0,2,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.825375 test begin: paddle.Tensor.squeeze(Tensor([894785, 120, 1, 40],"float16"), axis=2, )

[torch error] paddle.Tensor.squeeze(Tensor([894785, 120, 1, 40],"float16"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.834682 test begin: paddle.Tensor.std(Tensor([1, 1, 2281701379],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.std(Tensor([1, 1, 2281701379],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.843982 test begin: paddle.Tensor.std(Tensor([1, 10, 228170138],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.std(Tensor([1, 10, 228170138],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.853260 test begin: paddle.Tensor.std(Tensor([1, 11, 207427399],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.std(Tensor([1, 11, 207427399],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.862527 test begin: paddle.Tensor.std(Tensor([1, 42253730, 54],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.std(Tensor([1, 42253730, 54],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.871801 test begin: paddle.Tensor.std(Tensor([1, 50704476, 45],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.std(Tensor([1, 50704476, 45],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.881072 test begin: paddle.Tensor.std(Tensor([1, 63380594, 36],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.std(Tensor([1, 63380594, 36],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.890356 test begin: paddle.Tensor.std(Tensor([1024, 1024, 2177],"float32"), )

[torch error] paddle.Tensor.std(Tensor([1024, 1024, 2177],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.899562 test begin: paddle.Tensor.std(Tensor([1024, 1024, 4097],"float16"), )

[torch error] paddle.Tensor.std(Tensor([1024, 1024, 4097],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.908824 test begin: paddle.Tensor.std(Tensor([1024, 278529, 8],"float32"), )

[torch error] paddle.Tensor.std(Tensor([1024, 278529, 8],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.919393 test begin: paddle.Tensor.std(Tensor([1024, 524289, 8],"float16"), )

[torch error] paddle.Tensor.std(Tensor([1024, 524289, 8],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.928769 test begin: paddle.Tensor.std(Tensor([278529, 1024, 8],"float32"), )

[torch error] paddle.Tensor.std(Tensor([278529, 1024, 8],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.938106 test begin: paddle.Tensor.std(Tensor([42253730, 1, 54],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.std(Tensor([42253730, 1, 54],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.947413 test begin: paddle.Tensor.std(Tensor([50704476, 1, 45],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.std(Tensor([50704476, 1, 45],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.956723 test begin: paddle.Tensor.std(Tensor([524289, 1024, 8],"float16"), )

[torch error] paddle.Tensor.std(Tensor([524289, 1024, 8],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.966435 test begin: paddle.Tensor.std(Tensor([5761873, 11, 36],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.std(Tensor([5761873, 11, 36],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.976184 test begin: paddle.Tensor.std(Tensor([63380594, 1, 36],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.std(Tensor([63380594, 1, 36],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.985498 test begin: paddle.Tensor.std(Tensor([6338060, 10, 36],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.std(Tensor([6338060, 10, 36],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:24.995514 test begin: paddle.Tensor.sum(Tensor([1, 1, 1, 2281701379],"bool"), )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 1, 2281701379],"bool"), ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.005086 test begin: paddle.Tensor.sum(Tensor([1, 1, 1, 4294967297],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 1, 4294967297],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.014629 test begin: paddle.Tensor.sum(Tensor([1, 1, 1, 4294967297],"float16"), 1, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 1, 4294967297],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.024008 test begin: paddle.Tensor.sum(Tensor([1, 1, 1, 4294967297],"float16"), 2, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 1, 4294967297],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.033399 test begin: paddle.Tensor.sum(Tensor([1, 1, 1, 4294967297],"float16"), 3, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 1, 4294967297],"float16"), 3, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.042741 test begin: paddle.Tensor.sum(Tensor([1, 1, 1140850690, 2],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 1140850690, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.052084 test begin: paddle.Tensor.sum(Tensor([1, 1, 13, 175515491],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 13, 175515491],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.061427 test begin: paddle.Tensor.sum(Tensor([1, 1, 17, 252645136],"float16"), )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 17, 252645136],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.070801 test begin: paddle.Tensor.sum(Tensor([1, 1, 2, 1140850690],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 2, 1140850690],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.080095 test begin: paddle.Tensor.sum(Tensor([1, 1, 2281701379, 1],"bool"), )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 2281701379, 1],"bool"), ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.089697 test begin: paddle.Tensor.sum(Tensor([1, 1, 2281701379],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 2281701379],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.099124 test begin: paddle.Tensor.sum(Tensor([1, 1, 2281701379],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 2281701379],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.108434 test begin: paddle.Tensor.sum(Tensor([1, 1, 252645136, 17],"float16"), )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 252645136, 17],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.117930 test begin: paddle.Tensor.sum(Tensor([1, 1, 4294967297, 1],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 4294967297, 1],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.127412 test begin: paddle.Tensor.sum(Tensor([1, 1, 4294967297, 1],"float16"), 1, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 4294967297, 1],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.136804 test begin: paddle.Tensor.sum(Tensor([1, 1, 4294967297, 1],"float16"), 2, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 4294967297, 1],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.146372 test begin: paddle.Tensor.sum(Tensor([1, 1, 4294967297, 1],"float16"), 3, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 4294967297, 1],"float16"), 3, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.155766 test begin: paddle.Tensor.sum(Tensor([1, 1, 5, 858993460],"float16"), 2, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 5, 858993460],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.165047 test begin: paddle.Tensor.sum(Tensor([1, 1, 5, 858993460],"float16"), 3, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 5, 858993460],"float16"), 3, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.174400 test begin: paddle.Tensor.sum(Tensor([1, 1, 61595, 37044],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 61595, 37044],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.183899 test begin: paddle.Tensor.sum(Tensor([1, 1, 67908, 33600],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 67908, 33600],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.193314 test begin: paddle.Tensor.sum(Tensor([1, 1, 75245, 30324],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 75245, 30324],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.202659 test begin: paddle.Tensor.sum(Tensor([1, 1, 83837, 27216],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 83837, 27216],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.212012 test begin: paddle.Tensor.sum(Tensor([1, 1, 858993460, 5],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 858993460, 5],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.221755 test begin: paddle.Tensor.sum(Tensor([1, 1, 858993460, 5],"float16"), 1, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 858993460, 5],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.231054 test begin: paddle.Tensor.sum(Tensor([1, 1, 858993460, 5],"float16"), 2, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 858993460, 5],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.240734 test begin: paddle.Tensor.sum(Tensor([1, 1, 858993460, 5],"float16"), 3, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 858993460, 5],"float16"), 3, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.250192 test begin: paddle.Tensor.sum(Tensor([1, 1, 93991, 24276],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([1, 1, 93991, 24276],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.259607 test begin: paddle.Tensor.sum(Tensor([1, 1140850690, 2],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([1, 1140850690, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.268961 test begin: paddle.Tensor.sum(Tensor([1, 128, 128, 3, 46422],"float32"), list[2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([1, 128, 128, 3, 46422],"float32"), list[2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.278389 test begin: paddle.Tensor.sum(Tensor([1, 128, 128, 46422, 3],"float32"), list[2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([1, 128, 128, 46422, 3],"float32"), list[2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.288041 test begin: paddle.Tensor.sum(Tensor([1, 128, 1980644, 3, 3],"float32"), list[2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([1, 128, 1980644, 3, 3],"float32"), list[2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.297501 test begin: paddle.Tensor.sum(Tensor([1, 128, 256, 23211, 3],"float32"), list[2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([1, 128, 256, 23211, 3],"float32"), list[2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.307115 test begin: paddle.Tensor.sum(Tensor([1, 128, 256, 3, 23211],"float32"), list[2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([1, 128, 256, 3, 23211],"float32"), list[2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.316631 test begin: paddle.Tensor.sum(Tensor([1, 142606337, 4, 4, 1, 1],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 142606337, 4, 4, 1, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.325936 test begin: paddle.Tensor.sum(Tensor([1, 14861479, 17, 17],"float16"), )

[torch error] paddle.Tensor.sum(Tensor([1, 14861479, 17, 17],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.335371 test begin: paddle.Tensor.sum(Tensor([1, 150, 32, 475355],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 150, 32, 475355],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.344694 test begin: paddle.Tensor.sum(Tensor([1, 150, 475355, 32],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 150, 475355, 32],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.354012 test begin: paddle.Tensor.sum(Tensor([1, 152113426, 15],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 152113426, 15],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.363379 test begin: paddle.Tensor.sum(Tensor([1, 152113426, 15],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([1, 152113426, 15],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.372783 test begin: paddle.Tensor.sum(Tensor([1, 171798692, 5, 5],"float16"), 2, )

[torch error] paddle.Tensor.sum(Tensor([1, 171798692, 5, 5],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.382661 test begin: paddle.Tensor.sum(Tensor([1, 171798692, 5, 5],"float16"), 3, )

[torch error] paddle.Tensor.sum(Tensor([1, 171798692, 5, 5],"float16"), 3, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.391958 test begin: paddle.Tensor.sum(Tensor([1, 190141782, 1, 3, 1, 4],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 190141782, 1, 3, 1, 4],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.401187 test begin: paddle.Tensor.sum(Tensor([1, 1980644, 128, 3, 3],"float32"), list[2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([1, 1980644, 128, 3, 3],"float32"), list[2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.410805 test begin: paddle.Tensor.sum(Tensor([1, 2, 1, 285212673, 1, 4],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 2, 1, 285212673, 1, 4],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.420136 test begin: paddle.Tensor.sum(Tensor([1, 2, 1, 3, 1, 380283564],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 2, 1, 3, 1, 380283564],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.429784 test begin: paddle.Tensor.sum(Tensor([1, 2, 1, 3, 95070891, 4],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 2, 1, 3, 95070891, 4],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.439195 test begin: paddle.Tensor.sum(Tensor([1, 2, 1140850690],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 2, 1140850690],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.448701 test begin: paddle.Tensor.sum(Tensor([1, 2, 1140850690],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([1, 2, 1140850690],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.458116 test begin: paddle.Tensor.sum(Tensor([1, 2, 1140850690],"float32"), -2, )

[torch error] paddle.Tensor.sum(Tensor([1, 2, 1140850690],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.467379 test begin: paddle.Tensor.sum(Tensor([1, 2, 2147483649],"float16"), -1, )

[torch error] paddle.Tensor.sum(Tensor([1, 2, 2147483649],"float16"), -1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.476668 test begin: paddle.Tensor.sum(Tensor([1, 2, 3, 715827883],"float16"), )

[torch error] paddle.Tensor.sum(Tensor([1, 2, 3, 715827883],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.486116 test begin: paddle.Tensor.sum(Tensor([1, 2, 536870913, 4],"float16"), )

[torch error] paddle.Tensor.sum(Tensor([1, 2, 536870913, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.495498 test begin: paddle.Tensor.sum(Tensor([1, 2, 95070891, 3, 1, 4],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 2, 95070891, 3, 1, 4],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.504756 test begin: paddle.Tensor.sum(Tensor([1, 2147483649, 2],"float16"), -1, )

[torch error] paddle.Tensor.sum(Tensor([1, 2147483649, 2],"float16"), -1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.514083 test begin: paddle.Tensor.sum(Tensor([1, 21504, 106106],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 21504, 106106],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.523396 test begin: paddle.Tensor.sum(Tensor([1, 21504, 106106],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([1, 21504, 106106],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.532664 test begin: paddle.Tensor.sum(Tensor([1, 221848, 10285],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([1, 221848, 10285],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.542191 test begin: paddle.Tensor.sum(Tensor([1, 2228225, 32, 32],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 2228225, 32, 32],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.551529 test begin: paddle.Tensor.sum(Tensor([1, 2281701379, 1, 1],"bool"), )

[torch error] paddle.Tensor.sum(Tensor([1, 2281701379, 1, 1],"bool"), ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.561124 test begin: paddle.Tensor.sum(Tensor([1, 2281701379, 1],"float32"), -2, )

[torch error] paddle.Tensor.sum(Tensor([1, 2281701379, 1],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.570556 test begin: paddle.Tensor.sum(Tensor([1, 2281701379],"bool"), )

[torch error] paddle.Tensor.sum(Tensor([1, 2281701379],"bool"), ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.579886 test begin: paddle.Tensor.sum(Tensor([1, 2281701379],"bool"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([1, 2281701379],"bool"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.589239 test begin: paddle.Tensor.sum(Tensor([1, 2281701379],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 2281701379],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.598754 test begin: paddle.Tensor.sum(Tensor([1, 2281701379],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([1, 2281701379],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.608309 test begin: paddle.Tensor.sum(Tensor([1, 2281701379],"float32"), 1, )

[torch error] paddle.Tensor.sum(Tensor([1, 2281701379],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.617645 test begin: paddle.Tensor.sum(Tensor([1, 2281701379],"float32"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([1, 2281701379],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.627009 test begin: paddle.Tensor.sum(Tensor([1, 2281701379],"float32"), list[-1,], )

[torch error] paddle.Tensor.sum(Tensor([1, 2281701379],"float32"), list[-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.636398 test begin: paddle.Tensor.sum(Tensor([1, 2281701379],"int32"), 0, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([1, 2281701379],"int32"), 0, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.645689 test begin: paddle.Tensor.sum(Tensor([1, 2281701379],"int64"), )

[torch error] paddle.Tensor.sum(Tensor([1, 2281701379],"int64"), ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.654970 test begin: paddle.Tensor.sum(Tensor([1, 2281701379],"int64"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([1, 2281701379],"int64"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.664250 test begin: paddle.Tensor.sum(Tensor([1, 228170138, 10],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 228170138, 10],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.675385 test begin: paddle.Tensor.sum(Tensor([1, 228170138, 10],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([1, 228170138, 10],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.686665 test begin: paddle.Tensor.sum(Tensor([1, 24276, 93991],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 24276, 93991],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.696305 test begin: paddle.Tensor.sum(Tensor([1, 24276, 93991],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([1, 24276, 93991],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.706951 test begin: paddle.Tensor.sum(Tensor([1, 256, 256, 11606, 3],"float32"), list[2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([1, 256, 256, 11606, 3],"float32"), list[2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.717500 test begin: paddle.Tensor.sum(Tensor([1, 256, 256, 3, 11606],"float32"), list[2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([1, 256, 256, 3, 11606],"float32"), list[2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.727742 test begin: paddle.Tensor.sum(Tensor([1, 256, 512, 3, 5803],"float32"), list[2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([1, 256, 512, 3, 5803],"float32"), list[2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.737892 test begin: paddle.Tensor.sum(Tensor([1, 256, 512, 5803, 3],"float32"), list[2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([1, 256, 512, 5803, 3],"float32"), list[2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.748247 test begin: paddle.Tensor.sum(Tensor([1, 256, 990322, 3, 3],"float32"), list[2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([1, 256, 990322, 3, 3],"float32"), list[2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.758437 test begin: paddle.Tensor.sum(Tensor([1, 25928425, 88],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 25928425, 88],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.768642 test begin: paddle.Tensor.sum(Tensor([1, 3, 190141782, 4, 1, 1],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 3, 190141782, 4, 1, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.778258 test begin: paddle.Tensor.sum(Tensor([1, 3, 4, 190141782, 1, 1],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 3, 4, 190141782, 1, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.790462 test begin: paddle.Tensor.sum(Tensor([1, 3, 4, 4, 1, 47535446],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 3, 4, 4, 1, 47535446],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.800434 test begin: paddle.Tensor.sum(Tensor([1, 3, 4, 4, 47535446, 1],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 3, 4, 4, 47535446, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.810505 test begin: paddle.Tensor.sum(Tensor([1, 357913942, 3, 4],"float16"), )

[torch error] paddle.Tensor.sum(Tensor([1, 357913942, 3, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.820410 test begin: paddle.Tensor.sum(Tensor([1, 4294967297, 1, 1],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([1, 4294967297, 1, 1],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.830427 test begin: paddle.Tensor.sum(Tensor([1, 4294967297, 1, 1],"float16"), 1, )

[torch error] paddle.Tensor.sum(Tensor([1, 4294967297, 1, 1],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.840382 test begin: paddle.Tensor.sum(Tensor([1, 4294967297, 1, 1],"float16"), 2, )

[torch error] paddle.Tensor.sum(Tensor([1, 4294967297, 1, 1],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.849891 test begin: paddle.Tensor.sum(Tensor([1, 4294967297, 1, 1],"float16"), 3, )

[torch error] paddle.Tensor.sum(Tensor([1, 4294967297, 1, 1],"float16"), 3, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.862661 test begin: paddle.Tensor.sum(Tensor([1, 4294967297],"float16"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([1, 4294967297],"float16"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.871954 test begin: paddle.Tensor.sum(Tensor([1, 4294967297],"float16"), list[-1,], )

[torch error] paddle.Tensor.sum(Tensor([1, 4294967297],"float16"), list[-1,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.881247 test begin: paddle.Tensor.sum(Tensor([1, 45634028, 25, 2],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([1, 45634028, 25, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.890484 test begin: paddle.Tensor.sum(Tensor([1, 4739, 13, 37044],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([1, 4739, 13, 37044],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.899744 test begin: paddle.Tensor.sum(Tensor([1, 495161, 512, 3, 3],"float32"), list[2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([1, 495161, 512, 3, 3],"float32"), list[2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.909068 test begin: paddle.Tensor.sum(Tensor([1, 5, 1, 858993460],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([1, 5, 1, 858993460],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.919965 test begin: paddle.Tensor.sum(Tensor([1, 5, 1, 858993460],"float16"), 1, )

[torch error] paddle.Tensor.sum(Tensor([1, 5, 1, 858993460],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.929550 test begin: paddle.Tensor.sum(Tensor([1, 5, 1, 858993460],"float16"), 2, )

[torch error] paddle.Tensor.sum(Tensor([1, 5, 1, 858993460],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.938921 test begin: paddle.Tensor.sum(Tensor([1, 5, 1, 858993460],"float16"), 3, )

[torch error] paddle.Tensor.sum(Tensor([1, 5, 1, 858993460],"float16"), 3, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.948287 test begin: paddle.Tensor.sum(Tensor([1, 5, 171798692, 5],"float16"), 1, )

[torch error] paddle.Tensor.sum(Tensor([1, 5, 171798692, 5],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.957825 test begin: paddle.Tensor.sum(Tensor([1, 5, 171798692, 5],"float16"), 3, )

[torch error] paddle.Tensor.sum(Tensor([1, 5, 171798692, 5],"float16"), 3, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.967190 test begin: paddle.Tensor.sum(Tensor([1, 5, 5, 171798692],"float16"), 1, )

[torch error] paddle.Tensor.sum(Tensor([1, 5, 5, 171798692],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.976511 test begin: paddle.Tensor.sum(Tensor([1, 5, 858993460, 1],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([1, 5, 858993460, 1],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.985809 test begin: paddle.Tensor.sum(Tensor([1, 5, 858993460, 1],"float16"), 1, )

[torch error] paddle.Tensor.sum(Tensor([1, 5, 858993460, 1],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:25.995175 test begin: paddle.Tensor.sum(Tensor([1, 5, 858993460, 1],"float16"), 2, )

[torch error] paddle.Tensor.sum(Tensor([1, 5, 858993460, 1],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.004561 test begin: paddle.Tensor.sum(Tensor([1, 5, 858993460, 1],"float16"), 3, )

[torch error] paddle.Tensor.sum(Tensor([1, 5, 858993460, 1],"float16"), 3, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.013951 test begin: paddle.Tensor.sum(Tensor([1, 512, 495161, 3, 3],"float32"), list[2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([1, 512, 495161, 3, 3],"float32"), list[2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.023300 test begin: paddle.Tensor.sum(Tensor([1, 512, 512, 2902, 3],"float32"), list[2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([1, 512, 512, 2902, 3],"float32"), list[2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.032687 test begin: paddle.Tensor.sum(Tensor([1, 512, 512, 3, 2902],"float32"), list[2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([1, 512, 512, 3, 2902],"float32"), list[2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.042061 test begin: paddle.Tensor.sum(Tensor([1, 5224, 13, 33600],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([1, 5224, 13, 33600],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.051397 test begin: paddle.Tensor.sum(Tensor([1, 570425345, 2, 2],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([1, 570425345, 2, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.060774 test begin: paddle.Tensor.sum(Tensor([1, 5789, 13, 30324],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([1, 5789, 13, 30324],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.070206 test begin: paddle.Tensor.sum(Tensor([1, 61595, 37044],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([1, 61595, 37044],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.079549 test begin: paddle.Tensor.sum(Tensor([1, 6449, 13, 27216],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([1, 6449, 13, 27216],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.088881 test begin: paddle.Tensor.sum(Tensor([1, 65536, 17409, 2],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([1, 65536, 17409, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.098250 test begin: paddle.Tensor.sum(Tensor([1, 65536, 25, 1393],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([1, 65536, 25, 1393],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.107613 test begin: paddle.Tensor.sum(Tensor([1, 67908, 33600],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([1, 67908, 33600],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.117681 test begin: paddle.Tensor.sum(Tensor([1, 67908, 33600],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([1, 67908, 33600],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.127164 test begin: paddle.Tensor.sum(Tensor([1, 7231, 13, 24276],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([1, 7231, 13, 24276],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.136566 test begin: paddle.Tensor.sum(Tensor([1, 75245, 30324],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([1, 75245, 30324],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.145908 test begin: paddle.Tensor.sum(Tensor([1, 75245, 30324],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([1, 75245, 30324],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.155213 test begin: paddle.Tensor.sum(Tensor([1, 760567127, 3],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 760567127, 3],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.164503 test begin: paddle.Tensor.sum(Tensor([1, 83837, 27216],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([1, 83837, 27216],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.174132 test begin: paddle.Tensor.sum(Tensor([1, 83837, 27216],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([1, 83837, 27216],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.183563 test begin: paddle.Tensor.sum(Tensor([1, 858993460, 1, 5],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([1, 858993460, 1, 5],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.192980 test begin: paddle.Tensor.sum(Tensor([1, 858993460, 1, 5],"float16"), 1, )

[torch error] paddle.Tensor.sum(Tensor([1, 858993460, 1, 5],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.202404 test begin: paddle.Tensor.sum(Tensor([1, 858993460, 1, 5],"float16"), 2, )

[torch error] paddle.Tensor.sum(Tensor([1, 858993460, 1, 5],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.212039 test begin: paddle.Tensor.sum(Tensor([1, 858993460, 1, 5],"float16"), 3, )

[torch error] paddle.Tensor.sum(Tensor([1, 858993460, 1, 5],"float16"), 3, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.221543 test begin: paddle.Tensor.sum(Tensor([1, 858993460, 5, 1],"float16"), 1, )

[torch error] paddle.Tensor.sum(Tensor([1, 858993460, 5, 1],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.231190 test begin: paddle.Tensor.sum(Tensor([1, 858993460, 5, 1],"float16"), 2, )

[torch error] paddle.Tensor.sum(Tensor([1, 858993460, 5, 1],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.240812 test begin: paddle.Tensor.sum(Tensor([1, 9, 4, 4, 1, 15845149],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 9, 4, 4, 1, 15845149],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.250125 test begin: paddle.Tensor.sum(Tensor([1, 9, 4, 4, 15845149, 1],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 9, 4, 4, 15845149, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.259434 test begin: paddle.Tensor.sum(Tensor([1, 9, 4, 63380594, 1, 1],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 9, 4, 63380594, 1, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.268709 test begin: paddle.Tensor.sum(Tensor([1, 9, 63380594, 4, 1, 1],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1, 9, 63380594, 4, 1, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.278042 test begin: paddle.Tensor.sum(Tensor([1, 93991, 24276],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([1, 93991, 24276],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.287615 test begin: paddle.Tensor.sum(Tensor([1, 93991, 24276],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([1, 93991, 24276],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.297045 test begin: paddle.Tensor.sum(Tensor([1, 990322, 256, 3, 3],"float32"), list[2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([1, 990322, 256, 3, 3],"float32"), list[2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.306565 test begin: paddle.Tensor.sum(Tensor([10, 10, 10, 4294968],"float16"), -1, )

[torch error] paddle.Tensor.sum(Tensor([10, 10, 10, 4294968],"float16"), -1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.315941 test begin: paddle.Tensor.sum(Tensor([10, 10, 21474837, 2],"float16"), -1, )

[torch error] paddle.Tensor.sum(Tensor([10, 10, 21474837, 2],"float16"), -1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.325358 test begin: paddle.Tensor.sum(Tensor([10, 114085069, 1, 2],"float32"), list[0,-1,], )

[torch error] paddle.Tensor.sum(Tensor([10, 114085069, 1, 2],"float32"), list[0,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.336557 test begin: paddle.Tensor.sum(Tensor([10, 114085069, 2],"float32"), list[0,-1,], )

[torch error] paddle.Tensor.sum(Tensor([10, 114085069, 2],"float32"), list[0,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.345952 test begin: paddle.Tensor.sum(Tensor([10, 143165577, 3],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([10, 143165577, 3],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.355289 test begin: paddle.Tensor.sum(Tensor([10, 2, 1, 114085069],"float32"), list[0,-1,], )

[torch error] paddle.Tensor.sum(Tensor([10, 2, 1, 114085069],"float32"), list[0,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.364597 test begin: paddle.Tensor.sum(Tensor([10, 2, 114085069],"float32"), list[0,-1,], )

[torch error] paddle.Tensor.sum(Tensor([10, 2, 114085069],"float32"), list[0,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.374204 test begin: paddle.Tensor.sum(Tensor([10, 2, 214748365],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([10, 2, 214748365],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.384247 test begin: paddle.Tensor.sum(Tensor([10, 2, 57042535, 2],"float32"), list[0,-1,], )

[torch error] paddle.Tensor.sum(Tensor([10, 2, 57042535, 2],"float32"), list[0,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.393799 test begin: paddle.Tensor.sum(Tensor([10, 20, 1, 11408507],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([10, 20, 1, 11408507],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.403140 test begin: paddle.Tensor.sum(Tensor([10, 20, 11408507, 1],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([10, 20, 11408507, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.412501 test begin: paddle.Tensor.sum(Tensor([10, 21474837, 10, 2],"float16"), -1, )

[torch error] paddle.Tensor.sum(Tensor([10, 21474837, 10, 2],"float16"), -1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.422233 test begin: paddle.Tensor.sum(Tensor([10, 228170138, 1, 1],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([10, 228170138, 1, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.431619 test begin: paddle.Tensor.sum(Tensor([10, 228170138],"float32"), 0, )

[torch error] paddle.Tensor.sum(Tensor([10, 228170138],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.440877 test begin: paddle.Tensor.sum(Tensor([10, 228170138],"float32"), 1, )

[torch error] paddle.Tensor.sum(Tensor([10, 228170138],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.450337 test begin: paddle.Tensor.sum(Tensor([10, 228170138],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([10, 228170138],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.459731 test begin: paddle.Tensor.sum(Tensor([10, 228170138],"float32"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([10, 228170138],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.469126 test begin: paddle.Tensor.sum(Tensor([10, 228170138],"float32"), list[0,-1,], )

[torch error] paddle.Tensor.sum(Tensor([10, 228170138],"float32"), list[0,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.478639 test begin: paddle.Tensor.sum(Tensor([10, 28521268, 8],"float32"), axis=2, )

[torch error] paddle.Tensor.sum(Tensor([10, 28521268, 8],"float32"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.488146 test begin: paddle.Tensor.sum(Tensor([10, 429496730],"float16"), -1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([10, 429496730],"float16"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.497565 test begin: paddle.Tensor.sum(Tensor([10, 429496730],"float16"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([10, 429496730],"float16"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.506873 test begin: paddle.Tensor.sum(Tensor([10, 429496730],"float16"), list[0,-1,], )

[torch error] paddle.Tensor.sum(Tensor([10, 429496730],"float16"), list[0,-1,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.516295 test begin: paddle.Tensor.sum(Tensor([10, 500, 456341],"float32"), axis=2, )

[torch error] paddle.Tensor.sum(Tensor([10, 500, 456341],"float32"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.525594 test begin: paddle.Tensor.sum(Tensor([10, 5000, 85900],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([10, 5000, 85900],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.534902 test begin: paddle.Tensor.sum(Tensor([10, 57042535, 4],"float32"), axis=2, )

[torch error] paddle.Tensor.sum(Tensor([10, 57042535, 4],"float32"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.544201 test begin: paddle.Tensor.sum(Tensor([10, 6, 71582789],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([10, 6, 71582789],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.553469 test begin: paddle.Tensor.sum(Tensor([100, 22817014],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([100, 22817014],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.562726 test begin: paddle.Tensor.sum(Tensor([100, 42949673],"float16"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([100, 42949673],"float16"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.572197 test begin: paddle.Tensor.sum(Tensor([1000, 1140851, 2],"float32"), list[0,1,], )

[torch error] paddle.Tensor.sum(Tensor([1000, 1140851, 2],"float32"), list[0,1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.581655 test begin: paddle.Tensor.sum(Tensor([1000, 2, 1140851],"float32"), list[0,1,], )

[torch error] paddle.Tensor.sum(Tensor([1000, 2, 1140851],"float32"), list[0,1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.590991 test begin: paddle.Tensor.sum(Tensor([1000, 2, 2, 570426],"float32"), list[0,1,], )

[torch error] paddle.Tensor.sum(Tensor([1000, 2, 2, 570426],"float32"), list[0,1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.600558 test begin: paddle.Tensor.sum(Tensor([1000, 2, 380284, 3],"float32"), list[0,1,], )

[torch error] paddle.Tensor.sum(Tensor([1000, 2, 380284, 3],"float32"), list[0,1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.610176 test begin: paddle.Tensor.sum(Tensor([1000, 380284, 2, 3],"float32"), list[0,1,], )

[torch error] paddle.Tensor.sum(Tensor([1000, 380284, 2, 3],"float32"), list[0,1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.619572 test begin: paddle.Tensor.sum(Tensor([101862, 22400],"float32"), axis=1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([101862, 22400],"float32"), axis=1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.629073 test begin: paddle.Tensor.sum(Tensor([10611, 21504, 10],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([10611, 21504, 10],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.638408 test begin: paddle.Tensor.sum(Tensor([10611, 21504, 10],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([10611, 21504, 10],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.647717 test begin: paddle.Tensor.sum(Tensor([1069, 24276, 88],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([1069, 24276, 88],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.657086 test begin: paddle.Tensor.sum(Tensor([1073741825, 2, 2],"float16"), -1, )

[torch error] paddle.Tensor.sum(Tensor([1073741825, 2, 2],"float16"), -1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.666433 test begin: paddle.Tensor.sum(Tensor([1073741825, 4],"float16"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([1073741825, 4],"float16"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.676016 test begin: paddle.Tensor.sum(Tensor([107374183, 40],"float16"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([107374183, 40],"float16"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.685499 test begin: paddle.Tensor.sum(Tensor([114085069, 20, 1, 1],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([114085069, 20, 1, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.694927 test begin: paddle.Tensor.sum(Tensor([114085069, 20],"int64"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([114085069, 20],"int64"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.704286 test begin: paddle.Tensor.sum(Tensor([1140850690, 1, 2, 1, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([1140850690, 1, 2, 1, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.713586 test begin: paddle.Tensor.sum(Tensor([1140850690, 1, 2, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([1140850690, 1, 2, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.722915 test begin: paddle.Tensor.sum(Tensor([1140850690, 2, 1],"float32"), -2, )

[torch error] paddle.Tensor.sum(Tensor([1140850690, 2, 1],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.732240 test begin: paddle.Tensor.sum(Tensor([1140850690, 2, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([1140850690, 2, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.741600 test begin: paddle.Tensor.sum(Tensor([1140850690, 2],"bool"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([1140850690, 2],"bool"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.751300 test begin: paddle.Tensor.sum(Tensor([1140850690, 2],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([1140850690, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.761019 test begin: paddle.Tensor.sum(Tensor([1140850690, 2],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([1140850690, 2],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.770363 test begin: paddle.Tensor.sum(Tensor([1140850690, 2],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([1140850690, 2],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.779678 test begin: paddle.Tensor.sum(Tensor([1140850690, 2],"float32"), list[-1,], )

[torch error] paddle.Tensor.sum(Tensor([1140850690, 2],"float32"), list[-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.789030 test begin: paddle.Tensor.sum(Tensor([1140850690, 2],"float32"), list[-2,-1,], )

[torch error] paddle.Tensor.sum(Tensor([1140850690, 2],"float32"), list[-2,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.798337 test begin: paddle.Tensor.sum(Tensor([1140851, 500, 4],"float32"), axis=2, )

[torch error] paddle.Tensor.sum(Tensor([1140851, 500, 4],"float32"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.807785 test begin: paddle.Tensor.sum(Tensor([11883862, 3, 8, 8],"float32"), axis=-3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([11883862, 3, 8, 8],"float32"), axis=-3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.817199 test begin: paddle.Tensor.sum(Tensor([12, 118839, 40, 40, 1],"float32"), list[1,2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([12, 118839, 40, 40, 1],"float32"), list[1,2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.826573 test begin: paddle.Tensor.sum(Tensor([12, 190141782],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([12, 190141782],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.835925 test begin: paddle.Tensor.sum(Tensor([12, 190141782],"float32"), axis=0, )

[torch error] paddle.Tensor.sum(Tensor([12, 190141782],"float32"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.845435 test begin: paddle.Tensor.sum(Tensor([12, 1901418, 10, 10, 1],"float32"), list[1,2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([12, 1901418, 10, 10, 1],"float32"), list[1,2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.854886 test begin: paddle.Tensor.sum(Tensor([12, 23768, 10, 10, 80],"float32"), list[1,2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([12, 23768, 10, 10, 80],"float32"), list[1,2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.868786 test begin: paddle.Tensor.sum(Tensor([12, 3, 10, 10, 633806],"float32"), list[1,2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([12, 3, 10, 10, 633806],"float32"), list[1,2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.880050 test begin: paddle.Tensor.sum(Tensor([12, 3, 10, 6338060, 1],"float32"), list[1,2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([12, 3, 10, 6338060, 1],"float32"), list[1,2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.890530 test begin: paddle.Tensor.sum(Tensor([12, 3, 10, 79226, 80],"float32"), list[1,2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([12, 3, 10, 79226, 80],"float32"), list[1,2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.901191 test begin: paddle.Tensor.sum(Tensor([12, 3, 1584515, 40, 1],"float32"), list[1,2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([12, 3, 1584515, 40, 1],"float32"), list[1,2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.911452 test begin: paddle.Tensor.sum(Tensor([12, 3, 20, 20, 158452],"float32"), list[1,2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([12, 3, 20, 20, 158452],"float32"), list[1,2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.921505 test begin: paddle.Tensor.sum(Tensor([12, 3, 20, 3169030, 1],"float32"), list[1,2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([12, 3, 20, 3169030, 1],"float32"), list[1,2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.931474 test begin: paddle.Tensor.sum(Tensor([12, 3, 20, 39613, 80],"float32"), list[1,2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([12, 3, 20, 39613, 80],"float32"), list[1,2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.941538 test begin: paddle.Tensor.sum(Tensor([12, 3, 3169030, 20, 1],"float32"), list[1,2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([12, 3, 3169030, 20, 1],"float32"), list[1,2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.951493 test begin: paddle.Tensor.sum(Tensor([12, 3, 39613, 20, 80],"float32"), list[1,2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([12, 3, 39613, 20, 80],"float32"), list[1,2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.961608 test begin: paddle.Tensor.sum(Tensor([12, 3, 40, 1584515, 1],"float32"), list[1,2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([12, 3, 40, 1584515, 1],"float32"), list[1,2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.971058 test begin: paddle.Tensor.sum(Tensor([12, 3, 40, 40, 39613],"float32"), list[1,2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([12, 3, 40, 40, 39613],"float32"), list[1,2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.983332 test begin: paddle.Tensor.sum(Tensor([12, 3, 6338060, 10, 1],"float32"), list[1,2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([12, 3, 6338060, 10, 1],"float32"), list[1,2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:26.994255 test begin: paddle.Tensor.sum(Tensor([12, 3, 79226, 10, 80],"float32"), list[1,2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([12, 3, 79226, 10, 80],"float32"), list[1,2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.004765 test begin: paddle.Tensor.sum(Tensor([12, 357913942],"float16"), axis=0, )

[torch error] paddle.Tensor.sum(Tensor([12, 357913942],"float16"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.015315 test begin: paddle.Tensor.sum(Tensor([12, 475355, 20, 20, 1],"float32"), list[1,2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([12, 475355, 20, 20, 1],"float32"), list[1,2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.026762 test begin: paddle.Tensor.sum(Tensor([12, 5942, 20, 20, 80],"float32"), list[1,2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([12, 5942, 20, 20, 80],"float32"), list[1,2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.037558 test begin: paddle.Tensor.sum(Tensor([12484, 32, 476, 12],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([12484, 32, 476, 12],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.047462 test begin: paddle.Tensor.sum(Tensor([126, 369567, 7, 7],"float32"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([126, 369567, 7, 7],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.061715 test begin: paddle.Tensor.sum(Tensor([126, 8, 323371, 7],"float32"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([126, 8, 323371, 7],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.072963 test begin: paddle.Tensor.sum(Tensor([126, 8, 7, 323371],"float32"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([126, 8, 7, 323371],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.083547 test begin: paddle.Tensor.sum(Tensor([128, 17825793, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([128, 17825793, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.094023 test begin: paddle.Tensor.sum(Tensor([128, 2, 8912897],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([128, 2, 8912897],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.103567 test begin: paddle.Tensor.sum(Tensor([128, 3121, 476, 12],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([128, 3121, 476, 12],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.113256 test begin: paddle.Tensor.sum(Tensor([128, 32, 46422, 12],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([128, 32, 46422, 12],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.123020 test begin: paddle.Tensor.sum(Tensor([128, 32, 476, 1171],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([128, 32, 476, 1171],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.132552 test begin: paddle.Tensor.sum(Tensor([134217729, 2, 4, 4],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([134217729, 2, 4, 4],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.142220 test begin: paddle.Tensor.sum(Tensor([1393, 65536, 25],"float32"), 2, )

[torch error] paddle.Tensor.sum(Tensor([1393, 65536, 25],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.153512 test begin: paddle.Tensor.sum(Tensor([14, 12782641, 4, 2, 3],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([14, 12782641, 4, 2, 3],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.163640 test begin: paddle.Tensor.sum(Tensor([14, 19173962, 4, 4],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([14, 19173962, 4, 4],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.173392 test begin: paddle.Tensor.sum(Tensor([14, 2, 25565282, 2, 3],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([14, 2, 25565282, 2, 3],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.182910 test begin: paddle.Tensor.sum(Tensor([14, 2, 38347923, 4],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([14, 2, 38347923, 4],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.192511 test begin: paddle.Tensor.sum(Tensor([14, 2, 4, 12782641, 3],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([14, 2, 4, 12782641, 3],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.202068 test begin: paddle.Tensor.sum(Tensor([14, 2, 4, 2, 19173962],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([14, 2, 4, 2, 19173962],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.211463 test begin: paddle.Tensor.sum(Tensor([14, 2, 4, 38347923],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([14, 2, 4, 38347923],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.220738 test begin: paddle.Tensor.sum(Tensor([14, 2, 51130564, 3],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([14, 2, 51130564, 3],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.230048 test begin: paddle.Tensor.sum(Tensor([14, 25565282, 4, 3],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([14, 25565282, 4, 3],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.239338 test begin: paddle.Tensor.sum(Tensor([1431655766, 3],"float16"), list[0,-1,], )

[torch error] paddle.Tensor.sum(Tensor([1431655766, 3],"float16"), list[0,-1,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.248831 test begin: paddle.Tensor.sum(Tensor([14389, 158580],"int32"), 0, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([14389, 158580],"int32"), 0, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.258170 test begin: paddle.Tensor.sum(Tensor([14449, 157920],"bool"), 1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([14449, 157920],"bool"), 1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.267489 test begin: paddle.Tensor.sum(Tensor([148, 5, 3083381],"float32"), 0, )

[torch error] paddle.Tensor.sum(Tensor([148, 5, 3083381],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.276799 test begin: paddle.Tensor.sum(Tensor([148, 5138968, 3],"float32"), 0, )

[torch error] paddle.Tensor.sum(Tensor([148, 5138968, 3],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.286069 test begin: paddle.Tensor.sum(Tensor([14855, 150, 32, 32],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([14855, 150, 32, 32],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.296354 test begin: paddle.Tensor.sum(Tensor([14861479, 1, 17, 17],"float16"), )

[torch error] paddle.Tensor.sum(Tensor([14861479, 1, 17, 17],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.306207 test begin: paddle.Tensor.sum(Tensor([14870, 153450],"int32"), 0, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([14870, 153450],"int32"), 0, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.315473 test begin: paddle.Tensor.sum(Tensor([15171, 150402],"int32"), 0, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([15171, 150402],"int32"), 0, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.324752 test begin: paddle.Tensor.sum(Tensor([152113426, 5, 3],"float32"), 0, )

[torch error] paddle.Tensor.sum(Tensor([152113426, 5, 3],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.334013 test begin: paddle.Tensor.sum(Tensor([15474, 128, 128, 3, 3],"float32"), list[2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([15474, 128, 128, 3, 3],"float32"), list[2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.344118 test begin: paddle.Tensor.sum(Tensor([15474, 147456],"float32"), 1, )

[torch error] paddle.Tensor.sum(Tensor([15474, 147456],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.353674 test begin: paddle.Tensor.sum(Tensor([15845149, 9, 4, 4, 1, 1],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([15845149, 9, 4, 4, 1, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.363427 test begin: paddle.Tensor.sum(Tensor([16, 10, 111412, 64, 2],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([16, 10, 111412, 64, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.373209 test begin: paddle.Tensor.sum(Tensor([16, 10, 122937, 58, 2],"float32"), axis=tuple(2,3,), )

[torch error] paddle.Tensor.sum(Tensor([16, 10, 122937, 58, 2],"float32"), axis=tuple(2,3,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.382553 test begin: paddle.Tensor.sum(Tensor([16, 10, 14260634],"float32"), 2, )

[torch error] paddle.Tensor.sum(Tensor([16, 10, 14260634],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.391827 test begin: paddle.Tensor.sum(Tensor([16, 10, 25, 570426],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([16, 10, 25, 570426],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.401085 test begin: paddle.Tensor.sum(Tensor([16, 10, 4, 3565159],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([16, 10, 4, 3565159],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.410393 test begin: paddle.Tensor.sum(Tensor([16, 10, 4240, 3364],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([16, 10, 4240, 3364],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.420690 test begin: paddle.Tensor.sum(Tensor([16, 10, 58, 122937, 2],"float32"), axis=tuple(2,3,), )

[torch error] paddle.Tensor.sum(Tensor([16, 10, 58, 122937, 2],"float32"), axis=tuple(2,3,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.430414 test begin: paddle.Tensor.sum(Tensor([16, 10, 58, 58, 4240],"float32"), axis=tuple(2,3,), )

[torch error] paddle.Tensor.sum(Tensor([16, 10, 58, 58, 4240],"float32"), axis=tuple(2,3,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.439815 test begin: paddle.Tensor.sum(Tensor([16, 10, 64, 111412, 2],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([16, 10, 64, 111412, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.449861 test begin: paddle.Tensor.sum(Tensor([16, 10, 64, 64, 3482],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([16, 10, 64, 64, 3482],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.459956 test begin: paddle.Tensor.sum(Tensor([16, 10, 7130317, 2],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([16, 10, 7130317, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.469299 test begin: paddle.Tensor.sum(Tensor([16, 10598, 4, 3364],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([16, 10598, 4, 3364],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.478623 test begin: paddle.Tensor.sum(Tensor([16, 11, 2, 101283, 64],"float32"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([16, 11, 2, 101283, 64],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.487985 test begin: paddle.Tensor.sum(Tensor([16, 11, 2, 64, 101283],"float32"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([16, 11, 2, 64, 101283],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.497527 test begin: paddle.Tensor.sum(Tensor([16, 11, 3166, 64, 64],"float32"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([16, 11, 3166, 64, 64],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.507123 test begin: paddle.Tensor.sum(Tensor([16, 17409, 2, 64, 64],"float32"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([16, 17409, 2, 64, 64],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.516469 test begin: paddle.Tensor.sum(Tensor([16, 17409, 64, 64, 2],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([16, 17409, 64, 64, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.525840 test begin: paddle.Tensor.sum(Tensor([16, 21196, 58, 58, 2],"float32"), axis=tuple(2,3,), )

[torch error] paddle.Tensor.sum(Tensor([16, 21196, 58, 58, 2],"float32"), axis=tuple(2,3,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.535229 test begin: paddle.Tensor.sum(Tensor([16, 2852127, 25, 2],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([16, 2852127, 25, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.544602 test begin: paddle.Tensor.sum(Tensor([16, 5704254, 25],"float32"), 2, )

[torch error] paddle.Tensor.sum(Tensor([16, 5704254, 25],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.553970 test begin: paddle.Tensor.sum(Tensor([16, 65536, 2177],"float32"), 2, )

[torch error] paddle.Tensor.sum(Tensor([16, 65536, 2177],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.564356 test begin: paddle.Tensor.sum(Tensor([16520, 138120],"int32"), 0, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([16520, 138120],"int32"), 0, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.574427 test begin: paddle.Tensor.sum(Tensor([16849, 135424],"float32"), 1, )

[torch error] paddle.Tensor.sum(Tensor([16849, 135424],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.583762 test begin: paddle.Tensor.sum(Tensor([16957, 10, 4, 3364],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([16957, 10, 4, 3364],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.593021 test begin: paddle.Tensor.sum(Tensor([17, 252645136],"float16"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([17, 252645136],"float16"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.602394 test begin: paddle.Tensor.sum(Tensor([171798692, 1, 5, 5],"float16"), 2, )

[torch error] paddle.Tensor.sum(Tensor([171798692, 1, 5, 5],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.612521 test begin: paddle.Tensor.sum(Tensor([171798692, 1, 5, 5],"float16"), 3, )

[torch error] paddle.Tensor.sum(Tensor([171798692, 1, 5, 5],"float16"), 3, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.623713 test begin: paddle.Tensor.sum(Tensor([171798692, 5, 1, 5],"float16"), 1, )

[torch error] paddle.Tensor.sum(Tensor([171798692, 5, 1, 5],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.633541 test begin: paddle.Tensor.sum(Tensor([171798692, 5, 1, 5],"float16"), 3, )

[torch error] paddle.Tensor.sum(Tensor([171798692, 5, 1, 5],"float16"), 3, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.643093 test begin: paddle.Tensor.sum(Tensor([171798692, 5, 5, 1],"float16"), 1, )

[torch error] paddle.Tensor.sum(Tensor([171798692, 5, 5, 1],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.652776 test begin: paddle.Tensor.sum(Tensor([17409, 4, 4, 4, 4, 4, 128],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([17409, 4, 4, 4, 4, 4, 128],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.662356 test begin: paddle.Tensor.sum(Tensor([17825793, 128],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([17825793, 128],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.672054 test begin: paddle.Tensor.sum(Tensor([17825793, 128],"float32"), axis=0, )

[torch error] paddle.Tensor.sum(Tensor([17825793, 128],"float32"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.681511 test begin: paddle.Tensor.sum(Tensor([178956971, 2, 3, 4],"float16"), )

[torch error] paddle.Tensor.sum(Tensor([178956971, 2, 3, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.690953 test begin: paddle.Tensor.sum(Tensor([178956971, 2, 4, 3],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([178956971, 2, 4, 3],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.701617 test begin: paddle.Tensor.sum(Tensor([18108742, 126],"float32"), 0, )

[torch error] paddle.Tensor.sum(Tensor([18108742, 126],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.711031 test begin: paddle.Tensor.sum(Tensor([181896, 12544],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([181896, 12544],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.720413 test begin: paddle.Tensor.sum(Tensor([181896, 12544],"float32"), 1, )

[torch error] paddle.Tensor.sum(Tensor([181896, 12544],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.729857 test begin: paddle.Tensor.sum(Tensor([18416, 123904],"float32"), 1, )

[torch error] paddle.Tensor.sum(Tensor([18416, 123904],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.739214 test begin: paddle.Tensor.sum(Tensor([18857037, 121],"float32"), 0, )

[torch error] paddle.Tensor.sum(Tensor([18857037, 121],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.748589 test begin: paddle.Tensor.sum(Tensor([190141782, 12],"bool"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([190141782, 12],"bool"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.757952 test begin: paddle.Tensor.sum(Tensor([190141782, 12],"bool"), axis=1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([190141782, 12],"bool"), axis=1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.767386 test begin: paddle.Tensor.sum(Tensor([190141782, 12],"float32"), 0, )

[torch error] paddle.Tensor.sum(Tensor([190141782, 12],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.776727 test begin: paddle.Tensor.sum(Tensor([190141782, 12],"float32"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([190141782, 12],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.786138 test begin: paddle.Tensor.sum(Tensor([190141782, 2, 2, 3],"float32"), list[0,1,], )

[torch error] paddle.Tensor.sum(Tensor([190141782, 2, 2, 3],"float32"), list[0,1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.795571 test begin: paddle.Tensor.sum(Tensor([190141782, 3, 1, 4, 1],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([190141782, 3, 1, 4, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.804987 test begin: paddle.Tensor.sum(Tensor([190141782, 3, 4],"bool"), axis=0, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([190141782, 3, 4],"bool"), axis=0, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.814450 test begin: paddle.Tensor.sum(Tensor([190141782, 3, 4],"bool"), axis=1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([190141782, 3, 4],"bool"), axis=1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.827083 test begin: paddle.Tensor.sum(Tensor([19014179, 2, 1, 2, 1, 1, 2, 3, 5],"bool"), 2, )

[torch error] paddle.Tensor.sum(Tensor([19014179, 2, 1, 2, 1, 1, 2, 3, 5],"bool"), 2, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.836872 test begin: paddle.Tensor.sum(Tensor([1901418, 1200],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([1901418, 1200],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.846369 test begin: paddle.Tensor.sum(Tensor([1901418, 3, 20, 20, 1],"float32"), list[1,2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([1901418, 3, 20, 20, 1],"float32"), list[1,2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.855698 test begin: paddle.Tensor.sum(Tensor([19015, 400, 300],"float32"), axis=-3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([19015, 400, 300],"float32"), axis=-3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.865078 test begin: paddle.Tensor.sum(Tensor([1935, 256, 512, 3, 3],"float32"), list[2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([1935, 256, 512, 3, 3],"float32"), list[2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.874405 test begin: paddle.Tensor.sum(Tensor([2, 1, 1140850690, 1, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([2, 1, 1140850690, 1, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.883763 test begin: paddle.Tensor.sum(Tensor([2, 1, 1140850690],"int64"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([2, 1, 1140850690],"int64"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.893134 test begin: paddle.Tensor.sum(Tensor([2, 1, 2, 1, 570425345],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([2, 1, 2, 1, 570425345],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.902397 test begin: paddle.Tensor.sum(Tensor([2, 1, 2, 570425345, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([2, 1, 2, 570425345, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.911948 test begin: paddle.Tensor.sum(Tensor([2, 1140850690, 1],"int64"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([2, 1140850690, 1],"int64"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.921487 test begin: paddle.Tensor.sum(Tensor([2, 1140850690],"bool"), axis=0, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([2, 1140850690],"bool"), axis=0, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.930941 test begin: paddle.Tensor.sum(Tensor([2, 1140850690],"bool"), axis=1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([2, 1140850690],"bool"), axis=1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.940293 test begin: paddle.Tensor.sum(Tensor([2, 1140850690],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([2, 1140850690],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.949763 test begin: paddle.Tensor.sum(Tensor([2, 1140850690],"float32"), 1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([2, 1140850690],"float32"), 1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.959199 test begin: paddle.Tensor.sum(Tensor([2, 1140850690],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([2, 1140850690],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.968582 test begin: paddle.Tensor.sum(Tensor([2, 1140850690],"float32"), list[-1,], )

[torch error] paddle.Tensor.sum(Tensor([2, 1140850690],"float32"), list[-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.977935 test begin: paddle.Tensor.sum(Tensor([2, 1140850690],"float32"), list[-2,-1,], )

[torch error] paddle.Tensor.sum(Tensor([2, 1140850690],"float32"), list[-2,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.987286 test begin: paddle.Tensor.sum(Tensor([2, 17825793, 8, 8],"float32"), axis=-3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([2, 17825793, 8, 8],"float32"), axis=-3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:27.996738 test begin: paddle.Tensor.sum(Tensor([2, 19014179, 1, 2, 1, 1, 2, 3, 5],"bool"), 2, )

[torch error] paddle.Tensor.sum(Tensor([2, 19014179, 1, 2, 1, 1, 2, 3, 5],"bool"), 2, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.006173 test begin: paddle.Tensor.sum(Tensor([2, 2, 1, 19014179, 1, 1, 2, 3, 5],"bool"), 2, )

[torch error] paddle.Tensor.sum(Tensor([2, 2, 1, 19014179, 1, 1, 2, 3, 5],"bool"), 2, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.015915 test begin: paddle.Tensor.sum(Tensor([2, 2, 1, 2, 1, 1, 19014179, 3, 5],"bool"), 2, )

[torch error] paddle.Tensor.sum(Tensor([2, 2, 1, 2, 1, 1, 19014179, 3, 5],"bool"), 2, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.025441 test begin: paddle.Tensor.sum(Tensor([2, 2, 1, 2, 1, 1, 2, 28521268, 5],"bool"), 2, )

[torch error] paddle.Tensor.sum(Tensor([2, 2, 1, 2, 1, 1, 2, 28521268, 5],"bool"), 2, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.034935 test begin: paddle.Tensor.sum(Tensor([2, 2, 1, 2, 1, 1, 2, 3, 47535446],"bool"), 2, )

[torch error] paddle.Tensor.sum(Tensor([2, 2, 1, 2, 1, 1, 2, 3, 47535446],"bool"), 2, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.044405 test begin: paddle.Tensor.sum(Tensor([2, 2, 1, 2, 1, 9507090, 2, 3, 5],"bool"), 2, )

[torch error] paddle.Tensor.sum(Tensor([2, 2, 1, 2, 1, 9507090, 2, 3, 5],"bool"), 2, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.053857 test begin: paddle.Tensor.sum(Tensor([2, 2, 1, 2, 9507090, 1, 2, 3, 5],"bool"), 2, )

[torch error] paddle.Tensor.sum(Tensor([2, 2, 1, 2, 9507090, 1, 2, 3, 5],"bool"), 2, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.063766 test begin: paddle.Tensor.sum(Tensor([2, 2, 570425345],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([2, 2, 570425345],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.074798 test begin: paddle.Tensor.sum(Tensor([2, 2, 570425345],"int64"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([2, 2, 570425345],"int64"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.085881 test begin: paddle.Tensor.sum(Tensor([2, 2, 9507090, 2, 1, 1, 2, 3, 5],"bool"), 2, )

[torch error] paddle.Tensor.sum(Tensor([2, 2, 9507090, 2, 1, 1, 2, 3, 5],"bool"), 2, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.096432 test begin: paddle.Tensor.sum(Tensor([2, 2147483649],"float16"), -1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([2, 2147483649],"float16"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.106830 test begin: paddle.Tensor.sum(Tensor([2, 285212673, 1, 4, 1],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([2, 285212673, 1, 4, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.117068 test begin: paddle.Tensor.sum(Tensor([2, 285212673, 4],"bool"), axis=0, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([2, 285212673, 4],"bool"), axis=0, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.128042 test begin: paddle.Tensor.sum(Tensor([2, 3, 1, 380283564, 1],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([2, 3, 1, 380283564, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.140217 test begin: paddle.Tensor.sum(Tensor([2, 3, 1, 4, 95070891],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([2, 3, 1, 4, 95070891],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.151743 test begin: paddle.Tensor.sum(Tensor([2, 3, 380283564],"bool"), axis=0, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([2, 3, 380283564],"bool"), axis=0, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.165167 test begin: paddle.Tensor.sum(Tensor([2, 3, 47535446, 8],"float32"), axis=-3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([2, 3, 47535446, 8],"float32"), axis=-3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.176635 test begin: paddle.Tensor.sum(Tensor([2, 3, 715827883],"float16"), list[-2,-1,], )

[torch error] paddle.Tensor.sum(Tensor([2, 3, 715827883],"float16"), list[-2,-1,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.186829 test begin: paddle.Tensor.sum(Tensor([2, 3, 8, 47535446],"float32"), axis=-3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([2, 3, 8, 47535446],"float32"), axis=-3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.197331 test begin: paddle.Tensor.sum(Tensor([2, 3, 95070891, 4, 1],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([2, 3, 95070891, 4, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.206956 test begin: paddle.Tensor.sum(Tensor([2, 570425345, 2, 1, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([2, 570425345, 2, 1, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.219245 test begin: paddle.Tensor.sum(Tensor([2, 570425345, 2],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([2, 570425345, 2],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.229279 test begin: paddle.Tensor.sum(Tensor([2, 570425345, 2],"int64"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([2, 570425345, 2],"int64"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.239306 test begin: paddle.Tensor.sum(Tensor([2, 715827883, 3],"float16"), list[-2,-1,], )

[torch error] paddle.Tensor.sum(Tensor([2, 715827883, 3],"float16"), list[-2,-1,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.251139 test begin: paddle.Tensor.sum(Tensor([20, 114085069],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([20, 114085069],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.261369 test begin: paddle.Tensor.sum(Tensor([207427399, 11],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([207427399, 11],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.272589 test begin: paddle.Tensor.sum(Tensor([207427399, 11],"float32"), 0, )

[torch error] paddle.Tensor.sum(Tensor([207427399, 11],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.284056 test begin: paddle.Tensor.sum(Tensor([21298, 107136],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([21298, 107136],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.293810 test begin: paddle.Tensor.sum(Tensor([21298, 107136],"float32"), 1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([21298, 107136],"float32"), 1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.306855 test begin: paddle.Tensor.sum(Tensor([2147483649, 2],"float16"), -1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([2147483649, 2],"float16"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.316360 test begin: paddle.Tensor.sum(Tensor([2147483649, 2],"float16"), list[-1,], )

[torch error] paddle.Tensor.sum(Tensor([2147483649, 2],"float16"), list[-1,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.325749 test begin: paddle.Tensor.sum(Tensor([214748365, 20],"float16"), -1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([214748365, 20],"float16"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.335077 test begin: paddle.Tensor.sum(Tensor([214748365, 20],"float16"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([214748365, 20],"float16"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.344340 test begin: paddle.Tensor.sum(Tensor([214748365, 4, 5],"float16"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([214748365, 4, 5],"float16"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.353586 test begin: paddle.Tensor.sum(Tensor([21474837, 10, 10, 2],"float16"), -1, )

[torch error] paddle.Tensor.sum(Tensor([21474837, 10, 10, 2],"float16"), -1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.362838 test begin: paddle.Tensor.sum(Tensor([21474837, 10, 20],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([21474837, 10, 20],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.372096 test begin: paddle.Tensor.sum(Tensor([21474837, 10, 20],"float16"), list[0,-1,], )

[torch error] paddle.Tensor.sum(Tensor([21474837, 10, 20],"float16"), list[0,-1,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.381360 test begin: paddle.Tensor.sum(Tensor([21475, 500, 400],"float16"), axis=-3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([21475, 500, 400],"float16"), axis=-3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.390625 test begin: paddle.Tensor.sum(Tensor([21559, 105840],"int32"), 0, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([21559, 105840],"int32"), 0, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.402586 test begin: paddle.Tensor.sum(Tensor([221848, 1, 10285],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([221848, 1, 10285],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.412589 test begin: paddle.Tensor.sum(Tensor([2281701379, 1, 1, 1],"bool"), )

[torch error] paddle.Tensor.sum(Tensor([2281701379, 1, 1, 1],"bool"), ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.422684 test begin: paddle.Tensor.sum(Tensor([2281701379, 1, 1],"int64"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([2281701379, 1, 1],"int64"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.433650 test begin: paddle.Tensor.sum(Tensor([2281701379, 1],"bool"), )

[torch error] paddle.Tensor.sum(Tensor([2281701379, 1],"bool"), ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.443325 test begin: paddle.Tensor.sum(Tensor([2281701379, 1],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([2281701379, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.460304 test begin: paddle.Tensor.sum(Tensor([2281701379, 1],"float32"), list[-2,-1,], )

[torch error] paddle.Tensor.sum(Tensor([2281701379, 1],"float32"), list[-2,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.469719 test begin: paddle.Tensor.sum(Tensor([2281701379, 1],"int64"), )

[torch error] paddle.Tensor.sum(Tensor([2281701379, 1],"int64"), ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.479076 test begin: paddle.Tensor.sum(Tensor([2281701379],"bool"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([2281701379],"bool"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.488500 test begin: paddle.Tensor.sum(Tensor([2281701379],"bool"), axis=0, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([2281701379],"bool"), axis=0, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.497902 test begin: paddle.Tensor.sum(Tensor([2281701379],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([2281701379],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.507439 test begin: paddle.Tensor.sum(Tensor([2281701379],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([2281701379],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.516804 test begin: paddle.Tensor.sum(Tensor([2281701379],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([2281701379],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.526276 test begin: paddle.Tensor.sum(Tensor([2281701379],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([2281701379],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.535945 test begin: paddle.Tensor.sum(Tensor([2281701379],"float32"), list[-1,], )

[torch error] paddle.Tensor.sum(Tensor([2281701379],"float32"), list[-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.545331 test begin: paddle.Tensor.sum(Tensor([228170138, 10],"float32"), 0, )

[torch error] paddle.Tensor.sum(Tensor([228170138, 10],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.554649 test begin: paddle.Tensor.sum(Tensor([228170138, 10],"float32"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([228170138, 10],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.564006 test begin: paddle.Tensor.sum(Tensor([23283, 280, 350],"float32"), axis=-3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([23283, 280, 350],"float32"), axis=-3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.573681 test begin: paddle.Tensor.sum(Tensor([23768, 3, 20, 20, 80],"float32"), list[1,2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([23768, 3, 20, 20, 80],"float32"), list[1,2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.583058 test begin: paddle.Tensor.sum(Tensor([238609295, 6, 3],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([238609295, 6, 3],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.592301 test begin: paddle.Tensor.sum(Tensor([252645136, 17],"float16"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([252645136, 17],"float16"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.602723 test begin: paddle.Tensor.sum(Tensor([25321, 11, 2, 64, 64],"float32"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([25321, 11, 2, 64, 64],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.612654 test begin: paddle.Tensor.sum(Tensor([253522376, 3, 3],"bool"), axis=0, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([253522376, 3, 3],"bool"), axis=0, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.621976 test begin: paddle.Tensor.sum(Tensor([27853, 10, 64, 64, 2],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([27853, 10, 64, 64, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.632348 test begin: paddle.Tensor.sum(Tensor([286331154, 3, 5],"float16"), 1, )

[torch error] paddle.Tensor.sum(Tensor([286331154, 3, 5],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.642414 test begin: paddle.Tensor.sum(Tensor([286332, 5000, 3],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([286332, 5000, 3],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.651685 test begin: paddle.Tensor.sum(Tensor([2910334, 28, 28],"float32"), axis=-3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([2910334, 28, 28],"float32"), axis=-3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.661088 test begin: paddle.Tensor.sum(Tensor([3, 1, 2, 1, 380283564],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([3, 1, 2, 1, 380283564],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.670406 test begin: paddle.Tensor.sum(Tensor([3, 1, 2, 380283564, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([3, 1, 2, 380283564, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.679662 test begin: paddle.Tensor.sum(Tensor([3, 1, 2, 380283564],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([3, 1, 2, 380283564],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.690135 test begin: paddle.Tensor.sum(Tensor([3, 1, 760567127, 1, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([3, 1, 760567127, 1, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.700839 test begin: paddle.Tensor.sum(Tensor([3, 1, 760567127, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([3, 1, 760567127, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.710320 test begin: paddle.Tensor.sum(Tensor([3, 2173049, 350],"float32"), axis=-3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 2173049, 350],"float32"), axis=-3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.719817 test begin: paddle.Tensor.sum(Tensor([3, 253522376, 3],"bool"), axis=0, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 253522376, 3],"bool"), axis=0, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.729351 test begin: paddle.Tensor.sum(Tensor([3, 2535224, 300],"float32"), axis=-3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 2535224, 300],"float32"), axis=-3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.738898 test begin: paddle.Tensor.sum(Tensor([3, 27163112, 28],"float32"), axis=-3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 27163112, 28],"float32"), axis=-3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.748391 test begin: paddle.Tensor.sum(Tensor([3, 28, 27163112],"float32"), axis=-3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 28, 27163112],"float32"), axis=-3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.757788 test begin: paddle.Tensor.sum(Tensor([3, 280, 2716312],"float32"), axis=-3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 280, 2716312],"float32"), axis=-3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.767312 test begin: paddle.Tensor.sum(Tensor([3, 286331154, 5],"float16"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([3, 286331154, 5],"float16"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.776703 test begin: paddle.Tensor.sum(Tensor([3, 3, 253522376],"bool"), axis=0, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 3, 253522376],"bool"), axis=0, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.786148 test begin: paddle.Tensor.sum(Tensor([3, 3, 3, 84507459],"float32"), list[1,2,3,], )

[torch error] paddle.Tensor.sum(Tensor([3, 3, 3, 84507459],"float32"), list[1,2,3,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.795500 test begin: paddle.Tensor.sum(Tensor([3, 3, 84507459, 3],"float32"), list[1,2,3,], )

[torch error] paddle.Tensor.sum(Tensor([3, 3, 84507459, 3],"float32"), list[1,2,3,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.806298 test begin: paddle.Tensor.sum(Tensor([3, 3579140, 400],"float16"), axis=-3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 3579140, 400],"float16"), axis=-3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.816809 test begin: paddle.Tensor.sum(Tensor([3, 380283564, 2, 1, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([3, 380283564, 2, 1, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.826608 test begin: paddle.Tensor.sum(Tensor([3, 380283564, 2, 1],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([3, 380283564, 2, 1],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.836180 test begin: paddle.Tensor.sum(Tensor([3, 4, 357913942],"float16"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([3, 4, 357913942],"float16"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.845507 test begin: paddle.Tensor.sum(Tensor([3, 400, 1901418],"float32"), axis=-3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 400, 1901418],"float32"), axis=-3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.854799 test begin: paddle.Tensor.sum(Tensor([3, 500, 2863312],"float16"), axis=-3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 500, 2863312],"float16"), axis=-3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.864085 test begin: paddle.Tensor.sum(Tensor([3, 6, 3, 4, 2, 5281717],"bool"), axis=3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 6, 3, 4, 2, 5281717],"bool"), axis=3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.873875 test begin: paddle.Tensor.sum(Tensor([3, 6, 3, 4, 2, 5281717],"bool"), axis=4, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 6, 3, 4, 2, 5281717],"bool"), axis=4, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.884246 test begin: paddle.Tensor.sum(Tensor([3, 6, 3, 4, 2, 5281717],"bool"), axis=5, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 6, 3, 4, 2, 5281717],"bool"), axis=5, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.894239 test begin: paddle.Tensor.sum(Tensor([3, 6, 3, 4, 2112687, 5],"bool"), axis=3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 6, 3, 4, 2112687, 5],"bool"), axis=3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.905155 test begin: paddle.Tensor.sum(Tensor([3, 6, 3, 4, 2112687, 5],"bool"), axis=4, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 6, 3, 4, 2112687, 5],"bool"), axis=4, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.915609 test begin: paddle.Tensor.sum(Tensor([3, 6, 3, 4, 2112687, 5],"bool"), axis=5, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 6, 3, 4, 2112687, 5],"bool"), axis=5, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.925178 test begin: paddle.Tensor.sum(Tensor([3, 6, 3, 4225373, 2, 5],"bool"), axis=3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 6, 3, 4225373, 2, 5],"bool"), axis=3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.934721 test begin: paddle.Tensor.sum(Tensor([3, 6, 3, 4225373, 2, 5],"bool"), axis=4, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 6, 3, 4225373, 2, 5],"bool"), axis=4, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.945278 test begin: paddle.Tensor.sum(Tensor([3, 6, 3, 4225373, 2, 5],"bool"), axis=5, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 6, 3, 4225373, 2, 5],"bool"), axis=5, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.955369 test begin: paddle.Tensor.sum(Tensor([3, 6, 3169030, 4, 2, 5],"bool"), axis=3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 6, 3169030, 4, 2, 5],"bool"), axis=3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.964764 test begin: paddle.Tensor.sum(Tensor([3, 6, 3169030, 4, 2, 5],"bool"), axis=4, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 6, 3169030, 4, 2, 5],"bool"), axis=4, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.974146 test begin: paddle.Tensor.sum(Tensor([3, 6, 3169030, 4, 2, 5],"bool"), axis=5, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 6, 3169030, 4, 2, 5],"bool"), axis=5, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.983500 test begin: paddle.Tensor.sum(Tensor([3, 6338060, 3, 4, 2, 5],"bool"), axis=3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 6338060, 3, 4, 2, 5],"bool"), axis=3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:28.993054 test begin: paddle.Tensor.sum(Tensor([3, 6338060, 3, 4, 2, 5],"bool"), axis=4, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 6338060, 3, 4, 2, 5],"bool"), axis=4, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.002543 test begin: paddle.Tensor.sum(Tensor([3, 6338060, 3, 4, 2, 5],"bool"), axis=5, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 6338060, 3, 4, 2, 5],"bool"), axis=5, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.012773 test begin: paddle.Tensor.sum(Tensor([3, 8, 95070891],"float32"), axis=-3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 8, 95070891],"float32"), axis=-3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.022433 test begin: paddle.Tensor.sum(Tensor([3, 84507459, 3, 3],"float32"), list[1,2,3,], )

[torch error] paddle.Tensor.sum(Tensor([3, 84507459, 3, 3],"float32"), list[1,2,3,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.031780 test begin: paddle.Tensor.sum(Tensor([3, 95070891, 8],"float32"), axis=-3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3, 95070891, 8],"float32"), axis=-3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.041096 test begin: paddle.Tensor.sum(Tensor([30, 76056713],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([30, 76056713],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.050402 test begin: paddle.Tensor.sum(Tensor([3169030, 6, 3, 4, 2, 5],"bool"), axis=3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3169030, 6, 3, 4, 2, 5],"bool"), axis=3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.059761 test begin: paddle.Tensor.sum(Tensor([3169030, 6, 3, 4, 2, 5],"bool"), axis=4, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3169030, 6, 3, 4, 2, 5],"bool"), axis=4, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.069159 test begin: paddle.Tensor.sum(Tensor([3169030, 6, 3, 4, 2, 5],"bool"), axis=5, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([3169030, 6, 3, 4, 2, 5],"bool"), axis=5, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.078664 test begin: paddle.Tensor.sum(Tensor([318318, 14, 512],"float32"), 2, )

[torch error] paddle.Tensor.sum(Tensor([318318, 14, 512],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.088063 test begin: paddle.Tensor.sum(Tensor([325957340, 7],"bool"), axis=1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([325957340, 7],"bool"), axis=1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.098384 test begin: paddle.Tensor.sum(Tensor([33554433, 128],"float16"), axis=0, )

[torch error] paddle.Tensor.sum(Tensor([33554433, 128],"float16"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.109680 test begin: paddle.Tensor.sum(Tensor([33914, 10, 58, 58, 2],"float32"), axis=tuple(2,3,), )

[torch error] paddle.Tensor.sum(Tensor([33914, 10, 58, 58, 2],"float32"), axis=tuple(2,3,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.120657 test begin: paddle.Tensor.sum(Tensor([34817, 128, 8, 4, 4, 4],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([34817, 128, 8, 4, 4, 4],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.130445 test begin: paddle.Tensor.sum(Tensor([35651585, 8, 8],"float32"), axis=-3, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([35651585, 8, 8],"float32"), axis=-3, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.139717 test begin: paddle.Tensor.sum(Tensor([380283564, 2, 3],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([380283564, 2, 3],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.148944 test begin: paddle.Tensor.sum(Tensor([380283564, 3, 1, 1, 2, 1],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([380283564, 3, 1, 1, 2, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.159157 test begin: paddle.Tensor.sum(Tensor([3869, 256, 256, 3, 3],"float32"), list[2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([3869, 256, 256, 3, 3],"float32"), list[2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.168689 test begin: paddle.Tensor.sum(Tensor([4, 1073741825],"float16"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([4, 1073741825],"float16"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.178936 test begin: paddle.Tensor.sum(Tensor([4, 1114113, 512],"float32"), 2, )

[torch error] paddle.Tensor.sum(Tensor([4, 1114113, 512],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.190086 test begin: paddle.Tensor.sum(Tensor([4, 14, 40744668],"float32"), 2, )

[torch error] paddle.Tensor.sum(Tensor([4, 14, 40744668],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.199485 test begin: paddle.Tensor.sum(Tensor([4, 214748365, 5],"float16"), 1, )

[torch error] paddle.Tensor.sum(Tensor([4, 214748365, 5],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.209148 test begin: paddle.Tensor.sum(Tensor([4, 285212673, 1, 1, 2, 1],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([4, 285212673, 1, 1, 2, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.218605 test begin: paddle.Tensor.sum(Tensor([4, 3, 1, 1, 190141782, 1],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([4, 3, 1, 1, 190141782, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.228007 test begin: paddle.Tensor.sum(Tensor([4, 3, 1, 1, 2, 95070891],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([4, 3, 1, 1, 2, 95070891],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.237620 test begin: paddle.Tensor.sum(Tensor([4, 3, 1, 95070891, 2, 1],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([4, 3, 1, 95070891, 2, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.247256 test begin: paddle.Tensor.sum(Tensor([4, 3, 357913942],"float16"), 1, )

[torch error] paddle.Tensor.sum(Tensor([4, 3, 357913942],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.256665 test begin: paddle.Tensor.sum(Tensor([4, 3, 95070891, 1, 2, 1],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([4, 3, 95070891, 1, 2, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.266089 test begin: paddle.Tensor.sum(Tensor([4, 570425345],"bool"), 1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([4, 570425345],"bool"), 1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.275614 test begin: paddle.Tensor.sum(Tensor([4, 570425345],"bool"), axis=1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([4, 570425345],"bool"), axis=1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.285132 test begin: paddle.Tensor.sum(Tensor([4, 570425345],"float32"), axis=-1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([4, 570425345],"float32"), axis=-1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.294476 test begin: paddle.Tensor.sum(Tensor([4, 570425345],"float32"), axis=1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([4, 570425345],"float32"), axis=1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.304475 test begin: paddle.Tensor.sum(Tensor([4, 7, 81489335],"bool"), axis=1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([4, 7, 81489335],"bool"), axis=1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.314901 test begin: paddle.Tensor.sum(Tensor([4, 7, 81489335],"bool"), axis=2, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([4, 7, 81489335],"bool"), axis=2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.326305 test begin: paddle.Tensor.sum(Tensor([4, 95070891, 6],"bool"), axis=1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([4, 95070891, 6],"bool"), axis=1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.337884 test begin: paddle.Tensor.sum(Tensor([4, 95070891, 6],"bool"), axis=2, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([4, 95070891, 6],"bool"), axis=2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.348680 test begin: paddle.Tensor.sum(Tensor([4294967297, 1, 1, 1],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([4294967297, 1, 1, 1],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.359195 test begin: paddle.Tensor.sum(Tensor([4294967297, 1, 1, 1],"float16"), 1, )

[torch error] paddle.Tensor.sum(Tensor([4294967297, 1, 1, 1],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.369382 test begin: paddle.Tensor.sum(Tensor([4294967297, 1, 1, 1],"float16"), 2, )

[torch error] paddle.Tensor.sum(Tensor([4294967297, 1, 1, 1],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.379536 test begin: paddle.Tensor.sum(Tensor([4294967297, 1, 1, 1],"float16"), 3, )

[torch error] paddle.Tensor.sum(Tensor([4294967297, 1, 1, 1],"float16"), 3, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.389618 test begin: paddle.Tensor.sum(Tensor([4294967297],"float16"), -1, )

[torch error] paddle.Tensor.sum(Tensor([4294967297],"float16"), -1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.399635 test begin: paddle.Tensor.sum(Tensor([4294967297],"float16"), -1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([4294967297],"float16"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.409887 test begin: paddle.Tensor.sum(Tensor([4294967297],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([4294967297],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.419442 test begin: paddle.Tensor.sum(Tensor([42949673, 100],"float16"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([42949673, 100],"float16"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.431643 test begin: paddle.Tensor.sum(Tensor([429496730, 10],"float16"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([429496730, 10],"float16"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.441624 test begin: paddle.Tensor.sum(Tensor([4563403, 10, 25, 2],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([4563403, 10, 25, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.451628 test begin: paddle.Tensor.sum(Tensor([4563403, 500],"float32"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([4563403, 500],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.462115 test begin: paddle.Tensor.sum(Tensor([4739, 1, 13, 37044],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([4739, 1, 13, 37044],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.472054 test begin: paddle.Tensor.sum(Tensor([47535446, 3, 4, 4, 1, 1],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([47535446, 3, 4, 4, 1, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.482067 test begin: paddle.Tensor.sum(Tensor([475355, 3, 40, 40, 1],"float32"), list[1,2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([475355, 3, 40, 40, 1],"float32"), list[1,2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.491523 test begin: paddle.Tensor.sum(Tensor([477218589, 3, 3],"float16"), list[-2,-1,], )

[torch error] paddle.Tensor.sum(Tensor([477218589, 3, 3],"float16"), list[-2,-1,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.504701 test begin: paddle.Tensor.sum(Tensor([5, 114085069, 4],"bool"), axis=1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([5, 114085069, 4],"bool"), axis=1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.514266 test begin: paddle.Tensor.sum(Tensor([5, 3, 152113426],"bool"), axis=1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([5, 3, 152113426],"bool"), axis=1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.524561 test begin: paddle.Tensor.sum(Tensor([5, 456340276],"bool"), axis=1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([5, 456340276],"bool"), axis=1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.536185 test begin: paddle.Tensor.sum(Tensor([5224, 1, 13, 33600],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([5224, 1, 13, 33600],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.561577 test begin: paddle.Tensor.sum(Tensor([54326224, 42],"bool"), axis=1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([54326224, 42],"bool"), axis=1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.574256 test begin: paddle.Tensor.sum(Tensor([54326224, 7, 6],"bool"), axis=1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([54326224, 7, 6],"bool"), axis=1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.590079 test begin: paddle.Tensor.sum(Tensor([54326224, 7, 6],"bool"), axis=2, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([54326224, 7, 6],"bool"), axis=2, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.607424 test begin: paddle.Tensor.sum(Tensor([570425345, 1, 2, 2],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([570425345, 1, 2, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.617603 test begin: paddle.Tensor.sum(Tensor([570425345, 2, 1, 2],"float32"), list[0,-1,], )

[torch error] paddle.Tensor.sum(Tensor([570425345, 2, 1, 2],"float32"), list[0,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.627127 test begin: paddle.Tensor.sum(Tensor([570425345, 2, 2],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([570425345, 2, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.636999 test begin: paddle.Tensor.sum(Tensor([570425345, 2, 2],"float32"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([570425345, 2, 2],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.646532 test begin: paddle.Tensor.sum(Tensor([570425345, 2, 2],"float32"), list[0,-1,], )

[torch error] paddle.Tensor.sum(Tensor([570425345, 2, 2],"float32"), list[0,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.656991 test begin: paddle.Tensor.sum(Tensor([570425345, 2, 2],"float32"), list[0,1,], )

[torch error] paddle.Tensor.sum(Tensor([570425345, 2, 2],"float32"), list[0,1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.674697 test begin: paddle.Tensor.sum(Tensor([570425345, 2, 2],"int64"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([570425345, 2, 2],"int64"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.685330 test begin: paddle.Tensor.sum(Tensor([570425345, 4],"bool"), axis=-1, )

[torch error] paddle.Tensor.sum(Tensor([570425345, 4],"bool"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.694705 test begin: paddle.Tensor.sum(Tensor([570426, 500, 8],"float32"), axis=2, )

[torch error] paddle.Tensor.sum(Tensor([570426, 500, 8],"float32"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.704095 test begin: paddle.Tensor.sum(Tensor([5789, 1, 13, 30324],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([5789, 1, 13, 30324],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.713511 test begin: paddle.Tensor.sum(Tensor([5820667, 8, 7, 7],"float32"), axis=1, )

[torch error] paddle.Tensor.sum(Tensor([5820667, 8, 7, 7],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.723037 test begin: paddle.Tensor.sum(Tensor([61595, 1, 37044],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([61595, 1, 37044],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.733402 test begin: paddle.Tensor.sum(Tensor([6449, 1, 13, 27216],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([6449, 1, 13, 27216],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.750156 test begin: paddle.Tensor.sum(Tensor([67908, 1, 33600],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([67908, 1, 33600],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.760469 test begin: paddle.Tensor.sum(Tensor([67908, 1, 33600],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([67908, 1, 33600],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.769750 test begin: paddle.Tensor.sum(Tensor([697, 65536, 25, 2],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([697, 65536, 25, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.779817 test begin: paddle.Tensor.sum(Tensor([7074, 21504, 15],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([7074, 21504, 15],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.789381 test begin: paddle.Tensor.sum(Tensor([7074, 21504, 15],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([7074, 21504, 15],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.798755 test begin: paddle.Tensor.sum(Tensor([715827883, 2, 3],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([715827883, 2, 3],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.809116 test begin: paddle.Tensor.sum(Tensor([7231, 1, 13, 24276],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([7231, 1, 13, 24276],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.827541 test begin: paddle.Tensor.sum(Tensor([75245, 1, 30324],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([75245, 1, 30324],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.838049 test begin: paddle.Tensor.sum(Tensor([75245, 1, 30324],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([75245, 1, 30324],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.847465 test begin: paddle.Tensor.sum(Tensor([760567127, 3],"bool"), axis=0, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([760567127, 3],"bool"), axis=0, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.857064 test begin: paddle.Tensor.sum(Tensor([760567127, 3],"bool"), axis=1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([760567127, 3],"bool"), axis=1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.866538 test begin: paddle.Tensor.sum(Tensor([760567127, 3],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([760567127, 3],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.875908 test begin: paddle.Tensor.sum(Tensor([760567127, 3],"float32"), list[-1,], )

[torch error] paddle.Tensor.sum(Tensor([760567127, 3],"float32"), list[-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.886847 test begin: paddle.Tensor.sum(Tensor([760567127, 3],"float32"), list[-2,-1,], )

[torch error] paddle.Tensor.sum(Tensor([760567127, 3],"float32"), list[-2,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.903608 test begin: paddle.Tensor.sum(Tensor([760567127, 3],"float32"), list[0,-1,], )

[torch error] paddle.Tensor.sum(Tensor([760567127, 3],"float32"), list[0,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.913876 test begin: paddle.Tensor.sum(Tensor([7605672, 3, 10, 10, 1],"float32"), list[1,2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([7605672, 3, 10, 10, 1],"float32"), list[1,2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.923847 test begin: paddle.Tensor.sum(Tensor([7605672, 300],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([7605672, 300],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.933448 test begin: paddle.Tensor.sum(Tensor([7737, 128, 256, 3, 3],"float32"), list[2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([7737, 128, 256, 3, 3],"float32"), list[2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.942865 test begin: paddle.Tensor.sum(Tensor([8, 500, 570426],"float32"), axis=2, )

[torch error] paddle.Tensor.sum(Tensor([8, 500, 570426],"float32"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.952330 test begin: paddle.Tensor.sum(Tensor([8, 71303169, 4],"float32"), axis=2, )

[torch error] paddle.Tensor.sum(Tensor([8, 71303169, 4],"float32"), axis=2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.962836 test begin: paddle.Tensor.sum(Tensor([80, 128, 3482, 4, 4, 4],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([80, 128, 3482, 4, 4, 4],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.980876 test begin: paddle.Tensor.sum(Tensor([80, 128, 8, 1741, 4, 4],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([80, 128, 8, 1741, 4, 4],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:29.991519 test begin: paddle.Tensor.sum(Tensor([80, 128, 8, 4, 1741, 4],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([80, 128, 8, 4, 1741, 4],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.000971 test begin: paddle.Tensor.sum(Tensor([80, 128, 8, 4, 4, 1741],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([80, 128, 8, 4, 4, 1741],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.010388 test begin: paddle.Tensor.sum(Tensor([80, 128, 8, 4, 4, 4, 436],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([80, 128, 8, 4, 4, 4, 436],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.52 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.019985 test begin: paddle.Tensor.sum(Tensor([80, 128, 8, 4, 4, 436, 4],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([80, 128, 8, 4, 4, 436, 4],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.52 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.030041 test begin: paddle.Tensor.sum(Tensor([80, 128, 8, 4, 436, 4, 4],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([80, 128, 8, 4, 436, 4, 4],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.52 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.040498 test begin: paddle.Tensor.sum(Tensor([80, 128, 8, 436, 4, 4, 4],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([80, 128, 8, 436, 4, 4, 4],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.52 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.056540 test begin: paddle.Tensor.sum(Tensor([80, 128, 871, 4, 4, 4, 4],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([80, 128, 871, 4, 4, 4, 4],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.066744 test begin: paddle.Tensor.sum(Tensor([80, 13927, 8, 4, 4, 4, 4],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([80, 13927, 8, 4, 4, 4, 4],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.076119 test begin: paddle.Tensor.sum(Tensor([80, 4, 4, 4, 4, 4, 27853],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([80, 4, 4, 4, 4, 4, 27853],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.086139 test begin: paddle.Tensor.sum(Tensor([80, 4, 4, 4, 4, 871, 128],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([80, 4, 4, 4, 4, 871, 128],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.097159 test begin: paddle.Tensor.sum(Tensor([80, 4, 4, 4, 871, 4, 128],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([80, 4, 4, 4, 871, 4, 128],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.107732 test begin: paddle.Tensor.sum(Tensor([80, 4, 4, 871, 4, 4, 128],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([80, 4, 4, 871, 4, 4, 128],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.119119 test begin: paddle.Tensor.sum(Tensor([80, 4, 871, 4, 4, 4, 128],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([80, 4, 871, 4, 4, 4, 128],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.142521 test begin: paddle.Tensor.sum(Tensor([80, 55706, 8, 4, 4, 4],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([80, 55706, 8, 4, 4, 4],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.153828 test begin: paddle.Tensor.sum(Tensor([80, 871, 4, 4, 4, 4, 128],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([80, 871, 4, 4, 4, 4, 128],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.163190 test begin: paddle.Tensor.sum(Tensor([83837, 1, 27216],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([83837, 1, 27216],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.172552 test begin: paddle.Tensor.sum(Tensor([83837, 1, 27216],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([83837, 1, 27216],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.181921 test begin: paddle.Tensor.sum(Tensor([84507459, 3, 3, 3],"float32"), list[1,2,3,], )

[torch error] paddle.Tensor.sum(Tensor([84507459, 3, 3, 3],"float32"), list[1,2,3,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.193098 test begin: paddle.Tensor.sum(Tensor([858993460, 1, 1, 5],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([858993460, 1, 1, 5],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.207044 test begin: paddle.Tensor.sum(Tensor([858993460, 1, 1, 5],"float16"), 1, )

[torch error] paddle.Tensor.sum(Tensor([858993460, 1, 1, 5],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.216815 test begin: paddle.Tensor.sum(Tensor([858993460, 1, 1, 5],"float16"), 2, )

[torch error] paddle.Tensor.sum(Tensor([858993460, 1, 1, 5],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.226213 test begin: paddle.Tensor.sum(Tensor([858993460, 1, 1, 5],"float16"), 3, )

[torch error] paddle.Tensor.sum(Tensor([858993460, 1, 1, 5],"float16"), 3, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.235563 test begin: paddle.Tensor.sum(Tensor([858993460, 1, 5, 1],"float16"), 2, )

[torch error] paddle.Tensor.sum(Tensor([858993460, 1, 5, 1],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.245170 test begin: paddle.Tensor.sum(Tensor([858993460, 5, 1, 1],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([858993460, 5, 1, 1],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.254583 test begin: paddle.Tensor.sum(Tensor([858993460, 5, 1, 1],"float16"), 1, )

[torch error] paddle.Tensor.sum(Tensor([858993460, 5, 1, 1],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.263922 test begin: paddle.Tensor.sum(Tensor([858993460, 5, 1, 1],"float16"), 2, )

[torch error] paddle.Tensor.sum(Tensor([858993460, 5, 1, 1],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.273250 test begin: paddle.Tensor.sum(Tensor([858993460, 5, 1, 1],"float16"), 3, )

[torch error] paddle.Tensor.sum(Tensor([858993460, 5, 1, 1],"float16"), 3, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.282548 test begin: paddle.Tensor.sum(Tensor([8705, 128, 8, 4, 4, 4, 4],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([8705, 128, 8, 4, 4, 4, 4],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.292154 test begin: paddle.Tensor.sum(Tensor([89478486, 2, 4, 2, 3],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([89478486, 2, 4, 2, 3],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.301551 test begin: paddle.Tensor.sum(Tensor([9, 10, 47721859],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([9, 10, 47721859],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.311337 test begin: paddle.Tensor.sum(Tensor([9, 10, 47721859],"float16"), list[0,-1,], )

[torch error] paddle.Tensor.sum(Tensor([9, 10, 47721859],"float16"), list[0,-1,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.320591 test begin: paddle.Tensor.sum(Tensor([9, 23860930, 20],"float16"), 0, )

[torch error] paddle.Tensor.sum(Tensor([9, 23860930, 20],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.329825 test begin: paddle.Tensor.sum(Tensor([9, 23860930, 20],"float16"), list[0,-1,], )

[torch error] paddle.Tensor.sum(Tensor([9, 23860930, 20],"float16"), list[0,-1,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.340193 test begin: paddle.Tensor.sum(Tensor([9126806, 10, 25],"float32"), 2, )

[torch error] paddle.Tensor.sum(Tensor([9126806, 10, 25],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.350261 test begin: paddle.Tensor.sum(Tensor([93991, 1, 24276],"float32"), -1, keepdim=True, )

[torch error] paddle.Tensor.sum(Tensor([93991, 1, 24276],"float32"), -1, keepdim=True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.359507 test begin: paddle.Tensor.sum(Tensor([93991, 1, 24276],"float32"), axis=-2, )

[torch error] paddle.Tensor.sum(Tensor([93991, 1, 24276],"float32"), axis=-2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.368732 test begin: paddle.Tensor.sum(Tensor([9400, 24276, 10],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([9400, 24276, 10],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.378720 test begin: paddle.Tensor.sum(Tensor([9400, 24276, 10],"float32"), -1, )

[torch error] paddle.Tensor.sum(Tensor([9400, 24276, 10],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.388255 test begin: paddle.Tensor.sum(Tensor([95070891, 2, 1, 3, 1, 4],"float32"), )

[torch error] paddle.Tensor.sum(Tensor([95070891, 2, 1, 3, 1, 4],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.397584 test begin: paddle.Tensor.sum(Tensor([95070891, 24],"float32"), 1, )

[torch error] paddle.Tensor.sum(Tensor([95070891, 24],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.408061 test begin: paddle.Tensor.sum(Tensor([95071, 3, 10, 10, 80],"float32"), list[1,2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([95071, 3, 10, 10, 80],"float32"), list[1,2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.418194 test begin: paddle.Tensor.sum(Tensor([968, 512, 512, 3, 3],"float32"), list[2,3,4,], )

[torch error] paddle.Tensor.sum(Tensor([968, 512, 512, 3, 3],"float32"), list[2,3,4,], ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:30.427578 test begin: paddle.Tensor.take_along_axis(Tensor([128, 1000],"float32"), indices=Tensor([128, 17825793],"int32"), axis=-1, )

[torch error] paddle.Tensor.take_along_axis(Tensor([128, 1000],"float32"), indices=Tensor([128, 17825793],"int32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.68 GiB is allocated by PyTorch, and 508.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:52:58.466705 test begin: paddle.Tensor.take_along_axis(Tensor([128, 1000],"float32"), indices=Tensor([2281701379, 1],"int32"), axis=-1, )

[torch error] paddle.Tensor.take_along_axis(Tensor([128, 1000],"float32"), indices=Tensor([2281701379, 1],"int32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.68 GiB is allocated by PyTorch, and 508.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:53:26.543194 test begin: paddle.Tensor.take_along_axis(Tensor([128, 17825793],"float32"), indices=Tensor([128, 17825793],"int32"), axis=-1, )

[torch error] paddle.Tensor.take_along_axis(Tensor([128, 17825793],"float32"), indices=Tensor([128, 17825793],"int32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:53:54.655457 test begin: paddle.Tensor.take_along_axis(Tensor([128, 17825793],"float32"), indices=Tensor([128, 1],"int32"), axis=-1, )

[torch error] paddle.Tensor.take_along_axis(Tensor([128, 17825793],"float32"), indices=Tensor([128, 1],"int32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:54:23.770983 test begin: paddle.Tensor.take_along_axis(Tensor([2281702, 1000],"float32"), indices=Tensor([128, 1],"int32"), axis=-1, )

[torch error] paddle.Tensor.take_along_axis(Tensor([2281702, 1000],"float32"), indices=Tensor([128, 1],"int32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:54:55.387983 test begin: paddle.Tensor.take_along_axis(Tensor([2281702, 1000],"float32"), indices=Tensor([2281702, 1],"int32"), axis=-1, )

[torch error] paddle.Tensor.take_along_axis(Tensor([2281702, 1000],"float32"), indices=Tensor([2281702, 1],"int32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:55:29.005011 test begin: paddle.Tensor.take_along_axis(Tensor([2281702, 1000],"float32"), indices=Tensor([80, 1],"int32"), axis=-1, )

[torch error] paddle.Tensor.take_along_axis(Tensor([2281702, 1000],"float32"), indices=Tensor([80, 1],"int32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:55:59.126328 test begin: paddle.Tensor.take_along_axis(Tensor([80, 1000],"float32"), indices=Tensor([2281701379, 1],"int32"), axis=-1, )

[torch error] paddle.Tensor.take_along_axis(Tensor([80, 1000],"float32"), indices=Tensor([2281701379, 1],"int32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.68 GiB is allocated by PyTorch, and 508.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:56:57.109798 test begin: paddle.Tensor.take_along_axis(Tensor([80, 1000],"float32"), indices=Tensor([80, 28521268],"int32"), axis=-1, )

[torch error] paddle.Tensor.take_along_axis(Tensor([80, 1000],"float32"), indices=Tensor([80, 28521268],"int32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.68 GiB is allocated by PyTorch, and 508.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:58:01.136555 test begin: paddle.Tensor.take_along_axis(Tensor([80, 28521268],"float32"), indices=Tensor([80, 1],"int32"), axis=-1, )

[torch error] paddle.Tensor.take_along_axis(Tensor([80, 28521268],"float32"), indices=Tensor([80, 1],"int32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 17:59:04.813414 test begin: paddle.Tensor.take_along_axis(Tensor([80, 28521268],"float32"), indices=Tensor([80, 28521268],"int32"), axis=-1, )

[torch error] paddle.Tensor.take_along_axis(Tensor([80, 28521268],"float32"), indices=Tensor([80, 28521268],"int32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:02.953882 test begin: paddle.Tensor.topk(Tensor([1, 2281701379],"float32"), 5, 1, True, True, )

[torch error] paddle.Tensor.topk(Tensor([1, 2281701379],"float32"), 5, 1, True, True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:02.966328 test begin: paddle.Tensor.topk(Tensor([128, 17825793],"float32"), 5, 1, True, True, )

[torch error] paddle.Tensor.topk(Tensor([128, 17825793],"float32"), 5, 1, True, True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:02.977399 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 100083, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 100083, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:02.988832 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 10173, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 10173, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:02.999436 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 101931, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 101931, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.009750 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 101946, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 101946, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.029119 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 102327, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 102327, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.039636 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 102396, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 102396, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.050435 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 104598, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 104598, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.060803 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 104646, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 104646, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.071487 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 104883, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 104883, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.082080 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 105876, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 105876, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.092945 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 107739, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 107739, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.109599 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 108291, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 108291, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.119544 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 108540, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 108540, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.129350 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 110289, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 110289, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.138921 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 110832, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 110832, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.148739 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 111645, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 111645, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.158422 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 112293, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 112293, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.167985 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 113463, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 113463, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.177438 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 113676, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 113676, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.186840 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 113790, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 113790, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.196240 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 114213, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 114213, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.205613 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 114261, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 114261, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.214915 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 114330, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 114330, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.225438 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 114561, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 114561, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.235149 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 114750, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 114750, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.244572 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 115383, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 115383, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.253940 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 115494, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 115494, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.263238 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 115920, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 115920, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.272511 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 116010, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 116010, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.281764 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 116085, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 116085, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.291005 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 120090, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 120090, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.300245 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 120693, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 120693, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.309533 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 120765, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 120765, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.319848 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 121008, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 121008, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.329722 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 121383, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 121383, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.339368 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 121500, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 121500, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.348957 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 121539, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 121539, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.358469 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 121758, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 121758, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.367970 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 122073, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 122073, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.377407 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 122982, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 122982, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.386765 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 123516, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 123516, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.396083 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 124155, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 124155, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.405350 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 124257, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 124257, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.414704 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 129726, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 129726, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.425072 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 131643, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 131643, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.434707 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 131952, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 131952, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.444165 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 131961, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 131961, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.453527 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 132105, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 132105, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.462916 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 132522, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 132522, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.472277 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 134115, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 134115, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.481712 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 136164, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 136164, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.491090 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 137484, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 137484, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.500441 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 138210, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 138210, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.509792 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 138852, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 138852, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.519216 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 145707, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 145707, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.528592 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 146148, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 146148, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.537974 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 147111, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 147111, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.547414 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 147315, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 147315, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.556942 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 150507, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 150507, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.566446 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 150945, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 150945, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.575779 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 151971, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 151971, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.585175 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 158844, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 158844, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.595569 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 160434, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 160434, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.605043 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 16623, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 16623, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.614589 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 16704, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 16704, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.623898 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 173505, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 173505, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.633997 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 17520, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 17520, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.643695 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 17526, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 17526, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.653871 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 175965, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 175965, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.663533 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 17598, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 17598, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.672924 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 17601, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 17601, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.682332 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 176061, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 176061, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.692308 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 178701, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 178701, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.701920 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 17988, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 17988, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.711401 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 18054, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 18054, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.720797 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 18081, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 18081, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.730189 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 182052, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 182052, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.739524 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 18795, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 18795, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.748876 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 18969, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 18969, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.758331 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 192987, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 192987, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.768120 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 19461, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 19461, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.777685 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 195024, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 195024, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.787042 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 206889, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 206889, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.796469 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 20913, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 20913, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.805825 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 21504, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 21504, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.815163 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 21621, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 21621, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.824792 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 21639, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 21639, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.834280 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 22047, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 22047, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.843579 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 222156, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 222156, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.852905 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 22458, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 22458, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.862926 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 22485, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 22485, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.872635 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 22938, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 22938, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.882185 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 23199, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 23199, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.891744 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 23337, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 23337, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.901255 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 234837, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 234837, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.917539 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 23964, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 23964, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.928609 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 24081, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 24081, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.938136 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 24285, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 24285, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.949160 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 24426, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 24426, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.958945 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 24729, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 24729, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.969009 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 24861, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 24861, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.979065 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 24891, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 24891, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.988672 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 25389, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 25389, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:03.998361 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 25449, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 25449, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.009042 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 25653, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 25653, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.023008 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 25665, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 25665, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.032569 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 25707, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 25707, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.042049 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 26028, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 26028, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.051658 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 26037, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 26037, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.061415 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 26064, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 26064, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.071099 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 26340, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 26340, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.084951 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 26358, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 26358, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.094476 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 26721, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 26721, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.103802 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 26763, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 26763, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.113129 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 26820, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 26820, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.122758 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 27378, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 27378, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.135006 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 27540, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 27540, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.147343 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 27789, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 27789, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.158897 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 28236, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 28236, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.169925 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 28269, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 28269, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.180209 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 28776, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 28776, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.190411 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 28779, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 28779, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.200630 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 28836, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 28836, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.211880 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 28848, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 28848, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.222645 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 29025, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 29025, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.233973 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 29103, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 29103, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.245757 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 29124, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 29124, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.256619 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 29274, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 29274, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.267393 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 29724, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 29724, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.279649 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 29847, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 29847, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.291251 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 29865, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 29865, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.307765 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 30033, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 30033, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.317632 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 30063, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 30063, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.327514 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 30096, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 30096, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.337411 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 30141, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 30141, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.347575 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 30144, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 30144, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.357439 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 30231, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 30231, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.368484 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 30297, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 30297, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.378729 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 30339, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 30339, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.389370 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 30528, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 30528, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.399502 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 30624, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 30624, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.410394 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 30690, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 30690, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.420725 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 30852, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 30852, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.438502 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 30912, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 30912, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.448503 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 30981, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 30981, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.458421 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 31110, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 31110, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.468444 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 31170, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 31170, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.478290 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 31368, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 31368, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.488685 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 31698, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 31698, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.498703 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 31800, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 31800, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.508639 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 32778, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 32778, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.522900 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 32892, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 32892, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.534838 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 32937, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 32937, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.545445 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 33039, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 33039, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.555198 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 33102, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 33102, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.564951 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 33111, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 33111, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.574430 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 33132, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 33132, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.583832 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 33333, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 33333, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.593159 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 33336, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 33336, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.603699 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 33636, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 33636, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.613989 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 33642, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 33642, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.623435 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 33762, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 33762, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.633832 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 33873, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 33873, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.645426 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 33909, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 33909, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.655202 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 33984, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 33984, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.665403 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 34158, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 34158, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.675155 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 34533, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 34533, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.684836 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 35178, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 35178, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.694430 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 35379, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 35379, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.703948 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 35391, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 35391, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.713292 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 35412, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 35412, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.722595 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 35559, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 35559, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.731820 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 35565, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 35565, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.742031 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 35616, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 35616, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.751781 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 35811, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 35811, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.761484 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 36585, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 36585, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.771652 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 36780, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 36780, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.781195 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 36963, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 36963, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.790681 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 37524, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 37524, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.800154 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 37749, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 37749, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.809498 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 37920, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 37920, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.818817 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 37986, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 37986, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.828429 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 37998, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 37998, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.837885 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 38208, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 38208, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.847336 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 38250, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 38250, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.856644 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 38253, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 38253, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.865938 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 38301, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 38301, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.876175 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 38430, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 38430, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.885977 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 38682, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 38682, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.895506 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 38772, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 38772, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.904869 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 38958, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 38958, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.914369 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 39411, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 39411, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.923910 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 39684, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 39684, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.933629 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 39831, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 39831, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.943062 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 39897, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 39897, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.952478 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 39918, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 39918, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.961894 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 39921, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 39921, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.971326 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 40053, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 40053, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.980694 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 40095, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 40095, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:04.990130 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 40149, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 40149, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.000169 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 40275, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 40275, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.009687 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 40650, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 40650, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.019086 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 40686, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 40686, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.028465 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 40767, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 40767, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.037836 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 41334, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 41334, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.047221 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 41421, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 41421, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.056532 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 41532, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 41532, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.065868 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 41553, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 41553, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.075336 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 42129, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 42129, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.084691 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 42345, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 42345, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.094069 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 42774, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 42774, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.103444 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 43290, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 43290, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.112825 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 43299, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 43299, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.122156 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 43455, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 43455, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.132327 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 43542, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 43542, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.142137 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 43629, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 43629, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.152061 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 43770, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 43770, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.161542 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 43803, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 43803, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.170865 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 43809, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 43809, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.180183 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 43920, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 43920, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.189405 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 43950, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 43950, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.198655 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 43986, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 43986, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.207971 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 44064, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 44064, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.218106 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 44106, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 44106, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.227696 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 44193, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 44193, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.237159 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 44355, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 44355, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.246554 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 44373, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 44373, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.255988 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 44409, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 44409, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.265605 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 44598, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 44598, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.275192 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 44754, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 44754, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.284732 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 44793, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 44793, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.295359 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 45291, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 45291, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.305084 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 45354, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 45354, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.315268 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 45390, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 45390, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.327038 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 45423, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 45423, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.337969 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 45564, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 45564, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.348762 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 46104, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 46104, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.359526 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 46344, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 46344, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.369979 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 46404, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 46404, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.380690 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 46431, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 46431, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.391063 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 46722, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 46722, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.401223 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 46812, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 46812, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.411570 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 46848, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 46848, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.421248 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 46929, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 46929, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.431952 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 46959, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 46959, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.442428 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 47199, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 47199, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.452582 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 47625, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 47625, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.462831 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 47640, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 47640, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.472731 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 47715, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 47715, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.485873 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 47904, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 47904, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.499662 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 48150, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 48150, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.509043 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 48177, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 48177, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.518422 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 48282, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 48282, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.527710 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 48336, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 48336, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.537786 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 48465, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 48465, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.547221 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 48504, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 48504, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.557132 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 48636, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 48636, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.566719 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 48759, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 48759, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.576088 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 48978, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 48978, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.585478 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 49089, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 49089, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.594864 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 49614, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 49614, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.604241 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 49929, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 49929, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.613746 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 49986, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 49986, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.623088 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 50169, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 50169, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.632404 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 50184, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 50184, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.641713 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 50247, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 50247, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.651396 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 50400, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 50400, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.661037 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 50424, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 50424, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.670435 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 50604, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 50604, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.679853 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 50712, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 50712, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.689178 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 51051, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 51051, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.698583 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 51150, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 51150, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.707906 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 51171, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 51171, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.717219 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 51213, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 51213, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.726554 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 51357, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 51357, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.735856 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 51444, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 51444, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.745135 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 51459, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 51459, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.754714 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 51555, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 51555, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.764185 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 51558, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 51558, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.773687 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 51633, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 51633, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.783171 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 51639, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 51639, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.792569 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 51753, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 51753, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.801913 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 51912, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 51912, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.811223 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 52089, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 52089, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.820472 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 52191, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 52191, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.829813 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 52236, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 52236, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.839185 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 52518, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 52518, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.848454 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 52530, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 52530, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.857788 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 52980, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 52980, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.867318 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 53001, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 53001, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.876852 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 53010, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 53010, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.886304 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 53040, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 53040, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.895674 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 53190, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 53190, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.905086 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 53301, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 53301, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.914764 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 53778, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 53778, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.924449 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 53829, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 53829, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.933862 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 53901, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 53901, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.943200 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 54171, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 54171, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.952523 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 54249, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 54249, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.961831 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 54756, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 54756, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.971138 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 54873, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 54873, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.980412 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 54954, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 54954, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.989674 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 54999, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 54999, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:05.998958 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 55089, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 55089, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.008252 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 55194, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 55194, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.017523 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 55548, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 55548, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.026782 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 55572, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 55572, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.036107 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 55722, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 55722, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.045409 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 55821, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 55821, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.054668 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 55917, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 55917, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.063936 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 55974, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 55974, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.074502 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 56211, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 56211, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.083799 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 56316, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 56316, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.093113 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 56376, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 56376, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.102390 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 56559, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 56559, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.111672 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 56577, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 56577, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.120952 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 56886, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 56886, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.130266 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 56910, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 56910, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.139593 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 56925, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 56925, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.148880 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 57108, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 57108, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.158198 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 57147, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 57147, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.167500 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 57291, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 57291, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.176927 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 57471, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 57471, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.186996 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 57564, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 57564, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.196448 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 57576, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 57576, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.205764 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 57648, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 57648, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.215075 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 57708, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 57708, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.224357 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 58017, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 58017, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.233746 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 58056, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 58056, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.243129 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 58143, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 58143, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.252447 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 58221, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 58221, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.261782 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 58260, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 58260, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.271094 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 58677, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 58677, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.280435 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 59277, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 59277, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.289709 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 59442, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 59442, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.298983 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 59547, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 59547, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.308242 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 59736, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 59736, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.317477 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 59802, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 59802, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.326745 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 59841, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 59841, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.336042 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 59844, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 59844, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.345300 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 59994, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 59994, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.354597 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 60099, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 60099, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.363879 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 60102, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 60102, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.373247 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 60261, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 60261, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.382563 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 60366, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 60366, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.391863 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 60396, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 60396, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.401198 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 60483, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 60483, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.410474 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 60606, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 60606, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.420137 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 60630, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 60630, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.429683 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 60681, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 60681, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.439879 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 60720, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 60720, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.449467 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 60804, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 60804, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.458997 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 60849, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 60849, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.468389 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 60963, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 60963, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.477725 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 61149, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 61149, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.487063 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 61197, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 61197, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.496385 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 61251, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 61251, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.506345 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 61338, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 61338, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.516348 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 61608, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 61608, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.527354 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 61749, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 61749, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.538816 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 61776, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 61776, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.549132 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 61998, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 61998, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.559481 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 62004, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 62004, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.569748 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 62109, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 62109, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.579884 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 62199, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 62199, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.589915 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 62229, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 62229, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.599908 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 62421, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 62421, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.609913 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 62445, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 62445, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.619671 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 62481, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 62481, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.631828 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 62544, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 62544, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.641705 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 62910, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 62910, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.651603 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 62967, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 62967, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.661849 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 63618, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 63618, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.671728 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 63759, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 63759, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.683851 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 63786, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 63786, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.693815 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 63798, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 63798, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.707176 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 63855, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 63855, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.716536 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 64239, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 64239, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.725922 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 64314, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 64314, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.735324 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 64389, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 64389, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.744657 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 64866, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 64866, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.754053 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 65013, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 65013, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.763357 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 65382, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 65382, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.772704 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 65421, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 65421, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.782322 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 65778, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 65778, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.791702 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 65871, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 65871, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.801127 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 66123, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 66123, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.810527 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 66195, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 66195, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.819885 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 66234, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 66234, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.829630 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 66324, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 66324, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.839101 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 66501, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 66501, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.848571 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 67125, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 67125, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.857939 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 67434, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 67434, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.867507 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 67440, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 67440, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.876937 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 67491, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 67491, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.886316 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 67635, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 67635, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.895648 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 67641, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 67641, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.905008 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 67680, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 67680, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.914353 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 68058, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 68058, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.923664 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 68109, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 68109, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.932980 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 68373, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 68373, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.942389 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 68604, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 68604, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.951711 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 68985, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 68985, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.961018 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 69009, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 69009, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.970300 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 69075, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 69075, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.979595 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 69132, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 69132, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.988911 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 69147, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 69147, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:06.998249 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 69168, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 69168, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.007638 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 69384, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 69384, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.017057 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 69429, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 69429, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.026412 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 69450, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 69450, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.035774 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 69582, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 69582, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.045260 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 69993, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 69993, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.054663 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 70152, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 70152, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.064555 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 70311, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 70311, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.075080 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 70338, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 70338, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.084435 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 70395, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 70395, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.093779 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 70446, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 70446, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.103124 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 70503, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 70503, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.112428 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 70704, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 70704, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.121732 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 70998, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 70998, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.131055 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 71139, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 71139, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.140367 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 71175, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 71175, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.149662 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 71655, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 71655, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.158947 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 71886, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 71886, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.168262 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 71889, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 71889, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.177514 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 71895, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 71895, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.186788 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 72033, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 72033, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.196108 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 72054, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 72054, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.205476 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 73086, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 73086, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.214762 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 73167, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 73167, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.224050 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 73416, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 73416, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.233332 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 73707, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 73707, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.242603 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 73848, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 73848, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.252455 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 74232, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 74232, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.261922 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 74310, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 74310, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.271410 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 74502, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 74502, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.280782 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 74796, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 74796, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.290452 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 75063, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 75063, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.299896 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 75546, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 75546, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.309195 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 75975, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 75975, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.318590 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 75978, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 75978, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.327904 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 76713, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 76713, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.337172 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 76914, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 76914, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.346458 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 77040, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 77040, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.355762 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 77478, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 77478, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.366308 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 77481, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 77481, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.375909 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 77610, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 77610, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.387784 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 77745, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 77745, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.397153 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 78177, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 78177, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.406441 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 78426, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 78426, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.415682 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 78570, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 78570, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.425569 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 78801, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 78801, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.435128 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 78942, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 78942, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.444561 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 79038, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 79038, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.453964 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 79812, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 79812, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.463628 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 80262, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 80262, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.473112 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 80370, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 80370, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.482557 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 80526, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 80526, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.491912 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 80535, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 80535, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.501342 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 80781, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 80781, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.510741 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 81294, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 81294, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.520141 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 81633, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 81633, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.529503 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 81687, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 81687, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.538941 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 81873, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 81873, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.548372 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 82131, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 82131, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.557683 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 82152, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 82152, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.566954 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 82398, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 82398, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.576498 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 82482, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 82482, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.585886 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 82647, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 82647, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.595341 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 82944, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 82944, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.604807 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 83031, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 83031, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.614926 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 83259, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 83259, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.624543 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 83439, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 83439, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.634011 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 83523, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 83523, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.643357 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 83919, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 83919, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.652630 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 83922, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 83922, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.661875 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 84111, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 84111, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.671263 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 84552, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 84552, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.680652 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 84789, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 84789, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.689979 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 85515, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 85515, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.699328 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 85977, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 85977, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.708719 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 86346, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 86346, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.718506 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 86835, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 86835, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.729626 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 87768, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 87768, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.740365 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 87798, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 87798, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.750865 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 88182, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 88182, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.761297 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 88404, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 88404, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.771355 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 88719, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 88719, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.781827 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 88755, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 88755, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.792187 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 89019, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 89019, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.802411 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 89715, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 89715, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.812734 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 90303, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 90303, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.822216 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 90414, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 90414, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.834541 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 90771, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 90771, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.844906 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 90849, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 90849, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.855021 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 91281, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 91281, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.865135 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 91317, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 91317, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.875933 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 91515, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 91515, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.886304 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 91554, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 91554, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.896052 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 92397, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 92397, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.908924 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 92409, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 92409, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.918417 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 92916, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 92916, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.927799 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 93366, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 93366, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.937189 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 93528, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 93528, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.946560 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 93792, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 93792, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.955869 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 94272, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 94272, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.965246 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 94842, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 94842, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.974567 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 95037, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 95037, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.983856 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 95178, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 95178, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:07.993211 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 95928, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 95928, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.002555 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 96321, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 96321, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.011874 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 98148, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 98148, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.021115 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 98550, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 98550, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.030438 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 98760, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 98760, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.039751 test begin: paddle.Tensor.topk(Tensor([2281701379],"float32"), 99432, )

[torch error] paddle.Tensor.topk(Tensor([2281701379],"float32"), 99432, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.049060 test begin: paddle.Tensor.topk(Tensor([2281702, 1000],"float32"), 5, 1, True, True, )

[torch error] paddle.Tensor.topk(Tensor([2281702, 1000],"float32"), 5, 1, True, True, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.058455 test begin: paddle.Tensor.tril(Tensor([1, 1140850690, 2],"float32"), -1, )

[torch error] paddle.Tensor.tril(Tensor([1, 1140850690, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.067867 test begin: paddle.Tensor.tril(Tensor([1, 2, 1140850690],"float32"), -1, )

[torch error] paddle.Tensor.tril(Tensor([1, 2, 1140850690],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.077323 test begin: paddle.Tensor.tril(Tensor([1140850690, 2],"float32"), -1, )

[torch error] paddle.Tensor.tril(Tensor([1140850690, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.086655 test begin: paddle.Tensor.tril(Tensor([2, 1140850690],"float32"), -1, )

[torch error] paddle.Tensor.tril(Tensor([2, 1140850690],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.095972 test begin: paddle.Tensor.tril(Tensor([2, 2, 570425345],"float32"), -1, )

[torch error] paddle.Tensor.tril(Tensor([2, 2, 570425345],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.105316 test begin: paddle.Tensor.tril(Tensor([2, 570425345, 2],"float32"), -1, )

[torch error] paddle.Tensor.tril(Tensor([2, 570425345, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.114647 test begin: paddle.Tensor.tril(Tensor([570425345, 2, 2],"float32"), -1, )

[torch error] paddle.Tensor.tril(Tensor([570425345, 2, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.123918 test begin: paddle.Tensor.unbind(Tensor([1, 1188387, 30, 64],"float32"), 1, )

[torch error] paddle.Tensor.unbind(Tensor([1, 1188387, 30, 64],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.133237 test begin: paddle.Tensor.unbind(Tensor([1, 139265, 256, 32, 2],"float32"), axis=-1, )

[torch error] paddle.Tensor.unbind(Tensor([1, 139265, 256, 32, 2],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.142575 test begin: paddle.Tensor.unbind(Tensor([1, 16, 2228225, 32, 2],"float32"), axis=-1, )

[torch error] paddle.Tensor.unbind(Tensor([1, 16, 2228225, 32, 2],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.151859 test begin: paddle.Tensor.unbind(Tensor([1, 16, 256, 278529, 2],"float32"), axis=-1, )

[torch error] paddle.Tensor.unbind(Tensor([1, 16, 256, 278529, 2],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.161198 test begin: paddle.Tensor.unbind(Tensor([1, 16, 256, 32, 17409],"float32"), axis=-1, )

[torch error] paddle.Tensor.unbind(Tensor([1, 16, 256, 32, 17409],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.170581 test begin: paddle.Tensor.unbind(Tensor([1, 17825793, 2, 64],"float32"), 1, )

[torch error] paddle.Tensor.unbind(Tensor([1, 17825793, 2, 64],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.179866 test begin: paddle.Tensor.unbind(Tensor([1, 1782580, 20, 64],"float32"), 1, )

[torch error] paddle.Tensor.unbind(Tensor([1, 1782580, 20, 64],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.189148 test begin: paddle.Tensor.unbind(Tensor([1, 3565159, 10, 64],"float32"), 1, )

[torch error] paddle.Tensor.unbind(Tensor([1, 3565159, 10, 64],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.198457 test begin: paddle.Tensor.unbind(Tensor([1, 60, 10, 3802836],"float32"), 1, )

[torch error] paddle.Tensor.unbind(Tensor([1, 60, 10, 3802836],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.207700 test begin: paddle.Tensor.unbind(Tensor([1, 60, 2, 19014179],"float32"), 1, )

[torch error] paddle.Tensor.unbind(Tensor([1, 60, 2, 19014179],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.216980 test begin: paddle.Tensor.unbind(Tensor([1, 60, 20, 1901418],"float32"), 1, )

[torch error] paddle.Tensor.unbind(Tensor([1, 60, 20, 1901418],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.226267 test begin: paddle.Tensor.unbind(Tensor([1, 60, 30, 1267612],"float32"), 1, )

[torch error] paddle.Tensor.unbind(Tensor([1, 60, 30, 1267612],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.236312 test begin: paddle.Tensor.unbind(Tensor([1, 60, 4, 9507090],"float32"), 1, )

[torch error] paddle.Tensor.unbind(Tensor([1, 60, 4, 9507090],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.246081 test begin: paddle.Tensor.unbind(Tensor([1, 60, 594194, 64],"float32"), 1, )

[torch error] paddle.Tensor.unbind(Tensor([1, 60, 594194, 64],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.255392 test begin: paddle.Tensor.unbind(Tensor([1, 8912897, 4, 64],"float32"), 1, )

[torch error] paddle.Tensor.unbind(Tensor([1, 8912897, 4, 64],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.264698 test begin: paddle.Tensor.unbind(Tensor([11883862, 3, 8, 8],"float32"), axis=-3, )

[torch error] paddle.Tensor.unbind(Tensor([11883862, 3, 8, 8],"float32"), axis=-3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.274045 test begin: paddle.Tensor.unbind(Tensor([128, 3, 576, 32, 323],"float32"), axis=-1, )

[torch error] paddle.Tensor.unbind(Tensor([128, 3, 576, 32, 323],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.52 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.283433 test begin: paddle.Tensor.unbind(Tensor([128, 3, 576, 5158, 2],"float32"), axis=-1, )

[torch error] paddle.Tensor.unbind(Tensor([128, 3, 576, 5158, 2],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.293642 test begin: paddle.Tensor.unbind(Tensor([128, 3, 92843, 32, 2],"float32"), axis=-1, )

[torch error] paddle.Tensor.unbind(Tensor([128, 3, 92843, 32, 2],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.303330 test begin: paddle.Tensor.unbind(Tensor([128, 484, 576, 32, 2],"float32"), axis=-1, )

[torch error] paddle.Tensor.unbind(Tensor([128, 484, 576, 32, 2],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.312827 test begin: paddle.Tensor.unbind(Tensor([148549, 60, 4, 64],"float32"), 1, )

[torch error] paddle.Tensor.unbind(Tensor([148549, 60, 4, 64],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.322296 test begin: paddle.Tensor.unbind(Tensor([19807, 60, 30, 64],"float32"), 1, )

[torch error] paddle.Tensor.unbind(Tensor([19807, 60, 30, 64],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.331699 test begin: paddle.Tensor.unbind(Tensor([2, 131073, 256, 32, 2],"float16"), axis=-1, )

[torch error] paddle.Tensor.unbind(Tensor([2, 131073, 256, 32, 2],"float16"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.341082 test begin: paddle.Tensor.unbind(Tensor([2, 17825793, 8, 8],"float32"), axis=-3, )

[torch error] paddle.Tensor.unbind(Tensor([2, 17825793, 8, 8],"float32"), axis=-3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.350447 test begin: paddle.Tensor.unbind(Tensor([2, 3, 11184811, 32, 2],"float16"), axis=-1, )

[torch error] paddle.Tensor.unbind(Tensor([2, 3, 11184811, 32, 2],"float16"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.359788 test begin: paddle.Tensor.unbind(Tensor([2, 3, 256, 1398102, 2],"float16"), axis=-1, )

[torch error] paddle.Tensor.unbind(Tensor([2, 3, 256, 1398102, 2],"float16"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.369187 test begin: paddle.Tensor.unbind(Tensor([2, 3, 256, 32, 46422],"float32"), axis=-1, )

[torch error] paddle.Tensor.unbind(Tensor([2, 3, 256, 32, 46422],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.378502 test begin: paddle.Tensor.unbind(Tensor([2, 3, 256, 32, 87382],"float16"), axis=-1, )

[torch error] paddle.Tensor.unbind(Tensor([2, 3, 256, 32, 87382],"float16"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.387840 test begin: paddle.Tensor.unbind(Tensor([2, 3, 256, 742742, 2],"float32"), axis=-1, )

[torch error] paddle.Tensor.unbind(Tensor([2, 3, 256, 742742, 2],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.397173 test begin: paddle.Tensor.unbind(Tensor([2, 3, 47535446, 8],"float32"), axis=-3, )

[torch error] paddle.Tensor.unbind(Tensor([2, 3, 47535446, 8],"float32"), axis=-3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.406505 test begin: paddle.Tensor.unbind(Tensor([2, 3, 5941931, 32, 2],"float32"), axis=-1, )

[torch error] paddle.Tensor.unbind(Tensor([2, 3, 5941931, 32, 2],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.415882 test begin: paddle.Tensor.unbind(Tensor([2, 3, 8, 47535446],"float32"), axis=-3, )

[torch error] paddle.Tensor.unbind(Tensor([2, 3, 8, 47535446],"float32"), axis=-3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.425193 test begin: paddle.Tensor.unbind(Tensor([2, 69633, 256, 32, 2],"float32"), axis=-1, )

[torch error] paddle.Tensor.unbind(Tensor([2, 69633, 256, 32, 2],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.434570 test begin: paddle.Tensor.unbind(Tensor([20632, 3, 576, 32, 2],"float32"), axis=-1, )

[torch error] paddle.Tensor.unbind(Tensor([20632, 3, 576, 32, 2],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.443921 test begin: paddle.Tensor.unbind(Tensor([211, 864, 196, 64],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([211, 864, 196, 64],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.52 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.453750 test begin: paddle.Tensor.unbind(Tensor([258, 60, 2304, 64],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([258, 60, 2304, 64],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.463134 test begin: paddle.Tensor.unbind(Tensor([2910334, 28, 28],"float32"), axis=-3, )

[torch error] paddle.Tensor.unbind(Tensor([2910334, 28, 28],"float32"), axis=-3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.472523 test begin: paddle.Tensor.unbind(Tensor([297097, 60, 2, 64],"float32"), 1, )

[torch error] paddle.Tensor.unbind(Tensor([297097, 60, 2, 64],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.481883 test begin: paddle.Tensor.unbind(Tensor([29710, 60, 20, 64],"float32"), 1, )

[torch error] paddle.Tensor.unbind(Tensor([29710, 60, 20, 64],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.491223 test begin: paddle.Tensor.unbind(Tensor([3, 1, 140638, 169, 32],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([3, 1, 140638, 169, 32],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.500558 test begin: paddle.Tensor.unbind(Tensor([3, 1, 151387, 157, 32],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([3, 1, 151387, 157, 32],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.509895 test begin: paddle.Tensor.unbind(Tensor([3, 1, 163916, 145, 32],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([3, 1, 163916, 145, 32],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.519227 test begin: paddle.Tensor.unbind(Tensor([3, 1, 210334, 113, 32],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([3, 1, 210334, 113, 32],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.528572 test begin: paddle.Tensor.unbind(Tensor([3, 1, 218053, 109, 32],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([3, 1, 218053, 109, 32],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.537889 test begin: paddle.Tensor.unbind(Tensor([3, 1, 8, 109, 872211],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([3, 1, 8, 109, 872211],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.547928 test begin: paddle.Tensor.unbind(Tensor([3, 1, 8, 113, 841336],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([3, 1, 8, 113, 841336],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.557476 test begin: paddle.Tensor.unbind(Tensor([3, 1, 8, 145, 655662],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([3, 1, 8, 145, 655662],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.566914 test begin: paddle.Tensor.unbind(Tensor([3, 1, 8, 157, 605548],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([3, 1, 8, 157, 605548],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.576309 test begin: paddle.Tensor.unbind(Tensor([3, 1, 8, 169, 562550],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([3, 1, 8, 169, 562550],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.585649 test begin: paddle.Tensor.unbind(Tensor([3, 1, 8, 2970966, 32],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([3, 1, 8, 2970966, 32],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.594992 test begin: paddle.Tensor.unbind(Tensor([3, 11606, 1024, 64],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([3, 11606, 1024, 64],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.604322 test begin: paddle.Tensor.unbind(Tensor([3, 17580, 8, 169, 32],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([3, 17580, 8, 169, 32],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.613929 test begin: paddle.Tensor.unbind(Tensor([3, 18924, 8, 157, 32],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([3, 18924, 8, 157, 32],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.623506 test begin: paddle.Tensor.unbind(Tensor([3, 2, 380283564],"float32"), -2, )

[torch error] paddle.Tensor.unbind(Tensor([3, 2, 380283564],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.632888 test begin: paddle.Tensor.unbind(Tensor([3, 20490, 8, 145, 32],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([3, 20490, 8, 145, 32],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.642471 test begin: paddle.Tensor.unbind(Tensor([3, 253522376, 3],"float32"), -2, )

[torch error] paddle.Tensor.unbind(Tensor([3, 253522376, 3],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.652578 test begin: paddle.Tensor.unbind(Tensor([3, 26292, 8, 113, 32],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([3, 26292, 8, 113, 32],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.662127 test begin: paddle.Tensor.unbind(Tensor([3, 27163112, 28],"float32"), axis=-3, )

[torch error] paddle.Tensor.unbind(Tensor([3, 27163112, 28],"float32"), axis=-3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.671541 test begin: paddle.Tensor.unbind(Tensor([3, 27257, 8, 109, 32],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([3, 27257, 8, 109, 32],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.680923 test begin: paddle.Tensor.unbind(Tensor([3, 28, 27163112],"float32"), axis=-3, )

[torch error] paddle.Tensor.unbind(Tensor([3, 28, 27163112],"float32"), axis=-3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.690278 test begin: paddle.Tensor.unbind(Tensor([3, 432, 196, 8983],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([3, 432, 196, 8983],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.700166 test begin: paddle.Tensor.unbind(Tensor([3, 432, 27509, 64],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([3, 432, 27509, 64],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.709675 test begin: paddle.Tensor.unbind(Tensor([3, 48, 1024, 15474],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([3, 48, 1024, 15474],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.719054 test begin: paddle.Tensor.unbind(Tensor([3, 48, 247581, 64],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([3, 48, 247581, 64],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.728396 test begin: paddle.Tensor.unbind(Tensor([3, 5158, 2304, 64],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([3, 5158, 2304, 64],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.737900 test begin: paddle.Tensor.unbind(Tensor([3, 60, 198065, 64],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([3, 60, 198065, 64],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.747242 test begin: paddle.Tensor.unbind(Tensor([3, 60, 2304, 5502],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([3, 60, 2304, 5502],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.756555 test begin: paddle.Tensor.unbind(Tensor([3, 60632, 196, 64],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([3, 60632, 196, 64],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.765857 test begin: paddle.Tensor.unbind(Tensor([3, 8, 95070891],"float32"), axis=-3, )

[torch error] paddle.Tensor.unbind(Tensor([3, 8, 95070891],"float32"), axis=-3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.775147 test begin: paddle.Tensor.unbind(Tensor([3, 864, 13755, 64],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([3, 864, 13755, 64],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.785038 test begin: paddle.Tensor.unbind(Tensor([3, 864, 196, 4492],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([3, 864, 196, 4492],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.794473 test begin: paddle.Tensor.unbind(Tensor([3, 95070891, 8],"float32"), axis=-3, )

[torch error] paddle.Tensor.unbind(Tensor([3, 95070891, 8],"float32"), axis=-3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.803987 test begin: paddle.Tensor.unbind(Tensor([3, 96, 1024, 7737],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([3, 96, 1024, 7737],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.813358 test begin: paddle.Tensor.unbind(Tensor([3, 96, 123791, 64],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([3, 96, 123791, 64],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.822713 test begin: paddle.Tensor.unbind(Tensor([35651585, 8, 8],"float32"), axis=-3, )

[torch error] paddle.Tensor.unbind(Tensor([35651585, 8, 8],"float32"), axis=-3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.832069 test begin: paddle.Tensor.unbind(Tensor([363, 96, 1024, 64],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([363, 96, 1024, 64],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.841418 test begin: paddle.Tensor.unbind(Tensor([380283564, 2, 3],"float32"), -2, )

[torch error] paddle.Tensor.unbind(Tensor([380283564, 2, 3],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.850968 test begin: paddle.Tensor.unbind(Tensor([422, 432, 196, 64],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([422, 432, 196, 64],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.52 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.860297 test begin: paddle.Tensor.unbind(Tensor([46422, 3, 256, 32, 2],"float32"), axis=-1, )

[torch error] paddle.Tensor.unbind(Tensor([46422, 3, 256, 32, 2],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.869683 test begin: paddle.Tensor.unbind(Tensor([52740, 1, 8, 169, 32],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([52740, 1, 8, 169, 32],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.879206 test begin: paddle.Tensor.unbind(Tensor([56771, 1, 8, 157, 32],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([56771, 1, 8, 157, 32],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.888700 test begin: paddle.Tensor.unbind(Tensor([59420, 60, 10, 64],"float32"), 1, )

[torch error] paddle.Tensor.unbind(Tensor([59420, 60, 10, 64],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.898153 test begin: paddle.Tensor.unbind(Tensor([61469, 1, 8, 145, 32],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([61469, 1, 8, 145, 32],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.907506 test begin: paddle.Tensor.unbind(Tensor([726, 48, 1024, 64],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([726, 48, 1024, 64],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.917458 test begin: paddle.Tensor.unbind(Tensor([78876, 1, 8, 113, 32],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([78876, 1, 8, 113, 32],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.928719 test begin: paddle.Tensor.unbind(Tensor([80, 3, 148549, 32, 2],"float32"), axis=-1, )

[torch error] paddle.Tensor.unbind(Tensor([80, 3, 148549, 32, 2],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.940109 test begin: paddle.Tensor.unbind(Tensor([80, 3, 576, 32, 516],"float32"), axis=-1, )

[torch error] paddle.Tensor.unbind(Tensor([80, 3, 576, 32, 516],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.950342 test begin: paddle.Tensor.unbind(Tensor([80, 3, 576, 8253, 2],"float32"), axis=-1, )

[torch error] paddle.Tensor.unbind(Tensor([80, 3, 576, 8253, 2],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.960677 test begin: paddle.Tensor.unbind(Tensor([80, 774, 576, 32, 2],"float32"), axis=-1, )

[torch error] paddle.Tensor.unbind(Tensor([80, 774, 576, 32, 2],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.970748 test begin: paddle.Tensor.unbind(Tensor([81770, 1, 8, 109, 32],"float32"), 0, )

[torch error] paddle.Tensor.unbind(Tensor([81770, 1, 8, 109, 32],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.980861 test begin: paddle.Tensor.unbind(Tensor([8705, 16, 256, 32, 2],"float32"), axis=-1, )

[torch error] paddle.Tensor.unbind(Tensor([8705, 16, 256, 32, 2],"float32"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:08.991306 test begin: paddle.Tensor.unbind(Tensor([87382, 3, 256, 32, 2],"float16"), axis=-1, )

[torch error] paddle.Tensor.unbind(Tensor([87382, 3, 256, 32, 2],"float16"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.001624 test begin: paddle.Tensor.unique(Tensor([2281701379],"int64"), )

[torch error] paddle.Tensor.unique(Tensor([2281701379],"int64"), ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.012145 test begin: paddle.Tensor.unsqueeze(Tensor([1, 1, 144, 15845149],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 1, 144, 15845149],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.021909 test begin: paddle.Tensor.unsqueeze(Tensor([1, 1, 15845149, 144],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 1, 15845149, 144],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.034264 test begin: paddle.Tensor.unsqueeze(Tensor([1, 1, 2, 1140850690],"float32"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 1, 2, 1140850690],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.044370 test begin: paddle.Tensor.unsqueeze(Tensor([1, 1, 2281701379],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 1, 2281701379],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.054299 test begin: paddle.Tensor.unsqueeze(Tensor([1, 1, 2281701379],"float32"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 1, 2281701379],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.064293 test begin: paddle.Tensor.unsqueeze(Tensor([1, 1, 2281701379],"int32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 1, 2281701379],"int32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.075312 test begin: paddle.Tensor.unsqueeze(Tensor([1, 1, 2281701379],"int64"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 1, 2281701379],"int64"), -1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.085286 test begin: paddle.Tensor.unsqueeze(Tensor([1, 1, 2281701379],"int64"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 1, 2281701379],"int64"), 2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.094897 test begin: paddle.Tensor.unsqueeze(Tensor([1, 1, 285212673, 8],"float32"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 1, 285212673, 8],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.107748 test begin: paddle.Tensor.unsqueeze(Tensor([1, 1, 4294967297],"float16"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 1, 4294967297],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.117078 test begin: paddle.Tensor.unsqueeze(Tensor([1, 1, 46565335, 49],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 1, 46565335, 49],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.126392 test begin: paddle.Tensor.unsqueeze(Tensor([1, 1, 49, 46565335],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 1, 49, 46565335],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.135687 test begin: paddle.Tensor.unsqueeze(Tensor([1, 1, 49, 87652394],"float16"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 1, 49, 87652394],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.145041 test begin: paddle.Tensor.unsqueeze(Tensor([1, 1, 87652394, 49],"float16"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 1, 87652394, 49],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.154352 test begin: paddle.Tensor.unsqueeze(Tensor([1, 10, 228170138],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 10, 228170138],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.163711 test begin: paddle.Tensor.unsqueeze(Tensor([1, 100, 124006, 184],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 100, 124006, 184],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.173172 test begin: paddle.Tensor.unsqueeze(Tensor([1, 100, 129643, 176],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 100, 129643, 176],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.182474 test begin: paddle.Tensor.unsqueeze(Tensor([1, 100, 176, 129643],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 100, 176, 129643],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.191811 test begin: paddle.Tensor.unsqueeze(Tensor([1, 100, 184, 124006],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 100, 184, 124006],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.201167 test begin: paddle.Tensor.unsqueeze(Tensor([1, 100, 22817014],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 100, 22817014],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.210447 test begin: paddle.Tensor.unsqueeze(Tensor([1, 1024, 2, 2097153],"float16"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 1024, 2, 2097153],"float16"), -2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.219727 test begin: paddle.Tensor.unsqueeze(Tensor([1, 1024, 262145, 16],"float16"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 1024, 262145, 16],"float16"), -2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.228988 test begin: paddle.Tensor.unsqueeze(Tensor([1, 1100, 2074274],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 1100, 2074274],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.238250 test begin: paddle.Tensor.unsqueeze(Tensor([1, 110036, 144, 144],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 110036, 144, 144],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.247496 test begin: paddle.Tensor.unsqueeze(Tensor([1, 1114113, 2048],"int32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 1114113, 2048],"int32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.256755 test begin: paddle.Tensor.unsqueeze(Tensor([1, 114085069, 20],"float32"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 114085069, 20],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.266003 test begin: paddle.Tensor.unsqueeze(Tensor([1, 114085069, 20],"int64"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 114085069, 20],"int64"), -1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.275347 test begin: paddle.Tensor.unsqueeze(Tensor([1, 114085069, 20],"int64"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 114085069, 20],"int64"), 2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.284702 test begin: paddle.Tensor.unsqueeze(Tensor([1, 1140850690, 2],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 1140850690, 2],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.294059 test begin: paddle.Tensor.unsqueeze(Tensor([1, 1140850690, 2],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 1140850690, 2],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.303353 test begin: paddle.Tensor.unsqueeze(Tensor([1, 11408507, 200],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 11408507, 200],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.312627 test begin: paddle.Tensor.unsqueeze(Tensor([1, 12, 190141782],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 12, 190141782],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.321936 test begin: paddle.Tensor.unsqueeze(Tensor([1, 12964213, 176],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 12964213, 176],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.331269 test begin: paddle.Tensor.unsqueeze(Tensor([1, 134217729, 2, 16],"float16"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 134217729, 2, 16],"float16"), -2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.340561 test begin: paddle.Tensor.unsqueeze(Tensor([1, 142606337, 2, 8],"float32"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 142606337, 2, 8],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.349837 test begin: paddle.Tensor.unsqueeze(Tensor([1, 144, 15845149],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 144, 15845149],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.359173 test begin: paddle.Tensor.unsqueeze(Tensor([1, 176, 12964213],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 176, 12964213],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.368570 test begin: paddle.Tensor.unsqueeze(Tensor([1, 1788825, 49, 49],"float16"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 1788825, 49, 49],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.377860 test begin: paddle.Tensor.unsqueeze(Tensor([1, 18, 126761188],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 18, 126761188],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.387196 test begin: paddle.Tensor.unsqueeze(Tensor([1, 2, 142606337, 8],"float32"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 2, 142606337, 8],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.396532 test begin: paddle.Tensor.unsqueeze(Tensor([1, 2, 2, 570425345],"float32"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 2, 2, 570425345],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.406010 test begin: paddle.Tensor.unsqueeze(Tensor([1, 2281701379, 1],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 2281701379, 1],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.415450 test begin: paddle.Tensor.unsqueeze(Tensor([1, 2281701379],"bool"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 2281701379],"bool"), -1, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.424945 test begin: paddle.Tensor.unsqueeze(Tensor([1, 2281701379],"bool"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 2281701379],"bool"), 2, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.434420 test begin: paddle.Tensor.unsqueeze(Tensor([1, 2281701379],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 2281701379],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.443739 test begin: paddle.Tensor.unsqueeze(Tensor([1, 2281701379],"float32"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 2281701379],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.453035 test begin: paddle.Tensor.unsqueeze(Tensor([1, 2281701379],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 2281701379],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.462334 test begin: paddle.Tensor.unsqueeze(Tensor([1, 2281701379],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 2281701379],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.471663 test begin: paddle.Tensor.unsqueeze(Tensor([1, 2281701379],"float32"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 2281701379],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.481740 test begin: paddle.Tensor.unsqueeze(Tensor([1, 2281701379],"int32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 2281701379],"int32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.491270 test begin: paddle.Tensor.unsqueeze(Tensor([1, 2281701379],"int32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 2281701379],"int32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.500695 test begin: paddle.Tensor.unsqueeze(Tensor([1, 2281701379],"int64"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 2281701379],"int64"), 1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.510042 test begin: paddle.Tensor.unsqueeze(Tensor([1, 2281701379],"int64"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 2281701379],"int64"), 2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.519349 test begin: paddle.Tensor.unsqueeze(Tensor([1, 253522376, 9],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 253522376, 9],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.528693 test begin: paddle.Tensor.unsqueeze(Tensor([1, 285212673, 8],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 285212673, 8],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.538021 test begin: paddle.Tensor.unsqueeze(Tensor([1, 3, 2, 380283564],"float32"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 3, 2, 380283564],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.547637 test begin: paddle.Tensor.unsqueeze(Tensor([1, 3, 95070891, 8],"float32"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 3, 95070891, 8],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.557098 test begin: paddle.Tensor.unsqueeze(Tensor([1, 4294967297],"float16"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 4294967297],"float16"), -1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.566615 test begin: paddle.Tensor.unsqueeze(Tensor([1, 456340276, 5],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 456340276, 5],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.576043 test begin: paddle.Tensor.unsqueeze(Tensor([1, 536870913, 8],"float16"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 536870913, 8],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.585410 test begin: paddle.Tensor.unsqueeze(Tensor([1, 570425345, 4],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 570425345, 4],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.594942 test begin: paddle.Tensor.unsqueeze(Tensor([1, 570425345, 4],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 570425345, 4],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.604430 test begin: paddle.Tensor.unsqueeze(Tensor([1, 570425345, 4],"float32"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 570425345, 4],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.613860 test begin: paddle.Tensor.unsqueeze(Tensor([1, 67395, 184, 184],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 67395, 184, 184],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.623567 test begin: paddle.Tensor.unsqueeze(Tensor([1, 71303169, 2, 16],"float32"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 71303169, 2, 16],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.632995 test begin: paddle.Tensor.unsqueeze(Tensor([1, 71303169, 32],"float32"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 71303169, 32],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.642353 test begin: paddle.Tensor.unsqueeze(Tensor([1, 73661, 176, 176],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 73661, 176, 176],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.651872 test begin: paddle.Tensor.unsqueeze(Tensor([1, 760567127, 3],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 760567127, 3],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.661408 test begin: paddle.Tensor.unsqueeze(Tensor([1, 8, 17825793, 16],"float32"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 8, 17825793, 16],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.670773 test begin: paddle.Tensor.unsqueeze(Tensor([1, 8, 2, 142606337],"float32"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 8, 2, 142606337],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.680130 test begin: paddle.Tensor.unsqueeze(Tensor([1, 950313, 49, 49],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1, 950313, 49, 49],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.689672 test begin: paddle.Tensor.unsqueeze(Tensor([10, 1, 228170138],"bool"), 3, )

[torch error] paddle.Tensor.unsqueeze(Tensor([10, 1, 228170138],"bool"), 3, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.699135 test begin: paddle.Tensor.unsqueeze(Tensor([10, 228170138],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([10, 228170138],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.708444 test begin: paddle.Tensor.unsqueeze(Tensor([10, 457255, 499],"bool"), 3, )

[torch error] paddle.Tensor.unsqueeze(Tensor([10, 457255, 499],"bool"), 3, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.717812 test begin: paddle.Tensor.unsqueeze(Tensor([106106, 21504],"bool"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([106106, 21504],"bool"), -1, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.727288 test begin: paddle.Tensor.unsqueeze(Tensor([106106, 21504],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([106106, 21504],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.736603 test begin: paddle.Tensor.unsqueeze(Tensor([106106, 21504],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([106106, 21504],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.745925 test begin: paddle.Tensor.unsqueeze(Tensor([106106, 21504],"int32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([106106, 21504],"int32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.755505 test begin: paddle.Tensor.unsqueeze(Tensor([110036, 1, 144, 144],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([110036, 1, 144, 144],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.764919 test begin: paddle.Tensor.unsqueeze(Tensor([1114113, 1, 2048],"int32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1114113, 1, 2048],"int32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.774352 test begin: paddle.Tensor.unsqueeze(Tensor([1114113, 2048],"int64"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1114113, 2048],"int64"), 2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.783686 test begin: paddle.Tensor.unsqueeze(Tensor([111412, 512, 1, 40],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([111412, 512, 1, 40],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.793008 test begin: paddle.Tensor.unsqueeze(Tensor([114085069, 1, 20],"float32"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([114085069, 1, 20],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.802672 test begin: paddle.Tensor.unsqueeze(Tensor([114085069, 1, 20],"int64"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([114085069, 1, 20],"int64"), -1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.812117 test begin: paddle.Tensor.unsqueeze(Tensor([114085069, 1, 20],"int64"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([114085069, 1, 20],"int64"), 2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.821449 test begin: paddle.Tensor.unsqueeze(Tensor([114085069, 10, 2],"float32"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([114085069, 10, 2],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.830755 test begin: paddle.Tensor.unsqueeze(Tensor([114085069, 20],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([114085069, 20],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.840118 test begin: paddle.Tensor.unsqueeze(Tensor([114085069, 20],"int64"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([114085069, 20],"int64"), 1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.849397 test begin: paddle.Tensor.unsqueeze(Tensor([114085069, 20],"int64"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([114085069, 20],"int64"), 2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.858945 test begin: paddle.Tensor.unsqueeze(Tensor([1140850690, 1, 2],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1140850690, 1, 2],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.868429 test begin: paddle.Tensor.unsqueeze(Tensor([11408507, 100, 2],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([11408507, 100, 2],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.877813 test begin: paddle.Tensor.unsqueeze(Tensor([12, 118839, 40, 40],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([12, 118839, 40, 40],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.887119 test begin: paddle.Tensor.unsqueeze(Tensor([12, 1901418, 10, 10],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([12, 1901418, 10, 10],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.896428 test begin: paddle.Tensor.unsqueeze(Tensor([12, 3, 10, 6338060],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([12, 3, 10, 6338060],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.905741 test begin: paddle.Tensor.unsqueeze(Tensor([12, 3, 1584515, 40],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([12, 3, 1584515, 40],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.915013 test begin: paddle.Tensor.unsqueeze(Tensor([12, 3, 20, 3169030],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([12, 3, 20, 3169030],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.924241 test begin: paddle.Tensor.unsqueeze(Tensor([12, 3, 3169030, 20],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([12, 3, 3169030, 20],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.933464 test begin: paddle.Tensor.unsqueeze(Tensor([12, 3, 40, 1584515],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([12, 3, 40, 1584515],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.942800 test begin: paddle.Tensor.unsqueeze(Tensor([12, 3, 6338060, 10],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([12, 3, 6338060, 10],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.952173 test begin: paddle.Tensor.unsqueeze(Tensor([12, 475355, 20, 20],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([12, 475355, 20, 20],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.961730 test begin: paddle.Tensor.unsqueeze(Tensor([1220, 64, 94, 311],"float32"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1220, 64, 94, 311],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.971071 test begin: paddle.Tensor.unsqueeze(Tensor([13, 1, 175515491],"float32"), 3, )

[torch error] paddle.Tensor.unsqueeze(Tensor([13, 1, 175515491],"float32"), 3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.980381 test begin: paddle.Tensor.unsqueeze(Tensor([13, 175515491],"int64"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([13, 175515491],"int64"), -2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.989710 test begin: paddle.Tensor.unsqueeze(Tensor([13, 2, 16, 5484860],"int64"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([13, 2, 16, 5484860],"int64"), -1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:09.999004 test begin: paddle.Tensor.unsqueeze(Tensor([13, 2, 21939437, 4],"int64"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([13, 2, 21939437, 4],"int64"), -1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.008347 test begin: paddle.Tensor.unsqueeze(Tensor([13, 2, 4, 21939437],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([13, 2, 4, 21939437],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.017696 test begin: paddle.Tensor.unsqueeze(Tensor([13, 2, 5484860, 16],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([13, 2, 5484860, 16],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.027043 test begin: paddle.Tensor.unsqueeze(Tensor([13, 2, 87757746],"int64"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([13, 2, 87757746],"int64"), -2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.036425 test begin: paddle.Tensor.unsqueeze(Tensor([13, 21939437, 8],"float32"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([13, 21939437, 8],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.045810 test begin: paddle.Tensor.unsqueeze(Tensor([13, 25073642, 7],"float32"), 3, )

[torch error] paddle.Tensor.unsqueeze(Tensor([13, 25073642, 7],"float32"), 3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.055162 test begin: paddle.Tensor.unsqueeze(Tensor([13, 2742430, 16, 4],"int64"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([13, 2742430, 16, 4],"int64"), -1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.064490 test begin: paddle.Tensor.unsqueeze(Tensor([13, 2742430, 4, 16],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([13, 2742430, 4, 16],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.074799 test begin: paddle.Tensor.unsqueeze(Tensor([13, 2742430, 64],"int64"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([13, 2742430, 64],"int64"), -2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.084265 test begin: paddle.Tensor.unsqueeze(Tensor([13, 43878873, 4],"float32"), 3, )

[torch error] paddle.Tensor.unsqueeze(Tensor([13, 43878873, 4],"float32"), 3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.093738 test begin: paddle.Tensor.unsqueeze(Tensor([13, 7, 25073642],"float32"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([13, 7, 25073642],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.103236 test begin: paddle.Tensor.unsqueeze(Tensor([131073, 1024, 2, 16],"float16"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([131073, 1024, 2, 16],"float16"), -2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.112724 test begin: paddle.Tensor.unsqueeze(Tensor([142606337, 1, 2, 8],"float32"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([142606337, 1, 2, 8],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.123777 test begin: paddle.Tensor.unsqueeze(Tensor([15845149, 144],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([15845149, 144],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.134087 test begin: paddle.Tensor.unsqueeze(Tensor([15845149, 144],"float32"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([15845149, 144],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.143723 test begin: paddle.Tensor.unsqueeze(Tensor([15845149, 144],"int64"), axis=1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([15845149, 144],"int64"), axis=1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.153559 test begin: paddle.Tensor.unsqueeze(Tensor([15845149, 144],"int64"), axis=2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([15845149, 144],"int64"), axis=2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.163916 test begin: paddle.Tensor.unsqueeze(Tensor([16, 1, 3, 64, 742742],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([16, 1, 3, 64, 742742],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.173945 test begin: paddle.Tensor.unsqueeze(Tensor([16, 1, 3, 742742, 64],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([16, 1, 3, 742742, 64],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.184018 test begin: paddle.Tensor.unsqueeze(Tensor([16, 1, 34817, 64, 64],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([16, 1, 34817, 64, 64],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.194489 test begin: paddle.Tensor.unsqueeze(Tensor([16, 10, 1, 2, 7130317],"float32"), -3, )

[torch error] paddle.Tensor.unsqueeze(Tensor([16, 10, 1, 2, 7130317],"float32"), -3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.204527 test begin: paddle.Tensor.unsqueeze(Tensor([16, 10, 1, 7130317, 2],"float32"), -3, )

[torch error] paddle.Tensor.unsqueeze(Tensor([16, 10, 1, 7130317, 2],"float32"), -3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.214529 test begin: paddle.Tensor.unsqueeze(Tensor([16, 10, 111412, 64, 2],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([16, 10, 111412, 64, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.224057 test begin: paddle.Tensor.unsqueeze(Tensor([16, 10, 14260634],"float32"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([16, 10, 14260634],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.236229 test begin: paddle.Tensor.unsqueeze(Tensor([16, 10, 2, 7130317],"float32"), -3, )

[torch error] paddle.Tensor.unsqueeze(Tensor([16, 10, 2, 7130317],"float32"), -3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.246120 test begin: paddle.Tensor.unsqueeze(Tensor([16, 10, 245873, 58],"float32"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([16, 10, 245873, 58],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.255994 test begin: paddle.Tensor.unsqueeze(Tensor([16, 10, 3565159, 2, 2],"float32"), -3, )

[torch error] paddle.Tensor.unsqueeze(Tensor([16, 10, 3565159, 2, 2],"float32"), -3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.265875 test begin: paddle.Tensor.unsqueeze(Tensor([16, 10, 58, 245873],"float32"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([16, 10, 58, 245873],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.276391 test begin: paddle.Tensor.unsqueeze(Tensor([16, 10, 64, 111412, 2],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([16, 10, 64, 111412, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.286659 test begin: paddle.Tensor.unsqueeze(Tensor([16, 10, 64, 64, 3482],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([16, 10, 64, 64, 3482],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.296292 test begin: paddle.Tensor.unsqueeze(Tensor([16, 10, 7130317, 2],"float32"), -3, )

[torch error] paddle.Tensor.unsqueeze(Tensor([16, 10, 7130317, 2],"float32"), -3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.309138 test begin: paddle.Tensor.unsqueeze(Tensor([16, 11, 202566, 64],"float32"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([16, 11, 202566, 64],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.318679 test begin: paddle.Tensor.unsqueeze(Tensor([16, 11, 64, 202566],"float32"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([16, 11, 64, 202566],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.328094 test begin: paddle.Tensor.unsqueeze(Tensor([16, 11606, 3, 64, 64],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([16, 11606, 3, 64, 64],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.337628 test begin: paddle.Tensor.unsqueeze(Tensor([16, 17409, 64, 64, 2],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([16, 17409, 64, 64, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.347499 test begin: paddle.Tensor.unsqueeze(Tensor([16, 3, 64, 742742],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([16, 3, 64, 742742],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.356853 test begin: paddle.Tensor.unsqueeze(Tensor([16, 3, 742742, 64],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([16, 3, 742742, 64],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.366204 test begin: paddle.Tensor.unsqueeze(Tensor([16, 34817, 64, 64],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([16, 34817, 64, 64],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.375495 test begin: paddle.Tensor.unsqueeze(Tensor([16, 34817, 64, 64],"float32"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([16, 34817, 64, 64],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.384848 test begin: paddle.Tensor.unsqueeze(Tensor([16, 35651585, 1, 2, 2],"float32"), -3, )

[torch error] paddle.Tensor.unsqueeze(Tensor([16, 35651585, 1, 2, 2],"float32"), -3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.394280 test begin: paddle.Tensor.unsqueeze(Tensor([16, 35651585, 2, 2],"float32"), -3, )

[torch error] paddle.Tensor.unsqueeze(Tensor([16, 35651585, 2, 2],"float32"), -3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.403747 test begin: paddle.Tensor.unsqueeze(Tensor([16, 42392, 58, 58],"float32"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([16, 42392, 58, 58],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.413202 test begin: paddle.Tensor.unsqueeze(Tensor([16, 71303169, 2],"float32"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([16, 71303169, 2],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.422663 test begin: paddle.Tensor.unsqueeze(Tensor([17825793, 2, 16, 4],"int64"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([17825793, 2, 16, 4],"int64"), -1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.432048 test begin: paddle.Tensor.unsqueeze(Tensor([17825793, 2, 4, 16],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([17825793, 2, 4, 16],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.441396 test begin: paddle.Tensor.unsqueeze(Tensor([17825793, 2, 64],"int64"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([17825793, 2, 64],"int64"), -2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.450765 test begin: paddle.Tensor.unsqueeze(Tensor([1788825, 1, 49, 49],"float16"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1788825, 1, 49, 49],"float16"), 0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.460159 test begin: paddle.Tensor.unsqueeze(Tensor([185686, 1, 3, 64, 64],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([185686, 1, 3, 64, 64],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.469508 test begin: paddle.Tensor.unsqueeze(Tensor([185686, 3, 64, 64],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([185686, 3, 64, 64],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.478808 test begin: paddle.Tensor.unsqueeze(Tensor([190141782, 1, 1, 3, 4],"float32"), axis=1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([190141782, 1, 1, 3, 4],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.488136 test begin: paddle.Tensor.unsqueeze(Tensor([1901418, 3, 20, 20],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([1901418, 3, 20, 20],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.497474 test begin: paddle.Tensor.unsqueeze(Tensor([2, 1140850690],"bool"), axis=-1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([2, 1140850690],"bool"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.506842 test begin: paddle.Tensor.unsqueeze(Tensor([2, 1140850690],"float32"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([2, 1140850690],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.516194 test begin: paddle.Tensor.unsqueeze(Tensor([2, 1140850690],"int64"), axis=1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([2, 1140850690],"int64"), axis=1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.525915 test begin: paddle.Tensor.unsqueeze(Tensor([2, 1140850690],"int64"), axis=2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([2, 1140850690],"int64"), axis=2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.535394 test begin: paddle.Tensor.unsqueeze(Tensor([2, 142606337, 8],"int32"), axis=-3, )

[torch error] paddle.Tensor.unsqueeze(Tensor([2, 142606337, 8],"int32"), axis=-3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.544730 test begin: paddle.Tensor.unsqueeze(Tensor([2, 2, 8, 128, 1048577],"float16"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([2, 2, 8, 128, 1048577],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.554311 test begin: paddle.Tensor.unsqueeze(Tensor([2, 2, 8, 1398102, 96],"float16"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([2, 2, 8, 1398102, 96],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.563775 test begin: paddle.Tensor.unsqueeze(Tensor([2, 2, 87382, 128, 96],"float16"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([2, 2, 87382, 128, 96],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.573322 test begin: paddle.Tensor.unsqueeze(Tensor([2, 21846, 8, 128, 96],"float16"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([2, 21846, 8, 128, 96],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.582665 test begin: paddle.Tensor.unsqueeze(Tensor([2, 8, 142606337],"int32"), axis=-3, )

[torch error] paddle.Tensor.unsqueeze(Tensor([2, 8, 142606337],"int32"), axis=-3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.591978 test begin: paddle.Tensor.unsqueeze(Tensor([209716, 512, 1, 40],"float16"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([209716, 512, 1, 40],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.601275 test begin: paddle.Tensor.unsqueeze(Tensor([21126865, 12, 9],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([21126865, 12, 9],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.610612 test begin: paddle.Tensor.unsqueeze(Tensor([21846, 2, 8, 128, 96],"float16"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([21846, 2, 8, 128, 96],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.620123 test begin: paddle.Tensor.unsqueeze(Tensor([221848, 10285],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([221848, 10285],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.629400 test begin: paddle.Tensor.unsqueeze(Tensor([2228225, 1024],"int64"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([2228225, 1024],"int64"), 2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.638676 test begin: paddle.Tensor.unsqueeze(Tensor([2281701379, 1, 1],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([2281701379, 1, 1],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.648056 test begin: paddle.Tensor.unsqueeze(Tensor([2281701379, 1],"float32"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([2281701379, 1],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.657353 test begin: paddle.Tensor.unsqueeze(Tensor([2281701379, 1],"int32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([2281701379, 1],"int32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.667344 test begin: paddle.Tensor.unsqueeze(Tensor([2281701379],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([2281701379],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.676983 test begin: paddle.Tensor.unsqueeze(Tensor([2281701379],"float32"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([2281701379],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.686423 test begin: paddle.Tensor.unsqueeze(Tensor([2281701379],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([2281701379],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.695849 test begin: paddle.Tensor.unsqueeze(Tensor([2281701379],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([2281701379],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.705207 test begin: paddle.Tensor.unsqueeze(Tensor([2281701379],"int32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([2281701379],"int32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.714479 test begin: paddle.Tensor.unsqueeze(Tensor([2281701379],"int64"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([2281701379],"int64"), -1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.723766 test begin: paddle.Tensor.unsqueeze(Tensor([2281701379],"int64"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([2281701379],"int64"), 0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.733274 test begin: paddle.Tensor.unsqueeze(Tensor([2281701379],"int64"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([2281701379],"int64"), 1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.742667 test begin: paddle.Tensor.unsqueeze(Tensor([2281701379],"int64"), axis=-1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([2281701379],"int64"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.752075 test begin: paddle.Tensor.unsqueeze(Tensor([228170138, 10],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([228170138, 10],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.761434 test begin: paddle.Tensor.unsqueeze(Tensor([26844, 160000],"float16"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([26844, 160000],"float16"), -1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.770887 test begin: paddle.Tensor.unsqueeze(Tensor([27853, 10, 64, 64, 2],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([27853, 10, 64, 64, 2],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.780328 test begin: paddle.Tensor.unsqueeze(Tensor([28, 81489335],"int32"), axis=-3, )

[torch error] paddle.Tensor.unsqueeze(Tensor([28, 81489335],"int32"), axis=-3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.789695 test begin: paddle.Tensor.unsqueeze(Tensor([285212673, 8],"int32"), axis=-3, )

[torch error] paddle.Tensor.unsqueeze(Tensor([285212673, 8],"int32"), axis=-3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.799267 test begin: paddle.Tensor.unsqueeze(Tensor([28521268, 10, 8],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([28521268, 10, 8],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.808684 test begin: paddle.Tensor.unsqueeze(Tensor([285213, 8000],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([285213, 8000],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.818110 test begin: paddle.Tensor.unsqueeze(Tensor([3, 760567127],"int64"), axis=1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([3, 760567127],"int64"), axis=1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.827517 test begin: paddle.Tensor.unsqueeze(Tensor([3, 760567127],"int64"), axis=2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([3, 760567127],"int64"), axis=2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.836921 test begin: paddle.Tensor.unsqueeze(Tensor([31690297, 18, 4],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([31690297, 18, 4],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.846483 test begin: paddle.Tensor.unsqueeze(Tensor([325957340, 1, 7],"float32"), 3, )

[torch error] paddle.Tensor.unsqueeze(Tensor([325957340, 1, 7],"float32"), 3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.855859 test begin: paddle.Tensor.unsqueeze(Tensor([35651585, 8, 8],"int32"), axis=-3, )

[torch error] paddle.Tensor.unsqueeze(Tensor([35651585, 8, 8],"int32"), axis=-3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.865270 test begin: paddle.Tensor.unsqueeze(Tensor([4, 1, 1, 142606337, 4],"float32"), axis=1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([4, 1, 1, 142606337, 4],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.874581 test begin: paddle.Tensor.unsqueeze(Tensor([4, 1, 1, 3, 190141782],"float32"), axis=1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([4, 1, 1, 3, 190141782],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.883901 test begin: paddle.Tensor.unsqueeze(Tensor([4, 1, 47535446, 3, 4],"float32"), axis=1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([4, 1, 47535446, 3, 4],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.893279 test begin: paddle.Tensor.unsqueeze(Tensor([4, 19513, 94, 311],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([4, 19513, 94, 311],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.902896 test begin: paddle.Tensor.unsqueeze(Tensor([4, 19513, 94, 311],"float32"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([4, 19513, 94, 311],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.912410 test begin: paddle.Tensor.unsqueeze(Tensor([4, 47535446, 1, 3, 4],"float32"), axis=1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([4, 47535446, 1, 3, 4],"float32"), axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.921888 test begin: paddle.Tensor.unsqueeze(Tensor([4, 64, 28659, 311],"float32"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([4, 64, 28659, 311],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.931389 test begin: paddle.Tensor.unsqueeze(Tensor([4, 64, 94, 94819],"float32"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([4, 64, 94, 94819],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.940784 test begin: paddle.Tensor.unsqueeze(Tensor([4, 81, 22645, 311],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([4, 81, 22645, 311],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.950461 test begin: paddle.Tensor.unsqueeze(Tensor([4, 81, 94, 74918],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([4, 81, 94, 74918],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.959959 test begin: paddle.Tensor.unsqueeze(Tensor([40744668, 56],"int64"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([40744668, 56],"int64"), -2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.969407 test begin: paddle.Tensor.unsqueeze(Tensor([40744668, 7, 8],"float32"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([40744668, 7, 8],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.978725 test begin: paddle.Tensor.unsqueeze(Tensor([4294967297],"float16"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([4294967297],"float16"), -1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.988084 test begin: paddle.Tensor.unsqueeze(Tensor([4294967297],"float16"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([4294967297],"float16"), -2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:10.997417 test begin: paddle.Tensor.unsqueeze(Tensor([456340276, 1, 5],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([456340276, 1, 5],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.006724 test begin: paddle.Tensor.unsqueeze(Tensor([4572548, 1, 499],"bool"), 3, )

[torch error] paddle.Tensor.unsqueeze(Tensor([4572548, 1, 499],"bool"), 3, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.016105 test begin: paddle.Tensor.unsqueeze(Tensor([46565335, 49],"int64"), axis=1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([46565335, 49],"int64"), axis=1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.025433 test begin: paddle.Tensor.unsqueeze(Tensor([46565335, 49],"int64"), axis=2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([46565335, 49],"int64"), axis=2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.034755 test begin: paddle.Tensor.unsqueeze(Tensor([47535446, 3, 2, 8],"float32"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([47535446, 3, 2, 8],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.044098 test begin: paddle.Tensor.unsqueeze(Tensor([47535446, 3, 4, 4],"float32"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([47535446, 3, 4, 4],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.053388 test begin: paddle.Tensor.unsqueeze(Tensor([475355, 3, 40, 40],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([475355, 3, 40, 40],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.062874 test begin: paddle.Tensor.unsqueeze(Tensor([50642, 11, 64, 64],"float32"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([50642, 11, 64, 64],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.072225 test begin: paddle.Tensor.unsqueeze(Tensor([518569, 1100, 4],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([518569, 1100, 4],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.081774 test begin: paddle.Tensor.unsqueeze(Tensor([536870913, 1, 8],"float16"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([536870913, 1, 8],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.091112 test begin: paddle.Tensor.unsqueeze(Tensor([570425345, 1, 4],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([570425345, 1, 4],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.100439 test begin: paddle.Tensor.unsqueeze(Tensor([570425345, 1, 4],"float32"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([570425345, 1, 4],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.109773 test begin: paddle.Tensor.unsqueeze(Tensor([570425345, 1, 4],"float32"), 3, )

[torch error] paddle.Tensor.unsqueeze(Tensor([570425345, 1, 4],"float32"), 3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.119121 test begin: paddle.Tensor.unsqueeze(Tensor([570425345, 4],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([570425345, 4],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.128436 test begin: paddle.Tensor.unsqueeze(Tensor([57042535, 10, 1, 2, 2],"float32"), -3, )

[torch error] paddle.Tensor.unsqueeze(Tensor([57042535, 10, 1, 2, 2],"float32"), -3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.137853 test begin: paddle.Tensor.unsqueeze(Tensor([57042535, 10, 2, 2],"float32"), -3, )

[torch error] paddle.Tensor.unsqueeze(Tensor([57042535, 10, 2, 2],"float32"), -3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.147195 test begin: paddle.Tensor.unsqueeze(Tensor([5704254, 100, 4],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([5704254, 100, 4],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.156490 test begin: paddle.Tensor.unsqueeze(Tensor([5820667, 392],"int64"), axis=1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([5820667, 392],"int64"), axis=1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.165769 test begin: paddle.Tensor.unsqueeze(Tensor([5820667, 392],"int64"), axis=2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([5820667, 392],"int64"), axis=2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.175093 test begin: paddle.Tensor.unsqueeze(Tensor([64, 1677722, 1, 40],"float16"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([64, 1677722, 1, 40],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.184367 test begin: paddle.Tensor.unsqueeze(Tensor([64, 512, 1, 131073],"float16"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([64, 512, 1, 131073],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.193702 test begin: paddle.Tensor.unsqueeze(Tensor([64, 512, 1, 69633],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([64, 512, 1, 69633],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.203123 test begin: paddle.Tensor.unsqueeze(Tensor([64, 512, 1741, 40],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([64, 512, 1741, 40],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.212527 test begin: paddle.Tensor.unsqueeze(Tensor([64, 512, 3277, 40],"float16"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([64, 512, 3277, 40],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.221805 test begin: paddle.Tensor.unsqueeze(Tensor([64, 891290, 1, 40],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([64, 891290, 1, 40],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.231115 test begin: paddle.Tensor.unsqueeze(Tensor([674, 100, 184, 184],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([674, 100, 184, 184],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.240840 test begin: paddle.Tensor.unsqueeze(Tensor([67828, 10, 58, 58],"float32"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([67828, 10, 58, 58],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.250393 test begin: paddle.Tensor.unsqueeze(Tensor([71303169, 1, 32],"float32"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([71303169, 1, 32],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.259793 test begin: paddle.Tensor.unsqueeze(Tensor([71303169, 2, 2, 8],"float32"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([71303169, 2, 2, 8],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.269147 test begin: paddle.Tensor.unsqueeze(Tensor([73661, 176, 176],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([73661, 176, 176],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.278637 test begin: paddle.Tensor.unsqueeze(Tensor([737, 100, 176, 176],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([737, 100, 176, 176],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.288086 test begin: paddle.Tensor.unsqueeze(Tensor([760567127, 1, 3],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([760567127, 1, 3],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.297479 test begin: paddle.Tensor.unsqueeze(Tensor([7605672, 3, 10, 10],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([7605672, 3, 10, 10],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.309747 test begin: paddle.Tensor.unsqueeze(Tensor([7605672, 300],"bool"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([7605672, 300],"bool"), 2, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.320003 test begin: paddle.Tensor.unsqueeze(Tensor([79226, 144, 200],"float32"), -1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([79226, 144, 200],"float32"), -1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.331052 test begin: paddle.Tensor.unsqueeze(Tensor([8, 285212673],"int32"), axis=-3, )

[torch error] paddle.Tensor.unsqueeze(Tensor([8, 285212673],"int32"), axis=-3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.342661 test begin: paddle.Tensor.unsqueeze(Tensor([8, 512, 1, 557057],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([8, 512, 1, 557057],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.353050 test begin: paddle.Tensor.unsqueeze(Tensor([8, 512, 13927, 40],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([8, 512, 13927, 40],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.363544 test begin: paddle.Tensor.unsqueeze(Tensor([8, 7130317, 1, 40],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([8, 7130317, 1, 40],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.373739 test begin: paddle.Tensor.unsqueeze(Tensor([81489335, 28],"int32"), axis=-3, )

[torch error] paddle.Tensor.unsqueeze(Tensor([81489335, 28],"int32"), axis=-3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.383947 test begin: paddle.Tensor.unsqueeze(Tensor([8912897, 256],"bool"), axis=-1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([8912897, 256],"bool"), axis=-1, ) 
 CUDA out of memory. Tried to allocate 2.13 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.394197 test begin: paddle.Tensor.unsqueeze(Tensor([8912897, 8, 2, 16],"float32"), -2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([8912897, 8, 2, 16],"float32"), -2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.404316 test begin: paddle.Tensor.unsqueeze(Tensor([93991, 24276],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([93991, 24276],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.414556 test begin: paddle.Tensor.unsqueeze(Tensor([950313, 1, 49, 49],"float32"), 0, )

[torch error] paddle.Tensor.unsqueeze(Tensor([950313, 1, 49, 49],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.424156 test begin: paddle.Tensor.unsqueeze(Tensor([96, 1485483, 4, 4],"float32"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([96, 1485483, 4, 4],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.436344 test begin: paddle.Tensor.unsqueeze(Tensor([96, 3, 1980644, 4],"float32"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([96, 3, 1980644, 4],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.446875 test begin: paddle.Tensor.unsqueeze(Tensor([96, 3, 4, 1980644],"float32"), 2, )

[torch error] paddle.Tensor.unsqueeze(Tensor([96, 3, 4, 1980644],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.457420 test begin: paddle.Tensor.unsqueeze(Tensor([964, 81, 94, 311],"float32"), 1, )

[torch error] paddle.Tensor.unsqueeze(Tensor([964, 81, 94, 311],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.467564 test begin: paddle.Tensor.var(Tensor([1000, 2281702],"float32"), axis=0, )

[torch error] paddle.Tensor.var(Tensor([1000, 2281702],"float32"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.477603 test begin: paddle.Tensor.var(Tensor([10000, 143166, 3],"float16"), axis=0, )

[torch error] paddle.Tensor.var(Tensor([10000, 143166, 3],"float16"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.487656 test begin: paddle.Tensor.var(Tensor([10000, 2, 114086],"float32"), axis=0, )

[torch error] paddle.Tensor.var(Tensor([10000, 2, 114086],"float32"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.497119 test begin: paddle.Tensor.var(Tensor([10000, 2, 214749],"float16"), axis=0, )

[torch error] paddle.Tensor.var(Tensor([10000, 2, 214749],"float16"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.509901 test begin: paddle.Tensor.var(Tensor([10000, 76057, 3],"float32"), axis=0, )

[torch error] paddle.Tensor.var(Tensor([10000, 76057, 3],"float32"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.519242 test begin: paddle.Tensor.var(Tensor([100000, 42950],"float16"), axis=0, )

[torch error] paddle.Tensor.var(Tensor([100000, 42950],"float16"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.528542 test begin: paddle.Tensor.var(Tensor([1000000, 4295],"float16"), axis=0, )

[torch error] paddle.Tensor.var(Tensor([1000000, 4295],"float16"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.537814 test begin: paddle.Tensor.var(Tensor([1073741825, 4],"float16"), axis=0, )

[torch error] paddle.Tensor.var(Tensor([1073741825, 4],"float16"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.547140 test begin: paddle.Tensor.var(Tensor([1140850690, 2],"float32"), axis=0, )

[torch error] paddle.Tensor.var(Tensor([1140850690, 2],"float32"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.556426 test begin: paddle.Tensor.var(Tensor([2281701379],"float32"), axis=0, )

[torch error] paddle.Tensor.var(Tensor([2281701379],"float32"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.565662 test begin: paddle.Tensor.var(Tensor([2910334, 784],"float32"), axis=0, )

[torch error] paddle.Tensor.var(Tensor([2910334, 784],"float32"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.575416 test begin: paddle.Tensor.var(Tensor([380283564, 2, 3],"float32"), axis=0, )

[torch error] paddle.Tensor.var(Tensor([380283564, 2, 3],"float32"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.584908 test begin: paddle.Tensor.var(Tensor([4294967297, 1],"float16"), axis=0, )

[torch error] paddle.Tensor.var(Tensor([4294967297, 1],"float16"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.594301 test begin: paddle.Tensor.var(Tensor([4294967297],"float16"), axis=0, )

[torch error] paddle.Tensor.var(Tensor([4294967297],"float16"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.603653 test begin: paddle.Tensor.var(Tensor([5000, 456341],"float32"), axis=0, )

[torch error] paddle.Tensor.var(Tensor([5000, 456341],"float32"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.612991 test begin: paddle.Tensor.var(Tensor([5000, 858994],"float16"), axis=0, )

[torch error] paddle.Tensor.var(Tensor([5000, 858994],"float16"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.622507 test begin: paddle.Tensor.var(Tensor([50000, 2, 42950],"float16"), axis=0, )

[torch error] paddle.Tensor.var(Tensor([50000, 2, 42950],"float16"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.631848 test begin: paddle.Tensor.var(Tensor([50000, 28634, 3],"float16"), axis=0, )

[torch error] paddle.Tensor.var(Tensor([50000, 28634, 3],"float16"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.641219 test begin: paddle.Tensor.var(Tensor([715827883, 2, 3],"float16"), axis=0, )

[torch error] paddle.Tensor.var(Tensor([715827883, 2, 3],"float16"), axis=0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.650811 test begin: paddle.tan(Tensor([10, 228170138, 1],"float32"), )

[torch error] paddle.tan(Tensor([10, 228170138, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.660284 test begin: paddle.tan(Tensor([100, 1, 11408507, 2],"float32"), )

[torch error] paddle.tan(Tensor([100, 1, 11408507, 2],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.669695 test begin: paddle.tan(Tensor([100, 1, 2, 11408507],"float32"), )

[torch error] paddle.tan(Tensor([100, 1, 2, 11408507],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.679318 test begin: paddle.tan(Tensor([100, 1, 22817014],"float32"), )

[torch error] paddle.tan(Tensor([100, 1, 22817014],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.688764 test begin: paddle.tan(Tensor([100, 11408507, 2],"float32"), )

[torch error] paddle.tan(Tensor([100, 11408507, 2],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.698149 test begin: paddle.tan(Tensor([100, 2, 11408507, 1],"float32"), )

[torch error] paddle.tan(Tensor([100, 2, 11408507, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.707790 test begin: paddle.tan(Tensor([100, 2, 11408507],"float32"), )

[torch error] paddle.tan(Tensor([100, 2, 11408507],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.717337 test begin: paddle.tan(Tensor([100, 2, 3, 3802836],"float32"), )

[torch error] paddle.tan(Tensor([100, 2, 3, 3802836],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.726774 test begin: paddle.tan(Tensor([100, 22817014, 1],"float32"), )

[torch error] paddle.tan(Tensor([100, 22817014, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.736316 test begin: paddle.tan(Tensor([100, 22817014],"float32"), )

[torch error] paddle.tan(Tensor([100, 22817014],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.745856 test begin: paddle.tan(Tensor([100, 42949673],"float16"), )

[torch error] paddle.tan(Tensor([100, 42949673],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.755207 test begin: paddle.tan(Tensor([100, 5704254, 2, 2],"float32"), )

[torch error] paddle.tan(Tensor([100, 5704254, 2, 2],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.764630 test begin: paddle.tan(Tensor([100, 7605672, 3, 1],"float32"), )

[torch error] paddle.tan(Tensor([100, 7605672, 3, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.774052 test begin: paddle.tan(Tensor([100, 7605672, 3],"float32"), )

[torch error] paddle.tan(Tensor([100, 7605672, 3],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.783367 test begin: paddle.tan(Tensor([114085069, 20, 1],"float32"), )

[torch error] paddle.tan(Tensor([114085069, 20, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.792624 test begin: paddle.tan(Tensor([1140850690, 1, 2],"float32"), )

[torch error] paddle.tan(Tensor([1140850690, 1, 2],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.802069 test begin: paddle.tan(Tensor([1140850690, 2, 1],"float32"), )

[torch error] paddle.tan(Tensor([1140850690, 2, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.811492 test begin: paddle.tan(Tensor([1140850690, 2],"float32"), )

[torch error] paddle.tan(Tensor([1140850690, 2],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.821044 test begin: paddle.tan(Tensor([2281701379, 1],"float32"), )

[torch error] paddle.tan(Tensor([2281701379, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.830454 test begin: paddle.tan(Tensor([2281701379],"float32"), )

[torch error] paddle.tan(Tensor([2281701379],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.839793 test begin: paddle.tan(Tensor([380283564, 2, 3, 1],"float32"), )

[torch error] paddle.tan(Tensor([380283564, 2, 3, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.849109 test begin: paddle.tan(Tensor([380283564, 2, 3],"float32"), )

[torch error] paddle.tan(Tensor([380283564, 2, 3],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.858654 test begin: paddle.tan(Tensor([4294967297, 1],"float16"), )

[torch error] paddle.tan(Tensor([4294967297, 1],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.868036 test begin: paddle.tan(Tensor([570425345, 1, 2, 2],"float32"), )

[torch error] paddle.tan(Tensor([570425345, 1, 2, 2],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.877369 test begin: paddle.tan(x=Tensor([253522376, 3, 3],"float32"), )

[torch error] paddle.tan(x=Tensor([253522376, 3, 3],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.886711 test begin: paddle.tan(x=Tensor([3, 253522376, 3],"float32"), )

[torch error] paddle.tan(x=Tensor([3, 253522376, 3],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.896061 test begin: paddle.tan(x=Tensor([3, 3, 253522376],"float32"), )

[torch error] paddle.tan(x=Tensor([3, 3, 253522376],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.905445 test begin: paddle.tan(x=Tensor([3, 3, 477218589],"float16"), )

[torch error] paddle.tan(x=Tensor([3, 3, 477218589],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.914769 test begin: paddle.tan(x=Tensor([3, 477218589, 3],"float16"), )

[torch error] paddle.tan(x=Tensor([3, 477218589, 3],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.924292 test begin: paddle.tan(x=Tensor([477218589, 3, 3],"float16"), )

[torch error] paddle.tan(x=Tensor([477218589, 3, 3],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.933772 test begin: paddle.tensor_split(Tensor([190141782, 4, 3],"int64"), list[2,3,], )

[torch error] paddle.tensor_split(Tensor([190141782, 4, 3],"int64"), list[2,3,], ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.943403 test begin: paddle.tensor_split(Tensor([190141782, 4, 3],"int64"), list[2,4,6,], )

[torch error] paddle.tensor_split(Tensor([190141782, 4, 3],"int64"), list[2,4,6,], ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.952904 test begin: paddle.tensor_split(Tensor([190141782, 4, 3],"int64"), list[2,4,], )

[torch error] paddle.tensor_split(Tensor([190141782, 4, 3],"int64"), list[2,4,], ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.963102 test begin: paddle.tensor_split(Tensor([190141782, 4, 3],"int64"), tuple(2,6,), )

[torch error] paddle.tensor_split(Tensor([190141782, 4, 3],"int64"), tuple(2,6,), ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.972609 test begin: paddle.tensor_split(Tensor([20372334, 4, 4, 7],"int64"), list[2,3,], axis=3, )

[torch error] paddle.tensor_split(Tensor([20372334, 4, 4, 7],"int64"), list[2,3,], axis=3, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.981996 test begin: paddle.tensor_split(Tensor([20372334, 4, 4, 7],"int64"), list[2,4,6,], axis=3, )

[torch error] paddle.tensor_split(Tensor([20372334, 4, 4, 7],"int64"), list[2,4,6,], axis=3, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:11.991401 test begin: paddle.tensor_split(Tensor([20372334, 4, 4, 7],"int64"), tuple(2,6,), axis=3, )

[torch error] paddle.tensor_split(Tensor([20372334, 4, 4, 7],"int64"), tuple(2,6,), axis=3, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.000764 test begin: paddle.tensor_split(Tensor([20372334, 4, 7, 4],"int64"), list[2,3,], axis=-2, )

[torch error] paddle.tensor_split(Tensor([20372334, 4, 7, 4],"int64"), list[2,3,], axis=-2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.010131 test begin: paddle.tensor_split(Tensor([20372334, 4, 7, 4],"int64"), list[2,4,6,], axis=-2, )

[torch error] paddle.tensor_split(Tensor([20372334, 4, 7, 4],"int64"), list[2,4,6,], axis=-2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.019461 test begin: paddle.tensor_split(Tensor([20372334, 4, 7, 4],"int64"), tuple(2,6,), axis=-2, )

[torch error] paddle.tensor_split(Tensor([20372334, 4, 7, 4],"int64"), tuple(2,6,), axis=-2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.028851 test begin: paddle.tensor_split(Tensor([2281701379],"int64"), list[2,1,3,], axis=0, )

[torch error] paddle.tensor_split(Tensor([2281701379],"int64"), list[2,1,3,], axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.038408 test begin: paddle.tensor_split(Tensor([2281701379],"int64"), list[2,3,16,], axis=0, )

[torch error] paddle.tensor_split(Tensor([2281701379],"int64"), list[2,3,16,], axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.048039 test begin: paddle.tensor_split(Tensor([2281701379],"int64"), list[2,3,], )

[torch error] paddle.tensor_split(Tensor([2281701379],"int64"), list[2,3,], ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.057498 test begin: paddle.tensor_split(Tensor([2281701379],"int64"), list[2,3,], axis=0, )

[torch error] paddle.tensor_split(Tensor([2281701379],"int64"), list[2,3,], axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.066837 test begin: paddle.tensor_split(Tensor([2281701379],"int64"), list[2,4,5,], )

[torch error] paddle.tensor_split(Tensor([2281701379],"int64"), list[2,4,5,], ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.076269 test begin: paddle.tensor_split(Tensor([2281701379],"int64"), list[2,4,6,], )

[torch error] paddle.tensor_split(Tensor([2281701379],"int64"), list[2,4,6,], ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.085636 test begin: paddle.tensor_split(Tensor([2281701379],"int64"), list[2,4,6,], axis=0, )

[torch error] paddle.tensor_split(Tensor([2281701379],"int64"), list[2,4,6,], axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.095098 test begin: paddle.tensor_split(Tensor([2281701379],"int64"), list[2,4,], )

[torch error] paddle.tensor_split(Tensor([2281701379],"int64"), list[2,4,], ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.104541 test begin: paddle.tensor_split(Tensor([2281701379],"int64"), list[3,-1,16,], axis=0, )

[torch error] paddle.tensor_split(Tensor([2281701379],"int64"), list[3,-1,16,], axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.113873 test begin: paddle.tensor_split(Tensor([2281701379],"int64"), list[3,-1,5,2,16,], axis=0, )

[torch error] paddle.tensor_split(Tensor([2281701379],"int64"), list[3,-1,5,2,16,], axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.123230 test begin: paddle.tensor_split(Tensor([2281701379],"int64"), tuple(2,5,), )

[torch error] paddle.tensor_split(Tensor([2281701379],"int64"), tuple(2,5,), ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.132538 test begin: paddle.tensor_split(Tensor([2281701379],"int64"), tuple(2,6,), )

[torch error] paddle.tensor_split(Tensor([2281701379],"int64"), tuple(2,6,), ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.141859 test begin: paddle.tensor_split(Tensor([2281701379],"int64"), tuple(2,6,), axis=0, )

[torch error] paddle.tensor_split(Tensor([2281701379],"int64"), tuple(2,6,), axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.151248 test begin: paddle.tensor_split(Tensor([325957340, 7],"int64"), list[2,3,], axis=1, )

[torch error] paddle.tensor_split(Tensor([325957340, 7],"int64"), list[2,3,], axis=1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.160662 test begin: paddle.tensor_split(Tensor([325957340, 7],"int64"), list[2,4,6,], axis=1, )

[torch error] paddle.tensor_split(Tensor([325957340, 7],"int64"), list[2,4,6,], axis=1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.170057 test begin: paddle.tensor_split(Tensor([325957340, 7],"int64"), tuple(2,6,), axis=1, )

[torch error] paddle.tensor_split(Tensor([325957340, 7],"int64"), tuple(2,6,), axis=1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.179416 test begin: paddle.tensor_split(Tensor([4, 20372334, 4, 7],"int64"), list[2,3,], axis=3, )

[torch error] paddle.tensor_split(Tensor([4, 20372334, 4, 7],"int64"), list[2,3,], axis=3, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.188805 test begin: paddle.tensor_split(Tensor([4, 20372334, 4, 7],"int64"), list[2,4,6,], axis=3, )

[torch error] paddle.tensor_split(Tensor([4, 20372334, 4, 7],"int64"), list[2,4,6,], axis=3, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.198267 test begin: paddle.tensor_split(Tensor([4, 20372334, 4, 7],"int64"), tuple(2,6,), axis=3, )

[torch error] paddle.tensor_split(Tensor([4, 20372334, 4, 7],"int64"), tuple(2,6,), axis=3, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.207685 test begin: paddle.tensor_split(Tensor([4, 20372334, 7, 4],"int64"), list[2,3,], axis=-2, )

[torch error] paddle.tensor_split(Tensor([4, 20372334, 7, 4],"int64"), list[2,3,], axis=-2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.217145 test begin: paddle.tensor_split(Tensor([4, 20372334, 7, 4],"int64"), list[2,4,6,], axis=-2, )

[torch error] paddle.tensor_split(Tensor([4, 20372334, 7, 4],"int64"), list[2,4,6,], axis=-2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.226543 test begin: paddle.tensor_split(Tensor([4, 20372334, 7, 4],"int64"), tuple(2,6,), axis=-2, )

[torch error] paddle.tensor_split(Tensor([4, 20372334, 7, 4],"int64"), tuple(2,6,), axis=-2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.235930 test begin: paddle.tensor_split(Tensor([4, 4, 142606337],"int64"), list[2,3,], axis=2, )

[torch error] paddle.tensor_split(Tensor([4, 4, 142606337],"int64"), list[2,3,], axis=2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.245355 test begin: paddle.tensor_split(Tensor([4, 4, 142606337],"int64"), list[2,4,6,], axis=2, )

[torch error] paddle.tensor_split(Tensor([4, 4, 142606337],"int64"), list[2,4,6,], axis=2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.255147 test begin: paddle.tensor_split(Tensor([4, 4, 142606337],"int64"), tuple(2,6,), axis=2, )

[torch error] paddle.tensor_split(Tensor([4, 4, 142606337],"int64"), tuple(2,6,), axis=2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.264739 test begin: paddle.tensor_split(Tensor([4, 4, 20372334, 7],"int64"), list[2,3,], axis=3, )

[torch error] paddle.tensor_split(Tensor([4, 4, 20372334, 7],"int64"), list[2,3,], axis=3, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.274142 test begin: paddle.tensor_split(Tensor([4, 4, 20372334, 7],"int64"), list[2,4,6,], axis=3, )

[torch error] paddle.tensor_split(Tensor([4, 4, 20372334, 7],"int64"), list[2,4,6,], axis=3, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.283522 test begin: paddle.tensor_split(Tensor([4, 4, 20372334, 7],"int64"), tuple(2,6,), axis=3, )

[torch error] paddle.tensor_split(Tensor([4, 4, 20372334, 7],"int64"), tuple(2,6,), axis=3, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.292990 test begin: paddle.tensor_split(Tensor([4, 4, 35651585, 4],"int64"), list[2,3,], axis=-2, )

[torch error] paddle.tensor_split(Tensor([4, 4, 35651585, 4],"int64"), list[2,3,], axis=-2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.302375 test begin: paddle.tensor_split(Tensor([4, 4, 35651585, 4],"int64"), list[2,4,6,], axis=-2, )

[torch error] paddle.tensor_split(Tensor([4, 4, 35651585, 4],"int64"), list[2,4,6,], axis=-2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.311733 test begin: paddle.tensor_split(Tensor([4, 4, 35651585, 4],"int64"), tuple(2,6,), axis=-2, )

[torch error] paddle.tensor_split(Tensor([4, 4, 35651585, 4],"int64"), tuple(2,6,), axis=-2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.321206 test begin: paddle.tensor_split(Tensor([4, 4, 4, 35651585],"int64"), list[2,3,], axis=3, )

[torch error] paddle.tensor_split(Tensor([4, 4, 4, 35651585],"int64"), list[2,3,], axis=3, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.330682 test begin: paddle.tensor_split(Tensor([4, 4, 4, 35651585],"int64"), list[2,4,6,], axis=3, )

[torch error] paddle.tensor_split(Tensor([4, 4, 4, 35651585],"int64"), list[2,4,6,], axis=3, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.340098 test begin: paddle.tensor_split(Tensor([4, 4, 4, 35651585],"int64"), tuple(2,6,), axis=3, )

[torch error] paddle.tensor_split(Tensor([4, 4, 4, 35651585],"int64"), tuple(2,6,), axis=3, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.349780 test begin: paddle.tensor_split(Tensor([4, 4, 7, 20372334],"int64"), list[2,3,], axis=-2, )

[torch error] paddle.tensor_split(Tensor([4, 4, 7, 20372334],"int64"), list[2,3,], axis=-2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.359522 test begin: paddle.tensor_split(Tensor([4, 4, 7, 20372334],"int64"), list[2,4,6,], axis=-2, )

[torch error] paddle.tensor_split(Tensor([4, 4, 7, 20372334],"int64"), list[2,4,6,], axis=-2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.369365 test begin: paddle.tensor_split(Tensor([4, 4, 7, 20372334],"int64"), tuple(2,6,), axis=-2, )

[torch error] paddle.tensor_split(Tensor([4, 4, 7, 20372334],"int64"), tuple(2,6,), axis=-2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.378884 test begin: paddle.tensor_split(Tensor([4, 570425345],"int64"), list[2,3,], axis=1, )

[torch error] paddle.tensor_split(Tensor([4, 570425345],"int64"), list[2,3,], axis=1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.388399 test begin: paddle.tensor_split(Tensor([4, 570425345],"int64"), list[2,4,6,], axis=1, )

[torch error] paddle.tensor_split(Tensor([4, 570425345],"int64"), list[2,4,6,], axis=1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.397797 test begin: paddle.tensor_split(Tensor([4, 570425345],"int64"), tuple(2,6,), axis=1, )

[torch error] paddle.tensor_split(Tensor([4, 570425345],"int64"), tuple(2,6,), axis=1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.407209 test begin: paddle.tensor_split(Tensor([4, 81489335, 7],"int64"), list[2,3,], axis=2, )

[torch error] paddle.tensor_split(Tensor([4, 81489335, 7],"int64"), list[2,3,], axis=2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.416682 test begin: paddle.tensor_split(Tensor([4, 81489335, 7],"int64"), list[2,4,6,], axis=2, )

[torch error] paddle.tensor_split(Tensor([4, 81489335, 7],"int64"), list[2,4,6,], axis=2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.426113 test begin: paddle.tensor_split(Tensor([4, 81489335, 7],"int64"), tuple(2,6,), axis=2, )

[torch error] paddle.tensor_split(Tensor([4, 81489335, 7],"int64"), tuple(2,6,), axis=2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.436107 test begin: paddle.tensor_split(Tensor([570425345, 4],"int64"), list[2,3,], )

[torch error] paddle.tensor_split(Tensor([570425345, 4],"int64"), list[2,3,], ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.445675 test begin: paddle.tensor_split(Tensor([570425345, 4],"int64"), list[2,4,6,], )

[torch error] paddle.tensor_split(Tensor([570425345, 4],"int64"), list[2,4,6,], ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.455165 test begin: paddle.tensor_split(Tensor([570425345, 4],"int64"), list[2,4,], )

[torch error] paddle.tensor_split(Tensor([570425345, 4],"int64"), list[2,4,], ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.464590 test begin: paddle.tensor_split(Tensor([570425345, 4],"int64"), tuple(2,6,), )

[torch error] paddle.tensor_split(Tensor([570425345, 4],"int64"), tuple(2,6,), ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.474103 test begin: paddle.tensor_split(Tensor([7, 108652447, 3],"int64"), list[2,3,], )

[torch error] paddle.tensor_split(Tensor([7, 108652447, 3],"int64"), list[2,3,], ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.483508 test begin: paddle.tensor_split(Tensor([7, 108652447, 3],"int64"), list[2,4,6,], )

[torch error] paddle.tensor_split(Tensor([7, 108652447, 3],"int64"), list[2,4,6,], ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.492937 test begin: paddle.tensor_split(Tensor([7, 108652447, 3],"int64"), list[2,4,], )

[torch error] paddle.tensor_split(Tensor([7, 108652447, 3],"int64"), list[2,4,], ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.502349 test begin: paddle.tensor_split(Tensor([7, 108652447, 3],"int64"), tuple(2,6,), )

[torch error] paddle.tensor_split(Tensor([7, 108652447, 3],"int64"), tuple(2,6,), ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.511866 test begin: paddle.tensor_split(Tensor([7, 325957340],"int64"), list[2,3,], )

[torch error] paddle.tensor_split(Tensor([7, 325957340],"int64"), list[2,3,], ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.522653 test begin: paddle.tensor_split(Tensor([7, 325957340],"int64"), list[2,4,6,], )

[torch error] paddle.tensor_split(Tensor([7, 325957340],"int64"), list[2,4,6,], ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.533557 test begin: paddle.tensor_split(Tensor([7, 325957340],"int64"), list[2,4,], )

[torch error] paddle.tensor_split(Tensor([7, 325957340],"int64"), list[2,4,], ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.544056 test begin: paddle.tensor_split(Tensor([7, 325957340],"int64"), tuple(2,6,), )

[torch error] paddle.tensor_split(Tensor([7, 325957340],"int64"), tuple(2,6,), ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.554807 test begin: paddle.tensor_split(Tensor([7, 4, 81489335],"int64"), list[2,3,], )

[torch error] paddle.tensor_split(Tensor([7, 4, 81489335],"int64"), list[2,3,], ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.565700 test begin: paddle.tensor_split(Tensor([7, 4, 81489335],"int64"), list[2,4,6,], )

[torch error] paddle.tensor_split(Tensor([7, 4, 81489335],"int64"), list[2,4,6,], ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.576329 test begin: paddle.tensor_split(Tensor([7, 4, 81489335],"int64"), list[2,4,], )

[torch error] paddle.tensor_split(Tensor([7, 4, 81489335],"int64"), list[2,4,], ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.586739 test begin: paddle.tensor_split(Tensor([7, 4, 81489335],"int64"), tuple(2,6,), )

[torch error] paddle.tensor_split(Tensor([7, 4, 81489335],"int64"), tuple(2,6,), ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.597354 test begin: paddle.tensor_split(Tensor([81489335, 4, 7],"int64"), list[2,3,], axis=2, )

[torch error] paddle.tensor_split(Tensor([81489335, 4, 7],"int64"), list[2,3,], axis=2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.607631 test begin: paddle.tensor_split(Tensor([81489335, 4, 7],"int64"), list[2,4,6,], axis=2, )

[torch error] paddle.tensor_split(Tensor([81489335, 4, 7],"int64"), list[2,4,6,], axis=2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 18:00:12.617873 test begin: paddle.tensor_split(Tensor([81489335, 4, 7],"int64"), tuple(2,6,), axis=2, )

[torch error] paddle.tensor_split(Tensor([81489335, 4, 7],"int64"), tuple(2,6,), axis=2, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.66 GiB is free. Process 162195 has 77.52 GiB memory in use. Of the allocated memory 74.67 GiB is allocated by PyTorch, and 509.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-17 09:58:36.903420 test begin: paddle.sgn(Tensor([107374183, 20, 2],"float16"), )

W0317 10:00:32.161257 82397 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0317 10:00:32.162410 82397 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.sgn(Tensor([107374183, 20, 2],"float16"), )
2025-03-17 10:16:38.276788 test begin: paddle.sgn(Tensor([12, 178956971, 2],"float16"), )

[Pass] paddle.sgn(Tensor([12, 178956971, 2],"float16"), )
2025-03-17 10:32:40.542363 test begin: paddle.sgn(Tensor([12, 20, 17895698],"float16"), )

[Pass] paddle.sgn(Tensor([12, 20, 17895698],"float16"), )
2025-03-17 10:48:46.794821 test begin: paddle.sgn(Tensor([12, 20, 9507090],"float32"), )

[Pass] paddle.sgn(Tensor([12, 20, 9507090],"float32"), )
2025-03-17 10:53:20.898934 test begin: paddle.sgn(Tensor([12, 95070891, 2],"float32"), )

[Pass] paddle.sgn(Tensor([12, 95070891, 2],"float32"), )
2025-03-17 10:56:57.769383 test begin: paddle.sgn(Tensor([57042535, 20, 2],"float32"), )

[Pass] paddle.sgn(Tensor([57042535, 20, 2],"float32"), )
2025-03-17 11:00:19.337512 test begin: paddle.sign(Tensor([107374183, 20, 2],"float16"), )

[Pass] paddle.sign(Tensor([107374183, 20, 2],"float16"), )
2025-03-17 11:16:27.197246 test begin: paddle.sign(Tensor([11, 17, 12201612],"int32"), )

[Pass] paddle.sign(Tensor([11, 17, 12201612],"int32"), )
2025-03-17 11:19:45.682122 test begin: paddle.sign(Tensor([11, 17, 22967740],"int16"), )

[Pass] paddle.sign(Tensor([11, 17, 22967740],"int16"), )
2025-03-17 11:24:52.707448 test begin: paddle.sign(Tensor([11, 207427399],"float32"), )

W0317 11:26:33.155148 109662 backward.cc:441] While running Node (SignGradNode) raises a std::exception: paddle::memory::allocation::BadAlloc
[paddle error] paddle.sign(Tensor([11, 207427399],"float32"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SignGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::scale(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool)
4   void phi::ScaleKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, bool, phi::DenseTensor*)
5   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
6   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 8.500000GB memory on GPU 0, 76.961853GB memory has been allocated and available memory is only 2.223022GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-17 11:26:34.837178 test begin: paddle.sign(Tensor([11, 20742740, 10],"int32"), )

[Pass] paddle.sign(Tensor([11, 20742740, 10],"int32"), )
2025-03-17 11:28:50.859720 test begin: paddle.sign(Tensor([11, 39045158, 10],"int16"), )

[Pass] paddle.sign(Tensor([11, 39045158, 10],"int16"), )
2025-03-17 11:32:42.179320 test begin: paddle.sign(Tensor([12, 178956971, 2],"float16"), )

[Pass] paddle.sign(Tensor([12, 178956971, 2],"float16"), )
2025-03-17 11:50:00.054586 test begin: paddle.sign(Tensor([12, 20, 17895698],"float16"), )

W0317 11:58:24.783082 117084 backward.cc:441] While running Node (SignGradNode) raises a std::exception: paddle::memory::allocation::BadAlloc
[paddle error] paddle.sign(Tensor([12, 20, 17895698],"float16"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SignGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::scale(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool)
4   void phi::ScaleKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, bool, phi::DenseTensor*)
5   phi::dtype::float16* phi::DeviceContext::Alloc<phi::dtype::float16>(phi::TensorBase*, unsigned long, bool) const
6   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 8.000000GB memory on GPU 0, 73.569275GB memory has been allocated and available memory is only 5.615601GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-17 11:58:26.550699 test begin: paddle.sign(Tensor([12, 20, 9507090],"float32"), )

W0317 12:00:30.241593 119729 backward.cc:441] While running Node (SignGradNode) raises a std::exception: paddle::memory::allocation::BadAlloc
[paddle error] paddle.sign(Tensor([12, 20, 9507090],"float32"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SignGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::scale(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool)
4   void phi::ScaleKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, bool, phi::DenseTensor*)
5   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
6   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 8.500001GB memory on GPU 0, 75.569275GB memory has been allocated and available memory is only 3.615601GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-17 12:00:32.241536 test begin: paddle.sign(Tensor([12, 95070891, 2],"float32"), )

W0317 12:02:38.153815 120387 backward.cc:441] While running Node (SignGradNode) raises a std::exception: paddle::memory::allocation::BadAlloc
[paddle error] paddle.sign(Tensor([12, 95070891, 2],"float32"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SignGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::scale(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool)
4   void phi::ScaleKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, bool, phi::DenseTensor*)
5   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
6   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 8.500000GB memory on GPU 0, 75.569275GB memory has been allocated and available memory is only 3.615601GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-17 12:02:40.167210 test begin: paddle.sign(Tensor([1203073, 17, 5, 6, 7],"float16"), )

W0317 12:11:06.123595 121136 backward.cc:441] While running Node (SignGradNode) raises a std::exception: paddle::memory::allocation::BadAlloc
[paddle error] paddle.sign(Tensor([1203073, 17, 5, 6, 7],"float16"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SignGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::scale(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool)
4   void phi::ScaleKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, bool, phi::DenseTensor*)
5   phi::dtype::float16* phi::DeviceContext::Alloc<phi::dtype::float16>(phi::TensorBase*, unsigned long, bool) const
6   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 8.000006GB memory on GPU 0, 73.569275GB memory has been allocated and available memory is only 5.615601GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-17 12:11:08.286180 test begin: paddle.sign(Tensor([134217729, 17],"float32"), )

W0317 12:13:24.139673 125003 backward.cc:441] While running Node (SignGradNode) raises a std::exception: paddle::memory::allocation::BadAlloc
[paddle error] paddle.sign(Tensor([134217729, 17],"float32"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SignGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::scale(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool)
4   void phi::ScaleKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, bool, phi::DenseTensor*)
5   float* phi::DeviceContext::Alloc<float>(phi::TensorBase*, unsigned long, bool) const
6   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 8.500000GB memory on GPU 0, 75.569275GB memory has been allocated and available memory is only 3.615601GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-17 12:13:25.801576 test begin: paddle.sign(Tensor([13421773, 17, 10],"int32"), )

[Pass] paddle.sign(Tensor([13421773, 17, 10],"int32"), )
2025-03-17 12:16:05.603556 test begin: paddle.sign(Tensor([2, 107374183, 4, 5],"float16"), )

W0317 12:24:46.812834 126563 backward.cc:441] While running Node (SignGradNode) raises a std::exception: paddle::memory::allocation::BadAlloc
[paddle error] paddle.sign(Tensor([2, 107374183, 4, 5],"float16"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SignGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::scale(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool)
4   void phi::ScaleKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, bool, phi::DenseTensor*)
5   phi::dtype::float16* phi::DeviceContext::Alloc<phi::dtype::float16>(phi::TensorBase*, unsigned long, bool) const
6   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 8.000000GB memory on GPU 0, 73.569275GB memory has been allocated and available memory is only 5.615601GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-17 12:24:48.694528 test begin: paddle.sign(Tensor([2, 107374183, 4, 5],"int8"), )

[Pass] paddle.sign(Tensor([2, 107374183, 4, 5],"int8"), )
2025-03-17 12:28:55.015702 test begin: paddle.sign(Tensor([2, 3, 143165577, 5],"float16"), )

W0317 12:37:46.788103 130774 backward.cc:441] While running Node (SignGradNode) raises a std::exception: paddle::memory::allocation::BadAlloc
[paddle error] paddle.sign(Tensor([2, 3, 143165577, 5],"float16"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SignGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::scale(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool)
4   void phi::ScaleKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, bool, phi::DenseTensor*)
5   phi::dtype::float16* phi::DeviceContext::Alloc<phi::dtype::float16>(phi::TensorBase*, unsigned long, bool) const
6   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 8.000000GB memory on GPU 0, 73.569275GB memory has been allocated and available memory is only 5.615601GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-17 12:37:48.839080 test begin: paddle.sign(Tensor([2, 3, 143165577, 5],"int8"), )

[Pass] paddle.sign(Tensor([2, 3, 143165577, 5],"int8"), )
2025-03-17 12:41:53.570762 test begin: paddle.sign(Tensor([2, 3, 4, 178956971],"float16"), )

W0317 12:50:28.502100 134958 backward.cc:441] While running Node (SignGradNode) raises a std::exception: paddle::memory::allocation::BadAlloc
[paddle error] paddle.sign(Tensor([2, 3, 4, 178956971],"float16"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SignGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::scale(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool)
4   void phi::ScaleKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, bool, phi::DenseTensor*)
5   phi::dtype::float16* phi::DeviceContext::Alloc<phi::dtype::float16>(phi::TensorBase*, unsigned long, bool) const
6   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 8.000000GB memory on GPU 0, 73.569275GB memory has been allocated and available memory is only 5.615601GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-17 12:50:30.723743 test begin: paddle.sign(Tensor([2, 3, 4, 178956971],"int8"), )

[Pass] paddle.sign(Tensor([2, 3, 4, 178956971],"int8"), )
2025-03-17 12:54:10.672874 test begin: paddle.sign(Tensor([2281701379],"int64"), )

[Pass] paddle.sign(Tensor([2281701379],"int64"), )
2025-03-17 12:58:33.246109 test begin: paddle.sign(Tensor([25264514, 17, 10],"int16"), )

[Pass] paddle.sign(Tensor([25264514, 17, 10],"int16"), )
2025-03-17 13:02:59.866274 test begin: paddle.sign(Tensor([2910334, 1, 28, 28],"float32"), )

[Pass] paddle.sign(Tensor([2910334, 1, 28, 28],"float32"), )
2025-03-17 13:07:05.073904 test begin: paddle.sign(Tensor([380283564, 6],"float32"), )

[Pass] paddle.sign(Tensor([380283564, 6],"float32"), )
2025-03-17 13:11:06.285407 test begin: paddle.sign(Tensor([4294967297],"float16"), )

[Pass] paddle.sign(Tensor([4294967297],"float16"), )
2025-03-17 13:28:20.355946 test begin: paddle.sign(Tensor([4294967297],"uint8"), )

[Pass] paddle.sign(Tensor([4294967297],"uint8"), )
2025-03-17 13:33:15.280298 test begin: paddle.sign(Tensor([57042535, 20, 2],"float32"), )

[Pass] paddle.sign(Tensor([57042535, 20, 2],"float32"), )
2025-03-17 13:36:31.197837 test begin: paddle.sign(Tensor([64, 1, 1273271, 28],"float32"), )

[Pass] paddle.sign(Tensor([64, 1, 1273271, 28],"float32"), )
2025-03-17 13:40:34.101231 test begin: paddle.sign(Tensor([64, 1, 28, 1273271],"float32"), )

[Pass] paddle.sign(Tensor([64, 1, 28, 1273271],"float32"), )
2025-03-17 13:44:03.096098 test begin: paddle.sign(Tensor([64, 45474, 28, 28],"float32"), )

[Pass] paddle.sign(Tensor([64, 45474, 28, 28],"float32"), )
2025-03-17 13:47:45.877456 test begin: paddle.sign(Tensor([71582789, 3, 4, 5],"float16"), )

[Pass] paddle.sign(Tensor([71582789, 3, 4, 5],"float16"), )
2025-03-17 14:04:34.295581 test begin: paddle.sign(Tensor([71582789, 3, 4, 5],"int8"), )

[Pass] paddle.sign(Tensor([71582789, 3, 4, 5],"int8"), )
2025-03-17 14:08:07.378198 test begin: paddle.sign(Tensor([8, 17, 5, 6, 1052689],"float16"), )

W0317 14:16:38.855674 160204 backward.cc:441] While running Node (SignGradNode) raises a std::exception: paddle::memory::allocation::BadAlloc
[paddle error] paddle.sign(Tensor([8, 17, 5, 6, 1052689],"float16"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SignGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::scale(paddle::Tensor const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, paddle::experimental::ScalarBase<paddle::Tensor> const&, bool)
4   void phi::ScaleKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, paddle::experimental::ScalarBase<phi::DenseTensor> const&, bool, phi::DenseTensor*)
5   phi::dtype::float16* phi::DeviceContext::Alloc<phi::dtype::float16>(phi::TensorBase*, unsigned long, bool) const
6   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 8.000007GB memory on GPU 0, 74.778259GB memory has been allocated and available memory is only 4.406616GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-17 14:16:40.291322 test begin: paddle.sign(Tensor([8, 17, 5, 902305, 7],"float16"), )

[Pass] paddle.sign(Tensor([8, 17, 5, 902305, 7],"float16"), )
2025-03-17 14:34:41.157891 test begin: paddle.sign(Tensor([8, 17, 751921, 6, 7],"float16"), )

[Pass] paddle.sign(Tensor([8, 17, 751921, 6, 7],"float16"), )
2025-03-17 14:52:07.381893 test begin: paddle.sign(Tensor([8, 2556529, 5, 6, 7],"float16"), )

[Pass] paddle.sign(Tensor([8, 2556529, 5, 6, 7],"float16"), )
2025-03-17 15:09:26.714557 test begin: paddle.sign(Tensor([9, 253522376],"float32"), )

[Pass] paddle.sign(Tensor([9, 253522376],"float32"), )
2025-03-17 15:12:53.052459 test begin: paddle.sign(x=Tensor([2281701379],"float32"), )

[Pass] paddle.sign(x=Tensor([2281701379],"float32"), )
2025-03-17 15:16:02.464933 test begin: paddle.sign(x=Tensor([4294967297],"float16"), )

[Pass] paddle.sign(x=Tensor([4294967297],"float16"), )
2025-03-17 15:32:39.287546 test begin: paddle.signbit(Tensor([107374183, 20, 2],"float16"), )

element 0 of tensors does not require grad and does not have a grad_fn
[Pass] paddle.signbit(Tensor([107374183, 20, 2],"float16"), )
2025-03-17 15:39:45.304700 test begin: paddle.signbit(Tensor([107374183, 20, 2],"int16"), )

[Pass] paddle.signbit(Tensor([107374183, 20, 2],"int16"), )
2025-03-17 15:45:04.291471 test begin: paddle.signbit(Tensor([11, 17, 12201612],"int32"), )

[Pass] paddle.signbit(Tensor([11, 17, 12201612],"int32"), )
2025-03-17 15:47:59.284565 test begin: paddle.signbit(Tensor([11, 17, 22967740],"int16"), )

[Pass] paddle.signbit(Tensor([11, 17, 22967740],"int16"), )
2025-03-17 15:53:14.289896 test begin: paddle.signbit(Tensor([11, 207427399],"float32"), )

element 0 of tensors does not require grad and does not have a grad_fn
[Pass] paddle.signbit(Tensor([11, 207427399],"float32"), )
2025-03-17 15:56:04.230485 test begin: paddle.signbit(Tensor([11, 20742740, 10],"int32"), )

[Pass] paddle.signbit(Tensor([11, 20742740, 10],"int32"), )
2025-03-17 15:59:20.749252 test begin: paddle.signbit(Tensor([11, 39045158, 10],"int16"), )

[Pass] paddle.signbit(Tensor([11, 39045158, 10],"int16"), )
2025-03-17 16:04:44.140170 test begin: paddle.signbit(Tensor([12, 178956971, 2],"float16"), )

element 0 of tensors does not require grad and does not have a grad_fn
[Pass] paddle.signbit(Tensor([12, 178956971, 2],"float16"), )
2025-03-17 16:10:29.803993 test begin: paddle.signbit(Tensor([12, 178956971, 2],"int16"), )

[Pass] paddle.signbit(Tensor([12, 178956971, 2],"int16"), )
2025-03-17 16:15:30.562046 test begin: paddle.signbit(Tensor([12, 20, 17895698],"float16"), )

element 0 of tensors does not require grad and does not have a grad_fn
[Pass] paddle.signbit(Tensor([12, 20, 17895698],"float16"), )
2025-03-17 16:20:58.094994 test begin: paddle.signbit(Tensor([12, 20, 17895698],"int16"), )

[Pass] paddle.signbit(Tensor([12, 20, 17895698],"int16"), )
2025-03-17 16:26:01.669883 test begin: paddle.signbit(Tensor([12, 20, 9507090],"float32"), )

element 0 of tensors does not require grad and does not have a grad_fn
[Pass] paddle.signbit(Tensor([12, 20, 9507090],"float32"), )
2025-03-17 16:28:50.448558 test begin: paddle.signbit(Tensor([12, 95070891, 2],"float32"), )

element 0 of tensors does not require grad and does not have a grad_fn
[Pass] paddle.signbit(Tensor([12, 95070891, 2],"float32"), )
2025-03-17 16:31:40.024888 test begin: paddle.signbit(Tensor([1203073, 17, 5, 6, 7],"float16"), )

element 0 of tensors does not require grad and does not have a grad_fn
[Pass] paddle.signbit(Tensor([1203073, 17, 5, 6, 7],"float16"), )
2025-03-17 16:37:12.361583 test begin: paddle.signbit(Tensor([134217729, 17],"float32"), )

element 0 of tensors does not require grad and does not have a grad_fn
[Pass] paddle.signbit(Tensor([134217729, 17],"float32"), )
2025-03-17 16:40:07.495429 test begin: paddle.signbit(Tensor([13421773, 17, 10],"int32"), )

[Pass] paddle.signbit(Tensor([13421773, 17, 10],"int32"), )
2025-03-17 16:43:17.662979 test begin: paddle.signbit(Tensor([2, 107374183, 4, 5],"float16"), )

element 0 of tensors does not require grad and does not have a grad_fn
[Pass] paddle.signbit(Tensor([2, 107374183, 4, 5],"float16"), )
2025-03-17 16:49:00.464707 test begin: paddle.signbit(Tensor([2, 107374183, 4, 5],"int8"), )

[Pass] paddle.signbit(Tensor([2, 107374183, 4, 5],"int8"), )
2025-03-17 16:54:28.635909 test begin: paddle.signbit(Tensor([2, 3, 143165577, 5],"float16"), )

element 0 of tensors does not require grad and does not have a grad_fn
[Pass] paddle.signbit(Tensor([2, 3, 143165577, 5],"float16"), )
2025-03-17 17:00:45.095229 test begin: paddle.signbit(Tensor([2, 3, 143165577, 5],"int8"), )

[Pass] paddle.signbit(Tensor([2, 3, 143165577, 5],"int8"), )
2025-03-17 17:05:59.238597 test begin: paddle.signbit(Tensor([2, 3, 4, 178956971],"float16"), )

element 0 of tensors does not require grad and does not have a grad_fn
[Pass] paddle.signbit(Tensor([2, 3, 4, 178956971],"float16"), )
2025-03-17 17:11:56.954402 test begin: paddle.signbit(Tensor([2, 3, 4, 178956971],"int8"), )

[Pass] paddle.signbit(Tensor([2, 3, 4, 178956971],"int8"), )
2025-03-17 17:17:16.433840 test begin: paddle.signbit(Tensor([2281701379],"int64"), )

/usr/local/lib/python3.9/dist-packages/paddle/tensor/math.py:8242: UserWarning: The shape of broadcast output [-1] is different from the input tensor x with shape: [2281701379], please make sure you are using copysign api correctly.
  warnings.warn(
[paddle error] paddle.signbit(Tensor([2281701379],"int64"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_copysign(_object*, _object*, _object*)
1   copysign_ad_func(paddle::Tensor const&, paddle::Tensor const&)
2   paddle::experimental::copysign(paddle::Tensor const&, paddle::Tensor const&)
3   void phi::CopySignKernel<long, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*)
4   long* phi::DeviceContext::Alloc<long>(phi::TensorBase*, unsigned long, bool) const
5   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 17.000000GB memory on GPU 0, 79.182556GB memory has been allocated and available memory is only 2.375000MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-17 17:19:47.773944 test begin: paddle.signbit(Tensor([25264514, 17, 10],"int16"), )

[paddle error] paddle.signbit(Tensor([25264514, 17, 10],"int16"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::eager_api_sign(_object*, _object*, _object*)
1   sign_ad_func(paddle::Tensor const&)
2   paddle::experimental::sign(paddle::Tensor const&)
3   void phi::SignKernel<short, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*)
4   short* phi::DeviceContext::Alloc<short>(phi::TensorBase*, unsigned long, bool) const
5   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
6   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 8.000000GB memory on GPU 0, 77.280212GB memory has been allocated and available memory is only 1.904663GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-17 17:23:58.200700 test begin: paddle.signbit(Tensor([4294967297],"uint8"), )

/usr/local/lib/python3.9/dist-packages/paddle/tensor/math.py:8242: UserWarning: The shape of broadcast output [1] is different from the input tensor x with shape: [4294967297], please make sure you are using copysign api correctly.
  warnings.warn(
[Pass] paddle.signbit(Tensor([4294967297],"uint8"), )
2025-03-17 17:29:00.161305 test begin: paddle.signbit(Tensor([57042535, 20, 2],"float32"), )

element 0 of tensors does not require grad and does not have a grad_fn
[Pass] paddle.signbit(Tensor([57042535, 20, 2],"float32"), )
2025-03-17 17:32:07.859923 test begin: paddle.signbit(Tensor([71582789, 3, 4, 5],"float16"), )

element 0 of tensors does not require grad and does not have a grad_fn
[Pass] paddle.signbit(Tensor([71582789, 3, 4, 5],"float16"), )
2025-03-17 17:37:42.710727 test begin: paddle.signbit(Tensor([71582789, 3, 4, 5],"int8"), )

[Pass] paddle.signbit(Tensor([71582789, 3, 4, 5],"int8"), )
2025-03-17 17:43:09.020802 test begin: paddle.signbit(Tensor([8, 17, 5, 6, 1052689],"float16"), )

element 0 of tensors does not require grad and does not have a grad_fn
[paddle error] paddle.signbit(Tensor([8, 17, 5, 6, 1052689],"float16"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::pybind::TensorInit(_object*, _object*, _object*)
1   paddle::pybind::AutoInitTensorByPyArray(paddle::pybind::TensorObject*, std::unordered_map<std::string, _object*, std::hash<std::string >, std::equal_to<std::string >, std::allocator<std::pair<std::string const, _object*> > >, _object*, bool, long)
2   phi::DenseTensor::mutable_data(phi::Place const&, phi::DataType, unsigned long)
3   phi::memory_utils::AllocShared(phi::Place const&, unsigned long)
4   paddle::memory::AllocShared(phi::Place const&, unsigned long)
5   paddle::memory::allocation::AllocatorFacade::AllocShared(phi::Place const&, unsigned long)
6   paddle::memory::allocation::AllocatorFacade::Alloc(phi::Place const&, unsigned long)
7   paddle::memory::allocation::Allocator::Allocate(unsigned long)
8   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
9   std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
10  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 8.000007GB memory on GPU 0, 77.305603GB memory has been allocated and available memory is only 1.879272GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-17 17:47:39.560249 test begin: paddle.signbit(Tensor([8, 17, 5, 902305, 7],"float16"), )

element 0 of tensors does not require grad and does not have a grad_fn
[Pass] paddle.signbit(Tensor([8, 17, 5, 902305, 7],"float16"), )
2025-03-17 17:53:25.157860 test begin: paddle.signbit(Tensor([8, 17, 751921, 6, 7],"float16"), )

element 0 of tensors does not require grad and does not have a grad_fn
[Pass] paddle.signbit(Tensor([8, 17, 751921, 6, 7],"float16"), )
2025-03-17 17:58:56.274569 test begin: paddle.signbit(Tensor([8, 2556529, 5, 6, 7],"float16"), )

element 0 of tensors does not require grad and does not have a grad_fn
[Pass] paddle.signbit(Tensor([8, 2556529, 5, 6, 7],"float16"), )
2025-03-17 18:04:29.593229 test begin: paddle.sin(Tensor([1, 128, 1, 17825793],"float32"), )

[Pass] paddle.sin(Tensor([1, 128, 1, 17825793],"float32"), )
2025-03-17 18:07:46.180664 test begin: paddle.sin(Tensor([1, 128, 557057, 32],"float32"), )

[Pass] paddle.sin(Tensor([1, 128, 557057, 32],"float32"), )
2025-03-17 18:10:59.289176 test begin: paddle.sin(Tensor([1, 17825793, 1, 128],"float32"), )

[Pass] paddle.sin(Tensor([1, 17825793, 1, 128],"float32"), )
2025-03-17 18:14:35.636787 test begin: paddle.sin(Tensor([1, 2281701379, 1],"float32"), )

[Pass] paddle.sin(Tensor([1, 2281701379, 1],"float32"), )
2025-03-17 18:17:41.055566 test begin: paddle.sin(Tensor([1, 2281701379],"float32"), )

[Pass] paddle.sin(Tensor([1, 2281701379],"float32"), )
2025-03-17 18:20:50.443210 test begin: paddle.sin(Tensor([1, 25, 91268056],"float32"), )

[Pass] paddle.sin(Tensor([1, 25, 91268056],"float32"), )
2025-03-17 18:24:21.512703 test begin: paddle.sin(Tensor([1, 35651585, 1, 64],"float32"), )

[Pass] paddle.sin(Tensor([1, 35651585, 1, 64],"float32"), )
2025-03-17 18:27:36.788090 test begin: paddle.sin(Tensor([1, 4096, 1, 557057],"float32"), )

[Pass] paddle.sin(Tensor([1, 4096, 1, 557057],"float32"), )
2025-03-17 18:30:41.521597 test begin: paddle.sin(Tensor([1, 4096, 4353, 128],"float32"), )

[Pass] paddle.sin(Tensor([1, 4096, 4353, 128],"float32"), )
2025-03-17 18:34:03.882101 test begin: paddle.sin(Tensor([1, 4096, 8705, 64],"float32"), )

[Pass] paddle.sin(Tensor([1, 4096, 8705, 64],"float32"), )
2025-03-17 18:37:02.700307 test begin: paddle.sin(Tensor([1, 71303169, 1, 32],"float32"), )

[Pass] paddle.sin(Tensor([1, 71303169, 1, 32],"float32"), )
2025-03-17 18:40:05.080306 test begin: paddle.sin(Tensor([10, 20, 11408507],"float32"), )

[Pass] paddle.sin(Tensor([10, 20, 11408507],"float32"), )
2025-03-17 18:43:19.605104 test begin: paddle.sin(Tensor([10, 228170138, 1],"float32"), )

[Pass] paddle.sin(Tensor([10, 228170138, 1],"float32"), )
2025-03-17 18:47:08.364233 test begin: paddle.sin(Tensor([10, 228170138],"float32"), )

[Pass] paddle.sin(Tensor([10, 228170138],"float32"), )
2025-03-17 18:50:27.803206 test begin: paddle.sin(Tensor([100, 22817014],"float32"), )

[Pass] paddle.sin(Tensor([100, 22817014],"float32"), )
2025-03-17 18:54:38.857963 test begin: paddle.sin(Tensor([114085069, 20, 1],"float32"), )

[Pass] paddle.sin(Tensor([114085069, 20, 1],"float32"), )
2025-03-17 18:58:34.291756 test begin: paddle.sin(Tensor([1140850690, 2],"float32"), )

[Pass] paddle.sin(Tensor([1140850690, 2],"float32"), )
2025-03-17 19:02:10.490499 test begin: paddle.sin(Tensor([14449, 157920, 1],"float32"), )

[Pass] paddle.sin(Tensor([14449, 157920, 1],"float32"), )
2025-03-17 19:06:20.720250 test begin: paddle.sin(Tensor([17825793, 128],"float32"), )

[Pass] paddle.sin(Tensor([17825793, 128],"float32"), )
2025-03-17 19:10:14.699466 test begin: paddle.sin(Tensor([190141782, 3, 4],"float32"), )

[Pass] paddle.sin(Tensor([190141782, 3, 4],"float32"), )
2025-03-17 19:14:08.252453 test begin: paddle.sin(Tensor([2, 107136, 10649],"float32"), )

[Pass] paddle.sin(Tensor([2, 107136, 10649],"float32"), )
2025-03-17 19:18:35.694337 test begin: paddle.sin(Tensor([2, 1140850690, 1],"float32"), )

[Pass] paddle.sin(Tensor([2, 1140850690, 1],"float32"), )
2025-03-17 19:22:26.109468 test begin: paddle.sin(Tensor([2, 285212673, 4],"float32"), )

CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 3.98 GiB is free. Process 136480 has 27.09 GiB memory in use. Process 62167 has 5.06 GiB memory in use. Process 70540 has 43.03 GiB memory in use. Of the allocated memory 25.50 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-17 19:22:54.233485 test begin: paddle.reshape(Tensor([1140850690, 2],"float32"), list[-1,], )

W0317 19:24:57.518002 103722 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0317 19:24:57.519214 103722 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.reshape(Tensor([1140850690, 2],"float32"), list[-1,], )
2025-03-17 19:27:31.221557 test begin: paddle.reshape(Tensor([1140850690, 2],"float32"), shape=list[1,20,2,], )

[torch error] paddle.reshape(Tensor([1140850690, 2],"float32"), shape=list[1,20,2,], ) 
 shape '[1, 20, 2]' is invalid for input of size 2281701380
2025-03-17 19:27:35.957962 test begin: paddle.reshape(Tensor([1140850690, 2],"float32"), shape=list[3200,1,2,], )

[torch error] paddle.reshape(Tensor([1140850690, 2],"float32"), shape=list[3200,1,2,], ) 
 shape '[3200, 1, 2]' is invalid for input of size 2281701380
2025-03-17 19:27:38.551818 test begin: paddle.reshape(Tensor([1140850690, 2],"float32"), shape=list[40,], )

[torch error] paddle.reshape(Tensor([1140850690, 2],"float32"), shape=list[40,], ) 
 shape '[40]' is invalid for input of size 2281701380
2025-03-17 19:27:40.617069 test begin: paddle.reshape(Tensor([1140850690, 2],"float32"), tuple(2,4,), )

[torch error] paddle.reshape(Tensor([1140850690, 2],"float32"), tuple(2,4,), ) 
 shape '[2, 4]' is invalid for input of size 2281701380
2025-03-17 19:27:42.739750 test begin: paddle.reshape(Tensor([1140850690, 2],"int64"), list[14,2,4,2,], name="Categorical_sample", )

[torch error] paddle.reshape(Tensor([1140850690, 2],"int64"), list[14,2,4,2,], name="Categorical_sample", ) 
 shape '[14, 2, 4, 2]' is invalid for input of size 2281701380
2025-03-17 19:28:42.566437 test begin: paddle.reshape(Tensor([1140851, 10, 10, 20],"float32"), list[10,100,20,], )

[torch error] paddle.reshape(Tensor([1140851, 10, 10, 20],"float32"), list[10,100,20,], ) 
 shape '[10, 100, 20]' is invalid for input of size 2281702000
2025-03-17 19:28:45.815524 test begin: paddle.reshape(Tensor([115457, 100, 124, 3],"float16"), shape=tuple(4,-1,1,), )

[Pass] paddle.reshape(Tensor([115457, 100, 124, 3],"float16"), shape=tuple(4,-1,1,), )
2025-03-17 19:45:52.443734 test begin: paddle.reshape(Tensor([116, 121795, 304],"float16"), shape=tuple(116,-1,), )

[Pass] paddle.reshape(Tensor([116, 121795, 304],"float16"), shape=tuple(116,-1,), )
2025-03-17 20:01:53.652365 test begin: paddle.reshape(Tensor([116, 125087, 296],"float16"), shape=tuple(116,-1,), )

[Pass] paddle.reshape(Tensor([116, 125087, 296],"float16"), shape=tuple(116,-1,), )
2025-03-17 20:17:56.052651 test begin: paddle.reshape(Tensor([116, 136124, 272],"float16"), shape=tuple(116,-1,), )

[Pass] paddle.reshape(Tensor([116, 136124, 272],"float16"), shape=tuple(116,-1,), )
2025-03-17 20:33:53.965272 test begin: paddle.reshape(Tensor([116, 200, 185128],"float16"), shape=tuple(116,-1,), )

[Pass] paddle.reshape(Tensor([116, 200, 185128],"float16"), shape=tuple(116,-1,), )
2025-03-17 20:49:57.898260 test begin: paddle.reshape(Tensor([116, 66453, 296],"float32"), shape=tuple(116,-1,), )

[Pass] paddle.reshape(Tensor([116, 66453, 296],"float32"), shape=tuple(116,-1,), )
2025-03-17 20:53:14.724851 test begin: paddle.reshape(Tensor([1164134, 40, 49],"float32"), list[1960,], )

[torch error] paddle.reshape(Tensor([1164134, 40, 49],"float32"), list[1960,], ) 
 shape '[1960]' is invalid for input of size 2281702640
2025-03-17 20:53:18.869418 test begin: paddle.reshape(Tensor([1165, 35, 56000],"float32"), shape=tuple(-1,200,280,), )

[Pass] paddle.reshape(Tensor([1165, 35, 56000],"float32"), shape=tuple(-1,200,280,), )
2025-03-17 20:56:43.416835 test begin: paddle.reshape(Tensor([116509, 144, 256],"float16"), shape=tuple(-1,256,), )

[Pass] paddle.reshape(Tensor([116509, 144, 256],"float16"), shape=tuple(-1,256,), )
2025-03-17 21:13:54.379258 test begin: paddle.reshape(Tensor([116509, 256, 12, 12],"float16"), shape=tuple(4,256,-1,), )

[Pass] paddle.reshape(Tensor([116509, 256, 12, 12],"float16"), shape=tuple(4,256,-1,), )
2025-03-17 21:30:24.827730 test begin: paddle.reshape(Tensor([1168, 25, 264, 296],"float32"), shape=list[-1,264,296,], )

[Pass] paddle.reshape(Tensor([1168, 25, 264, 296],"float32"), shape=list[-1,264,296,], )
2025-03-17 21:33:30.595078 test begin: paddle.reshape(Tensor([1170, 22, 88704],"float32"), shape=tuple(-1,264,336,), )

[Pass] paddle.reshape(Tensor([1170, 22, 88704],"float32"), shape=tuple(-1,264,336,), )
2025-03-17 21:36:41.855268 test begin: paddle.reshape(Tensor([1173, 20, 320, 304],"float32"), shape=list[-1,320,304,], )

[Pass] paddle.reshape(Tensor([1173, 20, 320, 304],"float32"), shape=list[-1,320,304,], )
2025-03-17 21:39:50.632906 test begin: paddle.reshape(Tensor([11751, 2, 97088],"float32"), shape=tuple(-1,328,296,), )

[Pass] paddle.reshape(Tensor([11751, 2, 97088],"float32"), shape=tuple(-1,328,296,), )
2025-03-17 21:43:10.562492 test begin: paddle.reshape(Tensor([1176, 20, 328, 296],"float32"), shape=list[-1,328,296,], )

[Pass] paddle.reshape(Tensor([1176, 20, 328, 296],"float32"), shape=list[-1,328,296,], )
2025-03-17 21:46:30.636012 test begin: paddle.reshape(Tensor([1185, 22, 304, 288],"float32"), shape=list[-1,304,288,], )

[Pass] paddle.reshape(Tensor([1185, 22, 304, 288],"float32"), shape=list[-1,304,288,], )
2025-03-17 21:50:02.922972 test begin: paddle.reshape(Tensor([1185, 24, 264, 304],"float32"), shape=list[-1,264,304,], )

[Pass] paddle.reshape(Tensor([1185, 24, 264, 304],"float32"), shape=list[-1,264,304,], )
2025-03-17 21:53:08.761826 test begin: paddle.reshape(Tensor([11883862, 192],"float32"), list[1,64,3,], )

[torch error] paddle.reshape(Tensor([11883862, 192],"float32"), list[1,64,3,], ) 
 shape '[1, 64, 3]' is invalid for input of size 2281701504
2025-03-17 21:53:13.110649 test begin: paddle.reshape(Tensor([11883862, 192],"float32"), list[128,64,3,], )

[torch error] paddle.reshape(Tensor([11883862, 192],"float32"), list[128,64,3,], ) 
 shape '[128, 64, 3]' is invalid for input of size 2281701504
2025-03-17 21:53:16.336100 test begin: paddle.reshape(Tensor([11883862, 192],"float32"), list[64,64,3,], )

[torch error] paddle.reshape(Tensor([11883862, 192],"float32"), list[64,64,3,], ) 
 shape '[64, 64, 3]' is invalid for input of size 2281701504
2025-03-17 21:53:18.218759 test begin: paddle.reshape(Tensor([11883862, 64, 3],"float32"), tuple(64,-1,), )

[Pass] paddle.reshape(Tensor([11883862, 64, 3],"float32"), tuple(64,-1,), )
2025-03-17 21:56:43.490580 test begin: paddle.reshape(Tensor([1188387, 10, 16, 12],"float32"), shape=tuple(1,-1,4,), )

[Pass] paddle.reshape(Tensor([1188387, 10, 16, 12],"float32"), shape=tuple(1,-1,4,), )
2025-03-17 21:59:59.225603 test begin: paddle.reshape(Tensor([1189, 17, 336, 336],"float32"), shape=list[-1,336,336,], )

[Pass] paddle.reshape(Tensor([1189, 17, 336, 336],"float32"), shape=list[-1,336,336,], )
2025-03-17 22:03:03.163468 test begin: paddle.reshape(Tensor([1192, 22, 272, 320],"float32"), shape=list[-1,272,320,], )

[Pass] paddle.reshape(Tensor([1192, 22, 272, 320],"float32"), shape=list[-1,272,320,], )
2025-03-17 22:06:05.198399 test begin: paddle.reshape(Tensor([119304648, 4, 3, 3],"float16"), list[-1,3,], )

[Pass] paddle.reshape(Tensor([119304648, 4, 3, 3],"float16"), list[-1,3,], )
2025-03-17 22:22:13.392095 test begin: paddle.reshape(Tensor([119305, 100, 120, 3],"float16"), shape=tuple(4,-1,1,), )

[Pass] paddle.reshape(Tensor([119305, 100, 120, 3],"float16"), shape=tuple(4,-1,1,), )
2025-03-17 22:38:08.032800 test begin: paddle.reshape(Tensor([12, 10281, 68, 512],"float16"), list[-1,512,], )

[Pass] paddle.reshape(Tensor([12, 10281, 68, 512],"float16"), list[-1,512,], )
2025-03-17 22:54:19.746153 test begin: paddle.reshape(Tensor([12, 1177349, 304],"float16"), shape=tuple(12,-1,), )

[Pass] paddle.reshape(Tensor([12, 1177349, 304],"float16"), shape=tuple(12,-1,), )
2025-03-17 23:10:47.369634 test begin: paddle.reshape(Tensor([12, 1315861, 272],"float16"), shape=tuple(12,-1,), )

[Pass] paddle.reshape(Tensor([12, 1315861, 272],"float16"), shape=tuple(12,-1,), )
2025-03-17 23:27:07.800639 test begin: paddle.reshape(Tensor([12, 138512, 19, 34, 4],"float16"), list[-1,4,], )

[Pass] paddle.reshape(Tensor([12, 138512, 19, 34, 4],"float16"), list[-1,4,], )
2025-03-17 23:43:25.248870 test begin: paddle.reshape(Tensor([12, 14, 14, 970112],"float32"), list[12,14,14,512,], )

[torch error] paddle.reshape(Tensor([12, 14, 14, 970112],"float32"), list[12,14,14,512,], ) 
 shape '[12, 14, 14, 512]' is invalid for input of size 2281703424
2025-03-17 23:43:29.373603 test begin: paddle.reshape(Tensor([12, 14, 26527, 512],"float32"), list[12,14,14,512,], )

[torch error] paddle.reshape(Tensor([12, 14, 26527, 512],"float32"), list[12,14,14,512,], ) 
 shape '[12, 14, 14, 512]' is invalid for input of size 2281746432
2025-03-17 23:43:31.725790 test begin: paddle.reshape(Tensor([12, 17314, 76, 136, 2],"float16"), list[-1,2,], )

[Pass] paddle.reshape(Tensor([12, 17314, 76, 136, 2],"float16"), list[-1,2,], )
2025-03-17 23:59:42.153803 test begin: paddle.reshape(Tensor([12, 18397, 38, 68, 4],"float32"), list[-1,4,], )

[Pass] paddle.reshape(Tensor([12, 18397, 38, 68, 4],"float32"), list[-1,4,], )
2025-03-18 00:02:41.245884 test begin: paddle.reshape(Tensor([12, 19, 34, 294338],"float32"), list[-1,512,], )

[torch error] paddle.reshape(Tensor([12, 19, 34, 294338],"float32"), list[-1,512,], ) 
 shape '[-1, 512]' is invalid for input of size 2281708176
2025-03-18 00:02:45.504207 test begin: paddle.reshape(Tensor([12, 19, 34, 554047],"float16"), list[-1,512,], )

[torch error] paddle.reshape(Tensor([12, 19, 34, 554047],"float16"), list[-1,512,], ) 
 shape '[-1, 512]' is invalid for input of size 4294972344
2025-03-18 00:02:49.446779 test begin: paddle.reshape(Tensor([12, 19, 36793, 512],"float16"), list[-1,512,], )

[Pass] paddle.reshape(Tensor([12, 19, 36793, 512],"float16"), list[-1,512,], )
2025-03-18 00:18:55.758196 test begin: paddle.reshape(Tensor([12, 190141782],"float32"), list[15,4,], )

[torch error] paddle.reshape(Tensor([12, 190141782],"float32"), list[15,4,], ) 
 shape '[15, 4]' is invalid for input of size 2281701384
2025-03-18 00:19:00.360370 test begin: paddle.reshape(Tensor([12, 190141782],"float32"), shape=list[-1,102,], )

[torch error] paddle.reshape(Tensor([12, 190141782],"float32"), shape=list[-1,102,], ) 
 shape '[-1, 102]' is invalid for input of size 2281701384
2025-03-18 00:19:02.304797 test begin: paddle.reshape(Tensor([12, 190141782],"int64"), list[12,], )

[torch error] paddle.reshape(Tensor([12, 190141782],"int64"), list[12,], ) 
 shape '[12]' is invalid for input of size 2281701384
2025-03-18 00:19:10.183371 test begin: paddle.reshape(Tensor([12, 190141782],"int64"), list[15,4,], )

[torch error] paddle.reshape(Tensor([12, 190141782],"int64"), list[15,4,], ) 
 shape '[15, 4]' is invalid for input of size 2281701384
2025-03-18 00:19:13.177550 test begin: paddle.reshape(Tensor([12, 200, 1789570],"float16"), shape=tuple(12,-1,), )

[Pass] paddle.reshape(Tensor([12, 200, 1789570],"float16"), shape=tuple(12,-1,), )
2025-03-18 00:35:32.993659 test begin: paddle.reshape(Tensor([12, 20561, 34, 512],"float16"), list[-1,512,], )

[Pass] paddle.reshape(Tensor([12, 20561, 34, 512],"float16"), list[-1,512,], )
2025-03-18 00:52:19.270925 test begin: paddle.reshape(Tensor([12, 208, 1720741],"float16"), shape=tuple(12,-1,), )

[Pass] paddle.reshape(Tensor([12, 208, 1720741],"float16"), shape=tuple(12,-1,), )
2025-03-18 01:08:36.773036 test begin: paddle.reshape(Tensor([12, 208, 914144],"float32"), shape=tuple(12,-1,), )

[Pass] paddle.reshape(Tensor([12, 208, 914144],"float32"), shape=tuple(12,-1,), )
2025-03-18 01:12:34.673005 test begin: paddle.reshape(Tensor([12, 26527, 14, 512],"float32"), list[12,14,14,512,], )

[torch error] paddle.reshape(Tensor([12, 26527, 14, 512],"float32"), list[12,14,14,512,], ) 
 shape '[12, 14, 14, 512]' is invalid for input of size 2281746432
2025-03-18 01:12:39.078411 test begin: paddle.reshape(Tensor([12, 26527, 28, 256],"float32"), list[12,28,28,256,], )

[torch error] paddle.reshape(Tensor([12, 26527, 28, 256],"float32"), list[12,28,28,256,], ) 
 shape '[12, 28, 28, 256]' is invalid for input of size 2281746432
2025-03-18 01:12:41.375322 test begin: paddle.reshape(Tensor([12, 26527, 56, 128],"float32"), list[12,56,56,128,], )

[torch error] paddle.reshape(Tensor([12, 26527, 56, 128],"float32"), list[12,56,56,128,], ) 
 shape '[12, 56, 56, 128]' is invalid for input of size 2281746432
2025-03-18 01:12:42.627273 test begin: paddle.reshape(Tensor([12, 272, 1315861],"float16"), shape=tuple(12,-1,), )

[Pass] paddle.reshape(Tensor([12, 272, 1315861],"float16"), shape=tuple(12,-1,), )
2025-03-18 01:29:14.262759 test begin: paddle.reshape(Tensor([12, 277024, 19, 34, 2],"float16"), list[-1,2,], )

[Pass] paddle.reshape(Tensor([12, 277024, 19, 34, 2],"float16"), list[-1,2,], )
2025-03-18 01:45:45.646631 test begin: paddle.reshape(Tensor([12, 28, 26527, 256],"float32"), list[12,28,28,256,], )

[torch error] paddle.reshape(Tensor([12, 28, 26527, 256],"float32"), list[12,28,28,256,], ) 
 shape '[12, 28, 28, 256]' is invalid for input of size 2281746432
2025-03-18 01:45:49.696135 test begin: paddle.reshape(Tensor([12, 28, 28, 242528],"float32"), list[12,28,28,256,], )

[torch error] paddle.reshape(Tensor([12, 28, 28, 242528],"float32"), list[12,28,28,256,], ) 
 shape '[12, 28, 28, 256]' is invalid for input of size 2281703424
2025-03-18 01:45:50.876466 test begin: paddle.reshape(Tensor([12, 34628, 38, 68, 4],"float16"), list[-1,4,], )

[Pass] paddle.reshape(Tensor([12, 34628, 38, 68, 4],"float16"), list[-1,4,], )
2025-03-18 02:02:39.560308 test begin: paddle.reshape(Tensor([12, 357913942],"float16"), shape=list[-1,3,], )

[Pass] paddle.reshape(Tensor([12, 357913942],"float16"), shape=list[-1,3,], )
2025-03-18 02:19:25.098937 test begin: paddle.reshape(Tensor([12, 36793, 38, 68, 2],"float32"), list[-1,2,], )

[Pass] paddle.reshape(Tensor([12, 36793, 38, 68, 2],"float32"), list[-1,2,], )
2025-03-18 02:23:13.850766 test begin: paddle.reshape(Tensor([12, 38, 18397, 512],"float16"), list[-1,512,], )

[Pass] paddle.reshape(Tensor([12, 38, 18397, 512],"float16"), list[-1,512,], )
2025-03-18 02:39:24.957927 test begin: paddle.reshape(Tensor([12, 38, 68, 138512],"float16"), list[-1,512,], )

[Pass] paddle.reshape(Tensor([12, 38, 68, 138512],"float16"), list[-1,512,], )
2025-03-18 02:55:22.305152 test begin: paddle.reshape(Tensor([12, 38, 68, 73585],"float32"), list[-1,512,], )

[torch error] paddle.reshape(Tensor([12, 38, 68, 73585],"float32"), list[-1,512,], ) 
 shape '[-1, 512]' is invalid for input of size 2281723680
2025-03-18 02:55:26.657175 test begin: paddle.reshape(Tensor([12, 38, 9773, 512],"float32"), list[-1,512,], )

[Pass] paddle.reshape(Tensor([12, 38, 9773, 512],"float32"), list[-1,512,], )
2025-03-18 02:59:10.358018 test begin: paddle.reshape(Tensor([12, 4, 1315861, 34, 2],"float16"), list[-1,2,], )

[Pass] paddle.reshape(Tensor([12, 4, 1315861, 34, 2],"float16"), list[-1,2,], )
2025-03-18 03:15:28.401231 test begin: paddle.reshape(Tensor([12, 4, 174763, 68, 4],"float32"), list[-1,4,], )

[Pass] paddle.reshape(Tensor([12, 4, 174763, 68, 4],"float32"), list[-1,4,], )
2025-03-18 03:18:43.684746 test begin: paddle.reshape(Tensor([12, 4, 19, 1177349, 4],"float16"), list[-1,4,], )

[Pass] paddle.reshape(Tensor([12, 4, 19, 1177349, 4],"float16"), list[-1,4,], )
2025-03-18 03:34:50.764908 test begin: paddle.reshape(Tensor([12, 4, 19, 2354697, 2],"float16"), list[-1,2,], )

[Pass] paddle.reshape(Tensor([12, 4, 19, 2354697, 2],"float16"), list[-1,2,], )
2025-03-18 03:51:02.454927 test begin: paddle.reshape(Tensor([12, 4, 19, 34, 138512],"float16"), list[-1,2,], )

[Pass] paddle.reshape(Tensor([12, 4, 19, 34, 138512],"float16"), list[-1,2,], )
2025-03-18 04:06:59.822545 test begin: paddle.reshape(Tensor([12, 4, 19, 34, 138512],"float16"), list[-1,4,], )

[Pass] paddle.reshape(Tensor([12, 4, 19, 34, 138512],"float16"), list[-1,4,], )
2025-03-18 04:23:01.803886 test begin: paddle.reshape(Tensor([12, 4, 19, 34, 73585],"float32"), list[-1,4,], )

[Pass] paddle.reshape(Tensor([12, 4, 19, 34, 73585],"float32"), list[-1,4,], )
2025-03-18 04:26:48.896515 test begin: paddle.reshape(Tensor([12, 4, 19, 625467, 4],"float32"), list[-1,4,], )

[Pass] paddle.reshape(Tensor([12, 4, 19, 625467, 4],"float32"), list[-1,4,], )
2025-03-18 04:30:41.626571 test begin: paddle.reshape(Tensor([12, 4, 328966, 136, 2],"float16"), list[-1,2,], )

[Pass] paddle.reshape(Tensor([12, 4, 328966, 136, 2],"float16"), list[-1,2,], )
2025-03-18 04:47:07.473373 test begin: paddle.reshape(Tensor([12, 4, 328966, 68, 4],"float16"), list[-1,4,], )

[Pass] paddle.reshape(Tensor([12, 4, 328966, 68, 4],"float16"), list[-1,4,], )
2025-03-18 05:03:55.328918 test begin: paddle.reshape(Tensor([12, 4, 349526, 34, 4],"float32"), list[-1,4,], )

[Pass] paddle.reshape(Tensor([12, 4, 349526, 34, 4],"float32"), list[-1,4,], )
2025-03-18 05:07:56.140625 test begin: paddle.reshape(Tensor([12, 4, 349526, 68, 2],"float32"), list[-1,2,], )

[Pass] paddle.reshape(Tensor([12, 4, 349526, 68, 2],"float32"), list[-1,2,], )
2025-03-18 05:11:30.590420 test begin: paddle.reshape(Tensor([12, 4, 38, 1177349, 2],"float16"), list[-1,2,], )

[Pass] paddle.reshape(Tensor([12, 4, 38, 1177349, 2],"float16"), list[-1,2,], )
2025-03-18 05:27:39.529677 test begin: paddle.reshape(Tensor([12, 4, 38, 312734, 4],"float32"), list[-1,4,], )

[Pass] paddle.reshape(Tensor([12, 4, 38, 312734, 4],"float32"), list[-1,4,], )
2025-03-18 05:31:29.522906 test begin: paddle.reshape(Tensor([12, 4, 38, 588675, 4],"float16"), list[-1,4,], )

[Pass] paddle.reshape(Tensor([12, 4, 38, 588675, 4],"float16"), list[-1,4,], )
2025-03-18 05:48:18.531973 test begin: paddle.reshape(Tensor([12, 4, 38, 625467, 2],"float32"), list[-1,2,], )

[Pass] paddle.reshape(Tensor([12, 4, 38, 625467, 2],"float32"), list[-1,2,], )
2025-03-18 05:51:51.375741 test begin: paddle.reshape(Tensor([12, 4, 38, 68, 18397],"float32"), list[-1,2,], )

[Pass] paddle.reshape(Tensor([12, 4, 38, 68, 18397],"float32"), list[-1,2,], )
2025-03-18 05:55:08.995705 test begin: paddle.reshape(Tensor([12, 4, 38, 68, 18397],"float32"), list[-1,4,], )

[Pass] paddle.reshape(Tensor([12, 4, 38, 68, 18397],"float32"), list[-1,4,], )
2025-03-18 05:59:14.072505 test begin: paddle.reshape(Tensor([12, 4, 38, 68, 34628],"float16"), list[-1,2,], )

[Pass] paddle.reshape(Tensor([12, 4, 38, 68, 34628],"float16"), list[-1,2,], )
2025-03-18 06:16:41.748116 test begin: paddle.reshape(Tensor([12, 4, 38, 68, 34628],"float16"), list[-1,4,], )

[Pass] paddle.reshape(Tensor([12, 4, 38, 68, 34628],"float16"), list[-1,4,], )
2025-03-18 06:33:59.577102 test begin: paddle.reshape(Tensor([12, 4, 657931, 34, 4],"float16"), list[-1,4,], )

[Pass] paddle.reshape(Tensor([12, 4, 657931, 34, 4],"float16"), list[-1,4,], )
2025-03-18 06:50:43.650839 test begin: paddle.reshape(Tensor([12, 4, 657931, 68, 2],"float16"), list[-1,2,], )

[Pass] paddle.reshape(Tensor([12, 4, 657931, 68, 2],"float16"), list[-1,2,], )
2025-03-18 07:06:57.646859 test begin: paddle.reshape(Tensor([12, 4, 76, 136, 8657],"float16"), list[-1,2,], )

[Pass] paddle.reshape(Tensor([12, 4, 76, 136, 8657],"float16"), list[-1,2,], )
2025-03-18 07:23:05.624760 test begin: paddle.reshape(Tensor([12, 4, 76, 588675, 2],"float16"), list[-1,2,], )

[Pass] paddle.reshape(Tensor([12, 4, 76, 588675, 2],"float16"), list[-1,2,], )
2025-03-18 07:39:37.390581 test begin: paddle.reshape(Tensor([12, 5141, 136, 512],"float16"), list[-1,512,], )

[Pass] paddle.reshape(Tensor([12, 5141, 136, 512],"float16"), list[-1,512,], )
2025-03-18 07:56:00.619330 test begin: paddle.reshape(Tensor([12, 5462, 68, 512],"float32"), list[-1,512,], )

[Pass] paddle.reshape(Tensor([12, 5462, 68, 512],"float32"), list[-1,512,], )
2025-03-18 07:59:49.502622 test begin: paddle.reshape(Tensor([12, 56, 26527, 128],"float32"), list[12,56,56,128,], )

[torch error] paddle.reshape(Tensor([12, 56, 26527, 128],"float32"), list[12,56,56,128,], ) 
 shape '[12, 56, 56, 128]' is invalid for input of size 2281746432
2025-03-18 07:59:53.741559 test begin: paddle.reshape(Tensor([12, 56, 56, 60632],"float32"), list[12,56,56,128,], )

[torch error] paddle.reshape(Tensor([12, 56, 56, 60632],"float32"), list[12,56,56,128,], ) 
 shape '[12, 56, 56, 128]' is invalid for input of size 2281703424
2025-03-18 07:59:55.368877 test begin: paddle.reshape(Tensor([12, 69256, 38, 68, 2],"float16"), list[-1,2,], )

[Pass] paddle.reshape(Tensor([12, 69256, 38, 68, 2],"float16"), list[-1,2,], )
2025-03-18 08:17:01.223673 test begin: paddle.reshape(Tensor([12, 699051, 272],"float32"), shape=tuple(12,-1,), )

[Pass] paddle.reshape(Tensor([12, 699051, 272],"float32"), shape=tuple(12,-1,), )
2025-03-18 08:21:06.317832 test begin: paddle.reshape(Tensor([12, 73585, 19, 34, 4],"float32"), list[-1,4,], )

[Pass] paddle.reshape(Tensor([12, 73585, 19, 34, 4],"float32"), list[-1,4,], )
2025-03-18 08:24:36.588992 test begin: paddle.reshape(Tensor([12, 76, 136, 34628],"float16"), list[-1,512,], )

[Pass] paddle.reshape(Tensor([12, 76, 136, 34628],"float16"), list[-1,512,], )
2025-03-18 08:40:43.525696 test begin: paddle.reshape(Tensor([12, 76, 9199, 512],"float16"), list[-1,512,], )

[Pass] paddle.reshape(Tensor([12, 76, 9199, 512],"float16"), list[-1,512,], )
2025-03-18 08:57:44.947632 test begin: paddle.reshape(Tensor([12, 845075, 15, 15],"float32"), shape=list[12,32,-1,], )

[torch error] paddle.reshape(Tensor([12, 845075, 15, 15],"float32"), shape=list[12,32,-1,], ) 
 shape '[12, 32, -1]' is invalid for input of size 2281702500
2025-03-18 08:57:49.025886 test begin: paddle.reshape(Tensor([120, 117735, 304],"float16"), shape=tuple(120,-1,), )

[Pass] paddle.reshape(Tensor([120, 117735, 304],"float16"), shape=tuple(120,-1,), )
2025-03-18 09:14:39.218694 test begin: paddle.reshape(Tensor([120, 124276, 288],"float16"), shape=tuple(120,-1,), )

[Pass] paddle.reshape(Tensor([120, 124276, 288],"float16"), shape=tuple(120,-1,), )
2025-03-18 09:31:11.964238 test begin: paddle.reshape(Tensor([120, 131587, 272],"float16"), shape=tuple(120,-1,), )

[Pass] paddle.reshape(Tensor([120, 131587, 272],"float16"), shape=tuple(120,-1,), )
2025-03-18 09:48:01.052954 test begin: paddle.reshape(Tensor([120, 200, 178957],"float16"), shape=tuple(120,-1,), )

[accuracy error] paddle.reshape(Tensor([120, 200, 178957],"float16"), shape=tuple(120,-1,), ) 
 Unable to allocate 8.00 GiB for an array with shape (4294968000,) and data type float16
2025-03-18 09:54:09.545518 test begin: paddle.reshape(Tensor([120, 66022, 288],"float32"), shape=tuple(120,-1,), )

[accuracy error] paddle.reshape(Tensor([120, 66022, 288],"float32"), shape=tuple(120,-1,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y -inf location mismatch:
 x: array([[-0.36895 ,  0.287748,  0.378264, ..., -0.275839,  0.140963,
        -0.025245],
       [-0.352937, -0.021749,  0.381437, ..., -0.074022, -0.163108,...
 y: array([[-0.36895 ,  0.287748,  0.378264, ..., -0.275839,  0.140963,
        -0.025245],
       [-0.352937, -0.021749,  0.381437, ..., -0.074022, -0.163108,...
2025-03-18 09:55:15.605246 test begin: paddle.reshape(Tensor([12029, 2, 312, 304],"float32"), shape=list[-1,312,304,], )

[Pass] paddle.reshape(Tensor([12029, 2, 312, 304],"float32"), shape=list[-1,312,304,], )
2025-03-18 09:59:17.903075 test begin: paddle.reshape(Tensor([121264, 12, 2, 28, 28],"float32"), shape=list[2,24,28,28,], )

[torch error] paddle.reshape(Tensor([121264, 12, 2, 28, 28],"float32"), shape=list[2,24,28,28,], ) 
 shape '[2, 24, 28, 28]' is invalid for input of size 2281703424
2025-03-18 09:59:22.227980 test begin: paddle.reshape(Tensor([121264, 24, 28, 28],"float32"), shape=list[2,2,12,28,28,], )

[torch error] paddle.reshape(Tensor([121264, 24, 28, 28],"float32"), shape=list[2,2,12,28,28,], ) 
 shape '[2, 2, 12, 28, 28]' is invalid for input of size 2281703424
2025-03-18 09:59:24.125156 test begin: paddle.reshape(Tensor([121264, 48, 2, 14, 14],"float32"), shape=list[2,96,14,14,], )

[torch error] paddle.reshape(Tensor([121264, 48, 2, 14, 14],"float32"), shape=list[2,96,14,14,], ) 
 shape '[2, 96, 14, 14]' is invalid for input of size 2281703424
2025-03-18 09:59:26.032427 test begin: paddle.reshape(Tensor([121264, 96, 14, 14],"float32"), shape=list[2,2,48,14,14,], )

[torch error] paddle.reshape(Tensor([121264, 96, 14, 14],"float32"), shape=list[2,2,48,14,14,], ) 
 shape '[2, 2, 48, 14, 14]' is invalid for input of size 2281703424
2025-03-18 09:59:27.638339 test begin: paddle.reshape(Tensor([1222, 24, 256, 304],"float32"), shape=list[-1,256,304,], )

[accuracy error] paddle.reshape(Tensor([1222, 24, 256, 304],"float32"), shape=list[-1,256,304,], ) 
 Unable to allocate 2.13 GiB for an array with shape (2282422272,) and data type bool
2025-03-18 10:00:55.960334 test begin: paddle.reshape(Tensor([1228, 22, 264, 320],"float32"), shape=list[-1,264,320,], )

[accuracy error] paddle.reshape(Tensor([1228, 22, 264, 320],"float32"), shape=list[-1,264,320,], ) 
 Unable to allocate 8.50 GiB for an array with shape (2282311680,) and data type float32
2025-03-18 10:02:24.750746 test begin: paddle.reshape(Tensor([1236, 20, 312, 296],"float32"), shape=list[-1,312,296,], )

[accuracy error] paddle.reshape(Tensor([1236, 20, 312, 296],"float32"), shape=list[-1,312,296,], ) 
 Unable to allocate 8.50 GiB for an array with shape (2282941440,) and data type float32
2025-03-18 10:04:04.947633 test begin: paddle.reshape(Tensor([12380, 128, 120, 12],"float32"), shape=tuple(2,-1,4,), )

[accuracy error] paddle.reshape(Tensor([12380, 128, 120, 12],"float32"), shape=tuple(2,-1,4,), ) 
 Unable to allocate 8.50 GiB for an array with shape (2281881600,) and data type float32
2025-03-18 10:05:26.532750 test begin: paddle.reshape(Tensor([124, 111016, 312],"float16"), shape=tuple(124,-1,), )

[accuracy error] paddle.reshape(Tensor([124, 111016, 312],"float16"), shape=tuple(124,-1,), ) 
 (ResourceExhausted) Fail to alloc memory of 8589974016 size, error code is 12.
  [Hint: Expected error == 0, but received error:12 != 0:0.] (at ../paddle/phi/core/memory/allocation/cpu_allocator.cc:48)

2025-03-18 10:05:52.646485 test begin: paddle.reshape(Tensor([124, 113937, 304],"float16"), shape=tuple(124,-1,), )

2025-03-18 10:06:04.293082 test begin: paddle.reshape(Tensor([124, 127342, 272],"float16"), shape=tuple(124,-1,), )

2025-03-18 10:06:07.660506 test begin: paddle.reshape(Tensor([124, 14, 14, 93882],"float32"), list[124,14,14,384,], )

[torch error] paddle.reshape(Tensor([124, 14, 14, 93882],"float32"), list[124,14,14,384,], ) 
 shape '[124, 14, 14, 384]' is invalid for input of size 2281708128
2025-03-18 10:06:11.466877 test begin: paddle.reshape(Tensor([124, 14, 14, 93882],"float32"), list[124,14,14,768,], )

[torch error] paddle.reshape(Tensor([124, 14, 14, 93882],"float32"), list[124,14,14,768,], ) 
 shape '[124, 14, 14, 768]' is invalid for input of size 2281708128
2025-03-18 10:06:12.947043 test begin: paddle.reshape(Tensor([124, 14, 1712, 768],"float32"), list[124,14,14,768,], )

[torch error] paddle.reshape(Tensor([124, 14, 1712, 768],"float32"), list[124,14,14,768,], ) 
 shape '[124, 14, 14, 768]' is invalid for input of size 2282520576
2025-03-18 10:06:14.832208 test begin: paddle.reshape(Tensor([124, 14, 3423, 384],"float32"), list[124,14,14,384,], )

[torch error] paddle.reshape(Tensor([124, 14, 3423, 384],"float32"), list[124,14,14,384,], ) 
 shape '[124, 14, 14, 384]' is invalid for input of size 2281853952
2025-03-18 10:06:16.894196 test begin: paddle.reshape(Tensor([124, 1712, 14, 768],"float32"), list[124,14,14,768,], )

[torch error] paddle.reshape(Tensor([124, 1712, 14, 768],"float32"), list[124,14,14,768,], ) 
 shape '[124, 14, 14, 768]' is invalid for input of size 2282520576
2025-03-18 10:06:18.400113 test begin: paddle.reshape(Tensor([124, 1712, 28, 384],"float32"), list[124,28,28,384,], )

[torch error] paddle.reshape(Tensor([124, 1712, 28, 384],"float32"), list[124,28,28,384,], ) 
 shape '[124, 28, 28, 384]' is invalid for input of size 2282520576
2025-03-18 10:06:20.265167 test begin: paddle.reshape(Tensor([124, 1712, 56, 192],"float32"), list[124,56,56,192,], )

[torch error] paddle.reshape(Tensor([124, 1712, 56, 192],"float32"), list[124,56,56,192,], ) 
 shape '[124, 56, 56, 192]' is invalid for input of size 2282520576
2025-03-18 10:06:22.135744 test begin: paddle.reshape(Tensor([124, 18400818],"float32"), shape=list[-1,102,], )

[torch error] paddle.reshape(Tensor([124, 18400818],"float32"), shape=list[-1,102,], ) 
 shape '[-1, 102]' is invalid for input of size 2281701432
2025-03-18 10:06:24.025569 test begin: paddle.reshape(Tensor([124, 200, 173185],"float16"), shape=tuple(124,-1,), )

2025-03-18 10:06:36.159661 test begin: paddle.reshape(Tensor([124, 28, 1712, 384],"float32"), list[124,28,28,384,], )

[torch error] paddle.reshape(Tensor([124, 28, 1712, 384],"float32"), list[124,28,28,384,], ) 
 shape '[124, 28, 28, 384]' is invalid for input of size 2282520576
2025-03-18 10:06:38.244574 test begin: paddle.reshape(Tensor([124, 28, 28, 23471],"float32"), list[124,28,28,192,], )

[torch error] paddle.reshape(Tensor([124, 28, 28, 23471],"float32"), list[124,28,28,192,], ) 
 shape '[124, 28, 28, 192]' is invalid for input of size 2281756736
2025-03-18 10:06:40.130941 test begin: paddle.reshape(Tensor([124, 28, 28, 23471],"float32"), list[124,28,28,384,], )

[torch error] paddle.reshape(Tensor([124, 28, 28, 23471],"float32"), list[124,28,28,384,], ) 
 shape '[124, 28, 28, 384]' is invalid for input of size 2281756736
2025-03-18 10:06:42.020534 test begin: paddle.reshape(Tensor([124, 28, 3423, 192],"float32"), list[124,28,28,192,], )

[torch error] paddle.reshape(Tensor([124, 28, 3423, 192],"float32"), list[124,28,28,192,], ) 
 shape '[124, 28, 28, 192]' is invalid for input of size 2281853952
2025-03-18 10:06:43.631931 test begin: paddle.reshape(Tensor([124, 3423, 14, 384],"float32"), list[124,14,14,384,], )

[torch error] paddle.reshape(Tensor([124, 3423, 14, 384],"float32"), list[124,14,14,384,], ) 
 shape '[124, 14, 14, 384]' is invalid for input of size 2281853952
2025-03-18 10:06:44.996761 test begin: paddle.reshape(Tensor([124, 3423, 28, 192],"float32"), list[124,28,28,192,], )

[torch error] paddle.reshape(Tensor([124, 3423, 28, 192],"float32"), list[124,28,28,192,], ) 
 shape '[124, 28, 28, 192]' is invalid for input of size 2281853952
2025-03-18 10:06:46.614482 test begin: paddle.reshape(Tensor([124, 3423, 56, 96],"float32"), list[124,56,56,96,], )

[torch error] paddle.reshape(Tensor([124, 3423, 56, 96],"float32"), list[124,56,56,96,], ) 
 shape '[124, 56, 56, 96]' is invalid for input of size 2281853952
2025-03-18 10:06:48.802691 test begin: paddle.reshape(Tensor([124, 56, 1712, 192],"float32"), list[124,56,56,192,], )

[torch error] paddle.reshape(Tensor([124, 56, 1712, 192],"float32"), list[124,56,56,192,], ) 
 shape '[124, 56, 56, 192]' is invalid for input of size 2282520576
2025-03-18 10:06:50.702690 test begin: paddle.reshape(Tensor([124, 56, 3423, 96],"float32"), list[124,56,56,96,], )

[torch error] paddle.reshape(Tensor([124, 56, 3423, 96],"float32"), list[124,56,56,96,], ) 
 shape '[124, 56, 56, 96]' is invalid for input of size 2281853952
2025-03-18 10:06:52.396383 test begin: paddle.reshape(Tensor([124, 56, 56, 5868],"float32"), list[124,56,56,192,], )

[torch error] paddle.reshape(Tensor([124, 56, 56, 5868],"float32"), list[124,56,56,192,], ) 
 shape '[124, 56, 56, 192]' is invalid for input of size 2281853952
2025-03-18 10:06:53.836197 test begin: paddle.reshape(Tensor([124, 56, 56, 5868],"float32"), list[124,56,56,96,], )

[torch error] paddle.reshape(Tensor([124, 56, 56, 5868],"float32"), list[124,56,56,96,], ) 
 shape '[124, 56, 56, 96]' is invalid for input of size 2281853952
2025-03-18 10:06:55.817418 test begin: paddle.reshape(Tensor([124, 60530, 304],"float32"), shape=tuple(124,-1,), )

2025-03-18 10:07:11.033637 test begin: paddle.reshape(Tensor([1245, 22, 83328],"float32"), shape=tuple(-1,248,336,), )

[accuracy error] paddle.reshape(Tensor([1245, 22, 83328],"float32"), shape=tuple(-1,248,336,), ) 
 Unable to allocate 8.50 GiB for an array with shape (2282353920,) and data type float32
2025-03-18 10:08:10.419932 test begin: paddle.reshape(Tensor([1245, 43, 80256],"float16"), shape=tuple(-1,304,264,), )

[accuracy error] paddle.reshape(Tensor([1245, 43, 80256],"float16"), shape=tuple(-1,304,264,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y nan location mismatch:
 x: array([[[-0.4658  , -0.3828  , -0.485   , ...,  0.4775  ,  0.474   ,
         -0.4053  ],
        [ 0.09814 ,  0.405   , -0.3809  , ...,  0.05508 , -0.03833 ,...
 y: array([[[-0.4658  , -0.3828  , -0.485   , ...,  0.4775  ,  0.474   ,
         -0.4053  ],
        [ 0.09814 ,  0.405   , -0.3809  , ...,  0.05508 , -0.03833 ,...
2025-03-18 10:09:05.625242 test begin: paddle.reshape(Tensor([12510, 100, 152, 12],"float32"), shape=tuple(1,-1,4,), )

2025-03-18 10:09:22.143796 test begin: paddle.reshape(Tensor([12653, 6, 56576],"float16"), shape=tuple(-1,208,272,), )

[accuracy error] paddle.reshape(Tensor([12653, 6, 56576],"float16"), shape=tuple(-1,208,272,), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[[-0.4658  , -0.3828  , -0.485   , ..., -0.1967  , -0.3203  ,
          0.491   ],
        [ 0.343   ,  0.3916  ,  0.2438  , ..., -0.1981  ,  0.2194  ,...
 y: array([[[-0.4658  , -0.3828  , -0.485   , ..., -0.1967  , -0.3203  ,
          0.491   ],
        [ 0.343   ,  0.3916  ,  0.2438  , ..., -0.1981  ,  0.2194  ,...
2025-03-18 10:11:00.060033 test begin: paddle.reshape(Tensor([126761188, 18],"float32"), list[3,2,3,3,], )

[torch error] paddle.reshape(Tensor([126761188, 18],"float32"), list[3,2,3,3,], ) 
 shape '[3, 2, 3, 3]' is invalid for input of size 2281701384
2025-03-18 10:11:02.813916 test begin: paddle.reshape(Tensor([1268, 20, 304, 296],"float32"), shape=list[-1,304,296,], )

[accuracy error] paddle.reshape(Tensor([1268, 20, 304, 296],"float32"), shape=list[-1,304,296,], ) 
 (ResourceExhausted) Fail to alloc memory of 9127976960 size, error code is 12.
  [Hint: Expected error == 0, but received error:12 != 0:0.] (at ../paddle/phi/core/memory/allocation/cpu_allocator.cc:48)

2025-03-18 10:11:22.746225 test begin: paddle.reshape(Tensor([1272, 18, 304, 328],"float32"), shape=list[-1,304,328,], )

2025-03-18 10:11:32.895080 test begin: paddle.reshape(Tensor([1273271, 56, 32],"float32"), shape=list[14,-1,4,8,], )

2025-03-18 10:11:37.879407 test begin: paddle.reshape(Tensor([1273271, 56, 32],"float32"), shape=list[7,56,4,8,], )

[torch error] paddle.reshape(Tensor([1273271, 56, 32],"float32"), shape=list[7,56,4,8,], ) 
 shape '[7, 56, 4, 8]' is invalid for input of size 2281701632
2025-03-18 10:11:39.774826 test begin: paddle.reshape(Tensor([128, 1, 17825793],"float32"), shape=list[-1,1000,], )

[torch error] paddle.reshape(Tensor([128, 1, 17825793],"float32"), shape=list[-1,1000,], ) 
 shape '[-1, 1000]' is invalid for input of size 2281701504
2025-03-18 10:11:41.360962 test begin: paddle.reshape(Tensor([128, 1, 17825793],"float32"), shape=list[-1,102,], )

[torch error] paddle.reshape(Tensor([128, 1, 17825793],"float32"), shape=list[-1,102,], ) 
 shape '[-1, 102]' is invalid for input of size 2281701504
2025-03-18 10:11:42.373445 test begin: paddle.reshape(Tensor([128, 1, 17825793],"float32"), shape=list[-1,159,], )

[torch error] paddle.reshape(Tensor([128, 1, 17825793],"float32"), shape=list[-1,159,], ) 
 shape '[-1, 159]' is invalid for input of size 2281701504
2025-03-18 10:11:43.375873 test begin: paddle.reshape(Tensor([128, 1, 17825793],"float32"), shape=list[-1,192612,], )

[torch error] paddle.reshape(Tensor([128, 1, 17825793],"float32"), shape=list[-1,192612,], ) 
 shape '[-1, 192612]' is invalid for input of size 2281701504
2025-03-18 10:11:44.370258 test begin: paddle.reshape(Tensor([128, 1, 33554433],"float16"), shape=list[-1,1000,], )

[torch error] paddle.reshape(Tensor([128, 1, 33554433],"float16"), shape=list[-1,1000,], ) 
 shape '[-1, 1000]' is invalid for input of size 4294967424
2025-03-18 10:11:45.843463 test begin: paddle.reshape(Tensor([128, 1, 33554433],"float16"), shape=list[-1,192612,], )

[torch error] paddle.reshape(Tensor([128, 1, 33554433],"float16"), shape=list[-1,192612,], ) 
 shape '[-1, 192612]' is invalid for input of size 4294967424
2025-03-18 10:11:47.013143 test begin: paddle.reshape(Tensor([128, 110377, 304],"float16"), shape=tuple(128,-1,), )

2025-03-18 10:11:50.281387 test begin: paddle.reshape(Tensor([128, 1114113, 4, 4],"float32"), shape=list[-1,800,], )

[torch error] paddle.reshape(Tensor([128, 1114113, 4, 4],"float32"), shape=list[-1,800,], ) 
 shape '[-1, 800]' is invalid for input of size 2281703424
2025-03-18 10:11:51.618335 test begin: paddle.reshape(Tensor([128, 113360, 296],"float16"), shape=tuple(128,-1,), )

[Pass] paddle.reshape(Tensor([128, 113360, 296],"float16"), shape=tuple(128,-1,), )
2025-03-18 10:28:01.299248 test begin: paddle.reshape(Tensor([128, 116509, 288],"float16"), shape=tuple(128,-1,), )

[Pass] paddle.reshape(Tensor([128, 116509, 288],"float16"), shape=tuple(128,-1,), )
2025-03-18 10:44:31.845284 test begin: paddle.reshape(Tensor([128, 128, 139265],"float32"), list[-1,768,], )

[torch error] paddle.reshape(Tensor([128, 128, 139265],"float32"), list[-1,768,], ) 
 shape '[-1, 768]' is invalid for input of size 2281717760
2025-03-18 10:44:36.076213 test begin: paddle.reshape(Tensor([128, 128, 262145],"float16"), list[-1,768,], )

[torch error] paddle.reshape(Tensor([128, 128, 262145],"float16"), list[-1,768,], ) 
 shape '[-1, 768]' is invalid for input of size 4294983680
2025-03-18 10:44:39.532258 test begin: paddle.reshape(Tensor([128, 14, 14, 171197],"float16"), list[128,14,14,384,], )

[torch error] paddle.reshape(Tensor([128, 14, 14, 171197],"float16"), list[128,14,14,384,], ) 
 shape '[128, 14, 14, 384]' is invalid for input of size 4294990336
2025-03-18 10:44:40.475527 test begin: paddle.reshape(Tensor([128, 14, 14, 171197],"float16"), list[128,14,14,512,], )

[torch error] paddle.reshape(Tensor([128, 14, 14, 171197],"float16"), list[128,14,14,512,], ) 
 shape '[128, 14, 14, 512]' is invalid for input of size 4294990336
2025-03-18 10:44:41.618443 test begin: paddle.reshape(Tensor([128, 14, 14, 90948],"float32"), list[128,14,14,384,], )

[torch error] paddle.reshape(Tensor([128, 14, 14, 90948],"float32"), list[128,14,14,384,], ) 
 shape '[128, 14, 14, 384]' is invalid for input of size 2281703424
2025-03-18 10:44:42.935978 test begin: paddle.reshape(Tensor([128, 14, 14, 90948],"float32"), list[128,14,14,768,], )

[torch error] paddle.reshape(Tensor([128, 14, 14, 90948],"float32"), list[128,14,14,768,], ) 
 shape '[128, 14, 14, 768]' is invalid for input of size 2281703424
2025-03-18 10:44:45.440711 test begin: paddle.reshape(Tensor([128, 14, 1658, 768],"float32"), list[128,14,14,768,], )

[torch error] paddle.reshape(Tensor([128, 14, 1658, 768],"float32"), list[128,14,14,768,], ) 
 shape '[128, 14, 14, 768]' is invalid for input of size 2281832448
2025-03-18 10:44:47.376851 test begin: paddle.reshape(Tensor([128, 14, 3316, 384],"float32"), list[128,14,14,384,], )

[torch error] paddle.reshape(Tensor([128, 14, 3316, 384],"float32"), list[128,14,14,384,], ) 
 shape '[128, 14, 14, 384]' is invalid for input of size 2281832448
2025-03-18 10:44:49.142669 test begin: paddle.reshape(Tensor([128, 14, 4682, 512],"float16"), list[128,14,14,512,], )

[torch error] paddle.reshape(Tensor([128, 14, 4682, 512],"float16"), list[128,14,14,512,], ) 
 shape '[128, 14, 14, 512]' is invalid for input of size 4295753728
2025-03-18 10:44:50.439234 test begin: paddle.reshape(Tensor([128, 14, 6242, 384],"float16"), list[128,14,14,384,], )

[torch error] paddle.reshape(Tensor([128, 14, 6242, 384],"float16"), list[128,14,14,384,], ) 
 shape '[128, 14, 14, 384]' is invalid for input of size 4295294976
2025-03-18 10:44:51.687134 test begin: paddle.reshape(Tensor([128, 1658, 14, 768],"float32"), list[128,14,14,768,], )

[torch error] paddle.reshape(Tensor([128, 1658, 14, 768],"float32"), list[128,14,14,768,], ) 
 shape '[128, 14, 14, 768]' is invalid for input of size 2281832448
2025-03-18 10:44:53.217521 test begin: paddle.reshape(Tensor([128, 1658, 28, 384],"float32"), list[128,28,28,384,], )

[torch error] paddle.reshape(Tensor([128, 1658, 28, 384],"float32"), list[128,28,28,384,], ) 
 shape '[128, 28, 28, 384]' is invalid for input of size 2281832448
2025-03-18 10:44:54.618934 test begin: paddle.reshape(Tensor([128, 1658, 56, 192],"float32"), list[128,56,56,192,], )

[torch error] paddle.reshape(Tensor([128, 1658, 56, 192],"float32"), list[128,56,56,192,], ) 
 shape '[128, 56, 56, 192]' is invalid for input of size 2281832448
2025-03-18 10:44:56.020275 test begin: paddle.reshape(Tensor([128, 175, 192612],"float16"), shape=list[-1,192612,], )

[torch error] paddle.reshape(Tensor([128, 175, 192612],"float16"), shape=list[-1,192612,], ) 
 cannot reshape array of size 4300000000 into shape (128,175,192612)
2025-03-18 10:44:56.028810 test begin: paddle.reshape(Tensor([128, 17825793, 1, 1],"float32"), list[-1,16,1280,], )

[torch error] paddle.reshape(Tensor([128, 17825793, 1, 1],"float32"), list[-1,16,1280,], ) 
 shape '[-1, 16, 1280]' is invalid for input of size 2281701504
2025-03-18 10:44:57.905065 test begin: paddle.reshape(Tensor([128, 17825793, 1, 1],"float32"), list[-1,8,2048,], )

[torch error] paddle.reshape(Tensor([128, 17825793, 1, 1],"float32"), list[-1,8,2048,], ) 
 shape '[-1, 8, 2048]' is invalid for input of size 2281701504
2025-03-18 10:44:59.320200 test begin: paddle.reshape(Tensor([128, 17825793],"float32"), list[128,64,3,], )

[torch error] paddle.reshape(Tensor([128, 17825793],"float32"), list[128,64,3,], ) 
 shape '[128, 64, 3]' is invalid for input of size 2281701504
2025-03-18 10:45:01.288310 test begin: paddle.reshape(Tensor([128, 17825793],"float32"), list[128,80,1,], )

[torch error] paddle.reshape(Tensor([128, 17825793],"float32"), list[128,80,1,], ) 
 shape '[128, 80, 1]' is invalid for input of size 2281701504
2025-03-18 10:45:02.998048 test begin: paddle.reshape(Tensor([128, 17825793],"float32"), shape=list[-1,1000,], )

[torch error] paddle.reshape(Tensor([128, 17825793],"float32"), shape=list[-1,1000,], ) 
 shape '[-1, 1000]' is invalid for input of size 2281701504
2025-03-18 10:45:04.126020 test begin: paddle.reshape(Tensor([128, 17825793],"float32"), shape=list[-1,102,], )

[torch error] paddle.reshape(Tensor([128, 17825793],"float32"), shape=list[-1,102,], ) 
 shape '[-1, 102]' is invalid for input of size 2281701504
2025-03-18 10:45:05.215891 test begin: paddle.reshape(Tensor([128, 17826, 1000],"float32"), shape=list[-1,1000,], )

[Pass] paddle.reshape(Tensor([128, 17826, 1000],"float32"), shape=list[-1,1000,], )
2025-03-18 10:48:08.253953 test begin: paddle.reshape(Tensor([128, 200, 167773],"float16"), shape=tuple(128,-1,), )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 86, in func_timeout
    thread.join(timeout)
  File "/usr/lib/python3.9/threading.py", line 1064, in join
    self._wait_for_tstate_lock(timeout=max(timeout, 0))
  File "/usr/lib/python3.9/threading.py", line 1080, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt
2025-03-18 11:13:12.939408 test begin: paddle.reshape(Tensor([128, 200, 167773],"float16"), shape=tuple(128,-1,), )

W0318 11:14:55.355804 161546 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0318 11:14:55.356918 161546 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.reshape(Tensor([128, 200, 167773],"float16"), shape=tuple(128,-1,), )
2025-03-18 11:30:28.087843 test begin: paddle.reshape(Tensor([128, 2048, 1, 16385],"float16"), list[-1,8,2048,], )

[Pass] paddle.reshape(Tensor([128, 2048, 1, 16385],"float16"), list[-1,8,2048,], )
2025-03-18 11:46:34.955556 test begin: paddle.reshape(Tensor([128, 2048, 16385, 1],"float16"), list[-1,8,2048,], )

[Pass] paddle.reshape(Tensor([128, 2048, 16385, 1],"float16"), list[-1,8,2048,], )
2025-03-18 12:02:36.284586 test begin: paddle.reshape(Tensor([128, 28, 1658, 384],"float32"), list[128,28,28,384,], )

[torch error] paddle.reshape(Tensor([128, 28, 1658, 384],"float32"), list[128,28,28,384,], ) 
 shape '[128, 28, 28, 384]' is invalid for input of size 2281832448
2025-03-18 12:04:11.989049 test begin: paddle.reshape(Tensor([128, 28, 28, 22737],"float32"), list[128,28,28,192,], )

[torch error] paddle.reshape(Tensor([128, 28, 28, 22737],"float32"), list[128,28,28,192,], ) 
 shape '[128, 28, 28, 192]' is invalid for input of size 2281703424
2025-03-18 12:04:13.538952 test begin: paddle.reshape(Tensor([128, 28, 28, 22737],"float32"), list[128,28,28,384,], )

[torch error] paddle.reshape(Tensor([128, 28, 28, 22737],"float32"), list[128,28,28,384,], ) 
 shape '[128, 28, 28, 384]' is invalid for input of size 2281703424
2025-03-18 12:04:15.560666 test begin: paddle.reshape(Tensor([128, 28, 28, 42800],"float16"), list[128,28,28,192,], )

[torch error] paddle.reshape(Tensor([128, 28, 28, 42800],"float16"), list[128,28,28,192,], ) 
 shape '[128, 28, 28, 192]' is invalid for input of size 4295065600
2025-03-18 12:04:19.716165 test begin: paddle.reshape(Tensor([128, 28, 28, 42800],"float16"), list[128,28,28,256,], )

[torch error] paddle.reshape(Tensor([128, 28, 28, 42800],"float16"), list[128,28,28,256,], ) 
 shape '[128, 28, 28, 256]' is invalid for input of size 4295065600
2025-03-18 12:04:21.828090 test begin: paddle.reshape(Tensor([128, 28, 3316, 192],"float32"), list[128,28,28,192,], )

[torch error] paddle.reshape(Tensor([128, 28, 3316, 192],"float32"), list[128,28,28,192,], ) 
 shape '[128, 28, 28, 192]' is invalid for input of size 2281832448
2025-03-18 12:04:26.554898 test begin: paddle.reshape(Tensor([128, 28, 4682, 256],"float16"), list[128,28,28,256,], )

[torch error] paddle.reshape(Tensor([128, 28, 4682, 256],"float16"), list[128,28,28,256,], ) 
 shape '[128, 28, 28, 256]' is invalid for input of size 4295753728
2025-03-18 12:04:28.823182 test begin: paddle.reshape(Tensor([128, 28, 6242, 192],"float16"), list[128,28,28,192,], )

[torch error] paddle.reshape(Tensor([128, 28, 6242, 192],"float16"), list[128,28,28,192,], ) 
 shape '[128, 28, 28, 192]' is invalid for input of size 4295294976
2025-03-18 12:04:31.846964 test begin: paddle.reshape(Tensor([128, 3316, 14, 384],"float32"), list[128,14,14,384,], )

[torch error] paddle.reshape(Tensor([128, 3316, 14, 384],"float32"), list[128,14,14,384,], ) 
 shape '[128, 14, 14, 384]' is invalid for input of size 2281832448
2025-03-18 12:04:33.775281 test begin: paddle.reshape(Tensor([128, 3316, 28, 192],"float32"), list[128,28,28,192,], )

[torch error] paddle.reshape(Tensor([128, 3316, 28, 192],"float32"), list[128,28,28,192,], ) 
 shape '[128, 28, 28, 192]' is invalid for input of size 2281832448
2025-03-18 12:04:36.333494 test begin: paddle.reshape(Tensor([128, 3316, 56, 96],"float32"), list[128,56,56,96,], )

[torch error] paddle.reshape(Tensor([128, 3316, 56, 96],"float32"), list[128,56,56,96,], ) 
 shape '[128, 56, 56, 96]' is invalid for input of size 2281832448
2025-03-18 12:04:38.953554 test begin: paddle.reshape(Tensor([128, 33554433, 1, 1],"float16"), list[-1,8,2048,], )

[torch error] paddle.reshape(Tensor([128, 33554433, 1, 1],"float16"), list[-1,8,2048,], ) 
 shape '[-1, 8, 2048]' is invalid for input of size 4294967424
2025-03-18 12:04:41.588194 test begin: paddle.reshape(Tensor([128, 33554433],"float16"), shape=list[-1,1000,], )

[torch error] paddle.reshape(Tensor([128, 33554433],"float16"), shape=list[-1,1000,], ) 
 shape '[-1, 1000]' is invalid for input of size 4294967424
2025-03-18 12:04:43.601235 test begin: paddle.reshape(Tensor([128, 33555, 1000],"float16"), shape=list[-1,1000,], )

[Pass] paddle.reshape(Tensor([128, 33555, 1000],"float16"), shape=list[-1,1000,], )
2025-03-18 12:20:50.526240 test begin: paddle.reshape(Tensor([128, 43691, 768],"float16"), list[-1,768,], )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 86, in func_timeout
    thread.join(timeout)
  File "/usr/lib/python3.9/threading.py", line 1064, in join
    self._wait_for_tstate_lock(timeout=max(timeout, 0))
  File "/usr/lib/python3.9/threading.py", line 1080, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt
2025-03-18 14:07:03.919235 test begin: paddle.reshape(Tensor([128, 28, 28, 22737],"float32"), list[128,28,28,192,], )

W0318 14:08:07.357800 149914 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0318 14:08:07.359163 149914 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.reshape(Tensor([128, 28, 28, 22737],"float32"), list[128,28,28,192,], ) 
 shape '[128, 28, 28, 192]' is invalid for input of size 2281703424
2025-03-18 14:08:09.320812 test begin: paddle.reshape(Tensor([128, 28, 28, 22737],"float32"), list[128,28,28,384,], )

[torch error] paddle.reshape(Tensor([128, 28, 28, 22737],"float32"), list[128,28,28,384,], ) 
 shape '[128, 28, 28, 384]' is invalid for input of size 2281703424
2025-03-18 14:08:11.470545 test begin: paddle.reshape(Tensor([128, 28, 28, 42800],"float16"), list[128,28,28,192,], )

[torch error] paddle.reshape(Tensor([128, 28, 28, 42800],"float16"), list[128,28,28,192,], ) 
 shape '[128, 28, 28, 192]' is invalid for input of size 4295065600
2025-03-18 14:09:28.252651 test begin: paddle.reshape(Tensor([128, 28, 28, 42800],"float16"), list[128,28,28,256,], )

[torch error] paddle.reshape(Tensor([128, 28, 28, 42800],"float16"), list[128,28,28,256,], ) 
 shape '[128, 28, 28, 256]' is invalid for input of size 4295065600
2025-03-18 14:09:29.255340 test begin: paddle.reshape(Tensor([128, 28, 3316, 192],"float32"), list[128,28,28,192,], )

[torch error] paddle.reshape(Tensor([128, 28, 3316, 192],"float32"), list[128,28,28,192,], ) 
 shape '[128, 28, 28, 192]' is invalid for input of size 2281832448
2025-03-18 14:09:36.145350 test begin: paddle.reshape(Tensor([128, 28, 4682, 256],"float16"), list[128,28,28,256,], )

[torch error] paddle.reshape(Tensor([128, 28, 4682, 256],"float16"), list[128,28,28,256,], ) 
 shape '[128, 28, 28, 256]' is invalid for input of size 4295753728
2025-03-18 14:09:39.738052 test begin: paddle.reshape(Tensor([128, 28, 6242, 192],"float16"), list[128,28,28,192,], )

[torch error] paddle.reshape(Tensor([128, 28, 6242, 192],"float16"), list[128,28,28,192,], ) 
 shape '[128, 28, 28, 192]' is invalid for input of size 4295294976
2025-03-18 14:09:41.265968 test begin: paddle.reshape(Tensor([128, 3316, 14, 384],"float32"), list[128,14,14,384,], )

[torch error] paddle.reshape(Tensor([128, 3316, 14, 384],"float32"), list[128,14,14,384,], ) 
 shape '[128, 14, 14, 384]' is invalid for input of size 2281832448
2025-03-18 14:09:42.787941 test begin: paddle.reshape(Tensor([128, 3316, 28, 192],"float32"), list[128,28,28,192,], )

[torch error] paddle.reshape(Tensor([128, 3316, 28, 192],"float32"), list[128,28,28,192,], ) 
 shape '[128, 28, 28, 192]' is invalid for input of size 2281832448
2025-03-18 14:09:44.690576 test begin: paddle.reshape(Tensor([128, 3316, 56, 96],"float32"), list[128,56,56,96,], )

[torch error] paddle.reshape(Tensor([128, 3316, 56, 96],"float32"), list[128,56,56,96,], ) 
 shape '[128, 56, 56, 96]' is invalid for input of size 2281832448
2025-03-18 14:09:46.505195 test begin: paddle.reshape(Tensor([128, 33554433, 1, 1],"float16"), list[-1,8,2048,], )

[torch error] paddle.reshape(Tensor([128, 33554433, 1, 1],"float16"), list[-1,8,2048,], ) 
 shape '[-1, 8, 2048]' is invalid for input of size 4294967424
2025-03-18 14:09:48.068637 test begin: paddle.reshape(Tensor([128, 33554433],"float16"), shape=list[-1,1000,], )

[torch error] paddle.reshape(Tensor([128, 33554433],"float16"), shape=list[-1,1000,], ) 
 shape '[-1, 1000]' is invalid for input of size 4294967424
2025-03-18 14:09:49.461173 test begin: paddle.reshape(Tensor([128, 33555, 1000],"float16"), shape=list[-1,1000,], )

[Pass] paddle.reshape(Tensor([128, 33555, 1000],"float16"), shape=list[-1,1000,], )
2025-03-18 14:25:41.759745 test begin: paddle.reshape(Tensor([128, 43691, 768],"float16"), list[-1,768,], )

[Pass] paddle.reshape(Tensor([128, 43691, 768],"float16"), list[-1,768,], )
2025-03-18 14:41:34.132863 test begin: paddle.reshape(Tensor([128, 4682, 14, 512],"float16"), list[128,14,14,512,], )

[torch error] paddle.reshape(Tensor([128, 4682, 14, 512],"float16"), list[128,14,14,512,], ) 
 shape '[128, 14, 14, 512]' is invalid for input of size 4295753728
2025-03-18 14:41:37.648055 test begin: paddle.reshape(Tensor([128, 4682, 28, 256],"float16"), list[128,28,28,256,], )

[torch error] paddle.reshape(Tensor([128, 4682, 28, 256],"float16"), list[128,28,28,256,], ) 
 shape '[128, 28, 28, 256]' is invalid for input of size 4295753728
2025-03-18 14:41:38.578631 test begin: paddle.reshape(Tensor([128, 4682, 56, 128],"float16"), list[128,56,56,128,], )

[torch error] paddle.reshape(Tensor([128, 4682, 56, 128],"float16"), list[128,56,56,128,], ) 
 shape '[128, 56, 56, 128]' is invalid for input of size 4295753728
2025-03-18 14:41:39.855665 test begin: paddle.reshape(Tensor([128, 56, 1658, 192],"float32"), list[128,56,56,192,], )

[torch error] paddle.reshape(Tensor([128, 56, 1658, 192],"float32"), list[128,56,56,192,], ) 
 shape '[128, 56, 56, 192]' is invalid for input of size 2281832448
2025-03-18 14:41:43.641284 test begin: paddle.reshape(Tensor([128, 56, 3316, 96],"float32"), list[128,56,56,96,], )

[torch error] paddle.reshape(Tensor([128, 56, 3316, 96],"float32"), list[128,56,56,96,], ) 
 shape '[128, 56, 56, 96]' is invalid for input of size 2281832448
2025-03-18 14:41:45.358390 test begin: paddle.reshape(Tensor([128, 56, 4682, 128],"float16"), list[128,56,56,128,], )

[torch error] paddle.reshape(Tensor([128, 56, 4682, 128],"float16"), list[128,56,56,128,], ) 
 shape '[128, 56, 56, 128]' is invalid for input of size 4295753728
2025-03-18 14:41:47.231122 test begin: paddle.reshape(Tensor([128, 56, 56, 10700],"float16"), list[128,56,56,128,], )

[torch error] paddle.reshape(Tensor([128, 56, 56, 10700],"float16"), list[128,56,56,128,], ) 
 shape '[128, 56, 56, 128]' is invalid for input of size 4295065600
2025-03-18 14:41:48.804963 test begin: paddle.reshape(Tensor([128, 56, 56, 10700],"float16"), list[128,56,56,96,], )

[torch error] paddle.reshape(Tensor([128, 56, 56, 10700],"float16"), list[128,56,56,96,], ) 
 shape '[128, 56, 56, 96]' is invalid for input of size 4295065600
2025-03-18 14:41:50.161139 test begin: paddle.reshape(Tensor([128, 56, 56, 5685],"float32"), list[128,56,56,192,], )

[torch error] paddle.reshape(Tensor([128, 56, 56, 5685],"float32"), list[128,56,56,192,], ) 
 shape '[128, 56, 56, 192]' is invalid for input of size 2282004480
2025-03-18 14:41:52.082182 test begin: paddle.reshape(Tensor([128, 56, 56, 5685],"float32"), list[128,56,56,96,], )

[torch error] paddle.reshape(Tensor([128, 56, 56, 5685],"float32"), list[128,56,56,96,], ) 
 shape '[128, 56, 56, 96]' is invalid for input of size 2282004480
2025-03-18 14:41:53.994803 test begin: paddle.reshape(Tensor([128, 56, 6242, 96],"float16"), list[128,56,56,96,], )

[torch error] paddle.reshape(Tensor([128, 56, 6242, 96],"float16"), list[128,56,56,96,], ) 
 shape '[128, 56, 56, 96]' is invalid for input of size 4295294976
2025-03-18 14:41:56.013377 test begin: paddle.reshape(Tensor([128, 60223, 296],"float32"), shape=tuple(128,-1,), )

[Pass] paddle.reshape(Tensor([128, 60223, 296],"float32"), shape=tuple(128,-1,), )
2025-03-18 14:44:47.443264 test begin: paddle.reshape(Tensor([128, 6242, 14, 384],"float16"), list[128,14,14,384,], )

[torch error] paddle.reshape(Tensor([128, 6242, 14, 384],"float16"), list[128,14,14,384,], ) 
 shape '[128, 14, 14, 384]' is invalid for input of size 4295294976
2025-03-18 14:44:51.483807 test begin: paddle.reshape(Tensor([128, 6242, 28, 192],"float16"), list[128,28,28,192,], )

[torch error] paddle.reshape(Tensor([128, 6242, 28, 192],"float16"), list[128,28,28,192,], ) 
 shape '[128, 28, 28, 192]' is invalid for input of size 4295294976
2025-03-18 14:44:52.904836 test begin: paddle.reshape(Tensor([128, 6242, 56, 96],"float16"), list[128,56,56,96,], )

[torch error] paddle.reshape(Tensor([128, 6242, 56, 96],"float16"), list[128,56,56,96,], ) 
 shape '[128, 56, 56, 96]' is invalid for input of size 4295294976
2025-03-18 14:44:54.682344 test begin: paddle.reshape(Tensor([1287, 21, 84480],"float32"), shape=tuple(-1,264,320,), )

[Pass] paddle.reshape(Tensor([1287, 21, 84480],"float32"), shape=tuple(-1,264,320,), )
2025-03-18 14:47:49.725974 test begin: paddle.reshape(Tensor([1290, 96, 96, 192],"float32"), list[28,96,96,192,], )

[torch error] paddle.reshape(Tensor([1290, 96, 96, 192],"float32"), list[28,96,96,192,], ) 
 shape '[28, 96, 96, 192]' is invalid for input of size 2282618880
2025-03-18 14:47:54.112249 test begin: paddle.reshape(Tensor([1290, 96, 96, 192],"float32"), list[32,96,96,192,], )

[torch error] paddle.reshape(Tensor([1290, 96, 96, 192],"float32"), list[32,96,96,192,], ) 
 shape '[32, 96, 96, 192]' is invalid for input of size 2282618880
2025-03-18 14:47:56.231784 test begin: paddle.reshape(Tensor([1290, 96, 96, 192],"float32"), list[4,96,96,192,], )

[torch error] paddle.reshape(Tensor([1290, 96, 96, 192],"float32"), list[4,96,96,192,], ) 
 shape '[4, 96, 96, 192]' is invalid for input of size 2282618880
2025-03-18 14:47:58.298412 test begin: paddle.reshape(Tensor([12946, 1296, 256],"float16"), shape=tuple(-1,256,), )

[Pass] paddle.reshape(Tensor([12946, 1296, 256],"float16"), shape=tuple(-1,256,), )
2025-03-18 15:03:59.125365 test begin: paddle.reshape(Tensor([12986, 19, 34, 512],"float16"), list[-1,512,], )

[Pass] paddle.reshape(Tensor([12986, 19, 34, 512],"float16"), list[-1,512,], )
2025-03-18 15:25:28.793208 test begin: paddle.reshape(Tensor([12988, 175678],"bool"), list[-1,32,1,], )

[torch error] paddle.reshape(Tensor([12988, 175678],"bool"), list[-1,32,1,], ) 
 shape '[-1, 32, 1]' is invalid for input of size 2281705864
2025-03-18 15:26:48.536348 test begin: paddle.reshape(Tensor([12993, 112, 2, 28, 28],"float32"), shape=list[2,224,28,28,], )

[torch error] paddle.reshape(Tensor([12993, 112, 2, 28, 28],"float32"), shape=list[2,224,28,28,], ) 
 shape '[2, 224, 28, 28]' is invalid for input of size 2281778688
2025-03-18 15:26:52.762213 test begin: paddle.reshape(Tensor([12993, 224, 28, 28],"float32"), shape=list[2,2,112,28,28,], )

[torch error] paddle.reshape(Tensor([12993, 224, 28, 28],"float32"), shape=list[2,2,112,28,28,], ) 
 shape '[2, 2, 112, 28, 28]' is invalid for input of size 2281778688
2025-03-18 15:26:54.409634 test begin: paddle.reshape(Tensor([13, 1, 1, 175515491],"float32"), list[13,7,1,], )

[torch error] paddle.reshape(Tensor([13, 1, 1, 175515491],"float32"), list[13,7,1,], ) 
 shape '[13, 7, 1]' is invalid for input of size 2281701383
2025-03-18 15:26:56.349138 test begin: paddle.reshape(Tensor([13, 1, 175515491, 1],"float32"), list[13,7,1,], )

[torch error] paddle.reshape(Tensor([13, 1, 175515491, 1],"float32"), list[13,7,1,], ) 
 shape '[13, 7, 1]' is invalid for input of size 2281701383
2025-03-18 15:26:58.284112 test begin: paddle.reshape(Tensor([13, 1, 175515491],"float32"), list[13,-1,32,], )

[torch error] paddle.reshape(Tensor([13, 1, 175515491],"float32"), list[13,-1,32,], ) 
 shape '[13, -1, 32]' is invalid for input of size 2281701383
2025-03-18 15:27:00.227990 test begin: paddle.reshape(Tensor([13, 1, 175515491],"float32"), list[13,1,1,1,], )

[torch error] paddle.reshape(Tensor([13, 1, 175515491],"float32"), list[13,1,1,1,], ) 
 shape '[13, 1, 1, 1]' is invalid for input of size 2281701383
2025-03-18 15:27:02.182642 test begin: paddle.reshape(Tensor([13, 1, 175515491],"int64"), shape=list[13,1,-1,4,], )

[torch error] paddle.reshape(Tensor([13, 1, 175515491],"int64"), shape=list[13,1,-1,4,], ) 
 shape '[13, 1, -1, 4]' is invalid for input of size 2281701383
2025-03-18 15:28:13.655393 test begin: paddle.reshape(Tensor([13, 1, 25073642, 7],"float32"), list[13,7,1,], )

[torch error] paddle.reshape(Tensor([13, 1, 25073642, 7],"float32"), list[13,7,1,], ) 
 shape '[13, 7, 1]' is invalid for input of size 2281701422
2025-03-18 15:28:18.222566 test begin: paddle.reshape(Tensor([13, 1, 7, 25073642],"float32"), list[13,7,1,], )

[torch error] paddle.reshape(Tensor([13, 1, 7, 25073642],"float32"), list[13,7,1,], ) 
 shape '[13, 7, 1]' is invalid for input of size 2281701422
2025-03-18 15:28:20.182151 test begin: paddle.reshape(Tensor([13, 1, 8, 21939437],"float32"), shape=list[13,-1,64,], )

[torch error] paddle.reshape(Tensor([13, 1, 8, 21939437],"float32"), shape=list[13,-1,64,], ) 
 shape '[13, -1, 64]' is invalid for input of size 2281701448
2025-03-18 15:28:22.174409 test begin: paddle.reshape(Tensor([13, 10969719, 16],"float32"), tuple(13,2,4,4,), )

[torch error] paddle.reshape(Tensor([13, 10969719, 16],"float32"), tuple(13,2,4,4,), ) 
 shape '[13, 2, 4, 4]' is invalid for input of size 2281701552
2025-03-18 15:28:24.105516 test begin: paddle.reshape(Tensor([13, 10969719, 16],"float32"), tuple(13,4,4,4,), )

[torch error] paddle.reshape(Tensor([13, 10969719, 16],"float32"), tuple(13,4,4,4,), ) 
 shape '[13, 4, 4, 4]' is invalid for input of size 2281701552
2025-03-18 15:28:26.041225 test begin: paddle.reshape(Tensor([13, 10969719, 16],"float32"), tuple(13,7,4,4,), )

[torch error] paddle.reshape(Tensor([13, 10969719, 16],"float32"), tuple(13,7,4,4,), ) 
 shape '[13, 7, 4, 4]' is invalid for input of size 2281701552
2025-03-18 15:28:27.991453 test begin: paddle.reshape(Tensor([13, 10969719, 2, 8],"float32"), list[13,4,8,2,], )

[torch error] paddle.reshape(Tensor([13, 10969719, 2, 8],"float32"), list[13,4,8,2,], ) 
 shape '[13, 4, 8, 2]' is invalid for input of size 2281701552
2025-03-18 15:28:29.919146 test begin: paddle.reshape(Tensor([13, 14626291, 6, 2],"float32"), list[13,4,2,6,], )

[torch error] paddle.reshape(Tensor([13, 14626291, 6, 2],"float32"), list[13,4,2,6,], ) 
 shape '[13, 4, 2, 6]' is invalid for input of size 2281701396
2025-03-18 15:28:31.832626 test begin: paddle.reshape(Tensor([13, 171402, 64, 16],"float32"), shape=list[13,2,4,16,16,], )

[torch error] paddle.reshape(Tensor([13, 171402, 64, 16],"float32"), shape=list[13,2,4,16,16,], ) 
 shape '[13, 2, 4, 16, 16]' is invalid for input of size 2281703424
2025-03-18 15:28:33.762522 test begin: paddle.reshape(Tensor([13, 175515491, 1],"float32"), list[13,1,1,1,], )

[torch error] paddle.reshape(Tensor([13, 175515491, 1],"float32"), list[13,1,1,1,], ) 
 shape '[13, 1, 1, 1]' is invalid for input of size 2281701383
2025-03-18 15:28:36.255795 test begin: paddle.reshape(Tensor([13, 175515491, 1],"float32"), list[13,5,1,1,], )

[torch error] paddle.reshape(Tensor([13, 175515491, 1],"float32"), list[13,5,1,1,], ) 
 shape '[13, 5, 1, 1]' is invalid for input of size 2281701383
2025-03-18 15:28:37.887232 test begin: paddle.reshape(Tensor([13, 175515491],"int64"), list[13,], )

[torch error] paddle.reshape(Tensor([13, 175515491],"int64"), list[13,], ) 
 shape '[13]' is invalid for input of size 2281701383
2025-03-18 15:28:41.737191 test begin: paddle.reshape(Tensor([13, 1790975, 7, 14],"float32"), list[13,4,14,7,], )

[torch error] paddle.reshape(Tensor([13, 1790975, 7, 14],"float32"), list[13,4,14,7,], ) 
 shape '[13, 4, 14, 7]' is invalid for input of size 2281702150
2025-03-18 15:28:43.270848 test begin: paddle.reshape(Tensor([13, 1828287, 96],"float32"), list[13,7,4,-1,], )

[torch error] paddle.reshape(Tensor([13, 1828287, 96],"float32"), list[13,7,4,-1,], ) 
 shape '[13, 7, 4, -1]' is invalid for input of size 2281702176
2025-03-18 15:28:45.221312 test begin: paddle.reshape(Tensor([13, 1928742, 13, 7],"float32"), list[13,4,7,13,], )

[torch error] paddle.reshape(Tensor([13, 1928742, 13, 7],"float32"), list[13,4,7,13,], ) 
 shape '[13, 4, 7, 13]' is invalid for input of size 2281701786
2025-03-18 15:28:47.146453 test begin: paddle.reshape(Tensor([13, 2, 2742430, 4, 8],"float32"), tuple(13,2,7,32,), )

[torch error] paddle.reshape(Tensor([13, 2, 2742430, 4, 8],"float32"), tuple(13,2,7,32,), ) 
 shape '[13, 2, 7, 32]' is invalid for input of size 2281701760
2025-03-18 15:28:49.079336 test begin: paddle.reshape(Tensor([13, 2, 5484860, 16],"float32"), shape=list[13,2,4,16,16,], )

[torch error] paddle.reshape(Tensor([13, 2, 5484860, 16],"float32"), shape=list[13,2,4,16,16,], ) 
 shape '[13, 2, 4, 16, 16]' is invalid for input of size 2281701760
2025-03-18 15:28:50.994668 test begin: paddle.reshape(Tensor([13, 2, 64, 1371215],"float32"), shape=list[13,2,4,16,16,], )

[torch error] paddle.reshape(Tensor([13, 2, 64, 1371215],"float32"), shape=list[13,2,4,16,16,], ) 
 shape '[13, 2, 4, 16, 16]' is invalid for input of size 2281701760
2025-03-18 15:28:52.909109 test begin: paddle.reshape(Tensor([13, 2, 7, 1567103, 8],"float32"), tuple(13,2,7,32,), )

[torch error] paddle.reshape(Tensor([13, 2, 7, 1567103, 8],"float32"), tuple(13,2,7,32,), ) 
 shape '[13, 2, 7, 32]' is invalid for input of size 2281701968
2025-03-18 15:28:54.829456 test begin: paddle.reshape(Tensor([13, 2, 7, 4, 3134206],"float32"), tuple(13,2,7,32,), )

[torch error] paddle.reshape(Tensor([13, 2, 7, 4, 3134206],"float32"), tuple(13,2,7,32,), ) 
 shape '[13, 2, 7, 32]' is invalid for input of size 2281701968
2025-03-18 15:28:56.782110 test begin: paddle.reshape(Tensor([13, 2, 8, 10969719],"float32"), shape=list[13,-1,64,], )

[torch error] paddle.reshape(Tensor([13, 2, 8, 10969719],"float32"), shape=list[13,-1,64,], ) 
 shape '[13, -1, 64]' is invalid for input of size 2281701552
2025-03-18 15:28:58.719835 test begin: paddle.reshape(Tensor([13, 2, 87757746],"float32"), shape=list[13,2,4,16,], )

[torch error] paddle.reshape(Tensor([13, 2, 87757746],"float32"), shape=list[13,2,4,16,], ) 
 shape '[13, 2, 4, 16]' is invalid for input of size 2281701396
2025-03-18 15:29:00.622690 test begin: paddle.reshape(Tensor([13, 2, 87757746],"float32"), tuple(13,2,4,4,), )

[torch error] paddle.reshape(Tensor([13, 2, 87757746],"float32"), tuple(13,2,4,4,), ) 
 shape '[13, 2, 4, 4]' is invalid for input of size 2281701396
2025-03-18 15:29:02.520653 test begin: paddle.reshape(Tensor([13, 2, 87757746],"int64"), shape=list[13,2,-1,4,], )

[torch error] paddle.reshape(Tensor([13, 2, 87757746],"int64"), shape=list[13,2,-1,4,], ) 
 shape '[13, 2, -1, 4]' is invalid for input of size 2281701396
2025-03-18 15:29:06.434769 test begin: paddle.reshape(Tensor([13, 21, 1044736, 8],"float32"), tuple(13,21,32,), )

[torch error] paddle.reshape(Tensor([13, 21, 1044736, 8],"float32"), tuple(13,21,32,), ) 
 shape '[13, 21, 32]' is invalid for input of size 2281703424
2025-03-18 15:29:08.363238 test begin: paddle.reshape(Tensor([13, 21, 4, 2089471],"float32"), tuple(13,21,32,), )

[torch error] paddle.reshape(Tensor([13, 21, 4, 2089471],"float32"), tuple(13,21,32,), ) 
 shape '[13, 21, 32]' is invalid for input of size 2281702332
2025-03-18 15:29:10.266856 test begin: paddle.reshape(Tensor([13, 21, 8357881],"float32"), list[13,21,4,8,], )

[torch error] paddle.reshape(Tensor([13, 21, 8357881],"float32"), list[13,21,4,8,], ) 
 shape '[13, 21, 4, 8]' is invalid for input of size 2281701513
2025-03-18 15:29:12.179671 test begin: paddle.reshape(Tensor([13, 21, 8357881],"float32"), tuple(13,-1,4,8,), )

[torch error] paddle.reshape(Tensor([13, 21, 8357881],"float32"), tuple(13,-1,4,8,), ) 
 shape '[13, -1, 4, 8]' is invalid for input of size 2281701513
2025-03-18 15:29:13.931648 test begin: paddle.reshape(Tensor([13, 21, 8357881],"float32"), tuple(13,21,4,8,), )

[torch error] paddle.reshape(Tensor([13, 21, 8357881],"float32"), tuple(13,21,4,8,), ) 
 shape '[13, 21, 4, 8]' is invalid for input of size 2281701513
2025-03-18 15:29:15.468469 test begin: paddle.reshape(Tensor([13, 21939437, 2, 4],"float32"), list[13,4,4,2,], )

[torch error] paddle.reshape(Tensor([13, 21939437, 2, 4],"float32"), list[13,4,4,2,], ) 
 shape '[13, 4, 4, 2]' is invalid for input of size 2281701448
2025-03-18 15:29:17.402019 test begin: paddle.reshape(Tensor([13, 25073642, 1, 7],"float32"), list[13,7,1,], )

[torch error] paddle.reshape(Tensor([13, 25073642, 1, 7],"float32"), list[13,7,1,], ) 
 shape '[13, 7, 1]' is invalid for input of size 2281701422
2025-03-18 15:29:19.336403 test begin: paddle.reshape(Tensor([13, 25073642, 7, 1],"float32"), list[13,7,1,], )

[torch error] paddle.reshape(Tensor([13, 25073642, 7, 1],"float32"), list[13,7,1,], ) 
 shape '[13, 7, 1]' is invalid for input of size 2281701422
2025-03-18 15:29:21.253664 test begin: paddle.reshape(Tensor([13, 2742430, 64],"float32"), shape=list[13,2,4,16,], )

[torch error] paddle.reshape(Tensor([13, 2742430, 64],"float32"), shape=list[13,2,4,16,], ) 
 shape '[13, 2, 4, 16]' is invalid for input of size 2281701760
2025-03-18 15:29:23.173044 test begin: paddle.reshape(Tensor([13, 29252582, 3, 2],"float32"), list[13,4,2,3,], )

[torch error] paddle.reshape(Tensor([13, 29252582, 3, 2],"float32"), list[13,4,2,3,], ) 
 shape '[13, 4, 2, 3]' is invalid for input of size 2281701396
2025-03-18 15:29:25.118774 test begin: paddle.reshape(Tensor([13, 2925259, 4, 15],"float32"), list[13,4,15,4,], )

[torch error] paddle.reshape(Tensor([13, 2925259, 4, 15],"float32"), list[13,4,15,4,], ) 
 shape '[13, 4, 15, 4]' is invalid for input of size 2281702020
2025-03-18 15:29:27.021954 test begin: paddle.reshape(Tensor([13, 3, 7, 8357881],"float32"), tuple(13,-1,32,), )

[torch error] paddle.reshape(Tensor([13, 3, 7, 8357881],"float32"), tuple(13,-1,32,), ) 
 shape '[13, -1, 32]' is invalid for input of size 2281701513
2025-03-18 15:29:28.949538 test begin: paddle.reshape(Tensor([13, 3134206, 7, 8],"float32"), tuple(52,-1,8,), )

[torch error] paddle.reshape(Tensor([13, 3134206, 7, 8],"float32"), tuple(52,-1,8,), ) 
 shape '[52, -1, 8]' is invalid for input of size 2281701968
2025-03-18 15:29:30.876507 test begin: paddle.reshape(Tensor([13, 3375298, 13, 4],"float32"), list[13,4,4,13,], )

[torch error] paddle.reshape(Tensor([13, 3375298, 13, 4],"float32"), list[13,4,4,13,], ) 
 shape '[13, 4, 4, 13]' is invalid for input of size 2281701448
2025-03-18 15:29:32.861342 test begin: paddle.reshape(Tensor([13, 4, 10969719, 4],"float32"), list[13,4,4,13,], )

[torch error] paddle.reshape(Tensor([13, 4, 10969719, 4],"float32"), list[13,4,4,13,], ) 
 shape '[13, 4, 4, 13]' is invalid for input of size 2281701552
2025-03-18 15:29:34.944360 test begin: paddle.reshape(Tensor([13, 4, 10969719, 4],"float32"), list[13,4,4,2,], )

[torch error] paddle.reshape(Tensor([13, 4, 10969719, 4],"float32"), list[13,4,4,2,], ) 
 shape '[13, 4, 4, 2]' is invalid for input of size 2281701552
2025-03-18 15:29:37.009262 test begin: paddle.reshape(Tensor([13, 4, 10969719, 4],"float32"), list[13,4,4,7,], )

[torch error] paddle.reshape(Tensor([13, 4, 10969719, 4],"float32"), list[13,4,4,7,], ) 
 shape '[13, 4, 4, 7]' is invalid for input of size 2281701552
2025-03-18 15:29:39.025514 test begin: paddle.reshape(Tensor([13, 4, 1096972, 5, 1, 8],"float32"), list[13,4,5,5,-1,], )

[torch error] paddle.reshape(Tensor([13, 4, 1096972, 5, 1, 8],"float32"), list[13,4,5,5,-1,], ) 
 shape '[13, 4, 5, 5, -1]' is invalid for input of size 2281701760
2025-03-18 15:29:41.050758 test begin: paddle.reshape(Tensor([13, 4, 13, 3375298],"float32"), list[13,4,4,13,], )

[torch error] paddle.reshape(Tensor([13, 4, 13, 3375298],"float32"), list[13,4,4,13,], ) 
 shape '[13, 4, 4, 13]' is invalid for input of size 2281701448
2025-03-18 15:29:43.067542 test begin: paddle.reshape(Tensor([13, 4, 13, 3375298],"float32"), list[13,4,7,13,], )

[torch error] paddle.reshape(Tensor([13, 4, 13, 3375298],"float32"), list[13,4,7,13,], ) 
 shape '[13, 4, 7, 13]' is invalid for input of size 2281701448
2025-03-18 15:29:45.063573 test begin: paddle.reshape(Tensor([13, 4, 1371215, 32],"float32"), shape=list[13,-1,32,], )

[Pass] paddle.reshape(Tensor([13, 4, 1371215, 32],"float32"), shape=list[13,-1,32,], )
2025-03-18 15:32:57.676281 test begin: paddle.reshape(Tensor([13, 4, 2, 21939437],"float32"), list[13,4,4,2,], )

[torch error] paddle.reshape(Tensor([13, 4, 2, 21939437],"float32"), list[13,4,4,2,], ) 
 shape '[13, 4, 4, 2]' is invalid for input of size 2281701448
2025-03-18 15:33:02.292152 test begin: paddle.reshape(Tensor([13, 4, 2, 21939437],"float32"), list[13,4,8,2,], )

[torch error] paddle.reshape(Tensor([13, 4, 2, 21939437],"float32"), list[13,4,8,2,], ) 
 shape '[13, 4, 8, 2]' is invalid for input of size 2281701448
2025-03-18 15:33:04.187500 test begin: paddle.reshape(Tensor([13, 4, 21, 2089471],"float32"), tuple(52,-1,8,), )

[torch error] paddle.reshape(Tensor([13, 4, 21, 2089471],"float32"), tuple(52,-1,8,), ) 
 shape '[52, -1, 8]' is invalid for input of size 2281702332
2025-03-18 15:33:06.115673 test begin: paddle.reshape(Tensor([13, 4, 21939437, 2],"float32"), list[13,4,2,3,], )

[torch error] paddle.reshape(Tensor([13, 4, 21939437, 2],"float32"), list[13,4,2,3,], ) 
 shape '[13, 4, 2, 3]' is invalid for input of size 2281701448
2025-03-18 15:33:08.060643 test begin: paddle.reshape(Tensor([13, 4, 21939437, 2],"float32"), list[13,4,2,6,], )

[torch error] paddle.reshape(Tensor([13, 4, 21939437, 2],"float32"), list[13,4,2,6,], ) 
 shape '[13, 4, 2, 6]' is invalid for input of size 2281701448
2025-03-18 15:33:10.013089 test begin: paddle.reshape(Tensor([13, 4, 2925259, 15],"float32"), list[13,4,15,4,], )

[torch error] paddle.reshape(Tensor([13, 4, 2925259, 15],"float32"), list[13,4,15,4,], ) 
 shape '[13, 4, 15, 4]' is invalid for input of size 2281702020
2025-03-18 15:33:11.918361 test begin: paddle.reshape(Tensor([13, 4, 3, 14626291],"float32"), list[13,4,2,3,], )

[torch error] paddle.reshape(Tensor([13, 4, 3, 14626291],"float32"), list[13,4,2,3,], ) 
 shape '[13, 4, 2, 3]' is invalid for input of size 2281701396
2025-03-18 15:33:13.853250 test begin: paddle.reshape(Tensor([13, 4, 3134206, 14],"float32"), list[13,4,14,7,], )

[torch error] paddle.reshape(Tensor([13, 4, 3134206, 14],"float32"), list[13,4,14,7,], ) 
 shape '[13, 4, 14, 7]' is invalid for input of size 2281701968
2025-03-18 15:33:15.787738 test begin: paddle.reshape(Tensor([13, 4, 4, 10969719],"float32"), list[13,4,15,4,], )

[torch error] paddle.reshape(Tensor([13, 4, 4, 10969719],"float32"), list[13,4,15,4,], ) 
 shape '[13, 4, 15, 4]' is invalid for input of size 2281701552
2025-03-18 15:33:17.712623 test begin: paddle.reshape(Tensor([13, 4, 4, 10969719],"float32"), list[13,4,8,4,], )

[torch error] paddle.reshape(Tensor([13, 4, 4, 10969719],"float32"), list[13,4,8,4,], ) 
 shape '[13, 4, 8, 4]' is invalid for input of size 2281701552
2025-03-18 15:33:19.635517 test begin: paddle.reshape(Tensor([13, 4, 43878873],"float32"), tuple(13,4,4,4,), )

[torch error] paddle.reshape(Tensor([13, 4, 43878873],"float32"), tuple(13,4,4,4,), ) 
 shape '[13, 4, 4, 4]' is invalid for input of size 2281701396
2025-03-18 15:33:21.584904 test begin: paddle.reshape(Tensor([13, 4, 5, 1096972, 1, 8],"float32"), list[13,4,5,5,-1,], )

[torch error] paddle.reshape(Tensor([13, 4, 5, 1096972, 1, 8],"float32"), list[13,4,5,5,-1,], ) 
 shape '[13, 4, 5, 5, -1]' is invalid for input of size 2281701760
2025-03-18 15:33:23.494322 test begin: paddle.reshape(Tensor([13, 4, 5, 1096972, 8],"float32"), list[13,4,5,-1,], )

[Pass] paddle.reshape(Tensor([13, 4, 5, 1096972, 8],"float32"), list[13,4,5,-1,], )
2025-03-18 15:38:15.525141 test begin: paddle.reshape(Tensor([13, 4, 5, 5, 1, 1755155],"float32"), list[13,4,5,5,-1,], )

[Pass] paddle.reshape(Tensor([13, 4, 5, 5, 1, 1755155],"float32"), list[13,4,5,5,-1,], )
2025-03-18 15:41:20.415931 test begin: paddle.reshape(Tensor([13, 4, 5, 5, 219395, 8],"float32"), list[13,4,5,5,-1,], )

[Pass] paddle.reshape(Tensor([13, 4, 5, 5, 219395, 8],"float32"), list[13,4,5,5,-1,], )
2025-03-18 15:44:14.872716 test begin: paddle.reshape(Tensor([13, 4, 5484860, 1, 8],"float32"), list[13,4,5,-1,], )

[Pass] paddle.reshape(Tensor([13, 4, 5484860, 1, 8],"float32"), list[13,4,5,-1,], )
2025-03-18 15:47:09.598753 test begin: paddle.reshape(Tensor([13, 4, 5484860, 8],"float32"), list[13,4,7,1,-1,], )

[torch error] paddle.reshape(Tensor([13, 4, 5484860, 8],"float32"), list[13,4,7,1,-1,], ) 
 shape '[13, 4, 7, 1, -1]' is invalid for input of size 2281701760
2025-03-18 15:47:13.707869 test begin: paddle.reshape(Tensor([13, 4, 5484860, 8],"float32"), list[13,4,8,2,], )

[torch error] paddle.reshape(Tensor([13, 4, 5484860, 8],"float32"), list[13,4,8,2,], ) 
 shape '[13, 4, 8, 2]' is invalid for input of size 2281701760
2025-03-18 15:47:15.711254 test begin: paddle.reshape(Tensor([13, 4, 5484860, 8],"float32"), list[13,4,8,4,], )

[torch error] paddle.reshape(Tensor([13, 4, 5484860, 8],"float32"), list[13,4,8,4,], ) 
 shape '[13, 4, 8, 4]' is invalid for input of size 2281701760
2025-03-18 15:47:17.610903 test begin: paddle.reshape(Tensor([13, 4, 5484860, 8],"float32"), tuple(52,-1,8,), )

[Pass] paddle.reshape(Tensor([13, 4, 5484860, 8],"float32"), tuple(52,-1,8,), )
2025-03-18 15:50:36.830182 test begin: paddle.reshape(Tensor([13, 4, 6, 7313146],"float32"), list[13,4,2,6,], )

[torch error] paddle.reshape(Tensor([13, 4, 6, 7313146],"float32"), list[13,4,2,6,], ) 
 shape '[13, 4, 2, 6]' is invalid for input of size 2281701552
2025-03-18 15:50:41.393442 test begin: paddle.reshape(Tensor([13, 4, 6268411, 7],"float32"), list[13,4,7,13,], )

[torch error] paddle.reshape(Tensor([13, 4, 6268411, 7],"float32"), list[13,4,7,13,], ) 
 shape '[13, 4, 7, 13]' is invalid for input of size 2281701604
2025-03-18 15:50:43.119300 test begin: paddle.reshape(Tensor([13, 4, 7, 6268411],"float32"), list[13,4,14,7,], )

[torch error] paddle.reshape(Tensor([13, 4, 7, 6268411],"float32"), list[13,4,14,7,], ) 
 shape '[13, 4, 14, 7]' is invalid for input of size 2281701604
2025-03-18 15:50:45.195963 test begin: paddle.reshape(Tensor([13, 4, 7, 6268411],"float32"), list[13,4,4,7,], )

[torch error] paddle.reshape(Tensor([13, 4, 7, 6268411],"float32"), list[13,4,4,7,], ) 
 shape '[13, 4, 4, 7]' is invalid for input of size 2281701604
2025-03-18 15:50:47.131420 test begin: paddle.reshape(Tensor([13, 4, 7, 6268411],"float32"), list[13,4,7,1,-1,], )

[Pass] paddle.reshape(Tensor([13, 4, 7, 6268411],"float32"), list[13,4,7,1,-1,], )
2025-03-18 15:54:03.216965 test begin: paddle.reshape(Tensor([13, 4, 7, 6268411],"float32"), tuple(52,-1,8,), )

[torch error] paddle.reshape(Tensor([13, 4, 7, 6268411],"float32"), tuple(52,-1,8,), ) 
 shape '[52, -1, 8]' is invalid for input of size 2281701604
2025-03-18 15:54:07.742754 test begin: paddle.reshape(Tensor([13, 4, 8, 5484860],"float32"), shape=list[13,-1,32,], )

[Pass] paddle.reshape(Tensor([13, 4, 8, 5484860],"float32"), shape=list[13,-1,32,], )
2025-03-18 15:57:25.667102 test begin: paddle.reshape(Tensor([13, 4, 8, 5484860],"float32"), shape=list[13,32,-1,], )

[Pass] paddle.reshape(Tensor([13, 4, 8, 5484860],"float32"), shape=list[13,32,-1,], )
2025-03-18 16:01:17.731123 test begin: paddle.reshape(Tensor([13, 4387888, 5, 1, 8],"float32"), list[13,4,5,-1,], )

[Pass] paddle.reshape(Tensor([13, 4387888, 5, 1, 8],"float32"), list[13,4,5,-1,], )
2025-03-18 16:04:18.588986 test begin: paddle.reshape(Tensor([13, 5, 35103099],"float32"), list[13,5,1,1,], )

[torch error] paddle.reshape(Tensor([13, 5, 35103099],"float32"), list[13,5,1,1,], ) 
 shape '[13, 5, 1, 1]' is invalid for input of size 2281701435
2025-03-18 16:04:22.909224 test begin: paddle.reshape(Tensor([13, 5484860, 2, 16],"float32"), shape=list[13,-1,32,], )

[Pass] paddle.reshape(Tensor([13, 5484860, 2, 16],"float32"), shape=list[13,-1,32,], )
2025-03-18 16:06:55.887535 test begin: paddle.reshape(Tensor([13, 5484860, 32],"float32"), list[13,-1,4,8,], )

[Pass] paddle.reshape(Tensor([13, 5484860, 32],"float32"), list[13,-1,4,8,], )
2025-03-18 16:09:47.267070 test begin: paddle.reshape(Tensor([13, 5484860, 32],"float32"), list[13,21,4,8,], )

[torch error] paddle.reshape(Tensor([13, 5484860, 32],"float32"), list[13,21,4,8,], ) 
 shape '[13, 21, 4, 8]' is invalid for input of size 2281701760
2025-03-18 16:09:51.478732 test begin: paddle.reshape(Tensor([13, 5484860, 32],"float32"), list[13,7,4,-1,], )

[torch error] paddle.reshape(Tensor([13, 5484860, 32],"float32"), list[13,7,4,-1,], ) 
 shape '[13, 7, 4, -1]' is invalid for input of size 2281701760
2025-03-18 16:09:53.425064 test begin: paddle.reshape(Tensor([13, 5484860, 32],"float32"), list[13,7,4,8,], )

[torch error] paddle.reshape(Tensor([13, 5484860, 32],"float32"), list[13,7,4,8,], ) 
 shape '[13, 7, 4, 8]' is invalid for input of size 2281701760
2025-03-18 16:09:55.586695 test begin: paddle.reshape(Tensor([13, 5484860, 32],"float32"), list[13,7,8,4,], )

[torch error] paddle.reshape(Tensor([13, 5484860, 32],"float32"), list[13,7,8,4,], ) 
 shape '[13, 7, 8, 4]' is invalid for input of size 2281701760
2025-03-18 16:09:57.512852 test begin: paddle.reshape(Tensor([13, 5484860, 32],"float32"), tuple(13,-1,4,8,), )

[Pass] paddle.reshape(Tensor([13, 5484860, 32],"float32"), tuple(13,-1,4,8,), )
2025-03-18 16:12:56.079286 test begin: paddle.reshape(Tensor([13, 5484860, 32],"float32"), tuple(13,2,7,-1,), )

[torch error] paddle.reshape(Tensor([13, 5484860, 32],"float32"), tuple(13,2,7,-1,), ) 
 shape '[13, 2, 7, -1]' is invalid for input of size 2281701760
2025-03-18 16:13:00.858357 test begin: paddle.reshape(Tensor([13, 5484860, 32],"float32"), tuple(13,21,4,8,), )

[torch error] paddle.reshape(Tensor([13, 5484860, 32],"float32"), tuple(13,21,4,8,), ) 
 shape '[13, 21, 4, 8]' is invalid for input of size 2281701760
2025-03-18 16:13:02.781153 test begin: paddle.reshape(Tensor([13, 5484860, 32],"int64"), shape=list[13,1,-1,4,], )

[Pass] paddle.reshape(Tensor([13, 5484860, 32],"int64"), shape=list[13,1,-1,4,], )
2025-03-18 16:15:53.117768 test begin: paddle.reshape(Tensor([13, 5484860, 32],"int64"), shape=list[13,2,-1,4,], )

[Pass] paddle.reshape(Tensor([13, 5484860, 32],"int64"), shape=list[13,2,-1,4,], )
2025-03-18 16:18:55.411624 test begin: paddle.reshape(Tensor([13, 5484860, 4, 8],"float32"), list[13,4,8,4,], )

[torch error] paddle.reshape(Tensor([13, 5484860, 4, 8],"float32"), list[13,4,8,4,], ) 
 shape '[13, 4, 8, 4]' is invalid for input of size 2281701760
2025-03-18 16:19:00.299748 test begin: paddle.reshape(Tensor([13, 5484860, 4, 8],"float32"), list[13,7,-1,], )

[torch error] paddle.reshape(Tensor([13, 5484860, 4, 8],"float32"), list[13,7,-1,], ) 
 shape '[13, 7, -1]' is invalid for input of size 2281701760
2025-03-18 16:19:03.671957 test begin: paddle.reshape(Tensor([13, 5484860, 4, 8],"float32"), tuple(13,1,7,32,), )

[torch error] paddle.reshape(Tensor([13, 5484860, 4, 8],"float32"), tuple(13,1,7,32,), ) 
 shape '[13, 1, 7, 32]' is invalid for input of size 2281701760
2025-03-18 16:19:05.702269 test begin: paddle.reshape(Tensor([13, 5484860, 4, 8],"float32"), tuple(13,21,32,), )

[torch error] paddle.reshape(Tensor([13, 5484860, 4, 8],"float32"), tuple(13,21,32,), ) 
 shape '[13, 21, 32]' is invalid for input of size 2281701760
2025-03-18 16:19:07.762655 test begin: paddle.reshape(Tensor([13, 5484860, 4, 8],"float32"), tuple(13,7,32,), )

[torch error] paddle.reshape(Tensor([13, 5484860, 4, 8],"float32"), tuple(13,7,32,), ) 
 shape '[13, 7, 32]' is invalid for input of size 2281701760
2025-03-18 16:19:09.850129 test begin: paddle.reshape(Tensor([13, 6268411, 7, 4],"float32"), list[13,4,4,7,], )

[torch error] paddle.reshape(Tensor([13, 6268411, 7, 4],"float32"), list[13,4,4,7,], ) 
 shape '[13, 4, 4, 7]' is invalid for input of size 2281701604
2025-03-18 16:19:11.876082 test begin: paddle.reshape(Tensor([13, 685608, 8, 32],"float32"), shape=list[13,-1,32,], )

[Pass] paddle.reshape(Tensor([13, 685608, 8, 32],"float32"), shape=list[13,-1,32,], )
2025-03-18 16:22:25.408384 test begin: paddle.reshape(Tensor([13, 685608, 8, 32],"float32"), shape=list[13,32,-1,], )

[Pass] paddle.reshape(Tensor([13, 685608, 8, 32],"float32"), shape=list[13,32,-1,], )
2025-03-18 16:25:39.850229 test begin: paddle.reshape(Tensor([13, 7, 25073642],"float32"), list[13,-1,4,8,], )

[torch error] paddle.reshape(Tensor([13, 7, 25073642],"float32"), list[13,-1,4,8,], ) 
 shape '[13, -1, 4, 8]' is invalid for input of size 2281701422
2025-03-18 16:25:44.308952 test begin: paddle.reshape(Tensor([13, 7, 25073642],"float32"), list[13,7,4,-1,], )

[torch error] paddle.reshape(Tensor([13, 7, 25073642],"float32"), list[13,7,4,-1,], ) 
 shape '[13, 7, 4, -1]' is invalid for input of size 2281701422
2025-03-18 16:25:45.932351 test begin: paddle.reshape(Tensor([13, 7, 25073642],"float32"), list[13,7,4,8,], )

[torch error] paddle.reshape(Tensor([13, 7, 25073642],"float32"), list[13,7,4,8,], ) 
 shape '[13, 7, 4, 8]' is invalid for input of size 2281701422
2025-03-18 16:25:48.077678 test begin: paddle.reshape(Tensor([13, 7, 25073642],"float32"), list[13,7,8,4,], )

[torch error] paddle.reshape(Tensor([13, 7, 25073642],"float32"), list[13,7,8,4,], ) 
 shape '[13, 7, 8, 4]' is invalid for input of size 2281701422
2025-03-18 16:25:50.141987 test begin: paddle.reshape(Tensor([13, 7, 25073642],"float32"), tuple(13,7,4,4,), )

[torch error] paddle.reshape(Tensor([13, 7, 25073642],"float32"), tuple(13,7,4,4,), ) 
 shape '[13, 7, 4, 4]' is invalid for input of size 2281701422
2025-03-18 16:25:52.199039 test begin: paddle.reshape(Tensor([13, 7, 3134206, 8],"float32"), list[13,7,-1,], )

[Pass] paddle.reshape(Tensor([13, 7, 3134206, 8],"float32"), list[13,7,-1,], )
2025-03-18 16:28:47.311315 test begin: paddle.reshape(Tensor([13, 7, 3134206, 8],"float32"), tuple(13,1,7,32,), )

[torch error] paddle.reshape(Tensor([13, 7, 3134206, 8],"float32"), tuple(13,1,7,32,), ) 
 shape '[13, 1, 7, 32]' is invalid for input of size 2281701968
2025-03-18 16:28:51.951233 test begin: paddle.reshape(Tensor([13, 7, 3134206, 8],"float32"), tuple(13,7,32,), )

[torch error] paddle.reshape(Tensor([13, 7, 3134206, 8],"float32"), tuple(13,7,32,), ) 
 shape '[13, 7, 32]' is invalid for input of size 2281701968
2025-03-18 16:28:53.725377 test begin: paddle.reshape(Tensor([13, 7, 4, 6268411],"float32"), list[13,7,-1,], )

[Pass] paddle.reshape(Tensor([13, 7, 4, 6268411],"float32"), list[13,7,-1,], )
2025-03-18 16:32:07.735833 test begin: paddle.reshape(Tensor([13, 7, 4, 6268411],"float32"), tuple(13,1,7,32,), )

[torch error] paddle.reshape(Tensor([13, 7, 4, 6268411],"float32"), tuple(13,1,7,32,), ) 
 shape '[13, 1, 7, 32]' is invalid for input of size 2281701604
2025-03-18 16:32:12.437255 test begin: paddle.reshape(Tensor([13, 7, 4, 6268411],"float32"), tuple(13,7,32,), )

[torch error] paddle.reshape(Tensor([13, 7, 4, 6268411],"float32"), tuple(13,7,32,), ) 
 shape '[13, 7, 32]' is invalid for input of size 2281701604
2025-03-18 16:32:15.495295 test begin: paddle.reshape(Tensor([13, 783552, 7, 32],"float32"), tuple(13,-1,32,), )

[Pass] paddle.reshape(Tensor([13, 783552, 7, 32],"float32"), tuple(13,-1,32,), )
2025-03-18 16:35:38.374851 test begin: paddle.reshape(Tensor([13, 783552, 7, 4, 8],"float32"), tuple(13,2,7,32,), )

[torch error] paddle.reshape(Tensor([13, 783552, 7, 4, 8],"float32"), tuple(13,2,7,32,), ) 
 shape '[13, 2, 7, 32]' is invalid for input of size 2281703424
2025-03-18 16:35:43.024694 test begin: paddle.reshape(Tensor([13, 877578, 5, 5, 1, 8],"float32"), list[13,4,5,5,-1,], )

[Pass] paddle.reshape(Tensor([13, 877578, 5, 5, 1, 8],"float32"), list[13,4,5,5,-1,], )
2025-03-18 16:38:54.981685 test begin: paddle.reshape(Tensor([1303, 23, 272, 280],"float32"), shape=list[-1,272,280,], )

[Pass] paddle.reshape(Tensor([1303, 23, 272, 280],"float32"), shape=list[-1,272,280,], )
2025-03-18 16:42:13.017313 test begin: paddle.reshape(Tensor([1304, 24, 240, 304],"float32"), shape=list[-1,240,304,], )

[Pass] paddle.reshape(Tensor([1304, 24, 240, 304],"float32"), shape=list[-1,240,304,], )
2025-03-18 16:45:26.909648 test begin: paddle.reshape(Tensor([1306, 25, 208, 336],"float32"), shape=list[-1,208,336,], )

[Pass] paddle.reshape(Tensor([1306, 25, 208, 336],"float32"), shape=list[-1,208,336,], )
2025-03-18 16:48:42.316800 test begin: paddle.reshape(Tensor([13108, 2, 272, 320],"float32"), shape=list[-1,272,320,], )

[Pass] paddle.reshape(Tensor([13108, 2, 272, 320],"float32"), shape=list[-1,272,320,], )
2025-03-18 16:51:49.793288 test begin: paddle.reshape(Tensor([13108, 2, 320, 272],"float32"), shape=list[-1,320,272,], )

[Pass] paddle.reshape(Tensor([13108, 2, 320, 272],"float32"), shape=list[-1,320,272,], )
2025-03-18 16:54:59.920979 test begin: paddle.reshape(Tensor([13108, 2, 87040],"float32"), shape=tuple(-1,320,272,), )

[Pass] paddle.reshape(Tensor([13108, 2, 87040],"float32"), shape=tuple(-1,320,272,), )
2025-03-18 16:58:04.344781 test begin: paddle.reshape(Tensor([1315, 44, 74240],"float16"), shape=tuple(-1,232,320,), )

[Pass] paddle.reshape(Tensor([1315, 44, 74240],"float16"), shape=tuple(-1,232,320,), )
2025-03-18 17:14:33.660769 test begin: paddle.reshape(Tensor([132, 107032, 304],"float16"), shape=tuple(132,-1,), )

[Pass] paddle.reshape(Tensor([132, 107032, 304],"float16"), shape=tuple(132,-1,), )
2025-03-18 17:30:48.425100 test begin: paddle.reshape(Tensor([132, 119624, 272],"float16"), shape=tuple(132,-1,), )

[Pass] paddle.reshape(Tensor([132, 119624, 272],"float16"), shape=tuple(132,-1,), )
2025-03-18 17:46:47.501386 test begin: paddle.reshape(Tensor([132, 200, 162689],"float16"), shape=tuple(132,-1,), )

[Pass] paddle.reshape(Tensor([132, 200, 162689],"float16"), shape=tuple(132,-1,), )
2025-03-18 18:03:15.350446 test begin: paddle.reshape(Tensor([132, 200, 86429],"float32"), shape=tuple(132,-1,), )

[Pass] paddle.reshape(Tensor([132, 200, 86429],"float32"), shape=tuple(132,-1,), )
2025-03-18 18:06:08.203948 test begin: paddle.reshape(Tensor([132, 56861, 304],"float32"), shape=tuple(132,-1,), )

[Pass] paddle.reshape(Tensor([132, 56861, 304],"float32"), shape=tuple(132,-1,), )
2025-03-18 18:08:43.455803 test begin: paddle.reshape(Tensor([132, 63551, 272],"float32"), shape=tuple(132,-1,), )

[Pass] paddle.reshape(Tensor([132, 63551, 272],"float32"), shape=tuple(132,-1,), )
2025-03-18 18:11:43.912327 test begin: paddle.reshape(Tensor([132, 96839, 336],"float16"), shape=tuple(132,-1,), )

[Pass] paddle.reshape(Tensor([132, 96839, 336],"float16"), shape=tuple(132,-1,), )
2025-03-18 18:28:09.337814 test begin: paddle.reshape(Tensor([1323, 22, 280, 280],"float32"), shape=list[-1,280,280,], )

[Pass] paddle.reshape(Tensor([1323, 22, 280, 280],"float32"), shape=list[-1,280,280,], )
2025-03-18 18:30:51.212017 test begin: paddle.reshape(Tensor([1327, 20, 256, 336],"float32"), shape=list[-1,256,336,], )

[Pass] paddle.reshape(Tensor([1327, 20, 256, 336],"float32"), shape=list[-1,256,336,], )
2025-03-18 18:33:52.757616 test begin: paddle.reshape(Tensor([1330, 29, 200, 296],"float32"), shape=list[-1,200,296,], )

[Pass] paddle.reshape(Tensor([1330, 29, 200, 296],"float32"), shape=list[-1,200,296,], )
2025-03-18 18:36:45.474453 test begin: paddle.reshape(Tensor([1332, 30, 107520],"float16"), shape=tuple(-1,336,320,), )

[Pass] paddle.reshape(Tensor([1332, 30, 107520],"float16"), shape=tuple(-1,336,320,), )
2025-03-18 18:52:57.688598 test begin: paddle.reshape(Tensor([1333, 22, 256, 304],"float32"), shape=list[-1,256,304,], )

[Pass] paddle.reshape(Tensor([1333, 22, 256, 304],"float32"), shape=list[-1,256,304,], )
2025-03-18 18:56:23.284693 test begin: paddle.reshape(Tensor([13338, 108, 132, 12],"float32"), shape=tuple(2,-1,4,), )

[Pass] paddle.reshape(Tensor([13338, 108, 132, 12],"float32"), shape=tuple(2,-1,4,), )
2025-03-18 18:59:39.990805 test begin: paddle.reshape(Tensor([13379, 4, 80256],"float16"), shape=tuple(-1,304,264,), )

[Pass] paddle.reshape(Tensor([13379, 4, 80256],"float16"), shape=tuple(-1,304,264,), )
2025-03-18 19:16:00.010079 test begin: paddle.reshape(Tensor([1338, 26, 328, 200],"float32"), shape=list[-1,328,200,], )

[Pass] paddle.reshape(Tensor([1338, 26, 328, 200],"float32"), shape=list[-1,328,200,], )
2025-03-18 19:19:01.952134 test begin: paddle.reshape(Tensor([13403, 2, 304, 280],"float32"), shape=list[-1,304,280,], )

[Pass] paddle.reshape(Tensor([13403, 2, 304, 280],"float32"), shape=list[-1,304,280,], )
2025-03-18 19:22:25.472560 test begin: paddle.reshape(Tensor([134217729, 32],"float16"), shape=list[-1,4,32,], )

[torch error] paddle.reshape(Tensor([134217729, 32],"float16"), shape=list[-1,4,32,], ) 
 shape '[-1, 4, 32]' is invalid for input of size 4294967328
2025-03-18 19:22:29.405855 test begin: paddle.reshape(Tensor([13444, 3, 208, 272],"float32"), shape=list[-1,208,272,], )

[Pass] paddle.reshape(Tensor([13444, 3, 208, 272],"float32"), shape=list[-1,208,272,], )
2025-03-18 19:25:38.491485 test begin: paddle.reshape(Tensor([1347, 17, 304, 328],"float32"), shape=list[-1,304,328,], )

[Pass] paddle.reshape(Tensor([1347, 17, 304, 328],"float32"), shape=list[-1,304,328,], )
2025-03-18 19:28:20.030085 test begin: paddle.reshape(Tensor([1348, 18, 336, 280],"float32"), shape=list[-1,336,280,], )

[Pass] paddle.reshape(Tensor([1348, 18, 336, 280],"float32"), shape=list[-1,336,280,], )
2025-03-18 19:31:28.122888 test begin: paddle.reshape(Tensor([13581556, 21, 8],"float32"), tuple(13,4,21,8,), )

[torch error] paddle.reshape(Tensor([13581556, 21, 8],"float32"), tuple(13,4,21,8,), ) 
 shape '[13, 4, 21, 8]' is invalid for input of size 2281701408
2025-03-18 19:31:32.304236 test begin: paddle.reshape(Tensor([1358156, 10, 14, 12],"float32"), shape=tuple(1,-1,4,), )

[Pass] paddle.reshape(Tensor([1358156, 10, 14, 12],"float32"), shape=tuple(1,-1,4,), )
2025-03-18 19:34:23.786750 test begin: paddle.reshape(Tensor([13582, 100, 140, 12],"float32"), shape=tuple(8,-1,4,), )

[Pass] paddle.reshape(Tensor([13582, 100, 140, 12],"float32"), shape=tuple(8,-1,4,), )
2025-03-18 19:37:19.584313 test begin: paddle.reshape(Tensor([136, 101221, 312],"float16"), shape=tuple(136,-1,), )

[Pass] paddle.reshape(Tensor([136, 101221, 312],"float16"), shape=tuple(136,-1,), )
2025-03-18 19:53:37.208304 test begin: paddle.reshape(Tensor([136, 103884, 304],"float16"), shape=tuple(136,-1,), )

[Pass] paddle.reshape(Tensor([136, 103884, 304],"float16"), shape=tuple(136,-1,), )
2025-03-18 20:10:15.754979 test begin: paddle.reshape(Tensor([136, 116106, 272],"float16"), shape=tuple(136,-1,), )

[Pass] paddle.reshape(Tensor([136, 116106, 272],"float16"), shape=tuple(136,-1,), )
2025-03-18 20:26:23.323875 test begin: paddle.reshape(Tensor([136, 200, 157904],"float16"), shape=tuple(136,-1,), )

[Pass] paddle.reshape(Tensor([136, 200, 157904],"float16"), shape=tuple(136,-1,), )
2025-03-18 20:42:05.102413 test begin: paddle.reshape(Tensor([136, 200, 83887],"float32"), shape=tuple(136,-1,), )

[Pass] paddle.reshape(Tensor([136, 200, 83887],"float32"), shape=tuple(136,-1,), )
2025-03-18 20:45:04.534056 test begin: paddle.reshape(Tensor([136, 55189, 304],"float32"), shape=tuple(136,-1,), )

[Pass] paddle.reshape(Tensor([136, 55189, 304],"float32"), shape=tuple(136,-1,), )
2025-03-18 20:47:40.807813 test begin: paddle.reshape(Tensor([136, 61681, 272],"float32"), shape=tuple(136,-1,), )

[Pass] paddle.reshape(Tensor([136, 61681, 272],"float32"), shape=tuple(136,-1,), )
2025-03-18 20:50:44.690379 test begin: paddle.reshape(Tensor([1372, 19, 304, 288],"float32"), shape=list[-1,304,288,], )

[Pass] paddle.reshape(Tensor([1372, 19, 304, 288],"float32"), shape=list[-1,304,288,], )
2025-03-18 20:53:19.542852 test begin: paddle.reshape(Tensor([1373, 39, 80256],"float16"), shape=tuple(-1,304,264,), )

[Pass] paddle.reshape(Tensor([1373, 39, 80256],"float16"), shape=tuple(-1,304,264,), )
2025-03-18 21:09:15.452637 test begin: paddle.reshape(Tensor([13798, 152, 272, 4],"float32"), shape=list[20,-1,4,], )

[torch error] paddle.reshape(Tensor([13798, 152, 272, 4],"float32"), shape=list[20,-1,4,], ) 
 shape '[20, -1, 4]' is invalid for input of size 2281858048
2025-03-18 21:09:19.392040 test begin: paddle.reshape(Tensor([1380, 17, 320, 304],"float32"), shape=list[-1,320,304,], )

[Pass] paddle.reshape(Tensor([1380, 17, 320, 304],"float32"), shape=list[-1,320,304,], )
2025-03-18 21:12:30.245851 test begin: paddle.reshape(Tensor([1386, 21, 280, 280],"float32"), shape=list[-1,280,280,], )

[Pass] paddle.reshape(Tensor([1386, 21, 280, 280],"float32"), shape=list[-1,280,280,], )
2025-03-18 21:15:15.330694 test begin: paddle.reshape(Tensor([1388, 28, 272, 216],"float32"), shape=list[-1,272,216,], )

[Pass] paddle.reshape(Tensor([1388, 28, 272, 216],"float32"), shape=list[-1,272,216,], )
2025-03-18 21:18:15.984341 test begin: paddle.reshape(Tensor([139265, 256, 64],"float32"), list[256,64,1,1,], )

[torch error] paddle.reshape(Tensor([139265, 256, 64],"float32"), list[256,64,1,1,], ) 
 shape '[256, 64, 1, 1]' is invalid for input of size 2281717760
2025-03-18 21:18:20.099946 test begin: paddle.reshape(Tensor([139265, 64, 256],"float32"), list[64,256,1,1,], )

[torch error] paddle.reshape(Tensor([139265, 64, 256],"float32"), list[64,256,1,1,], ) 
 shape '[64, 256, 1, 1]' is invalid for input of size 2281717760
2025-03-18 21:18:21.742527 test begin: paddle.reshape(Tensor([1397, 16, 336, 304],"float32"), shape=list[-1,336,304,], )

[Pass] paddle.reshape(Tensor([1397, 16, 336, 304],"float32"), shape=list[-1,336,304,], )
2025-03-18 21:21:26.375719 test begin: paddle.reshape(Tensor([13982, 100, 136, 12],"float32"), shape=tuple(8,-1,4,), )

[Pass] paddle.reshape(Tensor([13982, 100, 136, 12],"float32"), shape=tuple(8,-1,4,), )
2025-03-18 21:24:22.737912 test begin: paddle.reshape(Tensor([14, 10, 16297867],"float32"), shape=list[14,-1,4,8,], )

[torch error] paddle.reshape(Tensor([14, 10, 16297867],"float32"), shape=list[14,-1,4,8,], ) 
 shape '[14, -1, 4, 8]' is invalid for input of size 2281701380
2025-03-18 21:24:26.838586 test begin: paddle.reshape(Tensor([14, 11, 14816243],"float32"), shape=list[14,-1,4,8,], )

[torch error] paddle.reshape(Tensor([14, 11, 14816243],"float32"), shape=list[14,-1,4,8,], ) 
 shape '[14, -1, 4, 8]' is invalid for input of size 2281701422
2025-03-18 21:24:28.677397 test begin: paddle.reshape(Tensor([14, 1273271, 8, 16],"float32"), list[14,4,16,8,], )

[torch error] paddle.reshape(Tensor([14, 1273271, 8, 16],"float32"), list[14,4,16,8,], ) 
 shape '[14, 4, 16, 8]' is invalid for input of size 2281701632
2025-03-18 21:24:30.228291 test begin: paddle.reshape(Tensor([14, 1358156, 15, 8],"float32"), list[14,4,8,15,], )

[torch error] paddle.reshape(Tensor([14, 1358156, 15, 8],"float32"), list[14,4,8,15,], ) 
 shape '[14, 4, 8, 15]' is invalid for input of size 2281702080
2025-03-18 21:24:31.257815 test begin: paddle.reshape(Tensor([14, 14, 11641334],"float32"), shape=list[14,-1,4,8,], )

[torch error] paddle.reshape(Tensor([14, 14, 11641334],"float32"), shape=list[14,-1,4,8,], ) 
 shape '[14, -1, 4, 8]' is invalid for input of size 2281701464
2025-03-18 21:24:32.326485 test begin: paddle.reshape(Tensor([14, 14, 11641334],"float32"), shape=list[14,14,4,8,], )

[torch error] paddle.reshape(Tensor([14, 14, 11641334],"float32"), shape=list[14,14,4,8,], ) 
 shape '[14, 14, 4, 8]' is invalid for input of size 2281701464
2025-03-18 21:24:33.381836 test begin: paddle.reshape(Tensor([14, 1455167, 8, 14],"float32"), list[14,4,14,8,], )

[torch error] paddle.reshape(Tensor([14, 1455167, 8, 14],"float32"), list[14,4,14,8,], ) 
 shape '[14, 4, 14, 8]' is invalid for input of size 2281701856
2025-03-18 21:24:34.457922 test begin: paddle.reshape(Tensor([14, 1567103, 13, 8],"float32"), list[14,4,8,13,], )

[torch error] paddle.reshape(Tensor([14, 1567103, 13, 8],"float32"), list[14,4,8,13,], ) 
 shape '[14, 4, 8, 13]' is invalid for input of size 2281701968
2025-03-18 21:24:35.477516 test begin: paddle.reshape(Tensor([14, 162978670],"int64"), list[14,], )

[torch error] paddle.reshape(Tensor([14, 162978670],"int64"), list[14,], ) 
 shape '[14]' is invalid for input of size 2281701380
2025-03-18 21:24:44.355865 test begin: paddle.reshape(Tensor([14, 1663048, 7, 14],"float32"), list[14,4,14,7,], )

[torch error] paddle.reshape(Tensor([14, 1663048, 7, 14],"float32"), list[14,4,14,7,], ) 
 shape '[14, 4, 14, 7]' is invalid for input of size 2281701856
2025-03-18 21:24:46.036919 test begin: paddle.reshape(Tensor([14, 1790975, 13, 7],"float32"), list[14,4,7,13,], )

[torch error] paddle.reshape(Tensor([14, 1790975, 13, 7],"float32"), list[14,4,7,13,], ) 
 shape '[14, 4, 7, 13]' is invalid for input of size 2281702150
2025-03-18 21:24:47.476661 test begin: paddle.reshape(Tensor([14, 209, 779803],"float32"), list[-1,50000,], )

[torch error] paddle.reshape(Tensor([14, 209, 779803],"float32"), list[-1,50000,], ) 
 shape '[-1, 50000]' is invalid for input of size 2281703578
2025-03-18 21:24:49.454075 test begin: paddle.reshape(Tensor([14, 3260, 50000],"float32"), list[-1,50000,], )

[Pass] paddle.reshape(Tensor([14, 3260, 50000],"float32"), list[-1,50000,], )
2025-03-18 21:27:56.372333 test begin: paddle.reshape(Tensor([14, 4, 13, 3134206],"float32"), list[14,4,7,13,], )

[torch error] paddle.reshape(Tensor([14, 4, 13, 3134206],"float32"), list[14,4,7,13,], ) 
 shape '[14, 4, 7, 13]' is invalid for input of size 2281701968
2025-03-18 21:28:00.585781 test begin: paddle.reshape(Tensor([14, 4, 13, 3134206],"float32"), list[14,4,8,13,], )

[torch error] paddle.reshape(Tensor([14, 4, 13, 3134206],"float32"), list[14,4,8,13,], ) 
 shape '[14, 4, 8, 13]' is invalid for input of size 2281701968
2025-03-18 21:28:01.759507 test begin: paddle.reshape(Tensor([14, 4, 15, 2716312],"float32"), list[14,4,8,15,], )

[torch error] paddle.reshape(Tensor([14, 4, 15, 2716312],"float32"), list[14,4,8,15,], ) 
 shape '[14, 4, 8, 15]' is invalid for input of size 2281702080
2025-03-18 21:28:02.807224 test begin: paddle.reshape(Tensor([14, 4, 2546542, 16],"float32"), list[14,4,16,8,], )

[torch error] paddle.reshape(Tensor([14, 4, 2546542, 16],"float32"), list[14,4,16,8,], ) 
 shape '[14, 4, 16, 8]' is invalid for input of size 2281701632
2025-03-18 21:28:05.020109 test begin: paddle.reshape(Tensor([14, 4, 2910334, 14],"float32"), list[14,4,14,7,], )

[torch error] paddle.reshape(Tensor([14, 4, 2910334, 14],"float32"), list[14,4,14,7,], ) 
 shape '[14, 4, 14, 7]' is invalid for input of size 2281701856
2025-03-18 21:28:07.137691 test begin: paddle.reshape(Tensor([14, 4, 2910334, 14],"float32"), list[14,4,14,8,], )

[torch error] paddle.reshape(Tensor([14, 4, 2910334, 14],"float32"), list[14,4,14,8,], ) 
 shape '[14, 4, 14, 8]' is invalid for input of size 2281701856
2025-03-18 21:28:08.822989 test begin: paddle.reshape(Tensor([14, 4, 40744668],"float32"), shape=list[14,-1,4,8,], )

[torch error] paddle.reshape(Tensor([14, 4, 40744668],"float32"), shape=list[14,-1,4,8,], ) 
 shape '[14, -1, 4, 8]' is invalid for input of size 2281701408
2025-03-18 21:28:09.976276 test begin: paddle.reshape(Tensor([14, 4, 5093084, 8],"float32"), list[14,4,8,13,], )

[torch error] paddle.reshape(Tensor([14, 4, 5093084, 8],"float32"), list[14,4,8,13,], ) 
 shape '[14, 4, 8, 13]' is invalid for input of size 2281701632
2025-03-18 21:28:11.099233 test begin: paddle.reshape(Tensor([14, 4, 5093084, 8],"float32"), list[14,4,8,15,], )

[torch error] paddle.reshape(Tensor([14, 4, 5093084, 8],"float32"), list[14,4,8,15,], ) 
 shape '[14, 4, 8, 15]' is invalid for input of size 2281701632
2025-03-18 21:28:12.209804 test begin: paddle.reshape(Tensor([14, 4, 5820667, 7],"float32"), list[14,4,7,13,], )

[torch error] paddle.reshape(Tensor([14, 4, 5820667, 7],"float32"), list[14,4,7,13,], ) 
 shape '[14, 4, 7, 13]' is invalid for input of size 2281701464
2025-03-18 21:28:13.320500 test begin: paddle.reshape(Tensor([14, 4, 7, 5820667],"float32"), list[14,4,14,7,], )

[torch error] paddle.reshape(Tensor([14, 4, 7, 5820667],"float32"), list[14,4,14,7,], ) 
 shape '[14, 4, 14, 7]' is invalid for input of size 2281701464
2025-03-18 21:28:14.427342 test begin: paddle.reshape(Tensor([14, 4, 8, 5093084],"float32"), list[14,4,14,8,], )

[torch error] paddle.reshape(Tensor([14, 4, 8, 5093084],"float32"), list[14,4,14,8,], ) 
 shape '[14, 4, 14, 8]' is invalid for input of size 2281701632
2025-03-18 21:28:15.539216 test begin: paddle.reshape(Tensor([14, 4, 8, 5093084],"float32"), list[14,4,16,8,], )

[torch error] paddle.reshape(Tensor([14, 4, 8, 5093084],"float32"), list[14,4,16,8,], ) 
 shape '[14, 4, 16, 8]' is invalid for input of size 2281701632
2025-03-18 21:28:16.970687 test begin: paddle.reshape(Tensor([14, 5093084, 32],"float32"), shape=list[14,-1,4,8,], )

[Pass] paddle.reshape(Tensor([14, 5093084, 32],"float32"), shape=list[14,-1,4,8,], )
2025-03-18 21:31:23.258803 test begin: paddle.reshape(Tensor([14, 5093084, 32],"float32"), shape=list[14,14,4,8,], )

[torch error] paddle.reshape(Tensor([14, 5093084, 32],"float32"), shape=list[14,14,4,8,], ) 
 shape '[14, 14, 4, 8]' is invalid for input of size 2281701632
2025-03-18 21:31:27.939209 test begin: paddle.reshape(Tensor([14, 56, 2910334],"float32"), shape=list[14,-1,4,8,], )

[torch error] paddle.reshape(Tensor([14, 56, 2910334],"float32"), shape=list[14,-1,4,8,], ) 
 shape '[14, -1, 4, 8]' is invalid for input of size 2281701856
2025-03-18 21:31:29.609748 test begin: paddle.reshape(Tensor([140, 100916, 304],"float16"), shape=tuple(140,-1,), )

[Pass] paddle.reshape(Tensor([140, 100916, 304],"float16"), shape=tuple(140,-1,), )
2025-03-18 21:48:11.038203 test begin: paddle.reshape(Tensor([140, 106523, 288],"float16"), shape=tuple(140,-1,), )

[Pass] paddle.reshape(Tensor([140, 106523, 288],"float16"), shape=tuple(140,-1,), )
2025-03-18 22:04:42.420694 test begin: paddle.reshape(Tensor([140, 109566, 280],"float16"), shape=tuple(140,-1,), )

[Pass] paddle.reshape(Tensor([140, 109566, 280],"float16"), shape=tuple(140,-1,), )
2025-03-18 22:21:29.680260 test begin: paddle.reshape(Tensor([140, 200, 153392],"float16"), shape=tuple(140,-1,), )

[Pass] paddle.reshape(Tensor([140, 200, 153392],"float16"), shape=tuple(140,-1,), )
2025-03-18 22:37:45.506565 test begin: paddle.reshape(Tensor([140, 200, 81490],"float32"), shape=tuple(140,-1,), )

[Pass] paddle.reshape(Tensor([140, 200, 81490],"float32"), shape=tuple(140,-1,), )
2025-03-18 22:41:01.318409 test begin: paddle.reshape(Tensor([140, 56590, 288],"float32"), shape=tuple(140,-1,), )

[Pass] paddle.reshape(Tensor([140, 56590, 288],"float32"), shape=tuple(140,-1,), )
2025-03-18 22:43:55.672577 test begin: paddle.reshape(Tensor([140, 58207, 280],"float32"), shape=tuple(140,-1,), )

[Pass] paddle.reshape(Tensor([140, 58207, 280],"float32"), shape=tuple(140,-1,), )
2025-03-18 22:46:30.762918 test begin: paddle.reshape(Tensor([1410, 24, 248, 272],"float32"), shape=list[-1,248,272,], )

[Pass] paddle.reshape(Tensor([1410, 24, 248, 272],"float32"), shape=list[-1,248,272,], )
2025-03-18 22:49:32.728277 test begin: paddle.reshape(Tensor([1410, 24, 67456],"float32"), shape=tuple(-1,248,272,), )

[Pass] paddle.reshape(Tensor([1410, 24, 67456],"float32"), shape=tuple(-1,248,272,), )
2025-03-18 22:52:35.692953 test begin: paddle.reshape(Tensor([1411, 19, 304, 280],"float32"), shape=list[-1,304,280,], )

[Pass] paddle.reshape(Tensor([1411, 19, 304, 280],"float32"), shape=list[-1,304,280,], )
2025-03-18 22:55:32.695323 test begin: paddle.reshape(Tensor([1411, 19, 85120],"float32"), shape=tuple(-1,304,280,), )

[Pass] paddle.reshape(Tensor([1411, 19, 85120],"float32"), shape=tuple(-1,304,280,), )
2025-03-18 22:58:36.794948 test begin: paddle.reshape(Tensor([1424, 46, 65600],"float16"), shape=tuple(-1,200,328,), )

[Pass] paddle.reshape(Tensor([1424, 46, 65600],"float16"), shape=tuple(-1,200,328,), )
2025-03-18 23:14:43.660004 test begin: paddle.reshape(Tensor([142606337, 16],"float32"), list[-1,1,], )

[Pass] paddle.reshape(Tensor([142606337, 16],"float32"), list[-1,1,], )
2025-03-18 23:17:27.358432 test begin: paddle.reshape(Tensor([142606337, 16],"float32"), shape=list[-1,4,16,], )

[torch error] paddle.reshape(Tensor([142606337, 16],"float32"), shape=list[-1,4,16,], ) 
 shape '[-1, 4, 16]' is invalid for input of size 2281701392
2025-03-18 23:17:31.159828 test begin: paddle.reshape(Tensor([142606337, 4, 4],"float32"), tuple(-1,4,), )

[Pass] paddle.reshape(Tensor([142606337, 4, 4],"float32"), tuple(-1,4,), )
2025-03-18 23:20:09.022419 test begin: paddle.reshape(Tensor([14260634, 4, 5, 1, 8],"float32"), list[13,4,5,-1,], )

[torch error] paddle.reshape(Tensor([14260634, 4, 5, 1, 8],"float32"), list[13,4,5,-1,], ) 
 shape '[13, 4, 5, -1]' is invalid for input of size 2281701440
2025-03-18 23:20:12.980679 test begin: paddle.reshape(Tensor([14260634, 4, 5, 1, 8],"float32"), list[52,4,5,-1,], )

[torch error] paddle.reshape(Tensor([14260634, 4, 5, 1, 8],"float32"), list[52,4,5,-1,], ) 
 shape '[52, 4, 5, -1]' is invalid for input of size 2281701440
2025-03-18 23:20:14.493084 test begin: paddle.reshape(Tensor([1426064, 1600],"int64"), shape=list[-1,], )

[Pass] paddle.reshape(Tensor([1426064, 1600],"int64"), shape=list[-1,], )
2025-03-18 23:22:49.533262 test begin: paddle.reshape(Tensor([14267, 56, 56, 96],"float16"), list[128,56,56,96,], )

[torch error] paddle.reshape(Tensor([14267, 56, 56, 96],"float16"), list[128,56,56,96,], ) 
 shape '[128, 56, 56, 96]' is invalid for input of size 4295165952
2025-03-18 23:22:53.175523 test begin: paddle.reshape(Tensor([1427, 17, 336, 280],"float32"), shape=list[-1,336,280,], )

[Pass] paddle.reshape(Tensor([1427, 17, 336, 280],"float32"), shape=list[-1,336,280,], )
2025-03-18 23:25:37.847343 test begin: paddle.reshape(Tensor([1427, 32, 94080],"float16"), shape=tuple(-1,336,280,), )

[Pass] paddle.reshape(Tensor([1427, 32, 94080],"float16"), shape=tuple(-1,336,280,), )
2025-03-18 23:41:23.689390 test begin: paddle.reshape(Tensor([143, 256, 200, 312],"float32"), shape=tuple(1,256,-1,), )

[Pass] paddle.reshape(Tensor([143, 256, 200, 312],"float32"), shape=tuple(1,256,-1,), )
2025-03-18 23:45:09.202333 test begin: paddle.reshape(Tensor([1431, 16, 304, 328],"float32"), shape=list[-1,304,328,], )

[Pass] paddle.reshape(Tensor([1431, 16, 304, 328],"float32"), shape=list[-1,304,328,], )
2025-03-18 23:48:56.798795 test begin: paddle.reshape(Tensor([1431, 16, 328, 304],"float32"), shape=list[-1,328,304,], )

[Pass] paddle.reshape(Tensor([1431, 16, 328, 304],"float32"), shape=list[-1,328,304,], )
2025-03-18 23:53:10.942268 test begin: paddle.reshape(Tensor([1431655766, 3],"float16"), list[-1,], )

[Pass] paddle.reshape(Tensor([1431655766, 3],"float16"), list[-1,], )
2025-03-19 00:09:48.986451 test begin: paddle.reshape(Tensor([1431655766, 3],"float16"), list[2,3,], )

[torch error] paddle.reshape(Tensor([1431655766, 3],"float16"), list[2,3,], ) 
 shape '[2, 3]' is invalid for input of size 4294967298
2025-03-19 00:09:53.420530 test begin: paddle.reshape(Tensor([1431655766, 3],"float16"), shape=list[4,-1,], )

[torch error] paddle.reshape(Tensor([1431655766, 3],"float16"), shape=list[4,-1,], ) 
 shape '[4, -1]' is invalid for input of size 4294967298
2025-03-19 00:09:54.817950 test begin: paddle.reshape(Tensor([14349, 24, 6626],"float32"), list[-1,6626,], )

[Pass] paddle.reshape(Tensor([14349, 24, 6626],"float32"), list[-1,6626,], )
2025-03-19 00:13:18.252802 test begin: paddle.reshape(Tensor([14350324, 1, 159],"float32"), shape=list[-1,159,], )

[Pass] paddle.reshape(Tensor([14350324, 1, 159],"float32"), shape=list[-1,159,], )
2025-03-19 00:16:23.335192 test begin: paddle.reshape(Tensor([144, 103564, 288],"float16"), shape=tuple(144,-1,), )

[Pass] paddle.reshape(Tensor([144, 103564, 288],"float16"), shape=tuple(144,-1,), )
2025-03-19 00:32:54.368843 test begin: paddle.reshape(Tensor([144, 109656, 272],"float16"), shape=tuple(144,-1,), )

[Pass] paddle.reshape(Tensor([144, 109656, 272],"float16"), shape=tuple(144,-1,), )
2025-03-19 00:49:08.945207 test begin: paddle.reshape(Tensor([144, 15845149],"float32"), shape=list[-1,1000,], )

[torch error] paddle.reshape(Tensor([144, 15845149],"float32"), shape=list[-1,1000,], ) 
 shape '[-1, 1000]' is invalid for input of size 2281701456
2025-03-19 00:49:13.096886 test begin: paddle.reshape(Tensor([144, 200, 149131],"float16"), shape=tuple(144,-1,), )

[Pass] paddle.reshape(Tensor([144, 200, 149131],"float16"), shape=tuple(144,-1,), )
2025-03-19 01:05:24.568433 test begin: paddle.reshape(Tensor([144, 200, 79226],"float32"), shape=tuple(144,-1,), )

[Pass] paddle.reshape(Tensor([144, 200, 79226],"float32"), shape=tuple(144,-1,), )
2025-03-19 01:08:14.202138 test begin: paddle.reshape(Tensor([144, 55018, 288],"float32"), shape=tuple(144,-1,), )

[Pass] paddle.reshape(Tensor([144, 55018, 288],"float32"), shape=tuple(144,-1,), )
2025-03-19 01:10:48.130654 test begin: paddle.reshape(Tensor([144, 58255, 272],"float32"), shape=tuple(144,-1,), )

[Pass] paddle.reshape(Tensor([144, 58255, 272],"float32"), shape=tuple(144,-1,), )
2025-03-19 01:13:42.467052 test begin: paddle.reshape(Tensor([144, 98113, 304],"float16"), shape=tuple(144,-1,), )

[Pass] paddle.reshape(Tensor([144, 98113, 304],"float16"), shape=tuple(144,-1,), )
2025-03-19 01:29:56.722980 test begin: paddle.reshape(Tensor([1442, 21, 248, 304],"float32"), shape=list[-1,248,304,], )

[Pass] paddle.reshape(Tensor([1442, 21, 248, 304],"float32"), shape=list[-1,248,304,], )
2025-03-19 01:33:14.321886 test begin: paddle.reshape(Tensor([1448, 24, 304, 216],"float32"), shape=list[-1,304,216,], )

[Pass] paddle.reshape(Tensor([1448, 24, 304, 216],"float32"), shape=list[-1,304,216,], )
2025-03-19 01:36:35.837117 test begin: paddle.reshape(Tensor([1454, 17, 312, 296],"float32"), shape=list[-1,312,296,], )

[Pass] paddle.reshape(Tensor([1454, 17, 312, 296],"float32"), shape=list[-1,312,296,], )
2025-03-19 01:39:44.302404 test begin: paddle.reshape(Tensor([1466, 16, 304, 320],"float32"), shape=list[-1,304,320,], )

[Pass] paddle.reshape(Tensor([1466, 16, 304, 320],"float32"), shape=list[-1,304,320,], )
2025-03-19 01:42:58.050736 test begin: paddle.reshape(Tensor([147, 256, 200, 304],"float32"), shape=tuple(1,256,-1,), )

[Pass] paddle.reshape(Tensor([147, 256, 200, 304],"float32"), shape=tuple(1,256,-1,), )
2025-03-19 01:46:01.454862 test begin: paddle.reshape(Tensor([1473, 35, 83328],"float16"), shape=tuple(-1,248,336,), )

[Pass] paddle.reshape(Tensor([1473, 35, 83328],"float16"), shape=tuple(-1,248,336,), )
2025-03-19 02:01:55.134373 test begin: paddle.reshape(Tensor([148, 200, 145101],"float16"), shape=tuple(148,-1,), )

[Pass] paddle.reshape(Tensor([148, 200, 145101],"float16"), shape=tuple(148,-1,), )
2025-03-19 02:18:03.938263 test begin: paddle.reshape(Tensor([148, 200, 77085],"float32"), shape=tuple(148,-1,), )

[Pass] paddle.reshape(Tensor([148, 200, 77085],"float32"), shape=tuple(148,-1,), )
2025-03-19 02:20:41.947511 test begin: paddle.reshape(Tensor([148, 50714, 304],"float32"), shape=tuple(148,-1,), )

[Pass] paddle.reshape(Tensor([148, 50714, 304],"float32"), shape=tuple(148,-1,), )
2025-03-19 02:24:22.113961 test begin: paddle.reshape(Tensor([148, 52085, 296],"float32"), shape=tuple(148,-1,), )

[Pass] paddle.reshape(Tensor([148, 52085, 296],"float32"), shape=tuple(148,-1,), )
2025-03-19 02:27:27.145287 test begin: paddle.reshape(Tensor([148, 86370, 336],"float16"), shape=tuple(148,-1,), )

[Pass] paddle.reshape(Tensor([148, 86370, 336],"float16"), shape=tuple(148,-1,), )
2025-03-19 02:43:59.172951 test begin: paddle.reshape(Tensor([148, 95461, 304],"float16"), shape=tuple(148,-1,), )

[Pass] paddle.reshape(Tensor([148, 95461, 304],"float16"), shape=tuple(148,-1,), )
2025-03-19 03:00:18.054557 test begin: paddle.reshape(Tensor([148, 98041, 296],"float16"), shape=tuple(148,-1,), )

[Pass] paddle.reshape(Tensor([148, 98041, 296],"float16"), shape=tuple(148,-1,), )
2025-03-19 03:16:58.735567 test begin: paddle.reshape(Tensor([1484, 39, 74240],"float16"), shape=tuple(-1,232,320,), )

[Pass] paddle.reshape(Tensor([1484, 39, 74240],"float16"), shape=tuple(-1,232,320,), )
2025-03-19 03:33:38.324508 test begin: paddle.reshape(Tensor([1485483, 3, 1, 16, 1, 32, 1],"float32"), list[1,6,1,8,1,4,1,8,1,], )

[torch error] paddle.reshape(Tensor([1485483, 3, 1, 16, 1, 32, 1],"float32"), list[1,6,1,8,1,4,1,8,1,], ) 
 shape '[1, 6, 1, 8, 1, 4, 1, 8, 1]' is invalid for input of size 2281701888
2025-03-19 03:33:42.233637 test begin: paddle.reshape(Tensor([1490, 15, 336, 304],"float32"), shape=list[-1,336,304,], )

[Pass] paddle.reshape(Tensor([1490, 15, 336, 304],"float32"), shape=list[-1,336,304,], )
2025-03-19 03:36:31.044520 test begin: paddle.reshape(Tensor([1490, 21, 240, 304],"float32"), shape=list[-1,240,304,], )

[Pass] paddle.reshape(Tensor([1490, 21, 240, 304],"float32"), shape=list[-1,240,304,], )
2025-03-19 03:40:12.617260 test begin: paddle.reshape(Tensor([1490, 22, 256, 272],"float32"), shape=list[-1,256,272,], )

[Pass] paddle.reshape(Tensor([1490, 22, 256, 272],"float32"), shape=list[-1,256,272,], )
2025-03-19 03:43:16.291477 test begin: paddle.reshape(Tensor([1490, 22, 69632],"float32"), shape=tuple(-1,256,272,), )

[Pass] paddle.reshape(Tensor([1490, 22, 69632],"float32"), shape=tuple(-1,256,272,), )
2025-03-19 03:46:40.324584 test begin: paddle.reshape(Tensor([1498, 20, 272, 280],"float32"), shape=list[-1,272,280,], )

[Pass] paddle.reshape(Tensor([1498, 20, 272, 280],"float32"), shape=list[-1,272,280,], )
2025-03-19 03:49:43.717041 test begin: paddle.reshape(Tensor([14980, 2, 280, 272],"float32"), shape=list[-1,280,272,], )

[Pass] paddle.reshape(Tensor([14980, 2, 280, 272],"float32"), shape=list[-1,280,272,], )
2025-03-19 03:53:13.375209 test begin: paddle.reshape(Tensor([15, 152113426],"float32"), list[1,-1,4,], )

[torch error] paddle.reshape(Tensor([15, 152113426],"float32"), list[1,-1,4,], ) 
 shape '[1, -1, 4]' is invalid for input of size 2281701390
2025-03-19 03:53:17.265175 test begin: paddle.reshape(Tensor([15, 152113426],"float32"), shape=list[1,3,5,6,], )

[torch error] paddle.reshape(Tensor([15, 152113426],"float32"), shape=list[1,3,5,6,], ) 
 shape '[1, 3, 5, 6]' is invalid for input of size 2281701390
2025-03-19 03:53:18.416198 test begin: paddle.reshape(Tensor([15, 152113426],"int64"), list[15,], )

[torch error] paddle.reshape(Tensor([15, 152113426],"int64"), list[15,], ) 
 shape '[15]' is invalid for input of size 2281701390
2025-03-19 03:53:27.567403 test begin: paddle.reshape(Tensor([15, 286331154],"float16"), list[1,-1,4,], )

[torch error] paddle.reshape(Tensor([15, 286331154],"float16"), list[1,-1,4,], ) 
 shape '[1, -1, 4]' is invalid for input of size 4294967310
2025-03-19 03:53:31.119048 test begin: paddle.reshape(Tensor([15, 286331154],"float16"), shape=list[1,3,5,6,], )

[torch error] paddle.reshape(Tensor([15, 286331154],"float16"), shape=list[1,3,5,6,], ) 
 shape '[1, 3, 5, 6]' is invalid for input of size 4294967310
2025-03-19 03:53:32.412482 test begin: paddle.reshape(Tensor([15, 38028357, 4],"float32"), tuple(-1,4,), )

[Pass] paddle.reshape(Tensor([15, 38028357, 4],"float32"), tuple(-1,4,), )
2025-03-19 03:57:27.325353 test begin: paddle.reshape(Tensor([15, 4, 38028357],"float32"), tuple(-1,4,), )

[Pass] paddle.reshape(Tensor([15, 4, 38028357],"float32"), tuple(-1,4,), )
2025-03-19 04:00:54.488665 test begin: paddle.reshape(Tensor([1505, 17, 272, 328],"float32"), shape=list[-1,272,328,], )

[Pass] paddle.reshape(Tensor([1505, 17, 272, 328],"float32"), shape=list[-1,272,328,], )
2025-03-19 04:04:02.429718 test begin: paddle.reshape(Tensor([15158, 14, 14, 768],"float32"), list[124,14,14,768,], )

[torch error] paddle.reshape(Tensor([15158, 14, 14, 768],"float32"), list[124,14,14,768,], ) 
 shape '[124, 14, 14, 768]' is invalid for input of size 2281703424
2025-03-19 04:04:06.425514 test begin: paddle.reshape(Tensor([15158, 14, 14, 768],"float32"), list[128,14,14,768,], )

[torch error] paddle.reshape(Tensor([15158, 14, 14, 768],"float32"), list[128,14,14,768,], ) 
 shape '[128, 14, 14, 768]' is invalid for input of size 2281703424
2025-03-19 04:04:07.727643 test begin: paddle.reshape(Tensor([15158, 14, 14, 768],"float32"), list[4,14,14,768,], )

[torch error] paddle.reshape(Tensor([15158, 14, 14, 768],"float32"), list[4,14,14,768,], ) 
 shape '[4, 14, 14, 768]' is invalid for input of size 2281703424
2025-03-19 04:04:08.842512 test begin: paddle.reshape(Tensor([15158, 14, 14, 768],"float32"), list[8,14,14,768,], )

[torch error] paddle.reshape(Tensor([15158, 14, 14, 768],"float32"), list[8,14,14,768,], ) 
 shape '[8, 14, 14, 768]' is invalid for input of size 2281703424
2025-03-19 04:04:10.223786 test begin: paddle.reshape(Tensor([15158, 28, 28, 192],"float32"), list[124,28,28,192,], )

[torch error] paddle.reshape(Tensor([15158, 28, 28, 192],"float32"), list[124,28,28,192,], ) 
 shape '[124, 28, 28, 192]' is invalid for input of size 2281703424
2025-03-19 04:04:12.059423 test begin: paddle.reshape(Tensor([15158, 28, 28, 192],"float32"), list[128,28,28,192,], )

[torch error] paddle.reshape(Tensor([15158, 28, 28, 192],"float32"), list[128,28,28,192,], ) 
 shape '[128, 28, 28, 192]' is invalid for input of size 2281703424
2025-03-19 04:04:13.646476 test begin: paddle.reshape(Tensor([15158, 28, 28, 192],"float32"), list[60,28,28,192,], )

[torch error] paddle.reshape(Tensor([15158, 28, 28, 192],"float32"), list[60,28,28,192,], ) 
 shape '[60, 28, 28, 192]' is invalid for input of size 2281703424
2025-03-19 04:04:14.752854 test begin: paddle.reshape(Tensor([15158, 28, 28, 192],"float32"), list[64,28,28,192,], )

[torch error] paddle.reshape(Tensor([15158, 28, 28, 192],"float32"), list[64,28,28,192,], ) 
 shape '[64, 28, 28, 192]' is invalid for input of size 2281703424
2025-03-19 04:04:15.851598 test begin: paddle.reshape(Tensor([1516, 16, 336, 280],"float32"), shape=list[-1,336,280,], )

[Pass] paddle.reshape(Tensor([1516, 16, 336, 280],"float32"), shape=list[-1,336,280,], )
2025-03-19 04:07:26.088735 test begin: paddle.reshape(Tensor([151810, 501, 30],"float32"), list[-1,30,], )

[Pass] paddle.reshape(Tensor([151810, 501, 30],"float32"), list[-1,30,], )
2025-03-19 04:11:24.429339 test begin: paddle.reshape(Tensor([152, 103884, 272],"float16"), shape=tuple(152,-1,), )

[Pass] paddle.reshape(Tensor([152, 103884, 272],"float16"), shape=tuple(152,-1,), )
2025-03-19 04:28:06.762170 test begin: paddle.reshape(Tensor([152, 200, 141282],"float16"), shape=tuple(152,-1,), )

[Pass] paddle.reshape(Tensor([152, 200, 141282],"float16"), shape=tuple(152,-1,), )
2025-03-19 04:44:23.892347 test begin: paddle.reshape(Tensor([152, 200, 75056],"float32"), shape=tuple(152,-1,), )

[Pass] paddle.reshape(Tensor([152, 200, 75056],"float32"), shape=tuple(152,-1,), )
2025-03-19 04:47:40.770915 test begin: paddle.reshape(Tensor([152, 49379, 304],"float32"), shape=tuple(152,-1,), )

[Pass] paddle.reshape(Tensor([152, 49379, 304],"float32"), shape=tuple(152,-1,), )
2025-03-19 04:50:41.975070 test begin: paddle.reshape(Tensor([152, 55189, 272],"float32"), shape=tuple(152,-1,), )

[Pass] paddle.reshape(Tensor([152, 55189, 272],"float32"), shape=tuple(152,-1,), )
2025-03-19 04:54:38.953617 test begin: paddle.reshape(Tensor([152, 90566, 312],"float16"), shape=tuple(152,-1,), )

[Pass] paddle.reshape(Tensor([152, 90566, 312],"float16"), shape=tuple(152,-1,), )
2025-03-19 05:10:47.190792 test begin: paddle.reshape(Tensor([152, 92949, 304],"float16"), shape=tuple(152,-1,), )

[Pass] paddle.reshape(Tensor([152, 92949, 304],"float16"), shape=tuple(152,-1,), )
2025-03-19 05:27:02.881189 test begin: paddle.reshape(Tensor([1526, 15, 304, 328],"float32"), shape=list[-1,304,328,], )

[Pass] paddle.reshape(Tensor([1526, 15, 304, 328],"float32"), shape=list[-1,304,328,], )
2025-03-19 05:30:37.166135 test begin: paddle.reshape(Tensor([1526, 15, 328, 304],"float32"), shape=list[-1,328,304,], )

[Pass] paddle.reshape(Tensor([1526, 15, 328, 304],"float32"), shape=list[-1,328,304,], )
2025-03-19 05:34:19.255193 test begin: paddle.reshape(Tensor([1526, 15, 99712],"float32"), shape=tuple(-1,304,328,), )

[Pass] paddle.reshape(Tensor([1526, 15, 99712],"float32"), shape=tuple(-1,304,328,), )
2025-03-19 05:37:29.380448 test begin: paddle.reshape(Tensor([15335, 100, 124, 12],"float32"), shape=tuple(4,-1,4,), )

[Pass] paddle.reshape(Tensor([15335, 100, 124, 12],"float32"), shape=tuple(4,-1,4,), )
2025-03-19 05:40:36.324696 test begin: paddle.reshape(Tensor([1534, 19, 288, 272],"float32"), shape=list[-1,288,272,], )

[Pass] paddle.reshape(Tensor([1534, 19, 288, 272],"float32"), shape=list[-1,288,272,], )
2025-03-19 05:43:55.471216 test begin: paddle.reshape(Tensor([154, 1, 14816243],"float32"), shape=list[-1,102,], )

[torch error] paddle.reshape(Tensor([154, 1, 14816243],"float32"), shape=list[-1,102,], ) 
 shape '[-1, 102]' is invalid for input of size 2281701422
2025-03-19 05:43:59.560983 test begin: paddle.reshape(Tensor([154, 1, 27889399],"float16"), shape=list[-1,1000,], )

[torch error] paddle.reshape(Tensor([154, 1, 27889399],"float16"), shape=list[-1,1000,], ) 
 shape '[-1, 1000]' is invalid for input of size 4294967446
2025-03-19 05:44:03.446466 test begin: paddle.reshape(Tensor([154, 145258, 102],"float32"), shape=list[-1,102,], )

[Pass] paddle.reshape(Tensor([154, 145258, 102],"float32"), shape=list[-1,102,], )
2025-03-19 05:47:26.128055 test begin: paddle.reshape(Tensor([154, 27890, 1000],"float16"), shape=list[-1,1000,], )

[Pass] paddle.reshape(Tensor([154, 27890, 1000],"float16"), shape=list[-1,1000,], )
2025-03-19 06:03:47.389851 test begin: paddle.reshape(Tensor([1545, 16, 312, 296],"float32"), shape=list[-1,312,296,], )

[Pass] paddle.reshape(Tensor([1545, 16, 312, 296],"float32"), shape=list[-1,312,296,], )
2025-03-19 06:06:41.191505 test begin: paddle.reshape(Tensor([1545, 16, 92352],"float32"), shape=tuple(-1,312,296,), )

[Pass] paddle.reshape(Tensor([1545, 16, 92352],"float32"), shape=tuple(-1,312,296,), )
2025-03-19 06:09:40.941074 test begin: paddle.reshape(Tensor([15474, 128, 1152],"float32"), list[128,128,3,3,], )

[torch error] paddle.reshape(Tensor([15474, 128, 1152],"float32"), list[128,128,3,3,], ) 
 shape '[128, 128, 3, 3]' is invalid for input of size 2281734144
2025-03-19 06:09:45.442518 test begin: paddle.reshape(Tensor([15474, 576, 16, 16],"float32"), list[32,576,256,], )

[torch error] paddle.reshape(Tensor([15474, 576, 16, 16],"float32"), list[32,576,256,], ) 
 shape '[32, 576, 256]' is invalid for input of size 2281734144
2025-03-19 06:09:47.184745 test begin: paddle.reshape(Tensor([1555, 21, 208, 336],"float32"), shape=list[-1,208,336,], )

[Pass] paddle.reshape(Tensor([1555, 21, 208, 336],"float32"), shape=list[-1,208,336,], )
2025-03-19 06:12:29.207896 test begin: paddle.reshape(Tensor([156, 200, 137660],"float16"), shape=tuple(156,-1,), )

[Pass] paddle.reshape(Tensor([156, 200, 137660],"float16"), shape=tuple(156,-1,), )
2025-03-19 06:29:13.646357 test begin: paddle.reshape(Tensor([156, 200, 73132],"float32"), shape=tuple(156,-1,), )

[Pass] paddle.reshape(Tensor([156, 200, 73132],"float32"), shape=tuple(156,-1,), )
2025-03-19 06:32:10.914078 test begin: paddle.reshape(Tensor([156, 48113, 304],"float32"), shape=tuple(156,-1,), )

[Pass] paddle.reshape(Tensor([156, 48113, 304],"float32"), shape=tuple(156,-1,), )
2025-03-19 06:35:01.701014 test begin: paddle.reshape(Tensor([156, 52237, 280],"float32"), shape=tuple(156,-1,), )

[Pass] paddle.reshape(Tensor([156, 52237, 280],"float32"), shape=tuple(156,-1,), )
2025-03-19 06:38:26.171150 test begin: paddle.reshape(Tensor([156, 88244, 312],"float16"), shape=tuple(156,-1,), )

[Pass] paddle.reshape(Tensor([156, 88244, 312],"float16"), shape=tuple(156,-1,), )
2025-03-19 06:55:37.971669 test begin: paddle.reshape(Tensor([156, 90566, 304],"float16"), shape=tuple(156,-1,), )

[Pass] paddle.reshape(Tensor([156, 90566, 304],"float16"), shape=tuple(156,-1,), )
2025-03-19 07:12:05.552623 test begin: paddle.reshape(Tensor([156, 98329, 280],"float16"), shape=tuple(156,-1,), )

[Pass] paddle.reshape(Tensor([156, 98329, 280],"float16"), shape=tuple(156,-1,), )
2025-03-19 07:28:39.374898 test begin: paddle.reshape(Tensor([1572, 15, 288, 336],"float32"), shape=list[-1,288,336,], )

[Pass] paddle.reshape(Tensor([1572, 15, 288, 336],"float32"), shape=list[-1,288,336,], )
2025-03-19 07:32:22.208204 test begin: paddle.reshape(Tensor([1572, 15, 96768],"float32"), shape=tuple(-1,288,336,), )

[Pass] paddle.reshape(Tensor([1572, 15, 96768],"float32"), shape=tuple(-1,288,336,), )
2025-03-19 07:35:06.672451 test begin: paddle.reshape(Tensor([1580, 22, 304, 216],"float32"), shape=list[-1,304,216,], )

[Pass] paddle.reshape(Tensor([1580, 22, 304, 216],"float32"), shape=list[-1,304,216,], )
2025-03-19 07:38:09.354837 test begin: paddle.reshape(Tensor([15845149, 144],"int64"), shape=list[-1,], )

[Pass] paddle.reshape(Tensor([15845149, 144],"int64"), shape=list[-1,], )
2025-03-19 07:40:54.726911 test begin: paddle.reshape(Tensor([158452, 1, 3, 3, 40, 40],"float32"), shape=list[3,1,3,3,1600,], )

[torch error] paddle.reshape(Tensor([158452, 1, 3, 3, 40, 40],"float32"), shape=list[3,1,3,3,1600,], ) 
 shape '[3, 1, 3, 3, 1600]' is invalid for input of size 2281708800
2025-03-19 07:40:58.590372 test begin: paddle.reshape(Tensor([158452, 1, 3, 40, 40, 3],"float32"), shape=list[3,1,3,1600,3,], )

[torch error] paddle.reshape(Tensor([158452, 1, 3, 40, 40, 3],"float32"), shape=list[3,1,3,1600,3,], ) 
 shape '[3, 1, 3, 1600, 3]' is invalid for input of size 2281708800
2025-03-19 07:40:59.701900 test begin: paddle.reshape(Tensor([158452, 3, 1600, 3],"float32"), list[3,3,40,40,3,], )

[torch error] paddle.reshape(Tensor([158452, 3, 1600, 3],"float32"), list[3,3,40,40,3,], ) 
 shape '[3, 3, 40, 40, 3]' is invalid for input of size 2281708800
2025-03-19 07:41:00.840810 test begin: paddle.reshape(Tensor([158452, 3, 3, 1600],"float32"), list[3,3,3,40,40,], )

[torch error] paddle.reshape(Tensor([158452, 3, 3, 1600],"float32"), list[3,3,3,40,40,], ) 
 shape '[3, 3, 3, 40, 40]' is invalid for input of size 2281708800
2025-03-19 07:41:01.961118 test begin: paddle.reshape(Tensor([15846, 100, 120, 12],"float32"), shape=tuple(4,-1,4,), )

[Pass] paddle.reshape(Tensor([15846, 100, 120, 12],"float32"), shape=tuple(4,-1,4,), )
2025-03-19 07:44:40.532824 test begin: paddle.reshape(Tensor([1585, 16, 296, 304],"float32"), shape=list[-1,296,304,], )

[Pass] paddle.reshape(Tensor([1585, 16, 296, 304],"float32"), shape=list[-1,296,304,], )
2025-03-19 07:48:00.711030 test begin: paddle.reshape(Tensor([1585, 16, 304, 296],"float32"), shape=list[-1,304,296,], )

[Pass] paddle.reshape(Tensor([1585, 16, 304, 296],"float32"), shape=list[-1,304,296,], )
2025-03-19 07:51:21.489067 test begin: paddle.reshape(Tensor([1596, 14, 336, 304],"float32"), shape=list[-1,336,304,], )

[Pass] paddle.reshape(Tensor([1596, 14, 336, 304],"float32"), shape=list[-1,336,304,], )
2025-03-19 07:54:19.900820 test begin: paddle.reshape(Tensor([1599, 16, 272, 328],"float32"), shape=list[-1,272,328,], )

[Pass] paddle.reshape(Tensor([1599, 16, 272, 328],"float32"), shape=list[-1,272,328,], )
2025-03-19 07:58:15.580860 test begin: paddle.reshape(Tensor([16, 1, 142606337],"float32"), shape=list[-1,102,], )

[torch error] paddle.reshape(Tensor([16, 1, 142606337],"float32"), shape=list[-1,102,], ) 
 shape '[-1, 102]' is invalid for input of size 2281701392
2025-03-19 07:58:19.916810 test begin: paddle.reshape(Tensor([16, 1, 142606337],"float32"), shape=list[-1,128,], )

[torch error] paddle.reshape(Tensor([16, 1, 142606337],"float32"), shape=list[-1,128,], ) 
 shape '[-1, 128]' is invalid for input of size 2281701392
2025-03-19 07:58:21.465134 test begin: paddle.reshape(Tensor([16, 1, 142606337],"float32"), shape=list[-1,4,], )

[Pass] paddle.reshape(Tensor([16, 1, 142606337],"float32"), shape=list[-1,4,], )
2025-03-19 08:02:12.040497 test begin: paddle.reshape(Tensor([16, 1114113, 128],"float32"), shape=list[-1,128,], )

[Pass] paddle.reshape(Tensor([16, 1114113, 128],"float32"), shape=list[-1,128,], )
2025-03-19 08:05:35.828070 test begin: paddle.reshape(Tensor([16, 1280, 1, 111412],"float32"), list[-1,8,1280,], )

[Pass] paddle.reshape(Tensor([16, 1280, 1, 111412],"float32"), list[-1,8,1280,], )
2025-03-19 08:09:26.897175 test begin: paddle.reshape(Tensor([16, 1280, 111412, 1],"float32"), list[-1,8,1280,], )

[Pass] paddle.reshape(Tensor([16, 1280, 111412, 1],"float32"), list[-1,8,1280,], )
2025-03-19 08:12:46.622749 test begin: paddle.reshape(Tensor([16, 1398102, 102],"float32"), shape=list[-1,102,], )

[Pass] paddle.reshape(Tensor([16, 1398102, 102],"float32"), shape=list[-1,102,], )
2025-03-19 08:16:33.551788 test begin: paddle.reshape(Tensor([16, 14, 10186167],"float32"), shape=list[16,14,4,8,], )

[torch error] paddle.reshape(Tensor([16, 14, 10186167],"float32"), shape=list[16,14,4,8,], ) 
 shape '[16, 14, 4, 8]' is invalid for input of size 2281701408
2025-03-19 08:16:37.920525 test begin: paddle.reshape(Tensor([16, 14, 14, 727584],"float32"), list[16,14,14,512,], )

[torch error] paddle.reshape(Tensor([16, 14, 14, 727584],"float32"), list[16,14,14,512,], ) 
 shape '[16, 14, 14, 512]' is invalid for input of size 2281703424
2025-03-19 08:16:40.581373 test begin: paddle.reshape(Tensor([16, 14, 19895, 512],"float32"), list[16,14,14,512,], )

[torch error] paddle.reshape(Tensor([16, 14, 19895, 512],"float32"), list[16,14,14,512,], ) 
 shape '[16, 14, 14, 512]' is invalid for input of size 2281717760
2025-03-19 08:16:42.239197 test begin: paddle.reshape(Tensor([16, 142606337, 1, 1],"float32"), list[-1,8,1280,], )

[torch error] paddle.reshape(Tensor([16, 142606337, 1, 1],"float32"), list[-1,8,1280,], ) 
 shape '[-1, 8, 1280]' is invalid for input of size 2281701392
2025-03-19 08:16:43.945554 test begin: paddle.reshape(Tensor([16, 142606337, 1, 1],"float32"), list[-1,8,2048,], )

[torch error] paddle.reshape(Tensor([16, 142606337, 1, 1],"float32"), list[-1,8,2048,], ) 
 shape '[-1, 8, 2048]' is invalid for input of size 2281701392
2025-03-19 08:16:45.655047 test begin: paddle.reshape(Tensor([16, 142606337],"float32"), shape=list[-1,102,], )

[torch error] paddle.reshape(Tensor([16, 142606337],"float32"), shape=list[-1,102,], ) 
 shape '[-1, 102]' is invalid for input of size 2281701392
2025-03-19 08:16:47.288859 test begin: paddle.reshape(Tensor([16, 142606337],"float32"), shape=list[-1,2048,], )

[torch error] paddle.reshape(Tensor([16, 142606337],"float32"), shape=list[-1,2048,], ) 
 shape '[-1, 2048]' is invalid for input of size 2281701392
2025-03-19 08:16:48.709956 test begin: paddle.reshape(Tensor([16, 142606337],"float32"), shape=list[-1,4,16,], )

[torch error] paddle.reshape(Tensor([16, 142606337],"float32"), shape=list[-1,4,16,], ) 
 shape '[-1, 4, 16]' is invalid for input of size 2281701392
2025-03-19 08:16:51.267673 test begin: paddle.reshape(Tensor([16, 142606337],"float32"), shape=list[-1,4,32,], )

[torch error] paddle.reshape(Tensor([16, 142606337],"float32"), shape=list[-1,4,32,], ) 
 shape '[-1, 4, 32]' is invalid for input of size 2281701392
2025-03-19 08:16:53.253116 test begin: paddle.reshape(Tensor([16, 142606337],"int64"), list[16,], )

[torch error] paddle.reshape(Tensor([16, 142606337],"int64"), list[16,], ) 
 shape '[16]' is invalid for input of size 2281701392
2025-03-19 08:17:01.066509 test begin: paddle.reshape(Tensor([16, 19895, 14, 512],"float32"), list[16,14,14,512,], )

[torch error] paddle.reshape(Tensor([16, 19895, 14, 512],"float32"), list[16,14,14,512,], ) 
 shape '[16, 14, 14, 512]' is invalid for input of size 2281717760
2025-03-19 08:17:04.074608 test begin: paddle.reshape(Tensor([16, 19895, 28, 256],"float32"), list[16,28,28,256,], )

[torch error] paddle.reshape(Tensor([16, 19895, 28, 256],"float32"), list[16,28,28,256,], ) 
 shape '[16, 28, 28, 256]' is invalid for input of size 2281717760
2025-03-19 08:17:05.411786 test begin: paddle.reshape(Tensor([16, 19895, 56, 128],"float32"), list[16,56,56,128,], )

[torch error] paddle.reshape(Tensor([16, 19895, 56, 128],"float32"), list[16,56,56,128,], ) 
 shape '[16, 56, 56, 128]' is invalid for input of size 2281717760
2025-03-19 08:17:06.778003 test begin: paddle.reshape(Tensor([16, 200, 1342178],"float16"), shape=tuple(16,-1,), )

[Pass] paddle.reshape(Tensor([16, 200, 1342178],"float16"), shape=tuple(16,-1,), )
2025-03-19 08:33:24.615044 test begin: paddle.reshape(Tensor([16, 200, 713032],"float32"), shape=tuple(16,-1,), )

[Pass] paddle.reshape(Tensor([16, 200, 713032],"float32"), shape=tuple(16,-1,), )
2025-03-19 08:36:13.621000 test begin: paddle.reshape(Tensor([16, 2048, 1, 69633],"float32"), list[-1,8,2048,], )

[Pass] paddle.reshape(Tensor([16, 2048, 1, 69633],"float32"), list[-1,8,2048,], )
2025-03-19 08:39:28.369640 test begin: paddle.reshape(Tensor([16, 2048, 69633, 1],"float32"), list[-1,8,2048,], )

[Pass] paddle.reshape(Tensor([16, 2048, 69633, 1],"float32"), list[-1,8,2048,], )
2025-03-19 08:42:51.853238 test begin: paddle.reshape(Tensor([16, 222823, 640],"float32"), shape=list[-1,], )

[Pass] paddle.reshape(Tensor([16, 222823, 640],"float32"), shape=list[-1,], )
2025-03-19 08:45:32.502722 test begin: paddle.reshape(Tensor([16, 28, 19895, 256],"float32"), list[16,28,28,256,], )

[torch error] paddle.reshape(Tensor([16, 28, 19895, 256],"float32"), list[16,28,28,256,], ) 
 shape '[16, 28, 28, 256]' is invalid for input of size 2281717760
2025-03-19 08:45:35.794228 test begin: paddle.reshape(Tensor([16, 28, 28, 181896],"float32"), list[16,28,28,256,], )

[torch error] paddle.reshape(Tensor([16, 28, 28, 181896],"float32"), list[16,28,28,256,], ) 
 shape '[16, 28, 28, 256]' is invalid for input of size 2281703424
2025-03-19 08:45:37.366379 test begin: paddle.reshape(Tensor([16, 35651585, 4],"float32"), shape=list[-1,4,], )

[Pass] paddle.reshape(Tensor([16, 35651585, 4],"float32"), shape=list[-1,4,], )
2025-03-19 08:48:30.847213 test begin: paddle.reshape(Tensor([16, 4456449, 32],"float32"), shape=list[16,14,4,8,], )

[torch error] paddle.reshape(Tensor([16, 4456449, 32],"float32"), shape=list[16,14,4,8,], ) 
 shape '[16, 14, 4, 8]' is invalid for input of size 2281701888
2025-03-19 08:48:34.740218 test begin: paddle.reshape(Tensor([16, 457072, 312],"float32"), shape=tuple(16,-1,), )

[Pass] paddle.reshape(Tensor([16, 457072, 312],"float32"), shape=tuple(16,-1,), )
2025-03-19 08:51:42.332772 test begin: paddle.reshape(Tensor([16, 469100, 304],"float32"), shape=tuple(16,-1,), )

[Pass] paddle.reshape(Tensor([16, 469100, 304],"float32"), shape=tuple(16,-1,), )
2025-03-19 08:55:39.548920 test begin: paddle.reshape(Tensor([16, 56, 19895, 128],"float32"), list[16,56,56,128,], )

[torch error] paddle.reshape(Tensor([16, 56, 19895, 128],"float32"), list[16,56,56,128,], ) 
 shape '[16, 56, 56, 128]' is invalid for input of size 2281717760
2025-03-19 08:55:43.466539 test begin: paddle.reshape(Tensor([16, 56, 56, 45474],"float32"), list[16,56,56,128,], )

[torch error] paddle.reshape(Tensor([16, 56, 56, 45474],"float32"), list[16,56,56,128,], ) 
 shape '[16, 56, 56, 128]' is invalid for input of size 2281703424
2025-03-19 08:55:44.567182 test begin: paddle.reshape(Tensor([16, 640, 222823],"float32"), shape=list[-1,], )

[Pass] paddle.reshape(Tensor([16, 640, 222823],"float32"), shape=list[-1,], )
2025-03-19 08:59:06.572978 test begin: paddle.reshape(Tensor([16, 838861, 320],"float16"), shape=tuple(16,-1,), )

[Pass] paddle.reshape(Tensor([16, 838861, 320],"float16"), shape=tuple(16,-1,), )
2025-03-19 09:15:57.428209 test begin: paddle.reshape(Tensor([16, 860371, 312],"float16"), shape=tuple(16,-1,), )

[Pass] paddle.reshape(Tensor([16, 860371, 312],"float16"), shape=tuple(16,-1,), )
2025-03-19 09:32:44.335369 test begin: paddle.reshape(Tensor([16, 883012, 304],"float16"), shape=tuple(16,-1,), )

[Pass] paddle.reshape(Tensor([16, 883012, 304],"float16"), shape=tuple(16,-1,), )
2025-03-19 09:49:24.478874 test begin: paddle.reshape(Tensor([160, 200, 134218],"float16"), shape=tuple(160,-1,), )

[Pass] paddle.reshape(Tensor([160, 200, 134218],"float16"), shape=tuple(160,-1,), )
2025-03-19 10:06:36.044360 test begin: paddle.reshape(Tensor([160, 200, 71304],"float32"), shape=tuple(160,-1,), )

[Pass] paddle.reshape(Tensor([160, 200, 71304],"float32"), shape=tuple(160,-1,), )
2025-03-19 10:09:30.930184 test begin: paddle.reshape(Tensor([160, 50931, 280],"float32"), shape=tuple(160,-1,), )

[Pass] paddle.reshape(Tensor([160, 50931, 280],"float32"), shape=tuple(160,-1,), )
2025-03-19 10:12:36.620078 test begin: paddle.reshape(Tensor([160, 52429, 272],"float32"), shape=tuple(160,-1,), )

[Pass] paddle.reshape(Tensor([160, 52429, 272],"float32"), shape=tuple(160,-1,), )
2025-03-19 10:15:22.480287 test begin: paddle.reshape(Tensor([160, 88302, 304],"float16"), shape=tuple(160,-1,), )

[Pass] paddle.reshape(Tensor([160, 88302, 304],"float16"), shape=tuple(160,-1,), )
2025-03-19 10:32:10.701622 test begin: paddle.reshape(Tensor([160, 95870, 280],"float16"), shape=tuple(160,-1,), )

[Pass] paddle.reshape(Tensor([160, 95870, 280],"float16"), shape=tuple(160,-1,), )
2025-03-19 10:48:57.381945 test begin: paddle.reshape(Tensor([160, 98690, 272],"float16"), shape=tuple(160,-1,), )

[Pass] paddle.reshape(Tensor([160, 98690, 272],"float16"), shape=tuple(160,-1,), )
2025-03-19 11:05:45.867924 test begin: paddle.reshape(Tensor([1604, 15, 304, 312],"float32"), shape=list[-1,304,312,], )

[Pass] paddle.reshape(Tensor([1604, 15, 304, 312],"float32"), shape=list[-1,304,312,], )
2025-03-19 11:09:16.237960 test begin: paddle.reshape(Tensor([1604, 15, 312, 304],"float32"), shape=list[-1,312,304,], )

[Pass] paddle.reshape(Tensor([1604, 15, 312, 304],"float32"), shape=list[-1,312,304,], )
2025-03-19 11:12:28.609014 test begin: paddle.reshape(Tensor([1606, 24, 200, 296],"float32"), shape=list[-1,200,296,], )

[Pass] paddle.reshape(Tensor([1606, 24, 200, 296],"float32"), shape=list[-1,200,296,], )
2025-03-19 11:16:21.107150 test begin: paddle.reshape(Tensor([1614, 30, 88704],"float16"), shape=tuple(-1,264,336,), )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 86, in func_timeout
    thread.join(timeout)
  File "/usr/lib/python3.9/threading.py", line 1064, in join
    self._wait_for_tstate_lock(timeout=max(timeout, 0))
  File "/usr/lib/python3.9/threading.py", line 1080, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt
2025-03-19 12:15:46.905686 test begin: paddle.reshape(Tensor([1614, 35, 76032],"float16"), shape=tuple(-1,264,288,), )

W0319 12:17:27.204916 73916 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0319 12:17:27.207237 73916 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.reshape(Tensor([1614, 35, 76032],"float16"), shape=tuple(-1,264,288,), )
2025-03-19 12:32:53.162767 test begin: paddle.reshape(Tensor([1615, 39, 68224],"float16"), shape=tuple(-1,208,328,), )

[Pass] paddle.reshape(Tensor([1615, 39, 68224],"float16"), shape=tuple(-1,208,328,), )
2025-03-19 12:48:53.961013 test begin: paddle.reshape(Tensor([1617, 15, 280, 336],"float32"), shape=list[-1,280,336,], )

[Pass] paddle.reshape(Tensor([1617, 15, 280, 336],"float32"), shape=list[-1,280,336,], )
2025-03-19 12:53:17.148446 test begin: paddle.reshape(Tensor([1617, 18, 280, 280],"float32"), shape=list[-1,280,280,], )

[Pass] paddle.reshape(Tensor([1617, 18, 280, 280],"float32"), shape=list[-1,280,280,], )
2025-03-19 12:56:33.398959 test begin: paddle.reshape(Tensor([1619, 24, 272, 216],"float32"), shape=list[-1,272,216,], )

[Pass] paddle.reshape(Tensor([1619, 24, 272, 216],"float32"), shape=list[-1,272,216,], )
2025-03-19 12:59:35.284833 test begin: paddle.reshape(Tensor([16246, 50, 53, 53],"float32"), shape=list[-1,140450,], )

[Pass] paddle.reshape(Tensor([16246, 50, 53, 53],"float32"), shape=list[-1,140450,], )
2025-03-19 13:02:51.362401 test begin: paddle.reshape(Tensor([1629, 16, 304, 288],"float32"), shape=list[-1,304,288,], )

[Pass] paddle.reshape(Tensor([1629, 16, 304, 288],"float32"), shape=list[-1,304,288,], )
2025-03-19 13:06:08.138632 test begin: paddle.reshape(Tensor([1629, 16, 87552],"float32"), shape=tuple(-1,304,288,), )

[Pass] paddle.reshape(Tensor([1629, 16, 87552],"float32"), shape=tuple(-1,304,288,), )
2025-03-19 13:09:02.120847 test begin: paddle.reshape(Tensor([162978670, 14],"float32"), tuple(2,52,7,-1,), )

[torch error] paddle.reshape(Tensor([162978670, 14],"float32"), tuple(2,52,7,-1,), ) 
 shape '[2, 52, 7, -1]' is invalid for input of size 2281701380
2025-03-19 13:09:06.851484 test begin: paddle.reshape(Tensor([1633, 20, 208, 336],"float32"), shape=list[-1,208,336,], )

[Pass] paddle.reshape(Tensor([1633, 20, 208, 336],"float32"), shape=list[-1,208,336,], )
2025-03-19 13:11:59.075476 test begin: paddle.reshape(Tensor([1633, 20, 69888],"float32"), shape=tuple(-1,208,336,), )

[Pass] paddle.reshape(Tensor([1633, 20, 69888],"float32"), shape=tuple(-1,208,336,), )
2025-03-19 13:14:54.274317 test begin: paddle.reshape(Tensor([16369, 4, 65600],"float16"), shape=tuple(-1,200,328,), )

[Pass] paddle.reshape(Tensor([16369, 4, 65600],"float16"), shape=tuple(-1,200,328,), )
2025-03-19 13:31:17.901858 test begin: paddle.reshape(Tensor([1639, 25, 104832],"float16"), shape=tuple(-1,336,312,), )

[Pass] paddle.reshape(Tensor([1639, 25, 104832],"float16"), shape=tuple(-1,336,312,), )
2025-03-19 13:48:15.327867 test begin: paddle.reshape(Tensor([164, 200, 130945],"float16"), shape=tuple(164,-1,), )

[Pass] paddle.reshape(Tensor([164, 200, 130945],"float16"), shape=tuple(164,-1,), )
2025-03-19 14:05:08.512222 test begin: paddle.reshape(Tensor([164, 200, 69565],"float32"), shape=tuple(164,-1,), )

[Pass] paddle.reshape(Tensor([164, 200, 69565],"float32"), shape=tuple(164,-1,), )
2025-03-19 14:09:06.354542 test begin: paddle.reshape(Tensor([164, 216, 121245],"float16"), shape=tuple(164,-1,), )

[Pass] paddle.reshape(Tensor([164, 216, 121245],"float16"), shape=tuple(164,-1,), )
2025-03-19 14:26:16.506168 test begin: paddle.reshape(Tensor([164, 256, 200, 272],"float32"), shape=tuple(1,256,-1,), )

[Pass] paddle.reshape(Tensor([164, 256, 200, 272],"float32"), shape=tuple(1,256,-1,), )
2025-03-19 14:29:27.232228 test begin: paddle.reshape(Tensor([164, 41408, 336],"float32"), shape=tuple(164,-1,), )

[Pass] paddle.reshape(Tensor([164, 41408, 336],"float32"), shape=tuple(164,-1,), )
2025-03-19 14:32:29.830700 test begin: paddle.reshape(Tensor([164, 45766, 304],"float32"), shape=tuple(164,-1,), )

[Pass] paddle.reshape(Tensor([164, 45766, 304],"float32"), shape=tuple(164,-1,), )
2025-03-19 14:35:40.999497 test begin: paddle.reshape(Tensor([164, 77943, 336],"float16"), shape=tuple(164,-1,), )

[Pass] paddle.reshape(Tensor([164, 77943, 336],"float16"), shape=tuple(164,-1,), )
2025-03-19 14:52:03.650720 test begin: paddle.reshape(Tensor([164, 86148, 304],"float16"), shape=tuple(164,-1,), )

[Pass] paddle.reshape(Tensor([164, 86148, 304],"float16"), shape=tuple(164,-1,), )
2025-03-19 15:07:54.968736 test begin: paddle.reshape(Tensor([16536, 176, 28, 28],"float32"), shape=list[2,2,88,28,28,], )

[torch error] paddle.reshape(Tensor([16536, 176, 28, 28],"float32"), shape=list[2,2,88,28,28,], ) 
 shape '[2, 2, 88, 28, 28]' is invalid for input of size 2281703424
2025-03-19 15:07:59.114130 test begin: paddle.reshape(Tensor([16536, 88, 2, 28, 28],"float32"), shape=list[2,176,28,28,], )

[torch error] paddle.reshape(Tensor([16536, 88, 2, 28, 28],"float32"), shape=list[2,176,28,28,], ) 
 shape '[2, 176, 28, 28]' is invalid for input of size 2281703424
2025-03-19 15:08:00.575036 test begin: paddle.reshape(Tensor([1657, 15, 280, 328],"float32"), shape=list[-1,280,328,], )

[Pass] paddle.reshape(Tensor([1657, 15, 280, 328],"float32"), shape=list[-1,280,328,], )
2025-03-19 15:10:45.955203 test begin: paddle.reshape(Tensor([1658, 16, 256, 336],"float32"), shape=list[-1,256,336,], )

[Pass] paddle.reshape(Tensor([1658, 16, 256, 336],"float32"), shape=list[-1,256,336,], )
2025-03-19 15:13:57.238549 test begin: paddle.reshape(Tensor([1665, 18, 272, 280],"float32"), shape=list[-1,272,280,], )

[Pass] paddle.reshape(Tensor([1665, 18, 272, 280],"float32"), shape=list[-1,272,280,], )
2025-03-19 15:17:06.062919 test begin: paddle.reshape(Tensor([16668, 124, 92, 12],"float32"), shape=tuple(2,-1,4,), )

[Pass] paddle.reshape(Tensor([16668, 124, 92, 12],"float32"), shape=tuple(2,-1,4,), )
2025-03-19 15:19:58.645527 test begin: paddle.reshape(Tensor([1673, 17, 264, 304],"float32"), shape=list[-1,264,304,], )

[Pass] paddle.reshape(Tensor([1673, 17, 264, 304],"float32"), shape=list[-1,264,304,], )
2025-03-19 15:23:33.567019 test begin: paddle.reshape(Tensor([1673, 32, 80256],"float16"), shape=tuple(-1,264,304,), )

[Pass] paddle.reshape(Tensor([1673, 32, 80256],"float16"), shape=tuple(-1,264,304,), )
2025-03-19 15:40:14.414431 test begin: paddle.reshape(Tensor([1675, 13, 104832],"float32"), shape=tuple(-1,336,312,), )

[Pass] paddle.reshape(Tensor([1675, 13, 104832],"float32"), shape=tuple(-1,336,312,), )
2025-03-19 15:43:55.159143 test begin: paddle.reshape(Tensor([1675, 13, 336, 312],"float32"), shape=list[-1,336,312,], )

[Pass] paddle.reshape(Tensor([1675, 13, 336, 312],"float32"), shape=list[-1,336,312,], )
2025-03-19 15:47:12.447275 test begin: paddle.reshape(Tensor([1676, 14, 304, 320],"float32"), shape=list[-1,304,320,], )

[Pass] paddle.reshape(Tensor([1676, 14, 304, 320],"float32"), shape=list[-1,304,320,], )
2025-03-19 15:51:04.268344 test begin: paddle.reshape(Tensor([1676, 23, 200, 296],"float32"), shape=list[-1,200,296,], )

[Pass] paddle.reshape(Tensor([1676, 23, 200, 296],"float32"), shape=list[-1,200,296,], )
2025-03-19 15:54:51.262833 test begin: paddle.reshape(Tensor([16777217, 256],"float16"), shape=list[4,-1,256,], )

[torch error] paddle.reshape(Tensor([16777217, 256],"float16"), shape=list[4,-1,256,], ) 
 shape '[4, -1, 256]' is invalid for input of size 4294967552
2025-03-19 15:54:55.163574 test begin: paddle.reshape(Tensor([1679, 14, 296, 328],"float32"), shape=list[-1,296,328,], )

[Pass] paddle.reshape(Tensor([1679, 14, 296, 328],"float32"), shape=list[-1,296,328,], )
2025-03-19 15:58:33.033615 test begin: paddle.reshape(Tensor([168, 200, 127827],"float16"), shape=tuple(168,-1,), )

[Pass] paddle.reshape(Tensor([168, 200, 127827],"float16"), shape=tuple(168,-1,), )
2025-03-19 16:15:01.867105 test begin: paddle.reshape(Tensor([168, 200, 67908],"float32"), shape=tuple(168,-1,), )

[Pass] paddle.reshape(Tensor([168, 200, 67908],"float32"), shape=tuple(168,-1,), )
2025-03-19 16:18:12.730864 test begin: paddle.reshape(Tensor([168, 272, 93991],"float16"), shape=tuple(168,-1,), )

[Pass] paddle.reshape(Tensor([168, 272, 93991],"float16"), shape=tuple(168,-1,), )
2025-03-19 16:34:43.624546 test begin: paddle.reshape(Tensor([168, 44677, 304],"float32"), shape=tuple(168,-1,), )

[Pass] paddle.reshape(Tensor([168, 44677, 304],"float32"), shape=tuple(168,-1,), )
2025-03-19 16:38:08.475428 test begin: paddle.reshape(Tensor([168, 47159, 288],"float32"), shape=tuple(168,-1,), )

[Pass] paddle.reshape(Tensor([168, 47159, 288],"float32"), shape=tuple(168,-1,), )
2025-03-19 16:41:52.866793 test begin: paddle.reshape(Tensor([168, 84097, 304],"float16"), shape=tuple(168,-1,), )

[Pass] paddle.reshape(Tensor([168, 84097, 304],"float16"), shape=tuple(168,-1,), )
2025-03-19 16:57:50.871649 test begin: paddle.reshape(Tensor([168, 88769, 288],"float16"), shape=tuple(168,-1,), )

[Pass] paddle.reshape(Tensor([168, 88769, 288],"float16"), shape=tuple(168,-1,), )
2025-03-19 17:13:43.520359 test begin: paddle.reshape(Tensor([168, 93991, 272],"float16"), shape=tuple(168,-1,), )

[Pass] paddle.reshape(Tensor([168, 93991, 272],"float16"), shape=tuple(168,-1,), )
2025-03-19 17:29:34.963207 test begin: paddle.reshape(Tensor([1685, 12, 336, 336],"float32"), shape=list[-1,336,336,], )

[Pass] paddle.reshape(Tensor([1685, 12, 336, 336],"float32"), shape=list[-1,336,336,], )
2025-03-19 17:32:19.357084 test begin: paddle.reshape(Tensor([1691, 15, 304, 296],"float32"), shape=list[-1,304,296,], )

[Pass] paddle.reshape(Tensor([1691, 15, 304, 296],"float32"), shape=list[-1,304,296,], )
2025-03-19 17:35:23.342137 test begin: paddle.reshape(Tensor([1691, 15, 89984],"float32"), shape=tuple(-1,304,296,), )

[Pass] paddle.reshape(Tensor([1691, 15, 89984],"float32"), shape=tuple(-1,304,296,), )
2025-03-19 17:38:23.151994 test begin: paddle.reshape(Tensor([16977, 2, 336, 200],"float32"), shape=list[-1,336,200,], )

[Pass] paddle.reshape(Tensor([16977, 2, 336, 200],"float32"), shape=list[-1,336,200,], )
2025-03-19 17:41:29.347576 test begin: paddle.reshape(Tensor([17, 134217729],"int64"), list[17,], )

[torch error] paddle.reshape(Tensor([17, 134217729],"int64"), list[17,], ) 
 shape '[17]' is invalid for input of size 2281701393
2025-03-19 17:42:30.385275 test begin: paddle.reshape(Tensor([170628, 13373],"bool"), list[-1,20,1,], )

[torch error] paddle.reshape(Tensor([170628, 13373],"bool"), list[-1,20,1,], ) 
 shape '[-1, 20, 1]' is invalid for input of size 2281808244
2025-03-19 17:43:34.442545 test begin: paddle.reshape(Tensor([1708, 18, 232, 320],"float32"), shape=list[-1,232,320,], )

[Pass] paddle.reshape(Tensor([1708, 18, 232, 320],"float32"), shape=list[-1,232,320,], )
2025-03-19 17:46:37.125733 test begin: paddle.reshape(Tensor([1708, 18, 74240],"float32"), shape=tuple(-1,232,320,), )

[Pass] paddle.reshape(Tensor([1708, 18, 74240],"float32"), shape=tuple(-1,232,320,), )
2025-03-19 17:49:57.031113 test begin: paddle.reshape(Tensor([1712, 33, 76032],"float16"), shape=tuple(-1,264,288,), )

[Pass] paddle.reshape(Tensor([1712, 33, 76032],"float16"), shape=tuple(-1,264,288,), )
2025-03-19 18:06:07.650174 test begin: paddle.reshape(Tensor([1714, 17, 288, 272],"float32"), shape=list[-1,288,272,], )

[Pass] paddle.reshape(Tensor([1714, 17, 288, 272],"float32"), shape=list[-1,288,272,], )
2025-03-19 18:09:49.150280 test begin: paddle.reshape(Tensor([1715, 13, 102400],"float32"), shape=tuple(-1,320,320,), )

[Pass] paddle.reshape(Tensor([1715, 13, 102400],"float32"), shape=tuple(-1,320,320,), )
2025-03-19 18:13:16.501745 test begin: paddle.reshape(Tensor([1715, 13, 320, 320],"float32"), shape=list[-1,320,320,], )

[Pass] paddle.reshape(Tensor([1715, 13, 320, 320],"float32"), shape=list[-1,320,320,], )
2025-03-19 18:16:48.577886 test begin: paddle.reshape(Tensor([1719, 13, 304, 336],"float32"), shape=list[-1,304,336,], )

[Pass] paddle.reshape(Tensor([1719, 13, 304, 336],"float32"), shape=list[-1,304,336,], )
2025-03-19 18:20:27.419741 test begin: paddle.reshape(Tensor([1719, 14, 304, 312],"float32"), shape=list[-1,304,312,], )

[Pass] paddle.reshape(Tensor([1719, 14, 304, 312],"float32"), shape=list[-1,304,312,], )
2025-03-19 18:23:47.568694 test begin: paddle.reshape(Tensor([172, 200, 124854],"float16"), shape=tuple(172,-1,), )

[Pass] paddle.reshape(Tensor([172, 200, 124854],"float16"), shape=tuple(172,-1,), )
2025-03-19 18:40:02.721144 test begin: paddle.reshape(Tensor([172, 200, 66329],"float32"), shape=tuple(172,-1,), )

[Pass] paddle.reshape(Tensor([172, 200, 66329],"float32"), shape=tuple(172,-1,), )
2025-03-19 18:43:16.945919 test begin: paddle.reshape(Tensor([172, 43638, 304],"float32"), shape=tuple(172,-1,), )

[Pass] paddle.reshape(Tensor([172, 43638, 304],"float32"), shape=tuple(172,-1,), )
2025-03-19 18:46:44.650220 test begin: paddle.reshape(Tensor([172, 48771, 272],"float32"), shape=tuple(172,-1,), )

[Pass] paddle.reshape(Tensor([172, 48771, 272],"float32"), shape=tuple(172,-1,), )
2025-03-19 18:50:09.630119 test begin: paddle.reshape(Tensor([172, 74318, 336],"float16"), shape=tuple(172,-1,), )

[Pass] paddle.reshape(Tensor([172, 74318, 336],"float16"), shape=tuple(172,-1,), )
2025-03-19 19:06:24.828550 test begin: paddle.reshape(Tensor([172, 82141, 304],"float16"), shape=tuple(172,-1,), )

[Pass] paddle.reshape(Tensor([172, 82141, 304],"float16"), shape=tuple(172,-1,), )
2025-03-19 19:22:42.357767 test begin: paddle.reshape(Tensor([172, 91805, 272],"float16"), shape=tuple(172,-1,), )

[Pass] paddle.reshape(Tensor([172, 91805, 272],"float16"), shape=tuple(172,-1,), )
2025-03-19 19:39:06.048233 test begin: paddle.reshape(Tensor([1725, 17, 256, 304],"float32"), shape=list[-1,256,304,], )

[Pass] paddle.reshape(Tensor([1725, 17, 256, 304],"float32"), shape=list[-1,256,304,], )
2025-03-19 19:42:05.770875 test begin: paddle.reshape(Tensor([1725, 38, 68, 512],"float32"), list[-1,512,], )

[Pass] paddle.reshape(Tensor([1725, 38, 68, 512],"float32"), list[-1,512,], )
2025-03-19 19:44:47.812118 test begin: paddle.reshape(Tensor([1733, 14, 280, 336],"float32"), shape=list[-1,280,336,], )

[Pass] paddle.reshape(Tensor([1733, 14, 280, 336],"float32"), shape=list[-1,280,336,], )
2025-03-19 19:47:45.777751 test begin: paddle.reshape(Tensor([1733, 27, 91840],"float16"), shape=tuple(-1,280,328,), )

[Pass] paddle.reshape(Tensor([1733, 27, 91840],"float16"), shape=tuple(-1,280,328,), )
2025-03-19 20:03:48.238473 test begin: paddle.reshape(Tensor([17409, 256, 512],"float32"), list[256,512,1,1,], )

[torch error] paddle.reshape(Tensor([17409, 256, 512],"float32"), list[256,512,1,1,], ) 
 shape '[256, 512, 1, 1]' is invalid for input of size 2281832448
2025-03-19 20:03:52.309116 test begin: paddle.reshape(Tensor([17409, 512, 256],"float32"), list[512,256,1,1,], )

[torch error] paddle.reshape(Tensor([17409, 512, 256],"float32"), list[512,256,1,1,], ) 
 shape '[512, 256, 1, 1]' is invalid for input of size 2281832448
2025-03-19 20:03:53.902913 test begin: paddle.reshape(Tensor([174763, 96, 16, 16],"float16"), list[64,96,-1,], )

[Pass] paddle.reshape(Tensor([174763, 96, 16, 16],"float16"), list[64,96,-1,], )
2025-03-19 20:20:06.926592 test begin: paddle.reshape(Tensor([1752, 22, 200, 296],"float32"), shape=list[-1,200,296,], )

[Pass] paddle.reshape(Tensor([1752, 22, 200, 296],"float32"), shape=list[-1,200,296,], )
2025-03-19 20:23:24.816758 test begin: paddle.reshape(Tensor([1752, 22, 59200],"float32"), shape=tuple(-1,200,296,), )

[Pass] paddle.reshape(Tensor([1752, 22, 59200],"float32"), shape=tuple(-1,200,296,), )
2025-03-19 20:26:20.001063 test begin: paddle.reshape(Tensor([176, 200, 122017],"float16"), shape=tuple(176,-1,), )

[Pass] paddle.reshape(Tensor([176, 200, 122017],"float16"), shape=tuple(176,-1,), )
2025-03-19 20:42:45.415840 test begin: paddle.reshape(Tensor([176, 200, 64822],"float32"), shape=tuple(176,-1,), )

[Pass] paddle.reshape(Tensor([176, 200, 64822],"float32"), shape=tuple(176,-1,), )
2025-03-19 20:45:40.288211 test begin: paddle.reshape(Tensor([176, 232, 105187],"float16"), shape=tuple(176,-1,), )

[Pass] paddle.reshape(Tensor([176, 232, 105187],"float16"), shape=tuple(176,-1,), )
2025-03-19 21:01:58.316308 test begin: paddle.reshape(Tensor([176, 41552, 312],"float32"), shape=tuple(176,-1,), )

[Pass] paddle.reshape(Tensor([176, 41552, 312],"float32"), shape=tuple(176,-1,), )
2025-03-19 21:05:03.705668 test begin: paddle.reshape(Tensor([176, 42646, 304],"float32"), shape=tuple(176,-1,), )

[Pass] paddle.reshape(Tensor([176, 42646, 304],"float32"), shape=tuple(176,-1,), )
2025-03-19 21:08:04.457701 test begin: paddle.reshape(Tensor([176, 76261, 320],"float16"), shape=tuple(176,-1,), )

[Pass] paddle.reshape(Tensor([176, 76261, 320],"float16"), shape=tuple(176,-1,), )
2025-03-19 21:25:07.126698 test begin: paddle.reshape(Tensor([176, 78216, 312],"float16"), shape=tuple(176,-1,), )

[Pass] paddle.reshape(Tensor([176, 78216, 312],"float16"), shape=tuple(176,-1,), )
2025-03-19 21:41:39.715615 test begin: paddle.reshape(Tensor([176, 80274, 304],"float16"), shape=tuple(176,-1,), )

[Pass] paddle.reshape(Tensor([176, 80274, 304],"float16"), shape=tuple(176,-1,), )
2025-03-19 21:58:03.719386 test begin: paddle.reshape(Tensor([1760573, 1296],"int64"), shape=list[-1,], )

[Pass] paddle.reshape(Tensor([1760573, 1296],"int64"), shape=list[-1,], )
2025-03-19 22:01:00.414681 test begin: paddle.reshape(Tensor([1763, 17, 272, 280],"float32"), shape=list[-1,272,280,], )

[Pass] paddle.reshape(Tensor([1763, 17, 272, 280],"float32"), shape=list[-1,272,280,], )
2025-03-19 22:04:38.904218 test begin: paddle.reshape(Tensor([1763, 17, 280, 272],"float32"), shape=list[-1,280,272,], )

[Pass] paddle.reshape(Tensor([1763, 17, 280, 272],"float32"), shape=list[-1,280,272,], )
2025-03-19 22:08:18.191163 test begin: paddle.reshape(Tensor([1763, 17, 76160],"float32"), shape=tuple(-1,272,280,), )

[Pass] paddle.reshape(Tensor([1763, 17, 76160],"float32"), shape=tuple(-1,272,280,), )
2025-03-19 22:11:13.549494 test begin: paddle.reshape(Tensor([1765, 13, 296, 336],"float32"), shape=list[-1,296,336,], )

[Pass] paddle.reshape(Tensor([1765, 13, 296, 336],"float32"), shape=list[-1,296,336,], )
2025-03-19 22:14:11.321872 test begin: paddle.reshape(Tensor([1765, 13, 99456],"float32"), shape=tuple(-1,296,336,), )

[Pass] paddle.reshape(Tensor([1765, 13, 99456],"float32"), shape=tuple(-1,296,336,), )
2025-03-19 22:17:15.066987 test begin: paddle.reshape(Tensor([1782, 19, 216, 312],"float32"), shape=list[-1,216,312,], )

[Pass] paddle.reshape(Tensor([1782, 19, 216, 312],"float32"), shape=list[-1,216,312,], )
2025-03-19 22:20:30.775704 test begin: paddle.reshape(Tensor([17825793, 1, 128],"float32"), list[13,-1,32,], )

[torch error] paddle.reshape(Tensor([17825793, 1, 128],"float32"), list[13,-1,32,], ) 
 shape '[13, -1, 32]' is invalid for input of size 2281701504
2025-03-19 22:20:34.923311 test begin: paddle.reshape(Tensor([17825793, 1, 128],"float32"), shape=list[-1,128,], )

[Pass] paddle.reshape(Tensor([17825793, 1, 128],"float32"), shape=list[-1,128,], )
2025-03-19 22:23:43.198642 test begin: paddle.reshape(Tensor([17825793, 1, 128],"float32"), shape=list[-1,32,128,], )

[torch error] paddle.reshape(Tensor([17825793, 1, 128],"float32"), shape=list[-1,32,128,], ) 
 shape '[-1, 32, 128]' is invalid for input of size 2281701504
2025-03-19 22:23:47.260538 test begin: paddle.reshape(Tensor([17825793, 2, 64],"float32"), shape=list[13,2,4,16,], )

[torch error] paddle.reshape(Tensor([17825793, 2, 64],"float32"), shape=list[13,2,4,16,], ) 
 shape '[13, 2, 4, 16]' is invalid for input of size 2281701504
2025-03-19 22:23:49.287735 test begin: paddle.reshape(Tensor([17825793, 2, 64],"int64"), shape=list[13,2,-1,4,], )

[torch error] paddle.reshape(Tensor([17825793, 2, 64],"int64"), shape=list[13,2,-1,4,], ) 
 shape '[13, 2, -1, 4]' is invalid for input of size 2281701504
2025-03-19 22:23:57.228938 test begin: paddle.reshape(Tensor([17825793, 4, 32],"float32"), list[-1,128,], )

[Pass] paddle.reshape(Tensor([17825793, 4, 32],"float32"), list[-1,128,], )
2025-03-19 22:26:45.017306 test begin: paddle.reshape(Tensor([17825793, 4, 32],"float32"), shape=list[-1,32,], )

[Pass] paddle.reshape(Tensor([17825793, 4, 32],"float32"), shape=list[-1,32,], )
2025-03-19 22:29:45.372303 test begin: paddle.reshape(Tensor([17825793, 4, 32],"float32"), shape=list[14,-1,4,8,], )

[torch error] paddle.reshape(Tensor([17825793, 4, 32],"float32"), shape=list[14,-1,4,8,], ) 
 shape '[14, -1, 4, 8]' is invalid for input of size 2281701504
2025-03-19 22:29:49.433268 test begin: paddle.reshape(Tensor([17825793, 4, 32],"float32"), shape=list[7,4,4,8,], )

[torch error] paddle.reshape(Tensor([17825793, 4, 32],"float32"), shape=list[7,4,4,8,], ) 
 shape '[7, 4, 4, 8]' is invalid for input of size 2281701504
2025-03-19 22:29:50.946830 test begin: paddle.reshape(Tensor([17825793, 4, 4, 8],"float32"), list[13,4,8,4,], )

[torch error] paddle.reshape(Tensor([17825793, 4, 4, 8],"float32"), list[13,4,8,4,], ) 
 shape '[13, 4, 8, 4]' is invalid for input of size 2281701504
2025-03-19 22:29:53.354981 test begin: paddle.reshape(Tensor([1782580, 1280, 1, 1],"float32"), list[-1,16,1280,], )

[torch error] paddle.reshape(Tensor([1782580, 1280, 1, 1],"float32"), list[-1,16,1280,], ) 
 shape '[-1, 16, 1280]' is invalid for input of size 2281702400
2025-03-19 22:29:54.939173 test begin: paddle.reshape(Tensor([1782580, 1280, 1, 1],"float32"), list[-1,8,1280,], )

[torch error] paddle.reshape(Tensor([1782580, 1280, 1, 1],"float32"), list[-1,8,1280,], ) 
 shape '[-1, 8, 1280]' is invalid for input of size 2281702400
2025-03-19 22:29:57.356232 test begin: paddle.reshape(Tensor([1782580, 1280, 1, 1],"float32"), shape=list[-1,1280,], )

[Pass] paddle.reshape(Tensor([1782580, 1280, 1, 1],"float32"), shape=list[-1,1280,], )
2025-03-19 22:32:46.744352 test begin: paddle.reshape(Tensor([1782580, 1280],"float32"), shape=list[-1,1280,], )

[Pass] paddle.reshape(Tensor([1782580, 1280],"float32"), shape=list[-1,1280,], )
2025-03-19 22:35:48.608247 test begin: paddle.reshape(Tensor([1784, 14, 272, 336],"float32"), shape=list[-1,272,336,], )

[Pass] paddle.reshape(Tensor([1784, 14, 272, 336],"float32"), shape=list[-1,272,336,], )
2025-03-19 22:38:42.861887 test begin: paddle.reshape(Tensor([1784, 14, 336, 272],"float32"), shape=list[-1,336,272,], )

[Pass] paddle.reshape(Tensor([1784, 14, 336, 272],"float32"), shape=list[-1,336,272,], )
2025-03-19 22:41:41.349827 test begin: paddle.reshape(Tensor([178956971, 4, 6],"float16"), shape=list[8,6,], )

[torch error] paddle.reshape(Tensor([178956971, 4, 6],"float16"), shape=list[8,6,], ) 
 shape '[8, 6]' is invalid for input of size 4294967304
2025-03-19 22:41:45.186895 test begin: paddle.reshape(Tensor([1793, 15, 272, 312],"float32"), shape=list[-1,272,312,], )

[Pass] paddle.reshape(Tensor([1793, 15, 272, 312],"float32"), shape=list[-1,272,312,], )
2025-03-19 22:44:45.316860 test begin: paddle.reshape(Tensor([1793, 15, 312, 272],"float32"), shape=list[-1,312,272,], )

[Pass] paddle.reshape(Tensor([1793, 15, 312, 272],"float32"), shape=list[-1,312,272,], )
2025-03-19 22:47:42.862706 test begin: paddle.reshape(Tensor([1798, 18, 232, 304],"float32"), shape=list[-1,232,304,], )

[Pass] paddle.reshape(Tensor([1798, 18, 232, 304],"float32"), shape=list[-1,232,304,], )
2025-03-19 22:50:27.109471 test begin: paddle.reshape(Tensor([1799, 35, 68224],"float16"), shape=tuple(-1,208,328,), )

[Pass] paddle.reshape(Tensor([1799, 35, 68224],"float16"), shape=tuple(-1,208,328,), )
2025-03-19 23:06:31.194218 test begin: paddle.reshape(Tensor([18, 126761188],"float32"), shape=list[-1,3,], )

[Pass] paddle.reshape(Tensor([18, 126761188],"float32"), shape=list[-1,3,], )
2025-03-19 23:09:28.814737 test begin: paddle.reshape(Tensor([18, 126761188],"int64"), list[18,], )

[torch error] paddle.reshape(Tensor([18, 126761188],"int64"), list[18,], ) 
 shape '[18]' is invalid for input of size 2281701384
2025-03-19 23:09:36.858958 test begin: paddle.reshape(Tensor([180, 200, 119305],"float16"), shape=tuple(180,-1,), )

[Pass] paddle.reshape(Tensor([180, 200, 119305],"float16"), shape=tuple(180,-1,), )
2025-03-19 23:25:34.400326 test begin: paddle.reshape(Tensor([180, 200, 63381],"float32"), shape=tuple(180,-1,), )

[Pass] paddle.reshape(Tensor([180, 200, 63381],"float32"), shape=tuple(180,-1,), )
2025-03-19 23:28:24.196990 test begin: paddle.reshape(Tensor([180, 41698, 304],"float32"), shape=tuple(180,-1,), )

[Pass] paddle.reshape(Tensor([180, 41698, 304],"float32"), shape=tuple(180,-1,), )
2025-03-19 23:31:19.309814 test begin: paddle.reshape(Tensor([180, 46604, 272],"float32"), shape=tuple(180,-1,), )

[Pass] paddle.reshape(Tensor([180, 46604, 272],"float32"), shape=tuple(180,-1,), )
2025-03-19 23:34:02.320265 test begin: paddle.reshape(Tensor([180, 74566, 320],"float16"), shape=tuple(180,-1,), )

[Pass] paddle.reshape(Tensor([180, 74566, 320],"float16"), shape=tuple(180,-1,), )
2025-03-19 23:49:56.279777 test begin: paddle.reshape(Tensor([180, 78490, 304],"float16"), shape=tuple(180,-1,), )

[Pass] paddle.reshape(Tensor([180, 78490, 304],"float16"), shape=tuple(180,-1,), )
2025-03-20 00:06:22.841508 test begin: paddle.reshape(Tensor([180, 87725, 272],"float16"), shape=tuple(180,-1,), )

[Pass] paddle.reshape(Tensor([180, 87725, 272],"float16"), shape=tuple(180,-1,), )
2025-03-20 00:22:14.808215 test begin: paddle.reshape(Tensor([18006, 120, 88, 12],"float32"), shape=tuple(2,-1,4,), )

[Pass] paddle.reshape(Tensor([18006, 120, 88, 12],"float32"), shape=tuple(2,-1,4,), )
2025-03-20 00:24:58.986484 test begin: paddle.reshape(Tensor([1801, 22, 200, 288],"float32"), shape=list[-1,200,288,], )

[Pass] paddle.reshape(Tensor([1801, 22, 200, 288],"float32"), shape=list[-1,200,288,], )
2025-03-20 00:27:56.658823 test begin: paddle.reshape(Tensor([1805, 13, 304, 320],"float32"), shape=list[-1,304,320,], )

[Pass] paddle.reshape(Tensor([1805, 13, 304, 320],"float32"), shape=list[-1,304,320,], )
2025-03-20 00:30:56.584814 test begin: paddle.reshape(Tensor([1805, 13, 320, 304],"float32"), shape=list[-1,320,304,], )

[Pass] paddle.reshape(Tensor([1805, 13, 320, 304],"float32"), shape=list[-1,320,304,], )
2025-03-20 00:33:47.151561 test begin: paddle.reshape(Tensor([1805, 20, 208, 304],"float32"), shape=list[-1,208,304,], )

[Pass] paddle.reshape(Tensor([1805, 20, 208, 304],"float32"), shape=list[-1,208,304,], )
2025-03-20 00:36:43.268860 test begin: paddle.reshape(Tensor([1812, 14, 296, 304],"float32"), shape=list[-1,296,304,], )

[Pass] paddle.reshape(Tensor([1812, 14, 296, 304],"float32"), shape=list[-1,296,304,], )
2025-03-20 00:39:41.952992 test begin: paddle.reshape(Tensor([1814, 12, 104832],"float32"), shape=tuple(-1,312,336,), )

[Pass] paddle.reshape(Tensor([1814, 12, 104832],"float32"), shape=tuple(-1,312,336,), )
2025-03-20 00:42:52.100307 test begin: paddle.reshape(Tensor([1814, 12, 312, 336],"float32"), shape=list[-1,312,336,], )

[Pass] paddle.reshape(Tensor([1814, 12, 312, 336],"float32"), shape=list[-1,312,336,], )
2025-03-20 00:45:34.892975 test begin: paddle.reshape(Tensor([181896, 32, 2, 14, 14],"float32"), shape=list[2,64,14,14,], )

[torch error] paddle.reshape(Tensor([181896, 32, 2, 14, 14],"float32"), shape=list[2,64,14,14,], ) 
 shape '[2, 64, 14, 14]' is invalid for input of size 2281703424
2025-03-20 00:45:39.477819 test begin: paddle.reshape(Tensor([181896, 64, 14, 14],"float32"), shape=list[2,2,32,14,14,], )

[torch error] paddle.reshape(Tensor([181896, 64, 14, 14],"float32"), shape=list[2,2,32,14,14,], ) 
 shape '[2, 2, 32, 14, 14]' is invalid for input of size 2281703424
2025-03-20 00:45:41.428497 test begin: paddle.reshape(Tensor([1819, 16, 280, 280],"float32"), shape=list[-1,280,280,], )

[Pass] paddle.reshape(Tensor([1819, 16, 280, 280],"float32"), shape=list[-1,280,280,], )
2025-03-20 00:48:41.717204 test begin: paddle.reshape(Tensor([1819, 16, 78400],"float32"), shape=tuple(-1,280,280,), )

[Pass] paddle.reshape(Tensor([1819, 16, 78400],"float32"), shape=tuple(-1,280,280,), )
2025-03-20 00:51:47.044730 test begin: paddle.reshape(Tensor([1821, 16, 288, 272],"float32"), shape=list[-1,288,272,], )

[Pass] paddle.reshape(Tensor([1821, 16, 288, 272],"float32"), shape=list[-1,288,272,], )
2025-03-20 00:55:21.467382 test begin: paddle.reshape(Tensor([1825362, 25, 25, 2],"float32"), tuple(-1,2,), )

[Pass] paddle.reshape(Tensor([1825362, 25, 25, 2],"float32"), tuple(-1,2,), )
2025-03-20 00:58:17.818614 test begin: paddle.reshape(Tensor([184, 200, 116712],"float16"), shape=tuple(184,-1,), )

[Pass] paddle.reshape(Tensor([184, 200, 116712],"float16"), shape=tuple(184,-1,), )
2025-03-20 01:14:33.178989 test begin: paddle.reshape(Tensor([184, 200, 62003],"float32"), shape=tuple(184,-1,), )

[Pass] paddle.reshape(Tensor([184, 200, 62003],"float32"), shape=tuple(184,-1,), )
2025-03-20 01:17:46.195440 test begin: paddle.reshape(Tensor([184, 40792, 304],"float32"), shape=tuple(184,-1,), )

[Pass] paddle.reshape(Tensor([184, 40792, 304],"float32"), shape=tuple(184,-1,), )
2025-03-20 01:20:46.977445 test begin: paddle.reshape(Tensor([184, 45591, 272],"float32"), shape=tuple(184,-1,), )

[Pass] paddle.reshape(Tensor([184, 45591, 272],"float32"), shape=tuple(184,-1,), )
2025-03-20 01:24:04.565105 test begin: paddle.reshape(Tensor([184, 71166, 328],"float16"), shape=tuple(184,-1,), )

[Pass] paddle.reshape(Tensor([184, 71166, 328],"float16"), shape=tuple(184,-1,), )
2025-03-20 01:40:28.383476 test begin: paddle.reshape(Tensor([184, 76784, 304],"float16"), shape=tuple(184,-1,), )

[Pass] paddle.reshape(Tensor([184, 76784, 304],"float16"), shape=tuple(184,-1,), )
2025-03-20 01:56:52.661774 test begin: paddle.reshape(Tensor([184, 85817, 272],"float16"), shape=tuple(184,-1,), )

[Pass] paddle.reshape(Tensor([184, 85817, 272],"float16"), shape=tuple(184,-1,), )
2025-03-20 02:14:03.355935 test begin: paddle.reshape(Tensor([1840, 17, 240, 304],"float32"), shape=list[-1,240,304,], )

[Pass] paddle.reshape(Tensor([1840, 17, 240, 304],"float32"), shape=list[-1,240,304,], )
2025-03-20 02:17:11.579834 test begin: paddle.reshape(Tensor([1840, 17, 72960],"float32"), shape=tuple(-1,240,304,), )

[Pass] paddle.reshape(Tensor([1840, 17, 72960],"float32"), shape=tuple(-1,240,304,), )
2025-03-20 02:20:17.848004 test begin: paddle.reshape(Tensor([1851, 13, 304, 312],"float32"), shape=list[-1,304,312,], )

[Pass] paddle.reshape(Tensor([1851, 13, 304, 312],"float32"), shape=list[-1,304,312,], )
2025-03-20 02:23:14.721241 test begin: paddle.reshape(Tensor([1853, 22, 200, 280],"float32"), shape=list[-1,200,280,], )

[Pass] paddle.reshape(Tensor([1853, 22, 200, 280],"float32"), shape=list[-1,200,280,], )
2025-03-20 02:26:01.795810 test begin: paddle.reshape(Tensor([1853, 22, 56000],"float32"), shape=tuple(-1,200,280,), )

[Pass] paddle.reshape(Tensor([1853, 22, 56000],"float32"), shape=tuple(-1,200,280,), )
2025-03-20 02:29:01.312286 test begin: paddle.reshape(Tensor([1862, 12, 102144],"float32"), shape=tuple(-1,336,304,), )

[Pass] paddle.reshape(Tensor([1862, 12, 102144],"float32"), shape=tuple(-1,336,304,), )
2025-03-20 02:31:59.164625 test begin: paddle.reshape(Tensor([1862, 12, 304, 336],"float32"), shape=list[-1,304,336,], )

[Pass] paddle.reshape(Tensor([1862, 12, 304, 336],"float32"), shape=list[-1,304,336,], )
2025-03-20 02:35:01.393524 test begin: paddle.reshape(Tensor([1862, 12, 336, 304],"float32"), shape=list[-1,336,304,], )

[Pass] paddle.reshape(Tensor([1862, 12, 336, 304],"float32"), shape=list[-1,336,304,], )
2025-03-20 02:38:01.223036 test begin: paddle.reshape(Tensor([1866, 13, 280, 336],"float32"), shape=list[-1,280,336,], )

[Pass] paddle.reshape(Tensor([1866, 13, 280, 336],"float32"), shape=list[-1,280,336,], )
2025-03-20 02:40:52.301020 test begin: paddle.reshape(Tensor([1866, 13, 94080],"float32"), shape=tuple(-1,280,336,), )

[Pass] paddle.reshape(Tensor([1866, 13, 94080],"float32"), shape=tuple(-1,280,336,), )
2025-03-20 02:43:47.671438 test begin: paddle.reshape(Tensor([1866, 14, 280, 312],"float32"), shape=list[-1,280,312,], )

[Pass] paddle.reshape(Tensor([1866, 14, 280, 312],"float32"), shape=list[-1,280,312,], )
2025-03-20 02:46:35.182752 test begin: paddle.reshape(Tensor([1866, 14, 87360],"float32"), shape=tuple(-1,280,312,), )

[Pass] paddle.reshape(Tensor([1866, 14, 87360],"float32"), shape=tuple(-1,280,312,), )
2025-03-20 02:49:20.021890 test begin: paddle.reshape(Tensor([1870, 33, 69632],"float16"), shape=tuple(-1,256,272,), )

[Pass] paddle.reshape(Tensor([1870, 33, 69632],"float16"), shape=tuple(-1,256,272,), )
2025-03-20 03:05:51.922195 test begin: paddle.reshape(Tensor([188, 200, 114228],"float16"), shape=tuple(188,-1,), )

[Pass] paddle.reshape(Tensor([188, 200, 114228],"float16"), shape=tuple(188,-1,), )
2025-03-20 03:22:45.380931 test begin: paddle.reshape(Tensor([188, 200, 60684],"float32"), shape=tuple(188,-1,), )

[Pass] paddle.reshape(Tensor([188, 200, 60684],"float32"), shape=tuple(188,-1,), )
2025-03-20 03:25:37.466122 test begin: paddle.reshape(Tensor([188, 272, 83992],"float16"), shape=tuple(188,-1,), )

[Pass] paddle.reshape(Tensor([188, 272, 83992],"float16"), shape=tuple(188,-1,), )
2025-03-20 03:41:51.676758 test begin: paddle.reshape(Tensor([188, 36122, 336],"float32"), shape=tuple(188,-1,), )

[Pass] paddle.reshape(Tensor([188, 36122, 336],"float32"), shape=tuple(188,-1,), )
2025-03-20 03:45:03.936609 test begin: paddle.reshape(Tensor([188, 39924, 304],"float32"), shape=tuple(188,-1,), )

[Pass] paddle.reshape(Tensor([188, 39924, 304],"float32"), shape=tuple(188,-1,), )
2025-03-20 03:48:12.018674 test begin: paddle.reshape(Tensor([188, 67993, 336],"float16"), shape=tuple(188,-1,), )

[Pass] paddle.reshape(Tensor([188, 67993, 336],"float16"), shape=tuple(188,-1,), )
2025-03-20 04:04:26.816522 test begin: paddle.reshape(Tensor([188, 75150, 304],"float16"), shape=tuple(188,-1,), )

[Pass] paddle.reshape(Tensor([188, 75150, 304],"float16"), shape=tuple(188,-1,), )
2025-03-20 04:20:38.744253 test begin: paddle.reshape(Tensor([188, 83992, 272],"float16"), shape=tuple(188,-1,), )

[Pass] paddle.reshape(Tensor([188, 83992, 272],"float16"), shape=tuple(188,-1,), )
2025-03-20 04:36:38.700351 test begin: paddle.reshape(Tensor([1887, 21, 200, 288],"float32"), shape=list[-1,200,288,], )

[Pass] paddle.reshape(Tensor([1887, 21, 200, 288],"float32"), shape=list[-1,200,288,], )
2025-03-20 04:39:28.780831 test begin: paddle.reshape(Tensor([1887, 26, 87552],"float16"), shape=tuple(-1,288,304,), )

[Pass] paddle.reshape(Tensor([1887, 26, 87552],"float16"), shape=tuple(-1,288,304,), )
2025-03-20 04:55:25.647048 test begin: paddle.reshape(Tensor([1890, 15, 272, 296],"float32"), shape=list[-1,272,296,], )

[Pass] paddle.reshape(Tensor([1890, 15, 272, 296],"float32"), shape=list[-1,272,296,], )
2025-03-20 04:58:18.143188 test begin: paddle.reshape(Tensor([1890, 15, 296, 272],"float32"), shape=list[-1,296,272,], )

[Pass] paddle.reshape(Tensor([1890, 15, 296, 272],"float32"), shape=list[-1,296,272,], )
2025-03-20 05:01:18.870292 test begin: paddle.reshape(Tensor([1891, 26, 87360],"float16"), shape=tuple(-1,280,312,), )

[Pass] paddle.reshape(Tensor([1891, 26, 87360],"float16"), shape=tuple(-1,280,312,), )
2025-03-20 05:17:34.820668 test begin: paddle.reshape(Tensor([1895, 8, 3, 224, 224],"float32"), list[-1,3,224,224,], )

[Pass] paddle.reshape(Tensor([1895, 8, 3, 224, 224],"float32"), list[-1,3,224,224,], )
2025-03-20 05:20:22.174257 test begin: paddle.reshape(Tensor([1900, 13, 304, 304],"float32"), shape=list[-1,304,304,], )

[Pass] paddle.reshape(Tensor([1900, 13, 304, 304],"float32"), shape=list[-1,304,304,], )
2025-03-20 05:23:16.818064 test begin: paddle.reshape(Tensor([1901, 13, 312, 296],"float32"), shape=list[-1,312,296,], )

[Pass] paddle.reshape(Tensor([1901, 13, 312, 296],"float32"), shape=list[-1,312,296,], )
2025-03-20 05:26:22.496314 test begin: paddle.reshape(Tensor([1901, 13, 92352],"float32"), shape=tuple(-1,312,296,), )

[Pass] paddle.reshape(Tensor([1901, 13, 92352],"float32"), shape=tuple(-1,312,296,), )
2025-03-20 05:29:10.665296 test begin: paddle.reshape(Tensor([190141782, 3, 4],"float32"), list[-1,4,], )

[Pass] paddle.reshape(Tensor([190141782, 3, 4],"float32"), list[-1,4,], )
2025-03-20 05:31:55.127096 test begin: paddle.reshape(Tensor([19014179, 3, 5, 2, 4],"float32"), shape=list[15,8,], )

[torch error] paddle.reshape(Tensor([19014179, 3, 5, 2, 4],"float32"), shape=list[15,8,], ) 
 shape '[15, 8]' is invalid for input of size 2281701480
2025-03-20 05:31:59.265123 test begin: paddle.reshape(Tensor([19014179, 3, 5, 2, 4],"float32"), shape=list[30,8,], )

[torch error] paddle.reshape(Tensor([19014179, 3, 5, 2, 4],"float32"), shape=list[30,8,], ) 
 shape '[30, 8]' is invalid for input of size 2281701480
2025-03-20 05:32:01.271298 test begin: paddle.reshape(Tensor([19014179, 3, 5, 8],"float32"), shape=list[1,3,5,2,4,], )

[torch error] paddle.reshape(Tensor([19014179, 3, 5, 8],"float32"), shape=list[1,3,5,2,4,], ) 
 shape '[1, 3, 5, 2, 4]' is invalid for input of size 2281701480
2025-03-20 05:32:03.623544 test begin: paddle.reshape(Tensor([19014179, 3, 5, 8],"float32"), shape=list[2,3,5,2,4,], )

[torch error] paddle.reshape(Tensor([19014179, 3, 5, 8],"float32"), shape=list[2,3,5,2,4,], ) 
 shape '[2, 3, 5, 2, 4]' is invalid for input of size 2281701480
2025-03-20 05:32:05.686409 test begin: paddle.reshape(Tensor([1901418, 24, 2, 5, 5],"float32"), shape=list[1,48,5,5,], )

[torch error] paddle.reshape(Tensor([1901418, 24, 2, 5, 5],"float32"), shape=list[1,48,5,5,], ) 
 shape '[1, 48, 5, 5]' is invalid for input of size 2281701600
2025-03-20 05:32:07.732320 test begin: paddle.reshape(Tensor([1901418, 48, 5, 5],"float32"), shape=list[1,2,24,5,5,], )

[torch error] paddle.reshape(Tensor([1901418, 48, 5, 5],"float32"), shape=list[1,2,24,5,5,], ) 
 shape '[1, 2, 24, 5, 5]' is invalid for input of size 2281701600
2025-03-20 05:32:09.789588 test begin: paddle.reshape(Tensor([1907, 12, 304, 328],"float32"), shape=list[-1,304,328,], )

[Pass] paddle.reshape(Tensor([1907, 12, 304, 328],"float32"), shape=list[-1,304,328,], )
2025-03-20 05:34:55.485160 test begin: paddle.reshape(Tensor([1907, 12, 328, 304],"float32"), shape=list[-1,328,304,], )

[Pass] paddle.reshape(Tensor([1907, 12, 328, 304],"float32"), shape=list[-1,328,304,], )
2025-03-20 05:37:55.309271 test begin: paddle.reshape(Tensor([1907, 12, 99712],"float32"), shape=tuple(-1,304,328,), )

[Pass] paddle.reshape(Tensor([1907, 12, 99712],"float32"), shape=tuple(-1,304,328,), )
2025-03-20 05:40:59.192899 test begin: paddle.reshape(Tensor([1907, 12, 99712],"float32"), shape=tuple(-1,328,304,), )

[Pass] paddle.reshape(Tensor([1907, 12, 99712],"float32"), shape=tuple(-1,328,304,), )
2025-03-20 05:43:42.484765 test begin: paddle.reshape(Tensor([1907, 22, 272, 200],"float32"), shape=list[-1,272,200,], )

[Pass] paddle.reshape(Tensor([1907, 22, 272, 200],"float32"), shape=list[-1,272,200,], )
2025-03-20 05:46:25.505638 test begin: paddle.reshape(Tensor([1912, 12, 296, 336],"float32"), shape=list[-1,296,336,], )

[Pass] paddle.reshape(Tensor([1912, 12, 296, 336],"float32"), shape=list[-1,296,336,], )
2025-03-20 05:49:08.791919 test begin: paddle.reshape(Tensor([192, 200, 111849],"float16"), shape=tuple(192,-1,), )

[Pass] paddle.reshape(Tensor([192, 200, 111849],"float16"), shape=tuple(192,-1,), )
2025-03-20 06:05:04.831453 test begin: paddle.reshape(Tensor([192, 200, 59420],"float32"), shape=tuple(192,-1,), )

[Pass] paddle.reshape(Tensor([192, 200, 59420],"float32"), shape=tuple(192,-1,), )
2025-03-20 06:07:55.981678 test begin: paddle.reshape(Tensor([192, 304, 73585],"float16"), shape=tuple(192,-1,), )

[Pass] paddle.reshape(Tensor([192, 304, 73585],"float16"), shape=tuple(192,-1,), )
2025-03-20 06:23:50.475987 test begin: paddle.reshape(Tensor([192, 35369, 336],"float32"), shape=tuple(192,-1,), )

[Pass] paddle.reshape(Tensor([192, 35369, 336],"float32"), shape=tuple(192,-1,), )
2025-03-20 06:26:48.638748 test begin: paddle.reshape(Tensor([192, 39092, 304],"float32"), shape=tuple(192,-1,), )

[Pass] paddle.reshape(Tensor([192, 39092, 304],"float32"), shape=tuple(192,-1,), )
2025-03-20 06:30:05.454305 test begin: paddle.reshape(Tensor([192, 66577, 336],"float16"), shape=tuple(192,-1,), )

[Pass] paddle.reshape(Tensor([192, 66577, 336],"float16"), shape=tuple(192,-1,), )
2025-03-20 06:46:09.273107 test begin: paddle.reshape(Tensor([192, 73585, 304],"float16"), shape=tuple(192,-1,), )

[Pass] paddle.reshape(Tensor([192, 73585, 304],"float16"), shape=tuple(192,-1,), )
2025-03-20 07:02:22.120653 test begin: paddle.reshape(Tensor([192, 82242, 272],"float16"), shape=tuple(192,-1,), )

[Pass] paddle.reshape(Tensor([192, 82242, 272],"float16"), shape=tuple(192,-1,), )
2025-03-20 07:18:19.245509 test begin: paddle.reshape(Tensor([1921, 13, 272, 336],"float32"), shape=list[-1,272,336,], )

[Pass] paddle.reshape(Tensor([1921, 13, 272, 336],"float32"), shape=list[-1,272,336,], )
2025-03-20 07:21:11.302592 test begin: paddle.reshape(Tensor([1921, 13, 336, 272],"float32"), shape=list[-1,336,272,], )

[Pass] paddle.reshape(Tensor([1921, 13, 336, 272],"float32"), shape=list[-1,336,272,], )
2025-03-20 07:23:50.783527 test begin: paddle.reshape(Tensor([1926, 34, 65600],"float16"), shape=tuple(-1,328,200,), )

[Pass] paddle.reshape(Tensor([1926, 34, 65600],"float16"), shape=tuple(-1,328,200,), )
2025-03-20 07:40:00.953861 test begin: paddle.reshape(Tensor([1930, 14, 264, 320],"float32"), shape=list[-1,264,320,], )

[Pass] paddle.reshape(Tensor([1930, 14, 264, 320],"float32"), shape=list[-1,264,320,], )
2025-03-20 07:42:51.841997 test begin: paddle.reshape(Tensor([1930, 14, 84480],"float32"), shape=tuple(-1,264,320,), )

[Pass] paddle.reshape(Tensor([1930, 14, 84480],"float32"), shape=tuple(-1,264,320,), )
2025-03-20 07:45:41.677389 test begin: paddle.reshape(Tensor([1931, 18, 216, 304],"float32"), shape=list[-1,216,304,], )

[Pass] paddle.reshape(Tensor([1931, 18, 216, 304],"float32"), shape=list[-1,216,304,], )
2025-03-20 07:48:34.923364 test begin: paddle.reshape(Tensor([1933, 18, 328, 200],"float32"), shape=list[-1,328,200,], )

[Pass] paddle.reshape(Tensor([1933, 18, 328, 200],"float32"), shape=list[-1,328,200,], )
2025-03-20 07:51:24.227853 test begin: paddle.reshape(Tensor([1935, 96, 96, 128],"float32"), list[4,96,96,128,], )

[torch error] paddle.reshape(Tensor([1935, 96, 96, 128],"float32"), list[4,96,96,128,], ) 
 shape '[4, 96, 96, 128]' is invalid for input of size 2282618880
2025-03-20 07:51:28.357466 test begin: paddle.reshape(Tensor([1935, 96, 96, 128],"float32"), list[60,96,96,128,], )

[torch error] paddle.reshape(Tensor([1935, 96, 96, 128],"float32"), list[60,96,96,128,], ) 
 shape '[60, 96, 96, 128]' is invalid for input of size 2282618880
2025-03-20 07:51:29.657351 test begin: paddle.reshape(Tensor([1935, 96, 96, 128],"float32"), list[64,96,96,128,], )

[torch error] paddle.reshape(Tensor([1935, 96, 96, 128],"float32"), list[64,96,96,128,], ) 
 shape '[64, 96, 96, 128]' is invalid for input of size 2282618880
2025-03-20 07:51:30.922907 test begin: paddle.reshape(Tensor([1940223, 49, 24],"float32"), list[1960,24,], )

[torch error] paddle.reshape(Tensor([1940223, 49, 24],"float32"), list[1960,24,], ) 
 shape '[1960, 24]' is invalid for input of size 2281702248
2025-03-20 07:51:33.140738 test begin: paddle.reshape(Tensor([1951, 13, 296, 304],"float32"), shape=list[-1,296,304,], )

[Pass] paddle.reshape(Tensor([1951, 13, 296, 304],"float32"), shape=list[-1,296,304,], )
2025-03-20 07:54:36.838991 test begin: paddle.reshape(Tensor([1951, 21, 104832],"float16"), shape=tuple(-1,312,336,), )

[Pass] paddle.reshape(Tensor([1951, 21, 104832],"float16"), shape=tuple(-1,312,336,), )
2025-03-20 08:10:36.647174 test begin: paddle.reshape(Tensor([1955, 12, 304, 320],"float32"), shape=list[-1,304,320,], )

[Pass] paddle.reshape(Tensor([1955, 12, 304, 320],"float32"), shape=list[-1,304,320,], )
2025-03-20 08:13:20.952564 test begin: paddle.reshape(Tensor([1955, 12, 97280],"float32"), shape=tuple(-1,304,320,), )

[Pass] paddle.reshape(Tensor([1955, 12, 97280],"float32"), shape=tuple(-1,304,320,), )
2025-03-20 08:16:23.725115 test begin: paddle.reshape(Tensor([1955, 15, 256, 304],"float32"), shape=list[-1,256,304,], )

[Pass] paddle.reshape(Tensor([1955, 15, 256, 304],"float32"), shape=list[-1,256,304,], )
2025-03-20 08:19:27.543270 test begin: paddle.reshape(Tensor([1955, 15, 77824],"float32"), shape=tuple(-1,256,304,), )

[Pass] paddle.reshape(Tensor([1955, 15, 77824],"float32"), shape=tuple(-1,256,304,), )
2025-03-20 08:22:26.818912 test begin: paddle.reshape(Tensor([1955, 16, 240, 304],"float32"), shape=list[-1,240,304,], )

[Pass] paddle.reshape(Tensor([1955, 16, 240, 304],"float32"), shape=list[-1,240,304,], )
2025-03-20 08:25:12.187316 test begin: paddle.reshape(Tensor([1955, 16, 72960],"float32"), shape=tuple(-1,240,304,), )

[Pass] paddle.reshape(Tensor([1955, 16, 72960],"float32"), shape=tuple(-1,240,304,), )
2025-03-20 08:28:24.049118 test begin: paddle.reshape(Tensor([1959, 12, 328, 296],"float32"), shape=list[-1,328,296,], )

[Pass] paddle.reshape(Tensor([1959, 12, 328, 296],"float32"), shape=list[-1,328,296,], )
2025-03-20 08:31:12.996814 test begin: paddle.reshape(Tensor([1959, 12, 97088],"float32"), shape=tuple(-1,328,296,), )

[Pass] paddle.reshape(Tensor([1959, 12, 97088],"float32"), shape=tuple(-1,328,296,), )
2025-03-20 08:34:15.570422 test begin: paddle.reshape(Tensor([196, 200, 109566],"float16"), shape=tuple(196,-1,), )

[Pass] paddle.reshape(Tensor([196, 200, 109566],"float16"), shape=tuple(196,-1,), )
2025-03-20 08:50:43.781925 test begin: paddle.reshape(Tensor([196, 200, 58207],"float32"), shape=tuple(196,-1,), )

[Pass] paddle.reshape(Tensor([196, 200, 58207],"float32"), shape=tuple(196,-1,), )
2025-03-20 08:53:37.474578 test begin: paddle.reshape(Tensor([196, 272, 80563],"float16"), shape=tuple(196,-1,), )

[Pass] paddle.reshape(Tensor([196, 272, 80563],"float16"), shape=tuple(196,-1,), )
2025-03-20 09:10:04.554611 test begin: paddle.reshape(Tensor([196, 34647, 336],"float32"), shape=tuple(196,-1,), )

[Pass] paddle.reshape(Tensor([196, 34647, 336],"float32"), shape=tuple(196,-1,), )
2025-03-20 09:12:53.299760 test begin: paddle.reshape(Tensor([196, 38294, 304],"float32"), shape=tuple(196,-1,), )

[Pass] paddle.reshape(Tensor([196, 38294, 304],"float32"), shape=tuple(196,-1,), )
2025-03-20 09:15:54.054070 test begin: paddle.reshape(Tensor([196, 65218, 336],"float16"), shape=tuple(196,-1,), )

[Pass] paddle.reshape(Tensor([196, 65218, 336],"float16"), shape=tuple(196,-1,), )
2025-03-20 09:32:15.093946 test begin: paddle.reshape(Tensor([196, 72083, 304],"float16"), shape=tuple(196,-1,), )

[Pass] paddle.reshape(Tensor([196, 72083, 304],"float16"), shape=tuple(196,-1,), )
2025-03-20 09:48:12.852418 test begin: paddle.reshape(Tensor([1960, 1164134],"float32"), list[-1,4,], )

[Pass] paddle.reshape(Tensor([1960, 1164134],"float32"), list[-1,4,], )
2025-03-20 09:51:06.202340 test begin: paddle.reshape(Tensor([1963, 25, 87552],"float16"), shape=tuple(-1,288,304,), )

[Pass] paddle.reshape(Tensor([1963, 25, 87552],"float16"), shape=tuple(-1,288,304,), )
2025-03-20 10:07:39.684777 test begin: paddle.reshape(Tensor([1967, 39, 56000],"float16"), shape=tuple(-1,200,280,), )

[Pass] paddle.reshape(Tensor([1967, 39, 56000],"float16"), shape=tuple(-1,200,280,), )
2025-03-20 10:23:49.522417 test begin: paddle.reshape(Tensor([1968, 13, 272, 328],"float32"), shape=list[-1,272,328,], )

[Pass] paddle.reshape(Tensor([1968, 13, 272, 328],"float32"), shape=list[-1,272,328,], )
2025-03-20 10:26:35.592286 test begin: paddle.reshape(Tensor([1972, 14, 304, 272],"float32"), shape=list[-1,304,272,], )

[Pass] paddle.reshape(Tensor([1972, 14, 304, 272],"float32"), shape=list[-1,304,272,], )
2025-03-20 10:29:22.493736 test begin: paddle.reshape(Tensor([1980644, 24, 24, 2],"float32"), shape=tuple(-1,2,), )

[Pass] paddle.reshape(Tensor([1980644, 24, 24, 2],"float32"), shape=tuple(-1,2,), )
2025-03-20 10:32:11.680563 test begin: paddle.reshape(Tensor([198065, 12, 12, 80],"float32"), shape=tuple(-1,80,), )

[Pass] paddle.reshape(Tensor([198065, 12, 12, 80],"float32"), shape=tuple(-1,80,), )
2025-03-20 10:34:51.728783 test begin: paddle.reshape(Tensor([1983, 18, 216, 296],"float32"), shape=list[-1,216,296,], )

[Pass] paddle.reshape(Tensor([1983, 18, 216, 296],"float32"), shape=list[-1,216,296,], )
2025-03-20 10:37:53.609946 test begin: paddle.reshape(Tensor([19973, 2, 107520],"float16"), shape=tuple(-1,336,320,), )

[Pass] paddle.reshape(Tensor([19973, 2, 107520],"float16"), shape=tuple(-1,336,320,), )
2025-03-20 10:54:00.403622 test begin: paddle.reshape(Tensor([1998, 17, 336, 200],"float32"), shape=list[-1,336,200,], )

[Pass] paddle.reshape(Tensor([1998, 17, 336, 200],"float32"), shape=list[-1,336,200,], )
2025-03-20 10:57:02.009671 test begin: paddle.reshape(Tensor([1998, 17, 67200],"float32"), shape=tuple(-1,336,200,), )

[Pass] paddle.reshape(Tensor([1998, 17, 67200],"float32"), shape=tuple(-1,336,200,), )
2025-03-20 11:00:00.897207 test begin: paddle.reshape(Tensor([2, 1, 1140850690],"float32"), list[-1,24,], )

[torch error] paddle.reshape(Tensor([2, 1, 1140850690],"float32"), list[-1,24,], ) 
 shape '[-1, 24]' is invalid for input of size 2281701380
2025-03-20 11:00:04.971929 test begin: paddle.reshape(Tensor([2, 1, 1140850690],"float32"), list[-1,8,], )

[torch error] paddle.reshape(Tensor([2, 1, 1140850690],"float32"), list[-1,8,], ) 
 shape '[-1, 8]' is invalid for input of size 2281701380
2025-03-20 11:00:07.038771 test begin: paddle.reshape(Tensor([2, 1, 1140850690],"float32"), shape=list[-1,128,], )

[torch error] paddle.reshape(Tensor([2, 1, 1140850690],"float32"), shape=list[-1,128,], ) 
 shape '[-1, 128]' is invalid for input of size 2281701380
2025-03-20 11:00:09.094284 test begin: paddle.reshape(Tensor([2, 1033380, 92, 12],"float32"), shape=tuple(2,-1,4,), )

[Pass] paddle.reshape(Tensor([2, 1033380, 92, 12],"float32"), shape=tuple(2,-1,4,), )
2025-03-20 11:02:55.012060 test begin: paddle.reshape(Tensor([2, 108, 132, 80027],"float32"), shape=tuple(2,-1,1,), )

[Pass] paddle.reshape(Tensor([2, 108, 132, 80027],"float32"), shape=tuple(2,-1,1,), )
2025-03-20 11:05:43.748857 test begin: paddle.reshape(Tensor([2, 108, 132, 80027],"float32"), shape=tuple(2,-1,4,), )

[Pass] paddle.reshape(Tensor([2, 108, 132, 80027],"float32"), shape=tuple(2,-1,4,), )
2025-03-20 11:08:45.303850 test begin: paddle.reshape(Tensor([2, 108, 3521145, 3],"float32"), shape=tuple(2,-1,1,), )

[Pass] paddle.reshape(Tensor([2, 108, 3521145, 3],"float32"), shape=tuple(2,-1,1,), )
2025-03-20 11:11:44.864961 test begin: paddle.reshape(Tensor([2, 108, 880287, 12],"float32"), shape=tuple(2,-1,4,), )

[Pass] paddle.reshape(Tensor([2, 108, 880287, 12],"float32"), shape=tuple(2,-1,4,), )
2025-03-20 11:14:27.012659 test begin: paddle.reshape(Tensor([2, 1080352, 88, 12],"float32"), shape=tuple(2,-1,4,), )

[Pass] paddle.reshape(Tensor([2, 1080352, 88, 12],"float32"), shape=tuple(2,-1,4,), )
2025-03-20 11:17:33.003126 test begin: paddle.reshape(Tensor([2, 1114113, 32, 32],"float32"), list[2,-1,], )

[Pass] paddle.reshape(Tensor([2, 1114113, 32, 32],"float32"), list[2,-1,], )
2025-03-20 11:20:31.536912 test begin: paddle.reshape(Tensor([2, 112, 12993, 28, 28],"float32"), shape=list[2,224,28,28,], )

[torch error] paddle.reshape(Tensor([2, 112, 12993, 28, 28],"float32"), shape=list[2,224,28,28,], ) 
 shape '[2, 224, 28, 28]' is invalid for input of size 2281778688
2025-03-20 11:20:35.690564 test begin: paddle.reshape(Tensor([2, 112, 2, 181896, 28],"float32"), shape=list[2,224,28,28,], )

[torch error] paddle.reshape(Tensor([2, 112, 2, 181896, 28],"float32"), shape=list[2,224,28,28,], ) 
 shape '[2, 224, 28, 28]' is invalid for input of size 2281703424
2025-03-20 11:20:37.639661 test begin: paddle.reshape(Tensor([2, 112, 2, 28, 181896],"float32"), shape=list[2,224,28,28,], )

[torch error] paddle.reshape(Tensor([2, 112, 2, 28, 181896],"float32"), shape=list[2,224,28,28,], ) 
 shape '[2, 224, 28, 28]' is invalid for input of size 2281703424
2025-03-20 11:20:40.078723 test begin: paddle.reshape(Tensor([2, 112, 3395389, 3],"float32"), shape=tuple(2,-1,1,), )

[Pass] paddle.reshape(Tensor([2, 112, 3395389, 3],"float32"), shape=tuple(2,-1,1,), )
2025-03-20 11:23:37.353060 test begin: paddle.reshape(Tensor([2, 112, 84, 121264],"float32"), shape=tuple(2,-1,1,), )

[Pass] paddle.reshape(Tensor([2, 112, 84, 121264],"float32"), shape=tuple(2,-1,1,), )
2025-03-20 11:26:36.308387 test begin: paddle.reshape(Tensor([2, 112, 84, 121264],"float32"), shape=tuple(2,-1,4,), )

[Pass] paddle.reshape(Tensor([2, 112, 84, 121264],"float32"), shape=tuple(2,-1,4,), )
2025-03-20 11:29:42.824095 test begin: paddle.reshape(Tensor([2, 112, 848848, 12],"float32"), shape=tuple(2,-1,4,), )

[Pass] paddle.reshape(Tensor([2, 112, 848848, 12],"float32"), shape=tuple(2,-1,4,), )
2025-03-20 11:32:25.561136 test begin: paddle.reshape(Tensor([2, 1131797, 84, 12],"float32"), shape=tuple(2,-1,4,), )

[Pass] paddle.reshape(Tensor([2, 1131797, 84, 12],"float32"), shape=tuple(2,-1,4,), )
2025-03-20 11:35:46.047905 test begin: paddle.reshape(Tensor([2, 114085069, 1, 10],"float32"), shape=list[-1,10,], )

[Pass] paddle.reshape(Tensor([2, 114085069, 1, 10],"float32"), shape=list[-1,10,], )
2025-03-20 11:38:29.367577 test begin: paddle.reshape(Tensor([2, 1140850690, 1, 1],"float32"), shape=list[-1,2048,], )

[torch error] paddle.reshape(Tensor([2, 1140850690, 1, 1],"float32"), shape=list[-1,2048,], ) 
 shape '[-1, 2048]' is invalid for input of size 2281701380
2025-03-20 11:38:33.758901 test begin: paddle.reshape(Tensor([2, 1140850690],"float32"), list[-1,1,8,], )

[torch error] paddle.reshape(Tensor([2, 1140850690],"float32"), list[-1,1,8,], ) 
 shape '[-1, 1, 8]' is invalid for input of size 2281701380
2025-03-20 11:38:35.723763 test begin: paddle.reshape(Tensor([2, 1140850690],"float32"), list[10,], )

[torch error] paddle.reshape(Tensor([2, 1140850690],"float32"), list[10,], ) 
 shape '[10]' is invalid for input of size 2281701380
2025-03-20 11:38:37.834929 test begin: paddle.reshape(Tensor([2, 1140850690],"float32"), list[12,10,], )

[torch error] paddle.reshape(Tensor([2, 1140850690],"float32"), list[12,10,], ) 
 shape '[12, 10]' is invalid for input of size 2281701380
2025-03-20 11:38:40.333588 test begin: paddle.reshape(Tensor([2, 1140850690],"float32"), list[2,3,3,3,], )

[torch error] paddle.reshape(Tensor([2, 1140850690],"float32"), list[2,3,3,3,], ) 
 shape '[2, 3, 3, 3]' is invalid for input of size 2281701380
2025-03-20 11:38:42.145386 test begin: paddle.reshape(Tensor([2, 1140850690],"float32"), list[2,3,32,32,], )

[torch error] paddle.reshape(Tensor([2, 1140850690],"float32"), list[2,3,32,32,], ) 
 shape '[2, 3, 32, 32]' is invalid for input of size 2281701380
2025-03-20 11:38:44.319662 test begin: paddle.reshape(Tensor([2, 1140850690],"float32"), list[2,5,5,], )

[torch error] paddle.reshape(Tensor([2, 1140850690],"float32"), list[2,5,5,], ) 
 shape '[2, 5, 5]' is invalid for input of size 2281701380
2025-03-20 11:38:46.334094 test begin: paddle.reshape(Tensor([2, 1140850690],"float32"), list[2,8,1,], )

[torch error] paddle.reshape(Tensor([2, 1140850690],"float32"), list[2,8,1,], ) 
 shape '[2, 8, 1]' is invalid for input of size 2281701380
2025-03-20 11:38:48.439265 test begin: paddle.reshape(Tensor([2, 1140850690],"float32"), list[3,2,], )

[torch error] paddle.reshape(Tensor([2, 1140850690],"float32"), list[3,2,], ) 
 shape '[3, 2]' is invalid for input of size 2281701380
2025-03-20 11:38:50.544916 test begin: paddle.reshape(Tensor([2, 1140850690],"float32"), list[6,-1,], )

[torch error] paddle.reshape(Tensor([2, 1140850690],"float32"), list[6,-1,], ) 
 shape '[6, -1]' is invalid for input of size 2281701380
2025-03-20 11:38:52.692457 test begin: paddle.reshape(Tensor([2, 1140850690],"float32"), shape=list[-1,1280,], )

[torch error] paddle.reshape(Tensor([2, 1140850690],"float32"), shape=list[-1,1280,], ) 
 shape '[-1, 1280]' is invalid for input of size 2281701380
2025-03-20 11:38:54.958835 test begin: paddle.reshape(Tensor([2, 1140850690],"float32"), shape=list[-1,2048,], )

[torch error] paddle.reshape(Tensor([2, 1140850690],"float32"), shape=list[-1,2048,], ) 
 shape '[-1, 2048]' is invalid for input of size 2281701380
2025-03-20 11:38:57.107550 test begin: paddle.reshape(Tensor([2, 1140850690],"int32"), shape=tuple(-1,1,), )

[Pass] paddle.reshape(Tensor([2, 1140850690],"int32"), shape=tuple(-1,1,), )
2025-03-20 11:41:43.023510 test begin: paddle.reshape(Tensor([2, 1140850690],"int64"), list[-1,1,], )

[Pass] paddle.reshape(Tensor([2, 1140850690],"int64"), list[-1,1,], )
2025-03-20 11:44:21.752752 test begin: paddle.reshape(Tensor([2, 1140850690],"int64"), list[10,], )

[torch error] paddle.reshape(Tensor([2, 1140850690],"int64"), list[10,], ) 
 shape '[10]' is invalid for input of size 2281701380
2025-03-20 11:44:28.595173 test begin: paddle.reshape(Tensor([2, 1140851, 1, 1000],"float32"), shape=list[-1,1000,], )

[Pass] paddle.reshape(Tensor([2, 1140851, 1, 1000],"float32"), shape=list[-1,1000,], )
2025-03-20 11:47:24.288877 test begin: paddle.reshape(Tensor([2, 116, 2, 14, 351248],"float32"), shape=list[2,232,14,14,], )

[torch error] paddle.reshape(Tensor([2, 116, 2, 14, 351248],"float32"), shape=list[2,232,14,14,], ) 
 shape '[2, 232, 14, 14]' is invalid for input of size 2281707008
2025-03-20 11:47:28.593816 test begin: paddle.reshape(Tensor([2, 116, 2, 351248, 14],"float32"), shape=list[2,232,14,14,], )

[torch error] paddle.reshape(Tensor([2, 116, 2, 351248, 14],"float32"), shape=list[2,232,14,14,], ) 
 shape '[2, 232, 14, 14]' is invalid for input of size 2281707008
2025-03-20 11:47:30.842900 test begin: paddle.reshape(Tensor([2, 116, 28, 351248],"float32"), shape=list[2,2,58,28,28,], )

[torch error] paddle.reshape(Tensor([2, 116, 28, 351248],"float32"), shape=list[2,2,58,28,28,], ) 
 shape '[2, 2, 58, 28, 28]' is invalid for input of size 2281707008
2025-03-20 11:47:32.534560 test begin: paddle.reshape(Tensor([2, 116, 351248, 28],"float32"), shape=list[2,2,58,28,28,], )

[torch error] paddle.reshape(Tensor([2, 116, 351248, 28],"float32"), shape=list[2,2,58,28,28,], ) 
 shape '[2, 2, 58, 28, 28]' is invalid for input of size 2281707008
2025-03-20 11:47:35.017554 test begin: paddle.reshape(Tensor([2, 116, 50179, 14, 14],"float32"), shape=list[2,232,14,14,], )

[torch error] paddle.reshape(Tensor([2, 116, 50179, 14, 14],"float32"), shape=list[2,232,14,14,], ) 
 shape '[2, 232, 14, 14]' is invalid for input of size 2281739488
2025-03-20 11:47:37.308868 test begin: paddle.reshape(Tensor([2, 11641334, 2, 7, 7],"float32"), shape=list[2,128,7,7,], )

[torch error] paddle.reshape(Tensor([2, 11641334, 2, 7, 7],"float32"), shape=list[2,128,7,7,], ) 
 shape '[2, 128, 7, 7]' is invalid for input of size 2281701464
2025-03-20 11:47:39.662663 test begin: paddle.reshape(Tensor([2, 11641334, 2, 7, 7],"float32"), shape=list[2,192,7,7,], )

[torch error] paddle.reshape(Tensor([2, 11641334, 2, 7, 7],"float32"), shape=list[2,192,7,7,], ) 
 shape '[2, 192, 7, 7]' is invalid for input of size 2281701464
2025-03-20 11:47:42.178021 test begin: paddle.reshape(Tensor([2, 11641334, 2, 7, 7],"float32"), shape=list[2,464,7,7,], )

[torch error] paddle.reshape(Tensor([2, 11641334, 2, 7, 7],"float32"), shape=list[2,464,7,7,], ) 
 shape '[2, 464, 7, 7]' is invalid for input of size 2281701464
2025-03-20 11:47:44.475139 test begin: paddle.reshape(Tensor([2, 11641334, 2, 7, 7],"float32"), shape=list[2,704,7,7,], )

[torch error] paddle.reshape(Tensor([2, 11641334, 2, 7, 7],"float32"), shape=list[2,704,7,7,], ) 
 shape '[2, 704, 7, 7]' is invalid for input of size 2281701464
2025-03-20 11:47:46.196966 test begin: paddle.reshape(Tensor([2, 11641334, 2, 7, 7],"float32"), shape=list[2,96,7,7,], )

[torch error] paddle.reshape(Tensor([2, 11641334, 2, 7, 7],"float32"), shape=list[2,96,7,7,], ) 
 shape '[2, 96, 7, 7]' is invalid for input of size 2281701464
2025-03-20 11:47:48.698068 test begin: paddle.reshape(Tensor([2, 11641334, 2, 7, 7],"float32"), shape=list[2,976,7,7,], )

[torch error] paddle.reshape(Tensor([2, 11641334, 2, 7, 7],"float32"), shape=list[2,976,7,7,], ) 
 shape '[2, 976, 7, 7]' is invalid for input of size 2281701464
2025-03-20 11:47:50.411436 test begin: paddle.reshape(Tensor([2, 11641334, 7, 14],"float32"), tuple(2,13,4,7,-1,), )

[torch error] paddle.reshape(Tensor([2, 11641334, 7, 14],"float32"), tuple(2,13,4,7,-1,), ) 
 shape '[2, 13, 4, 7, -1]' is invalid for input of size 2281701464
2025-03-20 11:47:52.904574 test begin: paddle.reshape(Tensor([2, 12, 121264, 28, 28],"float32"), shape=list[2,24,28,28,], )

[torch error] paddle.reshape(Tensor([2, 12, 121264, 28, 28],"float32"), shape=list[2,24,28,28,], ) 
 shape '[2, 24, 28, 28]' is invalid for input of size 2281703424
2025-03-20 11:47:54.800312 test begin: paddle.reshape(Tensor([2, 12, 2, 1697695, 28],"float32"), shape=list[2,24,28,28,], )

[torch error] paddle.reshape(Tensor([2, 12, 2, 1697695, 28],"float32"), shape=list[2,24,28,28,], ) 
 shape '[2, 24, 28, 28]' is invalid for input of size 2281702080
2025-03-20 11:47:56.753893 test begin: paddle.reshape(Tensor([2, 12, 2, 28, 1697695],"float32"), shape=list[2,24,28,28,], )

[torch error] paddle.reshape(Tensor([2, 12, 2, 28, 1697695],"float32"), shape=list[2,24,28,28,], ) 
 shape '[2, 24, 28, 28]' is invalid for input of size 2281702080
2025-03-20 11:47:58.699159 test begin: paddle.reshape(Tensor([2, 120, 3169030, 3],"float32"), shape=tuple(2,-1,1,), )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 86, in func_timeout
    thread.join(timeout)
  File "/usr/lib/python3.9/threading.py", line 1064, in join
    self._wait_for_tstate_lock(timeout=max(timeout, 0))
  File "/usr/lib/python3.9/threading.py", line 1080, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt
2025-03-20 11:49:52.304140 test begin: paddle.sin(Tensor([2, 3, 380283564],"float32"), )

W0320 11:51:26.311426 140756 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0320 11:51:26.312587 140756 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.sin(Tensor([2, 3, 380283564],"float32"), )
2025-03-20 11:53:52.753264 test begin: paddle.sin(Tensor([21298, 107136, 1],"float32"), )

[Pass] paddle.sin(Tensor([21298, 107136, 1],"float32"), )
2025-03-20 11:56:54.464581 test begin: paddle.sin(Tensor([2281701379],"float32"), )

[Pass] paddle.sin(Tensor([2281701379],"float32"), )
2025-03-20 11:59:54.396167 test begin: paddle.sin(Tensor([228170138, 10],"float32"), )

[Pass] paddle.sin(Tensor([228170138, 10],"float32"), )
2025-03-20 12:02:49.177295 test begin: paddle.sin(Tensor([4, 157920, 3613],"float32"), )

[Pass] paddle.sin(Tensor([4, 157920, 3613],"float32"), )
2025-03-20 12:06:00.946468 test begin: paddle.sin(Tensor([4, 570425345, 1],"float32"), )

[Pass] paddle.sin(Tensor([4, 570425345, 1],"float32"), )
2025-03-20 12:09:16.455802 test begin: paddle.sin(Tensor([4294967297],"float16"), )

[Pass] paddle.sin(Tensor([4294967297],"float16"), )
2025-03-20 12:26:55.829071 test begin: paddle.sin(Tensor([4353, 4096, 1, 128],"float32"), )

[Pass] paddle.sin(Tensor([4353, 4096, 1, 128],"float32"), )
2025-03-20 12:29:43.550575 test begin: paddle.sin(Tensor([557057, 128, 1, 32],"float32"), )

[Pass] paddle.sin(Tensor([557057, 128, 1, 32],"float32"), )
2025-03-20 12:32:46.607099 test begin: paddle.sin(Tensor([8705, 4096, 1, 64],"float32"), )

[Pass] paddle.sin(Tensor([8705, 4096, 1, 64],"float32"), )
2025-03-20 12:35:47.976093 test begin: paddle.sin(Tensor([8912897, 256],"float32"), )

[Pass] paddle.sin(Tensor([8912897, 256],"float32"), )
2025-03-20 12:38:50.921457 test begin: paddle.sin(Tensor([91268056, 25, 1],"float32"), )

[Pass] paddle.sin(Tensor([91268056, 25, 1],"float32"), )
2025-03-20 12:41:54.304996 test begin: paddle.sin(x=Tensor([2, 2, 1073741825],"float16"), )

[Pass] paddle.sin(x=Tensor([2, 2, 1073741825],"float16"), )
2025-03-20 12:58:04.312547 test begin: paddle.sin(x=Tensor([2, 2, 1073741825],"float16"), name="test_sin", )

[Pass] paddle.sin(x=Tensor([2, 2, 1073741825],"float16"), name="test_sin", )
2025-03-20 13:14:08.662682 test begin: paddle.sin(x=Tensor([2, 715827883, 3],"float16"), )

[Pass] paddle.sin(x=Tensor([2, 715827883, 3],"float16"), )
2025-03-20 13:30:25.104821 test begin: paddle.sin(x=Tensor([2, 715827883, 3],"float16"), name="test_sin", )

[Pass] paddle.sin(x=Tensor([2, 715827883, 3],"float16"), name="test_sin", )
2025-03-20 13:46:41.387899 test begin: paddle.sin(x=Tensor([2281701379],"float32"), )

[Pass] paddle.sin(x=Tensor([2281701379],"float32"), )
2025-03-20 13:49:27.503935 test begin: paddle.sin(x=Tensor([4294967297],"float16"), )

[Pass] paddle.sin(x=Tensor([4294967297],"float16"), )
2025-03-20 14:06:27.326970 test begin: paddle.sin(x=Tensor([715827883, 2, 3],"float16"), )

[Pass] paddle.sin(x=Tensor([715827883, 2, 3],"float16"), )
2025-03-20 14:24:44.070631 test begin: paddle.sin(x=Tensor([715827883, 2, 3],"float16"), name="test_sin", )

[Pass] paddle.sin(x=Tensor([715827883, 2, 3],"float16"), name="test_sin", )
2025-03-20 14:41:36.764380 test begin: paddle.sinc(Tensor([16, 142606337],"float32"), )

CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 4.51 GiB is free. Process 62167 has 5.06 GiB memory in use. Process 18810 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 16.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-20 16:55:43.749160 test begin: paddle.squeeze(Tensor([3, 760567127],"float32"), axis=list[1,], )

W0320 16:57:09.142875 59577 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0320 16:57:09.144083 59577 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([3, 760567127],"float32"), axis=list[1,], )
2025-03-20 16:59:28.381660 test begin: paddle.squeeze(Tensor([3, 760567127],"float32"), list[-1,], )

[Pass] paddle.squeeze(Tensor([3, 760567127],"float32"), list[-1,], )
2025-03-20 17:02:24.570721 test begin: paddle.squeeze(Tensor([32, 1, 71303169],"float32"), axis=1, )

[Pass] paddle.squeeze(Tensor([32, 1, 71303169],"float32"), axis=1, )
2025-03-20 17:05:21.971151 test begin: paddle.squeeze(Tensor([32, 1, 71303169],"float32"), axis=list[1,], )

[Pass] paddle.squeeze(Tensor([32, 1, 71303169],"float32"), axis=list[1,], )
2025-03-20 17:08:02.226213 test begin: paddle.squeeze(Tensor([32, 123791, 576],"float32"), axis=1, )

[Pass] paddle.squeeze(Tensor([32, 123791, 576],"float32"), axis=1, )
2025-03-20 17:11:01.912447 test begin: paddle.squeeze(Tensor([32, 448448, 159],"float32"), axis=list[1,], )

[Pass] paddle.squeeze(Tensor([32, 448448, 159],"float32"), axis=list[1,], )
2025-03-20 17:14:00.326328 test begin: paddle.squeeze(Tensor([325957340, 7, 1],"float32"), -1, )

[Pass] paddle.squeeze(Tensor([325957340, 7, 1],"float32"), -1, )
2025-03-20 17:16:59.729113 test begin: paddle.squeeze(Tensor([325957340, 7],"float32"), axis=list[0,], )

[Pass] paddle.squeeze(Tensor([325957340, 7],"float32"), axis=list[0,], )
2025-03-20 17:19:42.409734 test begin: paddle.squeeze(Tensor([3355444, 1280, 1, 1],"float16"), axis=list[2,3,], )

[Pass] paddle.squeeze(Tensor([3355444, 1280, 1, 1],"float16"), axis=list[2,3,], )
2025-03-20 17:36:56.821250 test begin: paddle.squeeze(Tensor([357913942, 3, 4],"float16"), axis=0, )

[Pass] paddle.squeeze(Tensor([357913942, 3, 4],"float16"), axis=0, )
2025-03-20 17:52:47.261709 test begin: paddle.squeeze(Tensor([380283564, 6],"float32"), axis=list[0,], )

[Pass] paddle.squeeze(Tensor([380283564, 6],"float32"), axis=list[0,], )
2025-03-20 17:55:29.959562 test begin: paddle.squeeze(Tensor([3961288, 1, 576],"float32"), axis=1, )

[Pass] paddle.squeeze(Tensor([3961288, 1, 576],"float32"), axis=1, )
2025-03-20 17:58:20.920972 test begin: paddle.squeeze(Tensor([4, 1, 1073741825],"float16"), axis=1, )

[Pass] paddle.squeeze(Tensor([4, 1, 1073741825],"float16"), axis=1, )
2025-03-20 18:14:27.949816 test begin: paddle.squeeze(Tensor([4, 1, 570425345],"float32"), axis=1, )

[Pass] paddle.squeeze(Tensor([4, 1, 570425345],"float32"), axis=1, )
2025-03-20 18:17:51.378429 test begin: paddle.squeeze(Tensor([4, 100, 5704254],"float32"), -1, )

[Pass] paddle.squeeze(Tensor([4, 100, 5704254],"float32"), -1, )
2025-03-20 18:20:41.954719 test begin: paddle.squeeze(Tensor([4, 1073741825, 1],"float16"), axis=2, )

[Pass] paddle.squeeze(Tensor([4, 1073741825, 1],"float16"), axis=2, )
2025-03-20 18:37:04.066889 test begin: paddle.squeeze(Tensor([4, 1073741825],"float16"), axis=1, )

[Pass] paddle.squeeze(Tensor([4, 1073741825],"float16"), axis=1, )
2025-03-20 18:53:20.466569 test begin: paddle.squeeze(Tensor([4, 1114113, 512],"float32"), axis=1, )

[Pass] paddle.squeeze(Tensor([4, 1114113, 512],"float32"), axis=1, )
2025-03-20 18:56:08.052474 test begin: paddle.squeeze(Tensor([4, 125, 125, 68720],"float16"), -1, )

[Pass] paddle.squeeze(Tensor([4, 125, 125, 68720],"float16"), -1, )
2025-03-20 19:13:35.103418 test begin: paddle.squeeze(Tensor([4, 125, 8589935, 1],"float16"), -1, )

[Pass] paddle.squeeze(Tensor([4, 125, 8589935, 1],"float16"), -1, )
2025-03-20 19:30:10.882366 test begin: paddle.squeeze(Tensor([4, 178956971, 6],"float16"), axis=1, )

[Pass] paddle.squeeze(Tensor([4, 178956971, 6],"float16"), axis=1, )
2025-03-20 19:47:20.976924 test begin: paddle.squeeze(Tensor([4, 2097153, 512, 1],"float16"), -1, )

[Pass] paddle.squeeze(Tensor([4, 2097153, 512, 1],"float16"), -1, )
2025-03-20 20:03:49.071946 test begin: paddle.squeeze(Tensor([4, 512, 2097153, 1],"float16"), -1, )

[Pass] paddle.squeeze(Tensor([4, 512, 2097153, 1],"float16"), -1, )
2025-03-20 20:20:30.563350 test begin: paddle.squeeze(Tensor([4, 512, 512, 4097],"float16"), -1, )

[Pass] paddle.squeeze(Tensor([4, 512, 512, 4097],"float16"), -1, )
2025-03-20 20:37:08.503958 test begin: paddle.squeeze(Tensor([4, 570425345, 1],"float32"), -1, )

[Pass] paddle.squeeze(Tensor([4, 570425345, 1],"float32"), -1, )
2025-03-20 20:40:09.039366 test begin: paddle.squeeze(Tensor([4, 570425345],"float32"), axis=1, )

[Pass] paddle.squeeze(Tensor([4, 570425345],"float32"), axis=1, )
2025-03-20 20:42:58.395275 test begin: paddle.squeeze(Tensor([4, 570425345],"float32"), axis=list[1,], )

[Pass] paddle.squeeze(Tensor([4, 570425345],"float32"), axis=list[1,], )
2025-03-20 20:46:02.391495 test begin: paddle.squeeze(Tensor([4, 7, 153391690],"float16"), axis=2, )

[Pass] paddle.squeeze(Tensor([4, 7, 153391690],"float16"), axis=2, )
2025-03-20 21:02:33.598496 test begin: paddle.squeeze(Tensor([4, 8589935, 125, 1],"float16"), -1, )

[Pass] paddle.squeeze(Tensor([4, 8589935, 125, 1],"float16"), -1, )
2025-03-20 21:18:52.355287 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-20 21:35:14.570507 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0320 21:36:56.029948 92048 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0320 21:36:56.031160 92048 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-20 21:52:43.570497 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0320 21:54:39.317293 11606 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0320 21:54:39.319177 11606 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-20 22:10:27.894349 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0320 22:12:14.008548 96577 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0320 22:12:14.009865 96577 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-20 22:28:13.522494 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0320 22:29:53.934465 17704 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0320 22:29:53.935617 17704 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-20 22:45:37.794408 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0320 22:47:27.506064 114128 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0320 22:47:27.508164 114128 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-20 23:03:27.651311 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0320 23:05:08.476168 54057 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0320 23:05:08.477639 54057 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-20 23:20:54.784092 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0320 23:22:37.299556 157158 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0320 23:22:37.300688 157158 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-20 23:38:08.012057 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0320 23:40:07.938916 112439 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0320 23:40:07.940945 112439 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-20 23:56:02.946716 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0320 23:57:45.719301 72103 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0320 23:57:45.720443 72103 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 00:13:30.805174 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 00:15:13.520900 29859 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 00:15:13.522058 29859 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 00:30:57.390448 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 00:32:40.565865 150912 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 00:32:40.566972 150912 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 00:48:24.539654 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 00:50:07.989168 108387 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 00:50:07.990351 108387 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 01:06:05.445124 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 01:07:44.514351 67501 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 01:07:44.515480 67501 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 01:23:33.454418 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 01:25:16.598039 24995 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 01:25:16.599459 24995 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 01:41:04.675498 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 01:42:49.163691 140458 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 01:42:49.164894 140458 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 01:58:40.467331 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 02:00:23.801693 91471 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 02:00:23.802872 91471 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 02:16:08.744240 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 02:17:51.993747 41830 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 02:17:51.995755 41830 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 02:33:41.087769 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 02:35:21.823571 155352 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 02:35:21.825009 155352 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 02:51:12.567504 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 02:52:56.738881 105161 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 02:52:56.740355 105161 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 03:08:50.399485 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 03:10:26.546862 56639 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 03:10:26.547997 56639 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 03:25:59.608280 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 03:27:41.709420  4992 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 03:27:41.710606  4992 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 03:43:23.331454 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 03:45:03.794399 118405 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 03:45:03.795598 118405 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 04:00:39.784902 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 04:02:39.289999 67620 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 04:02:39.291697 67620 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 04:18:37.881059 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 04:20:17.296095 21174 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 04:20:17.297904 21174 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 04:35:47.975792 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 04:37:31.602276 133209 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 04:37:31.603385 133209 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 04:53:15.208173 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 04:54:58.439306 75793 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 04:54:58.440606 75793 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 05:11:06.601882 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 05:13:05.779059 10481 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 05:13:05.780273 10481 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 05:28:56.424725 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 05:30:38.053252 108057 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 05:30:38.054354 108057 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 05:46:22.834625 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 05:48:04.686803 40479 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 05:48:04.687884 40479 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 06:03:42.481175 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 06:05:24.938584 135450 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 06:05:24.939709 135450 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 06:21:04.244145 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 06:22:44.176626 67271 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 06:22:44.177776 67271 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 06:38:24.759054 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 06:40:06.498941 162567 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 06:40:06.500011 162567 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 06:56:08.945783 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 06:57:51.794178 96266 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 06:57:51.795337 96266 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 07:13:33.484845 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 07:15:16.605921  1799 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 07:15:16.607913  1799 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 07:30:58.547210 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 07:32:42.119758 60450 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 07:32:42.121605 60450 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 07:48:29.127316 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 07:50:26.248926 118835 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 07:50:26.250622 118835 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 08:06:12.190911 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 08:07:52.824632 14450 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 08:07:52.825757 14450 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 08:23:21.404828 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 08:25:03.047576 55102 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 08:25:03.048952 55102 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 08:40:37.996867 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 08:42:16.437641 78452 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 08:42:16.438835 78452 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 08:57:35.356003 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 08:59:16.637203 98034 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 08:59:16.638430 98034 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 09:14:46.680542 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 09:16:26.173436 117999 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 09:16:26.174597 117999 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
2025-03-21 09:31:49.689436 test begin: paddle.squeeze(Tensor([4294967297, 1],"float16"), 1, )
W0321 09:33:28.672770 126353 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0321 09:33:28.673866 126353 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 86, in func_timeout
    thread.join(timeout)
  File "/usr/lib/python3.9/threading.py", line 1064, in join
    self._wait_for_tstate_lock(timeout=max(timeout, 0))
  File "/usr/lib/python3.9/threading.py", line 1080, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt
