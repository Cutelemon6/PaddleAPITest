2025-03-11 18:25:01.352432 test begin: paddle.broadcast_to(Tensor([4, 4563403, 5, 25],"bool"), list[4,8,5,25,], )

W0311 18:26:01.423602 95133 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 18:26:01.424671 95133 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.broadcast_to(Tensor([4, 4563403, 5, 25],"bool"), list[4,8,5,25,], ) 
 The expanded size of the tensor (8) must match the existing size (4563403) at non-singleton dimension 1.  Target sizes: [4, 8, 5, 25].  Tensor sizes: [4, 4563403, 5, 25]
2025-03-11 18:26:03.378582 test begin: paddle.broadcast_to(Tensor([4, 45730, 162, 77],"bool"), list[4,8,162,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 45730, 162, 77],"bool"), list[4,8,162,77,], ) 
 The expanded size of the tensor (8) must match the existing size (45730) at non-singleton dimension 1.  Target sizes: [4, 8, 162, 77].  Tensor sizes: [4, 45730, 162, 77]
2025-03-11 18:26:03.801298 test begin: paddle.broadcast_to(Tensor([4, 46014, 161, 77],"bool"), list[4,8,161,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 46014, 161, 77],"bool"), list[4,8,161,77,], ) 
 The expanded size of the tensor (8) must match the existing size (46014) at non-singleton dimension 1.  Target sizes: [4, 8, 161, 77].  Tensor sizes: [4, 46014, 161, 77]
2025-03-11 18:26:04.069844 test begin: paddle.broadcast_to(Tensor([4, 46298, 111, 111],"bool"), list[4,8,111,111,], )

[torch error] paddle.broadcast_to(Tensor([4, 46298, 111, 111],"bool"), list[4,8,111,111,], ) 
 The expanded size of the tensor (8) must match the existing size (46298) at non-singleton dimension 1.  Target sizes: [4, 8, 111, 111].  Tensor sizes: [4, 46298, 111, 111]
2025-03-11 18:26:04.672157 test begin: paddle.broadcast_to(Tensor([4, 463008, 16, 77],"bool"), list[4,8,16,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 463008, 16, 77],"bool"), list[4,8,16,77,], ) 
 The expanded size of the tensor (8) must match the existing size (463008) at non-singleton dimension 1.  Target sizes: [4, 8, 16, 77].  Tensor sizes: [4, 463008, 16, 77]
2025-03-11 18:26:05.333610 test begin: paddle.broadcast_to(Tensor([4, 46301, 160, 77],"bool"), list[4,8,160,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 46301, 160, 77],"bool"), list[4,8,160,77,], ) 
 The expanded size of the tensor (8) must match the existing size (46301) at non-singleton dimension 1.  Target sizes: [4, 8, 160, 77].  Tensor sizes: [4, 46301, 160, 77]
2025-03-11 18:26:05.752661 test begin: paddle.broadcast_to(Tensor([4, 465654, 35, 35],"bool"), list[4,8,35,35,], )

[torch error] paddle.broadcast_to(Tensor([4, 465654, 35, 35],"bool"), list[4,8,35,35,], ) 
 The expanded size of the tensor (8) must match the existing size (465654) at non-singleton dimension 1.  Target sizes: [4, 8, 35, 35].  Tensor sizes: [4, 465654, 35, 35]
2025-03-11 18:26:06.120288 test begin: paddle.broadcast_to(Tensor([4, 46592, 159, 77],"bool"), list[4,8,159,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 46592, 159, 77],"bool"), list[4,8,159,77,], ) 
 The expanded size of the tensor (8) must match the existing size (46592) at non-singleton dimension 1.  Target sizes: [4, 8, 159, 77].  Tensor sizes: [4, 46592, 159, 77]
2025-03-11 18:26:06.660113 test begin: paddle.broadcast_to(Tensor([4, 467179, 33, 37],"bool"), list[4,8,33,37,], )

[torch error] paddle.broadcast_to(Tensor([4, 467179, 33, 37],"bool"), list[4,8,33,37,], ) 
 The expanded size of the tensor (8) must match the existing size (467179) at non-singleton dimension 1.  Target sizes: [4, 8, 33, 37].  Tensor sizes: [4, 467179, 33, 37]
2025-03-11 18:26:07.369001 test begin: paddle.broadcast_to(Tensor([4, 4675618, 2, 61],"bool"), list[4,8,2,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 4675618, 2, 61],"bool"), list[4,8,2,61,], ) 
 The expanded size of the tensor (8) must match the existing size (4675618) at non-singleton dimension 1.  Target sizes: [4, 8, 2, 61].  Tensor sizes: [4, 4675618, 2, 61]
2025-03-11 18:26:07.795419 test begin: paddle.broadcast_to(Tensor([4, 467562, 20, 61],"bool"), list[4,8,20,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 467562, 20, 61],"bool"), list[4,8,20,61,], ) 
 The expanded size of the tensor (8) must match the existing size (467562) at non-singleton dimension 1.  Target sizes: [4, 8, 20, 61].  Tensor sizes: [4, 467562, 20, 61]
2025-03-11 18:26:08.371608 test begin: paddle.broadcast_to(Tensor([4, 46887, 158, 77],"bool"), list[4,8,158,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 46887, 158, 77],"bool"), list[4,8,158,77,], ) 
 The expanded size of the tensor (8) must match the existing size (46887) at non-singleton dimension 1.  Target sizes: [4, 8, 158, 77].  Tensor sizes: [4, 46887, 158, 77]
2025-03-11 18:26:08.881758 test begin: paddle.broadcast_to(Tensor([4, 4714260, 11, 11],"bool"), list[4,8,11,11,], )

[torch error] paddle.broadcast_to(Tensor([4, 4714260, 11, 11],"bool"), list[4,8,11,11,], ) 
 The expanded size of the tensor (8) must match the existing size (4714260) at non-singleton dimension 1.  Target sizes: [4, 8, 11, 11].  Tensor sizes: [4, 4714260, 11, 11]
2025-03-11 18:26:09.418061 test begin: paddle.broadcast_to(Tensor([4, 47143, 110, 110],"bool"), list[4,8,110,110,], )

[torch error] paddle.broadcast_to(Tensor([4, 47143, 110, 110],"bool"), list[4,8,110,110,], ) 
 The expanded size of the tensor (8) must match the existing size (47143) at non-singleton dimension 1.  Target sizes: [4, 8, 110, 110].  Tensor sizes: [4, 47143, 110, 110]
2025-03-11 18:26:10.030278 test begin: paddle.broadcast_to(Tensor([4, 47186, 157, 77],"bool"), list[4,8,157,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 47186, 157, 77],"bool"), list[4,8,157,77,], ) 
 The expanded size of the tensor (8) must match the existing size (47186) at non-singleton dimension 1.  Target sizes: [4, 8, 157, 77].  Tensor sizes: [4, 47186, 157, 77]
2025-03-11 18:26:10.693982 test begin: paddle.broadcast_to(Tensor([4, 47488, 156, 77],"bool"), list[4,8,156,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 47488, 156, 77],"bool"), list[4,8,156,77,], ) 
 The expanded size of the tensor (8) must match the existing size (47488) at non-singleton dimension 1.  Target sizes: [4, 8, 156, 77].  Tensor sizes: [4, 47488, 156, 77]
2025-03-11 18:26:11.228683 test begin: paddle.broadcast_to(Tensor([4, 47795, 155, 77],"bool"), list[4,8,155,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 47795, 155, 77],"bool"), list[4,8,155,77,], ) 
 The expanded size of the tensor (8) must match the existing size (47795) at non-singleton dimension 1.  Target sizes: [4, 8, 155, 77].  Tensor sizes: [4, 47795, 155, 77]
2025-03-11 18:26:11.778569 test begin: paddle.broadcast_to(Tensor([4, 48012, 109, 109],"bool"), list[4,8,109,109,], )

[torch error] paddle.broadcast_to(Tensor([4, 48012, 109, 109],"bool"), list[4,8,109,109,], ) 
 The expanded size of the tensor (8) must match the existing size (48012) at non-singleton dimension 1.  Target sizes: [4, 8, 109, 109].  Tensor sizes: [4, 48012, 109, 109]
2025-03-11 18:26:12.318142 test begin: paddle.broadcast_to(Tensor([4, 48105, 154, 77],"bool"), list[4,8,154,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 48105, 154, 77],"bool"), list[4,8,154,77,], ) 
 The expanded size of the tensor (8) must match the existing size (48105) at non-singleton dimension 1.  Target sizes: [4, 8, 154, 77].  Tensor sizes: [4, 48105, 154, 77]
2025-03-11 18:26:13.025655 test begin: paddle.broadcast_to(Tensor([4, 481779, 32, 37],"bool"), list[4,8,32,37,], )

[torch error] paddle.broadcast_to(Tensor([4, 481779, 32, 37],"bool"), list[4,8,32,37,], ) 
 The expanded size of the tensor (8) must match the existing size (481779) at non-singleton dimension 1.  Target sizes: [4, 8, 32, 37].  Tensor sizes: [4, 481779, 32, 37]
2025-03-11 18:26:13.719359 test begin: paddle.broadcast_to(Tensor([4, 48420, 153, 77],"bool"), list[4,8,153,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 48420, 153, 77],"bool"), list[4,8,153,77,], ) 
 The expanded size of the tensor (8) must match the existing size (48420) at non-singleton dimension 1.  Target sizes: [4, 8, 153, 77].  Tensor sizes: [4, 48420, 153, 77]
2025-03-11 18:26:14.142323 test begin: paddle.broadcast_to(Tensor([4, 48738, 152, 77],"bool"), list[4,8,152,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 48738, 152, 77],"bool"), list[4,8,152,77,], ) 
 The expanded size of the tensor (8) must match the existing size (48738) at non-singleton dimension 1.  Target sizes: [4, 8, 152, 77].  Tensor sizes: [4, 48738, 152, 77]
2025-03-11 18:26:14.729470 test begin: paddle.broadcast_to(Tensor([4, 4875431, 9, 13],"bool"), list[4,8,9,13,], )

[torch error] paddle.broadcast_to(Tensor([4, 4875431, 9, 13],"bool"), list[4,8,9,13,], ) 
 The expanded size of the tensor (8) must match the existing size (4875431) at non-singleton dimension 1.  Target sizes: [4, 8, 9, 13].  Tensor sizes: [4, 4875431, 9, 13]
2025-03-11 18:26:15.400243 test begin: paddle.broadcast_to(Tensor([4, 48905, 108, 108],"bool"), list[4,8,108,108,], )

[torch error] paddle.broadcast_to(Tensor([4, 48905, 108, 108],"bool"), list[4,8,108,108,], ) 
 The expanded size of the tensor (8) must match the existing size (48905) at non-singleton dimension 1.  Target sizes: [4, 8, 108, 108].  Tensor sizes: [4, 48905, 108, 108]
2025-03-11 18:26:15.819990 test begin: paddle.broadcast_to(Tensor([4, 49061, 151, 77],"bool"), list[4,8,151,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 49061, 151, 77],"bool"), list[4,8,151,77,], ) 
 The expanded size of the tensor (8) must match the existing size (49061) at non-singleton dimension 1.  Target sizes: [4, 8, 151, 77].  Tensor sizes: [4, 49061, 151, 77]
2025-03-11 18:26:16.293115 test begin: paddle.broadcast_to(Tensor([4, 492171, 19, 61],"bool"), list[4,8,19,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 492171, 19, 61],"bool"), list[4,8,19,61,], ) 
 The expanded size of the tensor (8) must match the existing size (492171) at non-singleton dimension 1.  Target sizes: [4, 8, 19, 61].  Tensor sizes: [4, 492171, 19, 61]
2025-03-11 18:26:16.870559 test begin: paddle.broadcast_to(Tensor([4, 493448, 34, 34],"bool"), list[4,8,34,34,], )

[torch error] paddle.broadcast_to(Tensor([4, 493448, 34, 34],"bool"), list[4,8,34,34,], ) 
 The expanded size of the tensor (8) must match the existing size (493448) at non-singleton dimension 1.  Target sizes: [4, 8, 34, 34].  Tensor sizes: [4, 493448, 34, 34]
2025-03-11 18:26:17.387909 test begin: paddle.broadcast_to(Tensor([4, 493875, 15, 77],"bool"), list[4,8,15,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 493875, 15, 77],"bool"), list[4,8,15,77,], ) 
 The expanded size of the tensor (8) must match the existing size (493875) at non-singleton dimension 1.  Target sizes: [4, 8, 15, 77].  Tensor sizes: [4, 493875, 15, 77]
2025-03-11 18:26:18.070898 test begin: paddle.broadcast_to(Tensor([4, 49388, 150, 77],"bool"), list[4,8,150,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 49388, 150, 77],"bool"), list[4,8,150,77,], ) 
 The expanded size of the tensor (8) must match the existing size (49388) at non-singleton dimension 1.  Target sizes: [4, 8, 150, 77].  Tensor sizes: [4, 49388, 150, 77]
2025-03-11 18:26:18.499059 test begin: paddle.broadcast_to(Tensor([4, 49719, 149, 77],"bool"), list[4,8,149,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 49719, 149, 77],"bool"), list[4,8,149,77,], ) 
 The expanded size of the tensor (8) must match the existing size (49719) at non-singleton dimension 1.  Target sizes: [4, 8, 149, 77].  Tensor sizes: [4, 49719, 149, 77]
2025-03-11 18:26:19.066742 test begin: paddle.broadcast_to(Tensor([4, 497320, 31, 37],"bool"), list[4,8,31,37,], )

[torch error] paddle.broadcast_to(Tensor([4, 497320, 31, 37],"bool"), list[4,8,31,37,], ) 
 The expanded size of the tensor (8) must match the existing size (497320) at non-singleton dimension 1.  Target sizes: [4, 8, 31, 37].  Tensor sizes: [4, 497320, 31, 37]
2025-03-11 18:26:19.632533 test begin: paddle.broadcast_to(Tensor([4, 49824, 107, 107],"bool"), list[4,8,107,107,], )

[torch error] paddle.broadcast_to(Tensor([4, 49824, 107, 107],"bool"), list[4,8,107,107,], ) 
 The expanded size of the tensor (8) must match the existing size (49824) at non-singleton dimension 1.  Target sizes: [4, 8, 107, 107].  Tensor sizes: [4, 49824, 107, 107]
2025-03-11 18:26:20.293448 test begin: paddle.broadcast_to(Tensor([4, 50055, 148, 77],"bool"), list[4,8,148,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 50055, 148, 77],"bool"), list[4,8,148,77,], ) 
 The expanded size of the tensor (8) must match the existing size (50055) at non-singleton dimension 1.  Target sizes: [4, 8, 148, 77].  Tensor sizes: [4, 50055, 148, 77]
2025-03-11 18:26:20.959700 test begin: paddle.broadcast_to(Tensor([4, 50396, 147, 77],"bool"), list[4,8,147,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 50396, 147, 77],"bool"), list[4,8,147,77,], ) 
 The expanded size of the tensor (8) must match the existing size (50396) at non-singleton dimension 1.  Target sizes: [4, 8, 147, 77].  Tensor sizes: [4, 50396, 147, 77]
2025-03-11 18:26:21.636799 test begin: paddle.broadcast_to(Tensor([4, 50741, 146, 77],"bool"), list[4,8,146,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 50741, 146, 77],"bool"), list[4,8,146,77,], ) 
 The expanded size of the tensor (8) must match the existing size (50741) at non-singleton dimension 1.  Target sizes: [4, 8, 146, 77].  Tensor sizes: [4, 50741, 146, 77]
2025-03-11 18:26:22.059129 test begin: paddle.broadcast_to(Tensor([4, 50768, 106, 106],"bool"), list[4,8,106,106,], )

[torch error] paddle.broadcast_to(Tensor([4, 50768, 106, 106],"bool"), list[4,8,106,106,], ) 
 The expanded size of the tensor (8) must match the existing size (50768) at non-singleton dimension 1.  Target sizes: [4, 8, 106, 106].  Tensor sizes: [4, 50768, 106, 106]
2025-03-11 18:26:22.645328 test begin: paddle.broadcast_to(Tensor([4, 51091, 145, 77],"bool"), list[4,8,145,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 51091, 145, 77],"bool"), list[4,8,145,77,], ) 
 The expanded size of the tensor (8) must match the existing size (51091) at non-singleton dimension 1.  Target sizes: [4, 8, 145, 77].  Tensor sizes: [4, 51091, 145, 77]
2025-03-11 18:26:23.155624 test begin: paddle.broadcast_to(Tensor([4, 5138968, 3, 37],"bool"), list[4,8,3,37,], )

[torch error] paddle.broadcast_to(Tensor([4, 5138968, 3, 37],"bool"), list[4,8,3,37,], ) 
 The expanded size of the tensor (8) must match the existing size (5138968) at non-singleton dimension 1.  Target sizes: [4, 8, 3, 37].  Tensor sizes: [4, 5138968, 3, 37]
2025-03-11 18:26:23.649677 test begin: paddle.broadcast_to(Tensor([4, 513897, 30, 37],"bool"), list[4,8,30,37,], )

[torch error] paddle.broadcast_to(Tensor([4, 513897, 30, 37],"bool"), list[4,8,30,37,], ) 
 The expanded size of the tensor (8) must match the existing size (513897) at non-singleton dimension 1.  Target sizes: [4, 8, 30, 37].  Tensor sizes: [4, 513897, 30, 37]
2025-03-11 18:26:24.231532 test begin: paddle.broadcast_to(Tensor([4, 51446, 144, 77],"bool"), list[4,8,144,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 51446, 144, 77],"bool"), list[4,8,144,77,], ) 
 The expanded size of the tensor (8) must match the existing size (51446) at non-singleton dimension 1.  Target sizes: [4, 8, 144, 77].  Tensor sizes: [4, 51446, 144, 77]
2025-03-11 18:26:24.873691 test begin: paddle.broadcast_to(Tensor([4, 51740, 105, 105],"bool"), list[4,8,105,105,], )

[torch error] paddle.broadcast_to(Tensor([4, 51740, 105, 105],"bool"), list[4,8,105,105,], ) 
 The expanded size of the tensor (8) must match the existing size (51740) at non-singleton dimension 1.  Target sizes: [4, 8, 105, 105].  Tensor sizes: [4, 51740, 105, 105]
2025-03-11 18:26:25.449129 test begin: paddle.broadcast_to(Tensor([4, 51806, 143, 77],"bool"), list[4,8,143,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 51806, 143, 77],"bool"), list[4,8,143,77,], ) 
 The expanded size of the tensor (8) must match the existing size (51806) at non-singleton dimension 1.  Target sizes: [4, 8, 143, 77].  Tensor sizes: [4, 51806, 143, 77]
2025-03-11 18:26:25.867423 test begin: paddle.broadcast_to(Tensor([4, 519514, 18, 61],"bool"), list[4,8,18,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 519514, 18, 61],"bool"), list[4,8,18,61,], ) 
 The expanded size of the tensor (8) must match the existing size (519514) at non-singleton dimension 1.  Target sizes: [4, 8, 18, 61].  Tensor sizes: [4, 519514, 18, 61]
2025-03-11 18:26:26.188494 test begin: paddle.broadcast_to(Tensor([4, 52170, 142, 77],"bool"), list[4,8,142,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 52170, 142, 77],"bool"), list[4,8,142,77,], ) 
 The expanded size of the tensor (8) must match the existing size (52170) at non-singleton dimension 1.  Target sizes: [4, 8, 142, 77].  Tensor sizes: [4, 52170, 142, 77]
2025-03-11 18:26:26.670105 test begin: paddle.broadcast_to(Tensor([4, 523807, 33, 33],"bool"), list[4,8,33,33,], )

[torch error] paddle.broadcast_to(Tensor([4, 523807, 33, 33],"bool"), list[4,8,33,33,], ) 
 The expanded size of the tensor (8) must match the existing size (523807) at non-singleton dimension 1.  Target sizes: [4, 8, 33, 33].  Tensor sizes: [4, 523807, 33, 33]
2025-03-11 18:26:27.097205 test begin: paddle.broadcast_to(Tensor([4, 52540, 141, 77],"bool"), list[4,8,141,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 52540, 141, 77],"bool"), list[4,8,141,77,], ) 
 The expanded size of the tensor (8) must match the existing size (52540) at non-singleton dimension 1.  Target sizes: [4, 8, 141, 77].  Tensor sizes: [4, 52540, 141, 77]
2025-03-11 18:26:27.670232 test begin: paddle.broadcast_to(Tensor([4, 52740, 104, 104],"bool"), list[4,8,104,104,], )

[torch error] paddle.broadcast_to(Tensor([4, 52740, 104, 104],"bool"), list[4,8,104,104,], ) 
 The expanded size of the tensor (8) must match the existing size (52740) at non-singleton dimension 1.  Target sizes: [4, 8, 104, 104].  Tensor sizes: [4, 52740, 104, 104]
2025-03-11 18:26:28.104845 test begin: paddle.broadcast_to(Tensor([4, 529152, 14, 77],"bool"), list[4,8,14,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 529152, 14, 77],"bool"), list[4,8,14,77,], ) 
 The expanded size of the tensor (8) must match the existing size (529152) at non-singleton dimension 1.  Target sizes: [4, 8, 14, 77].  Tensor sizes: [4, 529152, 14, 77]
2025-03-11 18:26:28.691617 test begin: paddle.broadcast_to(Tensor([4, 52916, 140, 77],"bool"), list[4,8,140,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 52916, 140, 77],"bool"), list[4,8,140,77,], ) 
 The expanded size of the tensor (8) must match the existing size (52916) at non-singleton dimension 1.  Target sizes: [4, 8, 140, 77].  Tensor sizes: [4, 52916, 140, 77]
2025-03-11 18:26:29.359642 test begin: paddle.broadcast_to(Tensor([4, 531618, 29, 37],"bool"), list[4,8,29,37,], )

[torch error] paddle.broadcast_to(Tensor([4, 531618, 29, 37],"bool"), list[4,8,29,37,], ) 
 The expanded size of the tensor (8) must match the existing size (531618) at non-singleton dimension 1.  Target sizes: [4, 8, 29, 37].  Tensor sizes: [4, 531618, 29, 37]
2025-03-11 18:26:30.034489 test begin: paddle.broadcast_to(Tensor([4, 53296, 139, 77],"bool"), list[4,8,139,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 53296, 139, 77],"bool"), list[4,8,139,77,], ) 
 The expanded size of the tensor (8) must match the existing size (53296) at non-singleton dimension 1.  Target sizes: [4, 8, 139, 77].  Tensor sizes: [4, 53296, 139, 77]
2025-03-11 18:26:30.720974 test begin: paddle.broadcast_to(Tensor([4, 53683, 138, 77],"bool"), list[4,8,138,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 53683, 138, 77],"bool"), list[4,8,138,77,], ) 
 The expanded size of the tensor (8) must match the existing size (53683) at non-singleton dimension 1.  Target sizes: [4, 8, 138, 77].  Tensor sizes: [4, 53683, 138, 77]
2025-03-11 18:26:31.318335 test begin: paddle.broadcast_to(Tensor([4, 53769, 103, 103],"bool"), list[4,8,103,103,], )

[torch error] paddle.broadcast_to(Tensor([4, 53769, 103, 103],"bool"), list[4,8,103,103,], ) 
 The expanded size of the tensor (8) must match the existing size (53769) at non-singleton dimension 1.  Target sizes: [4, 8, 103, 103].  Tensor sizes: [4, 53769, 103, 103]
2025-03-11 18:26:31.747953 test begin: paddle.broadcast_to(Tensor([4, 54074, 137, 77],"bool"), list[4,8,137,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 54074, 137, 77],"bool"), list[4,8,137,77,], ) 
 The expanded size of the tensor (8) must match the existing size (54074) at non-singleton dimension 1.  Target sizes: [4, 8, 137, 77].  Tensor sizes: [4, 54074, 137, 77]
2025-03-11 18:26:32.267342 test begin: paddle.broadcast_to(Tensor([4, 54472, 136, 77],"bool"), list[4,8,136,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 54472, 136, 77],"bool"), list[4,8,136,77,], ) 
 The expanded size of the tensor (8) must match the existing size (54472) at non-singleton dimension 1.  Target sizes: [4, 8, 136, 77].  Tensor sizes: [4, 54472, 136, 77]
2025-03-11 18:26:32.938117 test begin: paddle.broadcast_to(Tensor([4, 54828, 102, 102],"bool"), list[4,8,102,102,], )

[torch error] paddle.broadcast_to(Tensor([4, 54828, 102, 102],"bool"), list[4,8,102,102,], ) 
 The expanded size of the tensor (8) must match the existing size (54828) at non-singleton dimension 1.  Target sizes: [4, 8, 102, 102].  Tensor sizes: [4, 54828, 102, 102]
2025-03-11 18:26:33.673144 test begin: paddle.broadcast_to(Tensor([4, 5484860, 8, 13],"bool"), list[4,8,8,13,], )

[torch error] paddle.broadcast_to(Tensor([4, 5484860, 8, 13],"bool"), list[4,8,8,13,], ) 
 The expanded size of the tensor (8) must match the existing size (5484860) at non-singleton dimension 1.  Target sizes: [4, 8, 8, 13].  Tensor sizes: [4, 5484860, 8, 13]
2025-03-11 18:26:34.398197 test begin: paddle.broadcast_to(Tensor([4, 54875, 135, 77],"bool"), list[4,8,135,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 54875, 135, 77],"bool"), list[4,8,135,77,], ) 
 The expanded size of the tensor (8) must match the existing size (54875) at non-singleton dimension 1.  Target sizes: [4, 8, 135, 77].  Tensor sizes: [4, 54875, 135, 77]
2025-03-11 18:26:35.041559 test begin: paddle.broadcast_to(Tensor([4, 550073, 17, 61],"bool"), list[4,8,17,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 550073, 17, 61],"bool"), list[4,8,17,61,], ) 
 The expanded size of the tensor (8) must match the existing size (550073) at non-singleton dimension 1.  Target sizes: [4, 8, 17, 61].  Tensor sizes: [4, 550073, 17, 61]
2025-03-11 18:26:35.555987 test begin: paddle.broadcast_to(Tensor([4, 550604, 28, 37],"bool"), list[4,8,28,37,], )

[torch error] paddle.broadcast_to(Tensor([4, 550604, 28, 37],"bool"), list[4,8,28,37,], ) 
 The expanded size of the tensor (8) must match the existing size (550604) at non-singleton dimension 1.  Target sizes: [4, 8, 28, 37].  Tensor sizes: [4, 550604, 28, 37]
2025-03-11 18:26:36.225672 test begin: paddle.broadcast_to(Tensor([4, 55285, 134, 77],"bool"), list[4,8,134,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 55285, 134, 77],"bool"), list[4,8,134,77,], ) 
 The expanded size of the tensor (8) must match the existing size (55285) at non-singleton dimension 1.  Target sizes: [4, 8, 134, 77].  Tensor sizes: [4, 55285, 134, 77]
2025-03-11 18:26:36.947361 test begin: paddle.broadcast_to(Tensor([4, 55701, 133, 77],"bool"), list[4,8,133,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 55701, 133, 77],"bool"), list[4,8,133,77,], ) 
 The expanded size of the tensor (8) must match the existing size (55701) at non-singleton dimension 1.  Target sizes: [4, 8, 133, 77].  Tensor sizes: [4, 55701, 133, 77]
2025-03-11 18:26:37.386400 test begin: paddle.broadcast_to(Tensor([4, 557057, 32, 32],"bool"), list[4,8,32,32,], )

[torch error] paddle.broadcast_to(Tensor([4, 557057, 32, 32],"bool"), list[4,8,32,32,], ) 
 The expanded size of the tensor (8) must match the existing size (557057) at non-singleton dimension 1.  Target sizes: [4, 8, 32, 32].  Tensor sizes: [4, 557057, 32, 32]
2025-03-11 18:26:37.720762 test begin: paddle.broadcast_to(Tensor([4, 55919, 101, 101],"bool"), list[4,8,101,101,], )

[torch error] paddle.broadcast_to(Tensor([4, 55919, 101, 101],"bool"), list[4,8,101,101,], ) 
 The expanded size of the tensor (8) must match the existing size (55919) at non-singleton dimension 1.  Target sizes: [4, 8, 101, 101].  Tensor sizes: [4, 55919, 101, 101]
2025-03-11 18:26:38.060706 test begin: paddle.broadcast_to(Tensor([4, 56123, 132, 77],"bool"), list[4,8,132,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 56123, 132, 77],"bool"), list[4,8,132,77,], ) 
 The expanded size of the tensor (8) must match the existing size (56123) at non-singleton dimension 1.  Target sizes: [4, 8, 132, 77].  Tensor sizes: [4, 56123, 132, 77]
2025-03-11 18:26:38.642633 test begin: paddle.broadcast_to(Tensor([4, 56551, 131, 77],"bool"), list[4,8,131,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 56551, 131, 77],"bool"), list[4,8,131,77,], ) 
 The expanded size of the tensor (8) must match the existing size (56551) at non-singleton dimension 1.  Target sizes: [4, 8, 131, 77].  Tensor sizes: [4, 56551, 131, 77]
2025-03-11 18:26:39.303440 test begin: paddle.broadcast_to(Tensor([4, 569856, 13, 77],"bool"), list[4,8,13,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 569856, 13, 77],"bool"), list[4,8,13,77,], ) 
 The expanded size of the tensor (8) must match the existing size (569856) at non-singleton dimension 1.  Target sizes: [4, 8, 13, 77].  Tensor sizes: [4, 569856, 13, 77]
2025-03-11 18:26:39.851792 test begin: paddle.broadcast_to(Tensor([4, 56986, 130, 77],"bool"), list[4,8,130,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 56986, 130, 77],"bool"), list[4,8,130,77,], ) 
 The expanded size of the tensor (8) must match the existing size (56986) at non-singleton dimension 1.  Target sizes: [4, 8, 130, 77].  Tensor sizes: [4, 56986, 130, 77]
2025-03-11 18:26:40.353300 test begin: paddle.broadcast_to(Tensor([4, 570425345, 1, 1],"bool"), list[4,8,1,1,], )

[torch error] paddle.broadcast_to(Tensor([4, 570425345, 1, 1],"bool"), list[4,8,1,1,], ) 
 The expanded size of the tensor (8) must match the existing size (570425345) at non-singleton dimension 1.  Target sizes: [4, 8, 1, 1].  Tensor sizes: [4, 570425345, 1, 1]
2025-03-11 18:26:40.850714 test begin: paddle.broadcast_to(Tensor([4, 570425345, 1],"int32"), tuple(4,7,1,), )

[torch error] paddle.broadcast_to(Tensor([4, 570425345, 1],"int32"), tuple(4,7,1,), ) 
 The expanded size of the tensor (7) must match the existing size (570425345) at non-singleton dimension 1.  Target sizes: [4, 7, 1].  Tensor sizes: [4, 570425345, 1]
2025-03-11 18:27:26.436603 test begin: paddle.broadcast_to(Tensor([4, 570425345],"bool"), list[4,2,], )

[torch error] paddle.broadcast_to(Tensor([4, 570425345],"bool"), list[4,2,], ) 
 The expanded size of the tensor (2) must match the existing size (570425345) at non-singleton dimension 1.  Target sizes: [4, 2].  Tensor sizes: [4, 570425345]
2025-03-11 18:27:27.395623 test begin: paddle.broadcast_to(Tensor([4, 570425345],"float32"), list[4,2,], )

[torch error] paddle.broadcast_to(Tensor([4, 570425345],"float32"), list[4,2,], ) 
 The expanded size of the tensor (2) must match the existing size (570425345) at non-singleton dimension 1.  Target sizes: [4, 2].  Tensor sizes: [4, 570425345]
2025-03-11 18:28:26.470753 test begin: paddle.broadcast_to(Tensor([4, 570425345],"float32"), tuple(4,2,), )

[torch error] paddle.broadcast_to(Tensor([4, 570425345],"float32"), tuple(4,2,), ) 
 The expanded size of the tensor (2) must match the existing size (570425345) at non-singleton dimension 1.  Target sizes: [4, 2].  Tensor sizes: [4, 570425345]
2025-03-11 18:28:28.762123 test begin: paddle.broadcast_to(Tensor([4, 570425345],"float32"), tuple(4,3,), )

[torch error] paddle.broadcast_to(Tensor([4, 570425345],"float32"), tuple(4,3,), ) 
 The expanded size of the tensor (3) must match the existing size (570425345) at non-singleton dimension 1.  Target sizes: [4, 3].  Tensor sizes: [4, 570425345]
2025-03-11 18:28:30.471181 test begin: paddle.broadcast_to(Tensor([4, 570425345],"float32"), tuple(4,4,), )

[torch error] paddle.broadcast_to(Tensor([4, 570425345],"float32"), tuple(4,4,), ) 
 The expanded size of the tensor (4) must match the existing size (570425345) at non-singleton dimension 1.  Target sizes: [4, 4].  Tensor sizes: [4, 570425345]
2025-03-11 18:28:32.803355 test begin: paddle.broadcast_to(Tensor([4, 570425345],"float32"), tuple(4,7,), )

[torch error] paddle.broadcast_to(Tensor([4, 570425345],"float32"), tuple(4,7,), ) 
 The expanded size of the tensor (7) must match the existing size (570425345) at non-singleton dimension 1.  Target sizes: [4, 7].  Tensor sizes: [4, 570425345]
2025-03-11 18:28:35.445633 test begin: paddle.broadcast_to(Tensor([4, 570425345],"float32"), tuple(4,799,), )

[torch error] paddle.broadcast_to(Tensor([4, 570425345],"float32"), tuple(4,799,), ) 
 The expanded size of the tensor (799) must match the existing size (570425345) at non-singleton dimension 1.  Target sizes: [4, 799].  Tensor sizes: [4, 570425345]
2025-03-11 18:28:38.094505 test begin: paddle.broadcast_to(Tensor([4, 570425345],"int32"), list[4,5,], )

[torch error] paddle.broadcast_to(Tensor([4, 570425345],"int32"), list[4,5,], ) 
 The expanded size of the tensor (5) must match the existing size (570425345) at non-singleton dimension 1.  Target sizes: [4, 5].  Tensor sizes: [4, 570425345]
2025-03-11 18:28:43.665467 test begin: paddle.broadcast_to(Tensor([4, 570425345],"int32"), tuple(4,1,), )

[torch error] paddle.broadcast_to(Tensor([4, 570425345],"int32"), tuple(4,1,), ) 
 The expanded size of the tensor (1) must match the existing size (570425345) at non-singleton dimension 1.  Target sizes: [4, 1].  Tensor sizes: [4, 570425345]
2025-03-11 18:28:46.019094 test begin: paddle.broadcast_to(Tensor([4, 570425345],"int64"), shape=list[4,500,], )

[torch error] paddle.broadcast_to(Tensor([4, 570425345],"int64"), shape=list[4,500,], ) 
 The expanded size of the tensor (500) must match the existing size (570425345) at non-singleton dimension 1.  Target sizes: [4, 500].  Tensor sizes: [4, 570425345]
2025-03-11 18:29:41.917830 test begin: paddle.broadcast_to(Tensor([4, 570425345],"int64"), tuple(4,799,), )

[torch error] paddle.broadcast_to(Tensor([4, 570425345],"int64"), tuple(4,799,), ) 
 The expanded size of the tensor (799) must match the existing size (570425345) at non-singleton dimension 1.  Target sizes: [4, 799].  Tensor sizes: [4, 570425345]
2025-03-11 18:29:44.997730 test begin: paddle.broadcast_to(Tensor([4, 57042535, 2, 5],"bool"), list[4,8,2,5,], )

[torch error] paddle.broadcast_to(Tensor([4, 57042535, 2, 5],"bool"), list[4,8,2,5,], ) 
 The expanded size of the tensor (8) must match the existing size (57042535) at non-singleton dimension 1.  Target sizes: [4, 8, 2, 5].  Tensor sizes: [4, 57042535, 2, 5]
2025-03-11 18:29:45.965143 test begin: paddle.broadcast_to(Tensor([4, 5704254, 10, 10],"bool"), list[4,8,10,10,], )

[torch error] paddle.broadcast_to(Tensor([4, 5704254, 10, 10],"bool"), list[4,8,10,10,], ) 
 The expanded size of the tensor (8) must match the existing size (5704254) at non-singleton dimension 1.  Target sizes: [4, 8, 10, 10].  Tensor sizes: [4, 5704254, 10, 10]
2025-03-11 18:29:46.573266 test begin: paddle.broadcast_to(Tensor([4, 5704254, 4, 25],"bool"), list[4,8,4,25,], )

[torch error] paddle.broadcast_to(Tensor([4, 5704254, 4, 25],"bool"), list[4,8,4,25,], ) 
 The expanded size of the tensor (8) must match the existing size (5704254) at non-singleton dimension 1.  Target sizes: [4, 8, 4, 25].  Tensor sizes: [4, 5704254, 4, 25]
2025-03-11 18:29:47.091859 test begin: paddle.broadcast_to(Tensor([4, 57043, 100, 100],"bool"), list[4,8,100,100,], )

[torch error] paddle.broadcast_to(Tensor([4, 57043, 100, 100],"bool"), list[4,8,100,100,], ) 
 The expanded size of the tensor (8) must match the existing size (57043) at non-singleton dimension 1.  Target sizes: [4, 8, 100, 100].  Tensor sizes: [4, 57043, 100, 100]
2025-03-11 18:29:47.804094 test begin: paddle.broadcast_to(Tensor([4, 570997, 27, 37],"bool"), list[4,8,27,37,], )

[torch error] paddle.broadcast_to(Tensor([4, 570997, 27, 37],"bool"), list[4,8,27,37,], ) 
 The expanded size of the tensor (8) must match the existing size (570997) at non-singleton dimension 1.  Target sizes: [4, 8, 27, 37].  Tensor sizes: [4, 570997, 27, 37]
2025-03-11 18:29:48.485235 test begin: paddle.broadcast_to(Tensor([4, 57428, 129, 77],"bool"), list[4,8,129,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 57428, 129, 77],"bool"), list[4,8,129,77,], ) 
 The expanded size of the tensor (8) must match the existing size (57428) at non-singleton dimension 1.  Target sizes: [4, 8, 129, 77].  Tensor sizes: [4, 57428, 129, 77]
2025-03-11 18:29:49.154592 test begin: paddle.broadcast_to(Tensor([4, 57876, 128, 77],"bool"), list[4,8,128,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 57876, 128, 77],"bool"), list[4,8,128,77,], ) 
 The expanded size of the tensor (8) must match the existing size (57876) at non-singleton dimension 1.  Target sizes: [4, 8, 128, 77].  Tensor sizes: [4, 57876, 128, 77]
2025-03-11 18:29:49.664374 test begin: paddle.broadcast_to(Tensor([4, 58201, 99, 99],"bool"), list[4,8,99,99,], )

[torch error] paddle.broadcast_to(Tensor([4, 58201, 99, 99],"bool"), list[4,8,99,99,], ) 
 The expanded size of the tensor (8) must match the existing size (58201) at non-singleton dimension 1.  Target sizes: [4, 8, 99, 99].  Tensor sizes: [4, 58201, 99, 99]
2025-03-11 18:29:50.339853 test begin: paddle.broadcast_to(Tensor([4, 58332, 127, 77],"bool"), list[4,8,127,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 58332, 127, 77],"bool"), list[4,8,127,77,], ) 
 The expanded size of the tensor (8) must match the existing size (58332) at non-singleton dimension 1.  Target sizes: [4, 8, 127, 77].  Tensor sizes: [4, 58332, 127, 77]
2025-03-11 18:29:51.007307 test begin: paddle.broadcast_to(Tensor([4, 584453, 16, 61],"bool"), list[4,8,16,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 584453, 16, 61],"bool"), list[4,8,16,61,], ) 
 The expanded size of the tensor (8) must match the existing size (584453) at non-singleton dimension 1.  Target sizes: [4, 8, 16, 61].  Tensor sizes: [4, 584453, 16, 61]
2025-03-11 18:29:51.709138 test begin: paddle.broadcast_to(Tensor([4, 58795, 126, 77],"bool"), list[4,8,126,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 58795, 126, 77],"bool"), list[4,8,126,77,], ) 
 The expanded size of the tensor (8) must match the existing size (58795) at non-singleton dimension 1.  Target sizes: [4, 8, 126, 77].  Tensor sizes: [4, 58795, 126, 77]
2025-03-11 18:29:52.414090 test begin: paddle.broadcast_to(Tensor([4, 59265, 125, 77],"bool"), list[4,8,125,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 59265, 125, 77],"bool"), list[4,8,125,77,], ) 
 The expanded size of the tensor (8) must match the existing size (59265) at non-singleton dimension 1.  Target sizes: [4, 8, 125, 77].  Tensor sizes: [4, 59265, 125, 77]
2025-03-11 18:29:53.105261 test begin: paddle.broadcast_to(Tensor([4, 592958, 26, 37],"bool"), list[4,8,26,37,], )

[torch error] paddle.broadcast_to(Tensor([4, 592958, 26, 37],"bool"), list[4,8,26,37,], ) 
 The expanded size of the tensor (8) must match the existing size (592958) at non-singleton dimension 1.  Target sizes: [4, 8, 26, 37].  Tensor sizes: [4, 592958, 26, 37]
2025-03-11 18:29:53.528703 test begin: paddle.broadcast_to(Tensor([4, 593575, 31, 31],"bool"), list[4,8,31,31,], )

[torch error] paddle.broadcast_to(Tensor([4, 593575, 31, 31],"bool"), list[4,8,31,31,], ) 
 The expanded size of the tensor (8) must match the existing size (593575) at non-singleton dimension 1.  Target sizes: [4, 8, 31, 31].  Tensor sizes: [4, 593575, 31, 31]
2025-03-11 18:29:54.115518 test begin: paddle.broadcast_to(Tensor([4, 59395, 98, 98],"bool"), list[4,8,98,98,], )

[torch error] paddle.broadcast_to(Tensor([4, 59395, 98, 98],"bool"), list[4,8,98,98,], ) 
 The expanded size of the tensor (8) must match the existing size (59395) at non-singleton dimension 1.  Target sizes: [4, 8, 98, 98].  Tensor sizes: [4, 59395, 98, 98]
2025-03-11 18:29:54.764176 test begin: paddle.broadcast_to(Tensor([4, 59743, 124, 77],"bool"), list[4,8,124,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 59743, 124, 77],"bool"), list[4,8,124,77,], ) 
 The expanded size of the tensor (8) must match the existing size (59743) at non-singleton dimension 1.  Target sizes: [4, 8, 124, 77].  Tensor sizes: [4, 59743, 124, 77]
2025-03-11 18:29:55.463170 test begin: paddle.broadcast_to(Tensor([4, 60229, 123, 77],"bool"), list[4,8,123,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 60229, 123, 77],"bool"), list[4,8,123,77,], ) 
 The expanded size of the tensor (8) must match the existing size (60229) at non-singleton dimension 1.  Target sizes: [4, 8, 123, 77].  Tensor sizes: [4, 60229, 123, 77]
2025-03-11 18:29:56.150138 test begin: paddle.broadcast_to(Tensor([4, 60626, 97, 97],"bool"), list[4,8,97,97,], )

[torch error] paddle.broadcast_to(Tensor([4, 60626, 97, 97],"bool"), list[4,8,97,97,], ) 
 The expanded size of the tensor (8) must match the existing size (60626) at non-singleton dimension 1.  Target sizes: [4, 8, 97, 97].  Tensor sizes: [4, 60626, 97, 97]
2025-03-11 18:29:56.691636 test begin: paddle.broadcast_to(Tensor([4, 60723, 122, 77],"bool"), list[4,8,122,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 60723, 122, 77],"bool"), list[4,8,122,77,], ) 
 The expanded size of the tensor (8) must match the existing size (60723) at non-singleton dimension 1.  Target sizes: [4, 8, 122, 77].  Tensor sizes: [4, 60723, 122, 77]
2025-03-11 18:29:57.365047 test begin: paddle.broadcast_to(Tensor([4, 61225, 121, 77],"bool"), list[4,8,121,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 61225, 121, 77],"bool"), list[4,8,121,77,], ) 
 The expanded size of the tensor (8) must match the existing size (61225) at non-singleton dimension 1.  Target sizes: [4, 8, 121, 77].  Tensor sizes: [4, 61225, 121, 77]
2025-03-11 18:29:58.050628 test begin: paddle.broadcast_to(Tensor([4, 616677, 25, 37],"bool"), list[4,8,25,37,], )

[torch error] paddle.broadcast_to(Tensor([4, 616677, 25, 37],"bool"), list[4,8,25,37,], ) 
 The expanded size of the tensor (8) must match the existing size (616677) at non-singleton dimension 1.  Target sizes: [4, 8, 25, 37].  Tensor sizes: [4, 616677, 25, 37]
2025-03-11 18:29:58.486518 test begin: paddle.broadcast_to(Tensor([4, 617344, 12, 77],"bool"), list[4,8,12,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 617344, 12, 77],"bool"), list[4,8,12,77,], ) 
 The expanded size of the tensor (8) must match the existing size (617344) at non-singleton dimension 1.  Target sizes: [4, 8, 12, 77].  Tensor sizes: [4, 617344, 12, 77]
2025-03-11 18:29:58.814211 test begin: paddle.broadcast_to(Tensor([4, 61735, 120, 77],"bool"), list[4,8,120,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 61735, 120, 77],"bool"), list[4,8,120,77,], ) 
 The expanded size of the tensor (8) must match the existing size (61735) at non-singleton dimension 1.  Target sizes: [4, 8, 120, 77].  Tensor sizes: [4, 61735, 120, 77]
2025-03-11 18:29:59.381197 test begin: paddle.broadcast_to(Tensor([4, 61896, 96, 96],"bool"), list[4,8,96,96,], )

[torch error] paddle.broadcast_to(Tensor([4, 61896, 96, 96],"bool"), list[4,8,96,96,], ) 
 The expanded size of the tensor (8) must match the existing size (61896) at non-singleton dimension 1.  Target sizes: [4, 8, 96, 96].  Tensor sizes: [4, 61896, 96, 96]
2025-03-11 18:29:59.815124 test begin: paddle.broadcast_to(Tensor([4, 62254, 119, 77],"bool"), list[4,8,119,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 62254, 119, 77],"bool"), list[4,8,119,77,], ) 
 The expanded size of the tensor (8) must match the existing size (62254) at non-singleton dimension 1.  Target sizes: [4, 8, 119, 77].  Tensor sizes: [4, 62254, 119, 77]
2025-03-11 18:30:00.132789 test begin: paddle.broadcast_to(Tensor([4, 623416, 15, 61],"bool"), list[4,8,15,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 623416, 15, 61],"bool"), list[4,8,15,61,], ) 
 The expanded size of the tensor (8) must match the existing size (623416) at non-singleton dimension 1.  Target sizes: [4, 8, 15, 61].  Tensor sizes: [4, 623416, 15, 61]
2025-03-11 18:30:00.691947 test begin: paddle.broadcast_to(Tensor([4, 6268411, 7, 13],"bool"), list[4,8,7,13,], )

[torch error] paddle.broadcast_to(Tensor([4, 6268411, 7, 13],"bool"), list[4,8,7,13,], ) 
 The expanded size of the tensor (8) must match the existing size (6268411) at non-singleton dimension 1.  Target sizes: [4, 8, 7, 13].  Tensor sizes: [4, 6268411, 7, 13]
2025-03-11 18:30:01.299383 test begin: paddle.broadcast_to(Tensor([4, 62781, 118, 77],"bool"), list[4,8,118,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 62781, 118, 77],"bool"), list[4,8,118,77,], ) 
 The expanded size of the tensor (8) must match the existing size (62781) at non-singleton dimension 1.  Target sizes: [4, 8, 118, 77].  Tensor sizes: [4, 62781, 118, 77]
2025-03-11 18:30:01.808104 test begin: paddle.broadcast_to(Tensor([4, 63206, 95, 95],"bool"), list[4,8,95,95,], )

[torch error] paddle.broadcast_to(Tensor([4, 63206, 95, 95],"bool"), list[4,8,95,95,], ) 
 The expanded size of the tensor (8) must match the existing size (63206) at non-singleton dimension 1.  Target sizes: [4, 8, 95, 95].  Tensor sizes: [4, 63206, 95, 95]
2025-03-11 18:30:02.229188 test begin: paddle.broadcast_to(Tensor([4, 63318, 117, 77],"bool"), list[4,8,117,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 63318, 117, 77],"bool"), list[4,8,117,77,], ) 
 The expanded size of the tensor (8) must match the existing size (63318) at non-singleton dimension 1.  Target sizes: [4, 8, 117, 77].  Tensor sizes: [4, 63318, 117, 77]
2025-03-11 18:30:02.564418 test begin: paddle.broadcast_to(Tensor([4, 63380594, 3, 3],"bool"), list[4,8,3,3,], )

[torch error] paddle.broadcast_to(Tensor([4, 63380594, 3, 3],"bool"), list[4,8,3,3,], ) 
 The expanded size of the tensor (8) must match the existing size (63380594) at non-singleton dimension 1.  Target sizes: [4, 8, 3, 3].  Tensor sizes: [4, 63380594, 3, 3]
2025-03-11 18:30:02.864355 test begin: paddle.broadcast_to(Tensor([4, 633806, 30, 30],"bool"), list[4,8,30,30,], )

[torch error] paddle.broadcast_to(Tensor([4, 633806, 30, 30],"bool"), list[4,8,30,30,], ) 
 The expanded size of the tensor (8) must match the existing size (633806) at non-singleton dimension 1.  Target sizes: [4, 8, 30, 30].  Tensor sizes: [4, 633806, 30, 30]
2025-03-11 18:30:03.443825 test begin: paddle.broadcast_to(Tensor([4, 63864, 116, 77],"bool"), list[4,8,116,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 63864, 116, 77],"bool"), list[4,8,116,77,], ) 
 The expanded size of the tensor (8) must match the existing size (63864) at non-singleton dimension 1.  Target sizes: [4, 8, 116, 77].  Tensor sizes: [4, 63864, 116, 77]
2025-03-11 18:30:03.869769 test begin: paddle.broadcast_to(Tensor([4, 642371, 24, 37],"bool"), list[4,8,24,37,], )

[torch error] paddle.broadcast_to(Tensor([4, 642371, 24, 37],"bool"), list[4,8,24,37,], ) 
 The expanded size of the tensor (8) must match the existing size (642371) at non-singleton dimension 1.  Target sizes: [4, 8, 24, 37].  Tensor sizes: [4, 642371, 24, 37]
2025-03-11 18:30:04.395965 test begin: paddle.broadcast_to(Tensor([4, 64419, 115, 77],"bool"), list[4,8,115,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 64419, 115, 77],"bool"), list[4,8,115,77,], ) 
 The expanded size of the tensor (8) must match the existing size (64419) at non-singleton dimension 1.  Target sizes: [4, 8, 115, 77].  Tensor sizes: [4, 64419, 115, 77]
2025-03-11 18:30:04.857526 test begin: paddle.broadcast_to(Tensor([4, 64557, 94, 94],"bool"), list[4,8,94,94,], )

[torch error] paddle.broadcast_to(Tensor([4, 64557, 94, 94],"bool"), list[4,8,94,94,], ) 
 The expanded size of the tensor (8) must match the existing size (64557) at non-singleton dimension 1.  Target sizes: [4, 8, 94, 94].  Tensor sizes: [4, 64557, 94, 94]
2025-03-11 18:30:05.441703 test begin: paddle.broadcast_to(Tensor([4, 64984, 114, 77],"bool"), list[4,8,114,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 64984, 114, 77],"bool"), list[4,8,114,77,], ) 
 The expanded size of the tensor (8) must match the existing size (64984) at non-singleton dimension 1.  Target sizes: [4, 8, 114, 77].  Tensor sizes: [4, 64984, 114, 77]
2025-03-11 18:30:06.047778 test begin: paddle.broadcast_to(Tensor([4, 65559, 113, 77],"bool"), list[4,8,113,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 65559, 113, 77],"bool"), list[4,8,113,77,], ) 
 The expanded size of the tensor (8) must match the existing size (65559) at non-singleton dimension 1.  Target sizes: [4, 8, 113, 77].  Tensor sizes: [4, 65559, 113, 77]
2025-03-11 18:30:06.725245 test begin: paddle.broadcast_to(Tensor([4, 65953, 93, 93],"bool"), list[4,8,93,93,], )

[torch error] paddle.broadcast_to(Tensor([4, 65953, 93, 93],"bool"), list[4,8,93,93,], ) 
 The expanded size of the tensor (8) must match the existing size (65953) at non-singleton dimension 1.  Target sizes: [4, 8, 93, 93].  Tensor sizes: [4, 65953, 93, 93]
2025-03-11 18:30:07.397158 test begin: paddle.broadcast_to(Tensor([4, 66144, 112, 77],"bool"), list[4,8,112,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 66144, 112, 77],"bool"), list[4,8,112,77,], ) 
 The expanded size of the tensor (8) must match the existing size (66144) at non-singleton dimension 1.  Target sizes: [4, 8, 112, 77].  Tensor sizes: [4, 66144, 112, 77]
2025-03-11 18:30:08.089958 test begin: paddle.broadcast_to(Tensor([4, 66740, 111, 77],"bool"), list[4,8,111,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 66740, 111, 77],"bool"), list[4,8,111,77,], ) 
 The expanded size of the tensor (8) must match the existing size (66740) at non-singleton dimension 1.  Target sizes: [4, 8, 111, 77].  Tensor sizes: [4, 66740, 111, 77]
2025-03-11 18:30:08.612726 test begin: paddle.broadcast_to(Tensor([4, 667946, 14, 61],"bool"), list[4,8,14,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 667946, 14, 61],"bool"), list[4,8,14,61,], ) 
 The expanded size of the tensor (8) must match the existing size (667946) at non-singleton dimension 1.  Target sizes: [4, 8, 14, 61].  Tensor sizes: [4, 667946, 14, 61]
2025-03-11 18:30:09.030507 test begin: paddle.broadcast_to(Tensor([4, 670301, 23, 37],"bool"), list[4,8,23,37,], )

[torch error] paddle.broadcast_to(Tensor([4, 670301, 23, 37],"bool"), list[4,8,23,37,], ) 
 The expanded size of the tensor (8) must match the existing size (670301) at non-singleton dimension 1.  Target sizes: [4, 8, 23, 37].  Tensor sizes: [4, 670301, 23, 37]
2025-03-11 18:30:09.602913 test begin: paddle.broadcast_to(Tensor([4, 673466, 11, 77],"bool"), list[4,8,11,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 673466, 11, 77],"bool"), list[4,8,11,77,], ) 
 The expanded size of the tensor (8) must match the existing size (673466) at non-singleton dimension 1.  Target sizes: [4, 8, 11, 77].  Tensor sizes: [4, 673466, 11, 77]
2025-03-11 18:30:10.275458 test begin: paddle.broadcast_to(Tensor([4, 67347, 110, 77],"bool"), list[4,8,110,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 67347, 110, 77],"bool"), list[4,8,110,77,], ) 
 The expanded size of the tensor (8) must match the existing size (67347) at non-singleton dimension 1.  Target sizes: [4, 8, 110, 77].  Tensor sizes: [4, 67347, 110, 77]
2025-03-11 18:30:10.944282 test begin: paddle.broadcast_to(Tensor([4, 67395, 92, 92],"bool"), list[4,8,92,92,], )

[torch error] paddle.broadcast_to(Tensor([4, 67395, 92, 92],"bool"), list[4,8,92,92,], ) 
 The expanded size of the tensor (8) must match the existing size (67395) at non-singleton dimension 1.  Target sizes: [4, 8, 92, 92].  Tensor sizes: [4, 67395, 92, 92]
2025-03-11 18:30:11.551900 test begin: paddle.broadcast_to(Tensor([4, 678271, 29, 29],"bool"), list[4,8,29,29,], )

[torch error] paddle.broadcast_to(Tensor([4, 678271, 29, 29],"bool"), list[4,8,29,29,], ) 
 The expanded size of the tensor (8) must match the existing size (678271) at non-singleton dimension 1.  Target sizes: [4, 8, 29, 29].  Tensor sizes: [4, 678271, 29, 29]
2025-03-11 18:30:12.219715 test begin: paddle.broadcast_to(Tensor([4, 67965, 109, 77],"bool"), list[4,8,109,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 67965, 109, 77],"bool"), list[4,8,109,77,], ) 
 The expanded size of the tensor (8) must match the existing size (67965) at non-singleton dimension 1.  Target sizes: [4, 8, 109, 77].  Tensor sizes: [4, 67965, 109, 77]
2025-03-11 18:30:12.896521 test begin: paddle.broadcast_to(Tensor([4, 68258, 137, 61],"bool"), list[4,8,137,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 68258, 137, 61],"bool"), list[4,8,137,61,], ) 
 The expanded size of the tensor (8) must match the existing size (68258) at non-singleton dimension 1.  Target sizes: [4, 8, 137, 61].  Tensor sizes: [4, 68258, 137, 61]
2025-03-11 18:30:13.476989 test begin: paddle.broadcast_to(Tensor([4, 68594, 108, 77],"bool"), list[4,8,108,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 68594, 108, 77],"bool"), list[4,8,108,77,], ) 
 The expanded size of the tensor (8) must match the existing size (68594) at non-singleton dimension 1.  Target sizes: [4, 8, 108, 77].  Tensor sizes: [4, 68594, 108, 77]
2025-03-11 18:30:14.170908 test begin: paddle.broadcast_to(Tensor([4, 68760, 136, 61],"bool"), list[4,8,136,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 68760, 136, 61],"bool"), list[4,8,136,61,], ) 
 The expanded size of the tensor (8) must match the existing size (68760) at non-singleton dimension 1.  Target sizes: [4, 8, 136, 61].  Tensor sizes: [4, 68760, 136, 61]
2025-03-11 18:30:14.853565 test begin: paddle.broadcast_to(Tensor([4, 68884, 91, 91],"bool"), list[4,8,91,91,], )

[torch error] paddle.broadcast_to(Tensor([4, 68884, 91, 91],"bool"), list[4,8,91,91,], ) 
 The expanded size of the tensor (8) must match the existing size (68884) at non-singleton dimension 1.  Target sizes: [4, 8, 91, 91].  Tensor sizes: [4, 68884, 91, 91]
2025-03-11 18:30:15.525877 test begin: paddle.broadcast_to(Tensor([4, 69235, 107, 77],"bool"), list[4,8,107,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 69235, 107, 77],"bool"), list[4,8,107,77,], ) 
 The expanded size of the tensor (8) must match the existing size (69235) at non-singleton dimension 1.  Target sizes: [4, 8, 107, 77].  Tensor sizes: [4, 69235, 107, 77]
2025-03-11 18:30:16.221553 test begin: paddle.broadcast_to(Tensor([4, 69269, 135, 61],"bool"), list[4,8,135,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 69269, 135, 61],"bool"), list[4,8,135,61,], ) 
 The expanded size of the tensor (8) must match the existing size (69269) at non-singleton dimension 1.  Target sizes: [4, 8, 135, 61].  Tensor sizes: [4, 69269, 135, 61]
2025-03-11 18:30:16.890190 test begin: paddle.broadcast_to(Tensor([4, 69786, 134, 61],"bool"), list[4,8,134,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 69786, 134, 61],"bool"), list[4,8,134,61,], ) 
 The expanded size of the tensor (8) must match the existing size (69786) at non-singleton dimension 1.  Target sizes: [4, 8, 134, 61].  Tensor sizes: [4, 69786, 134, 61]
2025-03-11 18:30:17.561724 test begin: paddle.broadcast_to(Tensor([4, 69888, 106, 77],"bool"), list[4,8,106,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 69888, 106, 77],"bool"), list[4,8,106,77,], ) 
 The expanded size of the tensor (8) must match the existing size (69888) at non-singleton dimension 1.  Target sizes: [4, 8, 106, 77].  Tensor sizes: [4, 69888, 106, 77]
2025-03-11 18:30:17.987716 test begin: paddle.broadcast_to(Tensor([4, 7, 153391690],"float16"), tuple(4,7,6,), )

[torch error] paddle.broadcast_to(Tensor([4, 7, 153391690],"float16"), tuple(4,7,6,), ) 
 The expanded size of the tensor (6) must match the existing size (153391690) at non-singleton dimension 2.  Target sizes: [4, 7, 6].  Tensor sizes: [4, 7, 153391690]
2025-03-11 18:31:40.580704 test begin: paddle.broadcast_to(Tensor([4, 7, 81489335],"int32"), tuple(4,7,1,), )

[torch error] paddle.broadcast_to(Tensor([4, 7, 81489335],"int32"), tuple(4,7,1,), ) 
 The expanded size of the tensor (1) must match the existing size (81489335) at non-singleton dimension 2.  Target sizes: [4, 7, 1].  Tensor sizes: [4, 7, 81489335]
2025-03-11 18:31:45.330858 test begin: paddle.broadcast_to(Tensor([4, 700769, 22, 37],"bool"), list[4,8,22,37,], )

[torch error] paddle.broadcast_to(Tensor([4, 700769, 22, 37],"bool"), list[4,8,22,37,], ) 
 The expanded size of the tensor (8) must match the existing size (700769) at non-singleton dimension 1.  Target sizes: [4, 8, 22, 37].  Tensor sizes: [4, 700769, 22, 37]
2025-03-11 18:31:45.768505 test begin: paddle.broadcast_to(Tensor([4, 70311, 133, 61],"bool"), list[4,8,133,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 70311, 133, 61],"bool"), list[4,8,133,61,], ) 
 The expanded size of the tensor (8) must match the existing size (70311) at non-singleton dimension 1.  Target sizes: [4, 8, 133, 61].  Tensor sizes: [4, 70311, 133, 61]
2025-03-11 18:31:46.196446 test begin: paddle.broadcast_to(Tensor([4, 7042289, 9, 9],"bool"), list[4,8,9,9,], )

[torch error] paddle.broadcast_to(Tensor([4, 7042289, 9, 9],"bool"), list[4,8,9,9,], ) 
 The expanded size of the tensor (8) must match the existing size (7042289) at non-singleton dimension 1.  Target sizes: [4, 8, 9, 9].  Tensor sizes: [4, 7042289, 9, 9]
2025-03-11 18:31:46.804430 test begin: paddle.broadcast_to(Tensor([4, 70423, 90, 90],"bool"), list[4,8,90,90,], )

[torch error] paddle.broadcast_to(Tensor([4, 70423, 90, 90],"bool"), list[4,8,90,90,], ) 
 The expanded size of the tensor (8) must match the existing size (70423) at non-singleton dimension 1.  Target sizes: [4, 8, 90, 90].  Tensor sizes: [4, 70423, 90, 90]
2025-03-11 18:31:47.240343 test begin: paddle.broadcast_to(Tensor([4, 70554, 105, 77],"bool"), list[4,8,105,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 70554, 105, 77],"bool"), list[4,8,105,77,], ) 
 The expanded size of the tensor (8) must match the existing size (70554) at non-singleton dimension 1.  Target sizes: [4, 8, 105, 77].  Tensor sizes: [4, 70554, 105, 77]
2025-03-11 18:31:47.540626 test begin: paddle.broadcast_to(Tensor([4, 70843, 132, 61],"bool"), list[4,8,132,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 70843, 132, 61],"bool"), list[4,8,132,61,], ) 
 The expanded size of the tensor (8) must match the existing size (70843) at non-singleton dimension 1.  Target sizes: [4, 8, 132, 61].  Tensor sizes: [4, 70843, 132, 61]
2025-03-11 18:31:47.842077 test begin: paddle.broadcast_to(Tensor([4, 71232, 104, 77],"bool"), list[4,8,104,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 71232, 104, 77],"bool"), list[4,8,104,77,], ) 
 The expanded size of the tensor (8) must match the existing size (71232) at non-singleton dimension 1.  Target sizes: [4, 8, 104, 77].  Tensor sizes: [4, 71232, 104, 77]
2025-03-11 18:31:48.143168 test begin: paddle.broadcast_to(Tensor([4, 71384, 131, 61],"bool"), list[4,8,131,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 71384, 131, 61],"bool"), list[4,8,131,61,], ) 
 The expanded size of the tensor (8) must match the existing size (71384) at non-singleton dimension 1.  Target sizes: [4, 8, 131, 61].  Tensor sizes: [4, 71384, 131, 61]
2025-03-11 18:31:48.689286 test begin: paddle.broadcast_to(Tensor([4, 71924, 103, 77],"bool"), list[4,8,103,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 71924, 103, 77],"bool"), list[4,8,103,77,], ) 
 The expanded size of the tensor (8) must match the existing size (71924) at non-singleton dimension 1.  Target sizes: [4, 8, 103, 77].  Tensor sizes: [4, 71924, 103, 77]
2025-03-11 18:31:49.366156 test begin: paddle.broadcast_to(Tensor([4, 719326, 13, 61],"bool"), list[4,8,13,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 719326, 13, 61],"bool"), list[4,8,13,61,], ) 
 The expanded size of the tensor (8) must match the existing size (719326) at non-singleton dimension 1.  Target sizes: [4, 8, 13, 61].  Tensor sizes: [4, 719326, 13, 61]
2025-03-11 18:31:50.047034 test begin: paddle.broadcast_to(Tensor([4, 71933, 130, 61],"bool"), list[4,8,130,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 71933, 130, 61],"bool"), list[4,8,130,61,], ) 
 The expanded size of the tensor (8) must match the existing size (71933) at non-singleton dimension 1.  Target sizes: [4, 8, 130, 61].  Tensor sizes: [4, 71933, 130, 61]
2025-03-11 18:31:50.750465 test begin: paddle.broadcast_to(Tensor([4, 72015, 89, 89],"bool"), list[4,8,89,89,], )

[torch error] paddle.broadcast_to(Tensor([4, 72015, 89, 89],"bool"), list[4,8,89,89,], ) 
 The expanded size of the tensor (8) must match the existing size (72015) at non-singleton dimension 1.  Target sizes: [4, 8, 89, 89].  Tensor sizes: [4, 72015, 89, 89]
2025-03-11 18:31:51.454104 test begin: paddle.broadcast_to(Tensor([4, 72491, 129, 61],"bool"), list[4,8,129,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 72491, 129, 61],"bool"), list[4,8,129,61,], ) 
 The expanded size of the tensor (8) must match the existing size (72491) at non-singleton dimension 1.  Target sizes: [4, 8, 129, 61].  Tensor sizes: [4, 72491, 129, 61]
2025-03-11 18:31:51.881538 test begin: paddle.broadcast_to(Tensor([4, 72629, 102, 77],"bool"), list[4,8,102,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 72629, 102, 77],"bool"), list[4,8,102,77,], ) 
 The expanded size of the tensor (8) must match the existing size (72629) at non-singleton dimension 1.  Target sizes: [4, 8, 102, 77].  Tensor sizes: [4, 72629, 102, 77]
2025-03-11 18:31:52.465462 test begin: paddle.broadcast_to(Tensor([4, 727584, 28, 28],"bool"), list[4,8,28,28,], )

[torch error] paddle.broadcast_to(Tensor([4, 727584, 28, 28],"bool"), list[4,8,28,28,], ) 
 The expanded size of the tensor (8) must match the existing size (727584) at non-singleton dimension 1.  Target sizes: [4, 8, 28, 28].  Tensor sizes: [4, 727584, 28, 28]
2025-03-11 18:31:53.151657 test begin: paddle.broadcast_to(Tensor([4, 73057, 128, 61],"bool"), list[4,8,128,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 73057, 128, 61],"bool"), list[4,8,128,61,], ) 
 The expanded size of the tensor (8) must match the existing size (73057) at non-singleton dimension 1.  Target sizes: [4, 8, 128, 61].  Tensor sizes: [4, 73057, 128, 61]
2025-03-11 18:31:53.843834 test begin: paddle.broadcast_to(Tensor([4, 7313146, 6, 13],"bool"), list[4,8,6,13,], )

[torch error] paddle.broadcast_to(Tensor([4, 7313146, 6, 13],"bool"), list[4,8,6,13,], ) 
 The expanded size of the tensor (8) must match the existing size (7313146) at non-singleton dimension 1.  Target sizes: [4, 8, 6, 13].  Tensor sizes: [4, 7313146, 6, 13]
2025-03-11 18:31:54.267077 test begin: paddle.broadcast_to(Tensor([4, 73348, 101, 77],"bool"), list[4,8,101,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 73348, 101, 77],"bool"), list[4,8,101,77,], ) 
 The expanded size of the tensor (8) must match the existing size (73348) at non-singleton dimension 1.  Target sizes: [4, 8, 101, 77].  Tensor sizes: [4, 73348, 101, 77]
2025-03-11 18:31:54.861400 test begin: paddle.broadcast_to(Tensor([4, 734139, 21, 37],"bool"), list[4,8,21,37,], )

[torch error] paddle.broadcast_to(Tensor([4, 734139, 21, 37],"bool"), list[4,8,21,37,], ) 
 The expanded size of the tensor (8) must match the existing size (734139) at non-singleton dimension 1.  Target sizes: [4, 8, 21, 37].  Tensor sizes: [4, 734139, 21, 37]
2025-03-11 18:31:55.291077 test begin: paddle.broadcast_to(Tensor([4, 73632, 127, 61],"bool"), list[4,8,127,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 73632, 127, 61],"bool"), list[4,8,127,61,], ) 
 The expanded size of the tensor (8) must match the existing size (73632) at non-singleton dimension 1.  Target sizes: [4, 8, 127, 61].  Tensor sizes: [4, 73632, 127, 61]
2025-03-11 18:31:55.893049 test begin: paddle.broadcast_to(Tensor([4, 73661, 88, 88],"bool"), list[4,8,88,88,], )

[torch error] paddle.broadcast_to(Tensor([4, 73661, 88, 88],"bool"), list[4,8,88,88,], ) 
 The expanded size of the tensor (8) must match the existing size (73661) at non-singleton dimension 1.  Target sizes: [4, 8, 88, 88].  Tensor sizes: [4, 73661, 88, 88]
2025-03-11 18:31:56.313668 test begin: paddle.broadcast_to(Tensor([4, 7408122, 1, 77],"bool"), list[4,8,1,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 7408122, 1, 77],"bool"), list[4,8,1,77,], ) 
 The expanded size of the tensor (8) must match the existing size (7408122) at non-singleton dimension 1.  Target sizes: [4, 8, 1, 77].  Tensor sizes: [4, 7408122, 1, 77]
2025-03-11 18:31:56.640890 test begin: paddle.broadcast_to(Tensor([4, 740813, 10, 77],"bool"), list[4,8,10,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 740813, 10, 77],"bool"), list[4,8,10,77,], ) 
 The expanded size of the tensor (8) must match the existing size (740813) at non-singleton dimension 1.  Target sizes: [4, 8, 10, 77].  Tensor sizes: [4, 740813, 10, 77]
2025-03-11 18:31:57.232680 test begin: paddle.broadcast_to(Tensor([4, 74082, 100, 77],"bool"), list[4,8,100,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 74082, 100, 77],"bool"), list[4,8,100,77,], ) 
 The expanded size of the tensor (8) must match the existing size (74082) at non-singleton dimension 1.  Target sizes: [4, 8, 100, 77].  Tensor sizes: [4, 74082, 100, 77]
2025-03-11 18:31:57.919562 test begin: paddle.broadcast_to(Tensor([4, 74217, 126, 61],"bool"), list[4,8,126,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 74217, 126, 61],"bool"), list[4,8,126,61,], ) 
 The expanded size of the tensor (8) must match the existing size (74217) at non-singleton dimension 1.  Target sizes: [4, 8, 126, 61].  Tensor sizes: [4, 74217, 126, 61]
2025-03-11 18:31:58.337755 test begin: paddle.broadcast_to(Tensor([4, 74810, 125, 61],"bool"), list[4,8,125,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 74810, 125, 61],"bool"), list[4,8,125,61,], ) 
 The expanded size of the tensor (8) must match the existing size (74810) at non-singleton dimension 1.  Target sizes: [4, 8, 125, 61].  Tensor sizes: [4, 74810, 125, 61]
2025-03-11 18:31:58.636594 test begin: paddle.broadcast_to(Tensor([4, 74830, 99, 77],"bool"), list[4,8,99,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 74830, 99, 77],"bool"), list[4,8,99,77,], ) 
 The expanded size of the tensor (8) must match the existing size (74830) at non-singleton dimension 1.  Target sizes: [4, 8, 99, 77].  Tensor sizes: [4, 74830, 99, 77]
2025-03-11 18:31:59.223707 test begin: paddle.broadcast_to(Tensor([4, 75364, 87, 87],"bool"), list[4,8,87,87,], )

[torch error] paddle.broadcast_to(Tensor([4, 75364, 87, 87],"bool"), list[4,8,87,87,], ) 
 The expanded size of the tensor (8) must match the existing size (75364) at non-singleton dimension 1.  Target sizes: [4, 8, 87, 87].  Tensor sizes: [4, 75364, 87, 87]
2025-03-11 18:31:59.659648 test begin: paddle.broadcast_to(Tensor([4, 75414, 124, 61],"bool"), list[4,8,124,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 75414, 124, 61],"bool"), list[4,8,124,61,], ) 
 The expanded size of the tensor (8) must match the existing size (75414) at non-singleton dimension 1.  Target sizes: [4, 8, 124, 61].  Tensor sizes: [4, 75414, 124, 61]
2025-03-11 18:32:00.266141 test begin: paddle.broadcast_to(Tensor([4, 75594, 98, 77],"bool"), list[4,8,98,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 75594, 98, 77],"bool"), list[4,8,98,77,], ) 
 The expanded size of the tensor (8) must match the existing size (75594) at non-singleton dimension 1.  Target sizes: [4, 8, 98, 77].  Tensor sizes: [4, 75594, 98, 77]
2025-03-11 18:32:01.087759 test begin: paddle.broadcast_to(Tensor([4, 76027, 123, 61],"bool"), list[4,8,123,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 76027, 123, 61],"bool"), list[4,8,123,61,], ) 
 The expanded size of the tensor (8) must match the existing size (76027) at non-singleton dimension 1.  Target sizes: [4, 8, 123, 61].  Tensor sizes: [4, 76027, 123, 61]
2025-03-11 18:32:01.718892 test begin: paddle.broadcast_to(Tensor([4, 7605672, 3, 25],"bool"), list[4,8,3,25,], )

[torch error] paddle.broadcast_to(Tensor([4, 7605672, 3, 25],"bool"), list[4,8,3,25,], ) 
 The expanded size of the tensor (8) must match the existing size (7605672) at non-singleton dimension 1.  Target sizes: [4, 8, 3, 25].  Tensor sizes: [4, 7605672, 3, 25]
2025-03-11 18:32:02.427747 test begin: paddle.broadcast_to(Tensor([4, 76373, 97, 77],"bool"), list[4,8,97,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 76373, 97, 77],"bool"), list[4,8,97,77,], ) 
 The expanded size of the tensor (8) must match the existing size (76373) at non-singleton dimension 1.  Target sizes: [4, 8, 97, 77].  Tensor sizes: [4, 76373, 97, 77]
2025-03-11 18:32:03.115911 test begin: paddle.broadcast_to(Tensor([4, 76650, 122, 61],"bool"), list[4,8,122,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 76650, 122, 61],"bool"), list[4,8,122,61,], ) 
 The expanded size of the tensor (8) must match the existing size (76650) at non-singleton dimension 1.  Target sizes: [4, 8, 122, 61].  Tensor sizes: [4, 76650, 122, 61]
2025-03-11 18:32:03.877654 test begin: paddle.broadcast_to(Tensor([4, 7708451, 2, 37],"bool"), list[4,8,2,37,], )

[torch error] paddle.broadcast_to(Tensor([4, 7708451, 2, 37],"bool"), list[4,8,2,37,], ) 
 The expanded size of the tensor (8) must match the existing size (7708451) at non-singleton dimension 1.  Target sizes: [4, 8, 2, 37].  Tensor sizes: [4, 7708451, 2, 37]
2025-03-11 18:32:04.318847 test begin: paddle.broadcast_to(Tensor([4, 770846, 20, 37],"bool"), list[4,8,20,37,], )

[torch error] paddle.broadcast_to(Tensor([4, 770846, 20, 37],"bool"), list[4,8,20,37,], ) 
 The expanded size of the tensor (8) must match the existing size (770846) at non-singleton dimension 1.  Target sizes: [4, 8, 20, 37].  Tensor sizes: [4, 770846, 20, 37]
2025-03-11 18:32:04.914077 test begin: paddle.broadcast_to(Tensor([4, 77127, 86, 86],"bool"), list[4,8,86,86,], )

[torch error] paddle.broadcast_to(Tensor([4, 77127, 86, 86],"bool"), list[4,8,86,86,], ) 
 The expanded size of the tensor (8) must match the existing size (77127) at non-singleton dimension 1.  Target sizes: [4, 8, 86, 86].  Tensor sizes: [4, 77127, 86, 86]
2025-03-11 18:32:05.443365 test begin: paddle.broadcast_to(Tensor([4, 77168, 96, 77],"bool"), list[4,8,96,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 77168, 96, 77],"bool"), list[4,8,96,77,], ) 
 The expanded size of the tensor (8) must match the existing size (77168) at non-singleton dimension 1.  Target sizes: [4, 8, 96, 77].  Tensor sizes: [4, 77168, 96, 77]
2025-03-11 18:32:06.146820 test begin: paddle.broadcast_to(Tensor([4, 77283, 121, 61],"bool"), list[4,8,121,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 77283, 121, 61],"bool"), list[4,8,121,61,], ) 
 The expanded size of the tensor (8) must match the existing size (77283) at non-singleton dimension 1.  Target sizes: [4, 8, 121, 61].  Tensor sizes: [4, 77283, 121, 61]
2025-03-11 18:32:06.841266 test begin: paddle.broadcast_to(Tensor([4, 77927, 120, 61],"bool"), list[4,8,120,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 77927, 120, 61],"bool"), list[4,8,120,61,], ) 
 The expanded size of the tensor (8) must match the existing size (77927) at non-singleton dimension 1.  Target sizes: [4, 8, 120, 61].  Tensor sizes: [4, 77927, 120, 61]
2025-03-11 18:32:07.287214 test begin: paddle.broadcast_to(Tensor([4, 779270, 12, 61],"bool"), list[4,8,12,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 779270, 12, 61],"bool"), list[4,8,12,61,], ) 
 The expanded size of the tensor (8) must match the existing size (779270) at non-singleton dimension 1.  Target sizes: [4, 8, 12, 61].  Tensor sizes: [4, 779270, 12, 61]
2025-03-11 18:32:07.878411 test begin: paddle.broadcast_to(Tensor([4, 77981, 95, 77],"bool"), list[4,8,95,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 77981, 95, 77],"bool"), list[4,8,95,77,], ) 
 The expanded size of the tensor (8) must match the existing size (77981) at non-singleton dimension 1.  Target sizes: [4, 8, 95, 77].  Tensor sizes: [4, 77981, 95, 77]
2025-03-11 18:32:08.479564 test begin: paddle.broadcast_to(Tensor([4, 782477, 27, 27],"bool"), list[4,8,27,27,], )

[torch error] paddle.broadcast_to(Tensor([4, 782477, 27, 27],"bool"), list[4,8,27,27,], ) 
 The expanded size of the tensor (8) must match the existing size (782477) at non-singleton dimension 1.  Target sizes: [4, 8, 27, 27].  Tensor sizes: [4, 782477, 27, 27]
2025-03-11 18:32:09.177826 test begin: paddle.broadcast_to(Tensor([4, 78582, 119, 61],"bool"), list[4,8,119,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 78582, 119, 61],"bool"), list[4,8,119,61,], ) 
 The expanded size of the tensor (8) must match the existing size (78582) at non-singleton dimension 1.  Target sizes: [4, 8, 119, 61].  Tensor sizes: [4, 78582, 119, 61]
2025-03-11 18:32:09.914132 test begin: paddle.broadcast_to(Tensor([4, 78810, 94, 77],"bool"), list[4,8,94,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 78810, 94, 77],"bool"), list[4,8,94,77,], ) 
 The expanded size of the tensor (8) must match the existing size (78810) at non-singleton dimension 1.  Target sizes: [4, 8, 94, 77].  Tensor sizes: [4, 78810, 94, 77]
2025-03-11 18:32:10.603014 test begin: paddle.broadcast_to(Tensor([4, 78952, 85, 85],"bool"), list[4,8,85,85,], )

[torch error] paddle.broadcast_to(Tensor([4, 78952, 85, 85],"bool"), list[4,8,85,85,], ) 
 The expanded size of the tensor (8) must match the existing size (78952) at non-singleton dimension 1.  Target sizes: [4, 8, 85, 85].  Tensor sizes: [4, 78952, 85, 85]
2025-03-11 18:32:11.031533 test begin: paddle.broadcast_to(Tensor([4, 79248, 118, 61],"bool"), list[4,8,118,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 79248, 118, 61],"bool"), list[4,8,118,61,], ) 
 The expanded size of the tensor (8) must match the existing size (79248) at non-singleton dimension 1.  Target sizes: [4, 8, 118, 61].  Tensor sizes: [4, 79248, 118, 61]
2025-03-11 18:32:11.624219 test begin: paddle.broadcast_to(Tensor([4, 79658, 93, 77],"bool"), list[4,8,93,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 79658, 93, 77],"bool"), list[4,8,93,77,], ) 
 The expanded size of the tensor (8) must match the existing size (79658) at non-singleton dimension 1.  Target sizes: [4, 8, 93, 77].  Tensor sizes: [4, 79658, 93, 77]
2025-03-11 18:32:12.250513 test begin: paddle.broadcast_to(Tensor([4, 79926, 117, 61],"bool"), list[4,8,117,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 79926, 117, 61],"bool"), list[4,8,117,61,], ) 
 The expanded size of the tensor (8) must match the existing size (79926) at non-singleton dimension 1.  Target sizes: [4, 8, 117, 61].  Tensor sizes: [4, 79926, 117, 61]
2025-03-11 18:32:12.815336 test begin: paddle.broadcast_to(Tensor([4, 80524, 92, 77],"bool"), list[4,8,92,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 80524, 92, 77],"bool"), list[4,8,92,77,], ) 
 The expanded size of the tensor (8) must match the existing size (80524) at non-singleton dimension 1.  Target sizes: [4, 8, 92, 77].  Tensor sizes: [4, 80524, 92, 77]
2025-03-11 18:32:13.486987 test begin: paddle.broadcast_to(Tensor([4, 80615, 116, 61],"bool"), list[4,8,116,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 80615, 116, 61],"bool"), list[4,8,116,61,], ) 
 The expanded size of the tensor (8) must match the existing size (80615) at non-singleton dimension 1.  Target sizes: [4, 8, 116, 61].  Tensor sizes: [4, 80615, 116, 61]
2025-03-11 18:32:14.010430 test begin: paddle.broadcast_to(Tensor([4, 80843, 84, 84],"bool"), list[4,8,84,84,], )

[torch error] paddle.broadcast_to(Tensor([4, 80843, 84, 84],"bool"), list[4,8,84,84,], ) 
 The expanded size of the tensor (8) must match the existing size (80843) at non-singleton dimension 1.  Target sizes: [4, 8, 84, 84].  Tensor sizes: [4, 80843, 84, 84]
2025-03-11 18:32:14.713154 test begin: paddle.broadcast_to(Tensor([4, 811416, 19, 37],"bool"), list[4,8,19,37,], )

[torch error] paddle.broadcast_to(Tensor([4, 811416, 19, 37],"bool"), list[4,8,19,37,], ) 
 The expanded size of the tensor (8) must match the existing size (811416) at non-singleton dimension 1.  Target sizes: [4, 8, 19, 37].  Tensor sizes: [4, 811416, 19, 37]
2025-03-11 18:32:15.397700 test begin: paddle.broadcast_to(Tensor([4, 81316, 115, 61],"bool"), list[4,8,115,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 81316, 115, 61],"bool"), list[4,8,115,61,], ) 
 The expanded size of the tensor (8) must match the existing size (81316) at non-singleton dimension 1.  Target sizes: [4, 8, 115, 61].  Tensor sizes: [4, 81316, 115, 61]
2025-03-11 18:32:16.086741 test begin: paddle.broadcast_to(Tensor([4, 81408, 91, 77],"bool"), list[4,8,91,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 81408, 91, 77],"bool"), list[4,8,91,77,], ) 
 The expanded size of the tensor (8) must match the existing size (81408) at non-singleton dimension 1.  Target sizes: [4, 8, 91, 77].  Tensor sizes: [4, 81408, 91, 77]
2025-03-11 18:32:16.774192 test begin: paddle.broadcast_to(Tensor([4, 82029, 114, 61],"bool"), list[4,8,114,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 82029, 114, 61],"bool"), list[4,8,114,61,], ) 
 The expanded size of the tensor (8) must match the existing size (82029) at non-singleton dimension 1.  Target sizes: [4, 8, 114, 61].  Tensor sizes: [4, 82029, 114, 61]
2025-03-11 18:32:17.438770 test begin: paddle.broadcast_to(Tensor([4, 823125, 9, 77],"bool"), list[4,8,9,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 823125, 9, 77],"bool"), list[4,8,9,77,], ) 
 The expanded size of the tensor (8) must match the existing size (823125) at non-singleton dimension 1.  Target sizes: [4, 8, 9, 77].  Tensor sizes: [4, 823125, 9, 77]
2025-03-11 18:32:18.100525 test begin: paddle.broadcast_to(Tensor([4, 82313, 90, 77],"bool"), list[4,8,90,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 82313, 90, 77],"bool"), list[4,8,90,77,], ) 
 The expanded size of the tensor (8) must match the existing size (82313) at non-singleton dimension 1.  Target sizes: [4, 8, 90, 77].  Tensor sizes: [4, 82313, 90, 77]
2025-03-11 18:32:18.754750 test begin: paddle.broadcast_to(Tensor([4, 82755, 113, 61],"bool"), list[4,8,113,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 82755, 113, 61],"bool"), list[4,8,113,61,], ) 
 The expanded size of the tensor (8) must match the existing size (82755) at non-singleton dimension 1.  Target sizes: [4, 8, 113, 61].  Tensor sizes: [4, 82755, 113, 61]
2025-03-11 18:32:19.420466 test begin: paddle.broadcast_to(Tensor([4, 82803, 83, 83],"bool"), list[4,8,83,83,], )

[torch error] paddle.broadcast_to(Tensor([4, 82803, 83, 83],"bool"), list[4,8,83,83,], ) 
 The expanded size of the tensor (8) must match the existing size (82803) at non-singleton dimension 1.  Target sizes: [4, 8, 83, 83].  Tensor sizes: [4, 82803, 83, 83]
2025-03-11 18:32:20.079204 test begin: paddle.broadcast_to(Tensor([4, 83238, 89, 77],"bool"), list[4,8,89,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 83238, 89, 77],"bool"), list[4,8,89,77,], ) 
 The expanded size of the tensor (8) must match the existing size (83238) at non-singleton dimension 1.  Target sizes: [4, 8, 89, 77].  Tensor sizes: [4, 83238, 89, 77]
2025-03-11 18:32:20.605840 test begin: paddle.broadcast_to(Tensor([4, 83494, 112, 61],"bool"), list[4,8,112,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 83494, 112, 61],"bool"), list[4,8,112,61,], ) 
 The expanded size of the tensor (8) must match the existing size (83494) at non-singleton dimension 1.  Target sizes: [4, 8, 112, 61].  Tensor sizes: [4, 83494, 112, 61]
2025-03-11 18:32:21.029184 test begin: paddle.broadcast_to(Tensor([4, 84184, 88, 77],"bool"), list[4,8,88,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 84184, 88, 77],"bool"), list[4,8,88,77,], ) 
 The expanded size of the tensor (8) must match the existing size (84184) at non-singleton dimension 1.  Target sizes: [4, 8, 88, 77].  Tensor sizes: [4, 84184, 88, 77]
2025-03-11 18:32:21.584627 test begin: paddle.broadcast_to(Tensor([4, 84246, 111, 61],"bool"), list[4,8,111,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 84246, 111, 61],"bool"), list[4,8,111,61,], ) 
 The expanded size of the tensor (8) must match the existing size (84246) at non-singleton dimension 1.  Target sizes: [4, 8, 111, 61].  Tensor sizes: [4, 84246, 111, 61]
2025-03-11 18:32:22.266122 test begin: paddle.broadcast_to(Tensor([4, 843825, 26, 26],"bool"), list[4,8,26,26,], )

[torch error] paddle.broadcast_to(Tensor([4, 843825, 26, 26],"bool"), list[4,8,26,26,], ) 
 The expanded size of the tensor (8) must match the existing size (843825) at non-singleton dimension 1.  Target sizes: [4, 8, 26, 26].  Tensor sizes: [4, 843825, 26, 26]
2025-03-11 18:32:22.925307 test begin: paddle.broadcast_to(Tensor([4, 84835, 82, 82],"bool"), list[4,8,82,82,], )

[torch error] paddle.broadcast_to(Tensor([4, 84835, 82, 82],"bool"), list[4,8,82,82,], ) 
 The expanded size of the tensor (8) must match the existing size (84835) at non-singleton dimension 1.  Target sizes: [4, 8, 82, 82].  Tensor sizes: [4, 84835, 82, 82]
2025-03-11 18:32:23.602632 test begin: paddle.broadcast_to(Tensor([4, 850113, 11, 61],"bool"), list[4,8,11,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 850113, 11, 61],"bool"), list[4,8,11,61,], ) 
 The expanded size of the tensor (8) must match the existing size (850113) at non-singleton dimension 1.  Target sizes: [4, 8, 11, 61].  Tensor sizes: [4, 850113, 11, 61]
2025-03-11 18:32:24.274589 test begin: paddle.broadcast_to(Tensor([4, 85012, 110, 61],"bool"), list[4,8,110,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 85012, 110, 61],"bool"), list[4,8,110,61,], ) 
 The expanded size of the tensor (8) must match the existing size (85012) at non-singleton dimension 1.  Target sizes: [4, 8, 110, 61].  Tensor sizes: [4, 85012, 110, 61]
2025-03-11 18:32:24.933711 test begin: paddle.broadcast_to(Tensor([4, 85151, 87, 77],"bool"), list[4,8,87,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 85151, 87, 77],"bool"), list[4,8,87,77,], ) 
 The expanded size of the tensor (8) must match the existing size (85151) at non-singleton dimension 1.  Target sizes: [4, 8, 87, 77].  Tensor sizes: [4, 85151, 87, 77]
2025-03-11 18:32:25.464913 test begin: paddle.broadcast_to(Tensor([4, 856495, 18, 37],"bool"), list[4,8,18,37,], )

[torch error] paddle.broadcast_to(Tensor([4, 856495, 18, 37],"bool"), list[4,8,18,37,], ) 
 The expanded size of the tensor (8) must match the existing size (856495) at non-singleton dimension 1.  Target sizes: [4, 8, 18, 37].  Tensor sizes: [4, 856495, 18, 37]
2025-03-11 18:32:26.150607 test begin: paddle.broadcast_to(Tensor([4, 85792, 109, 61],"bool"), list[4,8,109,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 85792, 109, 61],"bool"), list[4,8,109,61,], ) 
 The expanded size of the tensor (8) must match the existing size (85792) at non-singleton dimension 1.  Target sizes: [4, 8, 109, 61].  Tensor sizes: [4, 85792, 109, 61]
2025-03-11 18:32:26.593286 test begin: paddle.broadcast_to(Tensor([4, 86141, 86, 77],"bool"), list[4,8,86,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 86141, 86, 77],"bool"), list[4,8,86,77,], ) 
 The expanded size of the tensor (8) must match the existing size (86141) at non-singleton dimension 1.  Target sizes: [4, 8, 86, 77].  Tensor sizes: [4, 86141, 86, 77]
2025-03-11 18:32:26.897699 test begin: paddle.broadcast_to(Tensor([4, 86586, 108, 61],"bool"), list[4,8,108,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 86586, 108, 61],"bool"), list[4,8,108,61,], ) 
 The expanded size of the tensor (8) must match the existing size (86586) at non-singleton dimension 1.  Target sizes: [4, 8, 108, 61].  Tensor sizes: [4, 86586, 108, 61]
2025-03-11 18:32:27.396564 test begin: paddle.broadcast_to(Tensor([4, 86942, 81, 81],"bool"), list[4,8,81,81,], )

[torch error] paddle.broadcast_to(Tensor([4, 86942, 81, 81],"bool"), list[4,8,81,81,], ) 
 The expanded size of the tensor (8) must match the existing size (86942) at non-singleton dimension 1.  Target sizes: [4, 8, 81, 81].  Tensor sizes: [4, 86942, 81, 81]
2025-03-11 18:32:28.069233 test begin: paddle.broadcast_to(Tensor([4, 87155, 85, 77],"bool"), list[4,8,85,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 87155, 85, 77],"bool"), list[4,8,85,77,], ) 
 The expanded size of the tensor (8) must match the existing size (87155) at non-singleton dimension 1.  Target sizes: [4, 8, 85, 77].  Tensor sizes: [4, 87155, 85, 77]
2025-03-11 18:32:28.726338 test begin: paddle.broadcast_to(Tensor([4, 87395, 107, 61],"bool"), list[4,8,107,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 87395, 107, 61],"bool"), list[4,8,107,61,], ) 
 The expanded size of the tensor (8) must match the existing size (87395) at non-singleton dimension 1.  Target sizes: [4, 8, 107, 61].  Tensor sizes: [4, 87395, 107, 61]
2025-03-11 18:32:29.411688 test begin: paddle.broadcast_to(Tensor([4, 8775775, 13, 5],"bool"), list[4,8,13,5,], )

[torch error] paddle.broadcast_to(Tensor([4, 8775775, 13, 5],"bool"), list[4,8,13,5,], ) 
 The expanded size of the tensor (8) must match the existing size (8775775) at non-singleton dimension 1.  Target sizes: [4, 8, 13, 5].  Tensor sizes: [4, 8775775, 13, 5]
2025-03-11 18:32:30.063668 test begin: paddle.broadcast_to(Tensor([4, 8775775, 5, 13],"bool"), list[4,8,5,13,], )

[torch error] paddle.broadcast_to(Tensor([4, 8775775, 5, 13],"bool"), list[4,8,5,13,], ) 
 The expanded size of the tensor (8) must match the existing size (8775775) at non-singleton dimension 1.  Target sizes: [4, 8, 5, 13].  Tensor sizes: [4, 8775775, 5, 13]
2025-03-11 18:32:30.747304 test begin: paddle.broadcast_to(Tensor([4, 88192, 84, 77],"bool"), list[4,8,84,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 88192, 84, 77],"bool"), list[4,8,84,77,], ) 
 The expanded size of the tensor (8) must match the existing size (88192) at non-singleton dimension 1.  Target sizes: [4, 8, 84, 77].  Tensor sizes: [4, 88192, 84, 77]
2025-03-11 18:32:31.438880 test begin: paddle.broadcast_to(Tensor([4, 88220, 106, 61],"bool"), list[4,8,106,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 88220, 106, 61],"bool"), list[4,8,106,61,], ) 
 The expanded size of the tensor (8) must match the existing size (88220) at non-singleton dimension 1.  Target sizes: [4, 8, 106, 61].  Tensor sizes: [4, 88220, 106, 61]
2025-03-11 18:32:31.856836 test begin: paddle.broadcast_to(Tensor([4, 89060, 105, 61],"bool"), list[4,8,105,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 89060, 105, 61],"bool"), list[4,8,105,61,], ) 
 The expanded size of the tensor (8) must match the existing size (89060) at non-singleton dimension 1.  Target sizes: [4, 8, 105, 61].  Tensor sizes: [4, 89060, 105, 61]
2025-03-11 18:32:32.244647 test begin: paddle.broadcast_to(Tensor([4, 8912897, 8, 8],"bool"), list[4,8,8,8,], )

[torch error] paddle.broadcast_to(Tensor([4, 8912897, 8, 8],"bool"), list[4,8,8,8,], ) 
 The expanded size of the tensor (8) must match the existing size (8912897) at non-singleton dimension 1.  Target sizes: [4, 8, 8, 8].  Tensor sizes: [4, 8912897, 8, 8]
2025-03-11 18:32:32.670050 test begin: paddle.broadcast_to(Tensor([4, 89129, 80, 80],"bool"), list[4,8,80,80,], )

[torch error] paddle.broadcast_to(Tensor([4, 89129, 80, 80],"bool"), list[4,8,80,80,], ) 
 The expanded size of the tensor (8) must match the existing size (89129) at non-singleton dimension 1.  Target sizes: [4, 8, 80, 80].  Tensor sizes: [4, 89129, 80, 80]
2025-03-11 18:32:32.977623 test begin: paddle.broadcast_to(Tensor([4, 89255, 83, 77],"bool"), list[4,8,83,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 89255, 83, 77],"bool"), list[4,8,83,77,], ) 
 The expanded size of the tensor (8) must match the existing size (89255) at non-singleton dimension 1.  Target sizes: [4, 8, 83, 77].  Tensor sizes: [4, 89255, 83, 77]
2025-03-11 18:32:33.386490 test begin: paddle.broadcast_to(Tensor([4, 89916, 104, 61],"bool"), list[4,8,104,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 89916, 104, 61],"bool"), list[4,8,104,61,], ) 
 The expanded size of the tensor (8) must match the existing size (89916) at non-singleton dimension 1.  Target sizes: [4, 8, 104, 61].  Tensor sizes: [4, 89916, 104, 61]
2025-03-11 18:32:34.064563 test begin: paddle.broadcast_to(Tensor([4, 90343, 82, 77],"bool"), list[4,8,82,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 90343, 82, 77],"bool"), list[4,8,82,77,], ) 
 The expanded size of the tensor (8) must match the existing size (90343) at non-singleton dimension 1.  Target sizes: [4, 8, 82, 77].  Tensor sizes: [4, 90343, 82, 77]
2025-03-11 18:32:34.739758 test begin: paddle.broadcast_to(Tensor([4, 906877, 17, 37],"bool"), list[4,8,17,37,], )

[torch error] paddle.broadcast_to(Tensor([4, 906877, 17, 37],"bool"), list[4,8,17,37,], ) 
 The expanded size of the tensor (8) must match the existing size (906877) at non-singleton dimension 1.  Target sizes: [4, 8, 17, 37].  Tensor sizes: [4, 906877, 17, 37]
2025-03-11 18:32:35.310169 test begin: paddle.broadcast_to(Tensor([4, 90789, 103, 61],"bool"), list[4,8,103,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 90789, 103, 61],"bool"), list[4,8,103,61,], ) 
 The expanded size of the tensor (8) must match the existing size (90789) at non-singleton dimension 1.  Target sizes: [4, 8, 103, 61].  Tensor sizes: [4, 90789, 103, 61]
2025-03-11 18:32:35.988455 test begin: paddle.broadcast_to(Tensor([4, 912681, 25, 25],"bool"), list[4,8,25,25,], )

[torch error] paddle.broadcast_to(Tensor([4, 912681, 25, 25],"bool"), list[4,8,25,25,], ) 
 The expanded size of the tensor (8) must match the existing size (912681) at non-singleton dimension 1.  Target sizes: [4, 8, 25, 25].  Tensor sizes: [4, 912681, 25, 25]
2025-03-11 18:32:36.647920 test begin: paddle.broadcast_to(Tensor([4, 91400, 79, 79],"bool"), list[4,8,79,79,], )

[torch error] paddle.broadcast_to(Tensor([4, 91400, 79, 79],"bool"), list[4,8,79,79,], ) 
 The expanded size of the tensor (8) must match the existing size (91400) at non-singleton dimension 1.  Target sizes: [4, 8, 79, 79].  Tensor sizes: [4, 91400, 79, 79]
2025-03-11 18:32:37.349418 test begin: paddle.broadcast_to(Tensor([4, 91459, 81, 77],"bool"), list[4,8,81,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 91459, 81, 77],"bool"), list[4,8,81,77,], ) 
 The expanded size of the tensor (8) must match the existing size (91459) at non-singleton dimension 1.  Target sizes: [4, 8, 81, 77].  Tensor sizes: [4, 91459, 81, 77]
2025-03-11 18:32:37.769446 test begin: paddle.broadcast_to(Tensor([4, 91679, 102, 61],"bool"), list[4,8,102,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 91679, 102, 61],"bool"), list[4,8,102,61,], ) 
 The expanded size of the tensor (8) must match the existing size (91679) at non-singleton dimension 1.  Target sizes: [4, 8, 102, 61].  Tensor sizes: [4, 91679, 102, 61]
2025-03-11 18:32:38.329869 test begin: paddle.broadcast_to(Tensor([4, 92587, 101, 61],"bool"), list[4,8,101,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 92587, 101, 61],"bool"), list[4,8,101,61,], ) 
 The expanded size of the tensor (8) must match the existing size (92587) at non-singleton dimension 1.  Target sizes: [4, 8, 101, 61].  Tensor sizes: [4, 92587, 101, 61]
2025-03-11 18:32:38.997202 test begin: paddle.broadcast_to(Tensor([4, 926016, 8, 77],"bool"), list[4,8,8,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 926016, 8, 77],"bool"), list[4,8,8,77,], ) 
 The expanded size of the tensor (8) must match the existing size (926016) at non-singleton dimension 1.  Target sizes: [4, 8, 8, 77].  Tensor sizes: [4, 926016, 8, 77]
2025-03-11 18:32:39.671815 test begin: paddle.broadcast_to(Tensor([4, 92602, 80, 77],"bool"), list[4,8,80,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 92602, 80, 77],"bool"), list[4,8,80,77,], ) 
 The expanded size of the tensor (8) must match the existing size (92602) at non-singleton dimension 1.  Target sizes: [4, 8, 80, 77].  Tensor sizes: [4, 92602, 80, 77]
2025-03-11 18:32:40.343723 test begin: paddle.broadcast_to(Tensor([4, 9351236, 1, 61],"bool"), list[4,8,1,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 9351236, 1, 61],"bool"), list[4,8,1,61,], ) 
 The expanded size of the tensor (8) must match the existing size (9351236) at non-singleton dimension 1.  Target sizes: [4, 8, 1, 61].  Tensor sizes: [4, 9351236, 1, 61]
2025-03-11 18:32:41.020062 test begin: paddle.broadcast_to(Tensor([4, 935124, 10, 61],"bool"), list[4,8,10,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 935124, 10, 61],"bool"), list[4,8,10,61,], ) 
 The expanded size of the tensor (8) must match the existing size (935124) at non-singleton dimension 1.  Target sizes: [4, 8, 10, 61].  Tensor sizes: [4, 935124, 10, 61]
2025-03-11 18:32:41.448727 test begin: paddle.broadcast_to(Tensor([4, 93513, 100, 61],"bool"), list[4,8,100,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 93513, 100, 61],"bool"), list[4,8,100,61,], ) 
 The expanded size of the tensor (8) must match the existing size (93513) at non-singleton dimension 1.  Target sizes: [4, 8, 100, 61].  Tensor sizes: [4, 93513, 100, 61]
2025-03-11 18:32:41.851180 test begin: paddle.broadcast_to(Tensor([4, 93759, 78, 78],"bool"), list[4,8,78,78,], )

[torch error] paddle.broadcast_to(Tensor([4, 93759, 78, 78],"bool"), list[4,8,78,78,], ) 
 The expanded size of the tensor (8) must match the existing size (93759) at non-singleton dimension 1.  Target sizes: [4, 8, 78, 78].  Tensor sizes: [4, 93759, 78, 78]
2025-03-11 18:32:42.271409 test begin: paddle.broadcast_to(Tensor([4, 93774, 79, 77],"bool"), list[4,8,79,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 93774, 79, 77],"bool"), list[4,8,79,77,], ) 
 The expanded size of the tensor (8) must match the existing size (93774) at non-singleton dimension 1.  Target sizes: [4, 8, 79, 77].  Tensor sizes: [4, 93774, 79, 77]
2025-03-11 18:32:42.857151 test begin: paddle.broadcast_to(Tensor([4, 94457, 99, 61],"bool"), list[4,8,99,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 94457, 99, 61],"bool"), list[4,8,99,61,], ) 
 The expanded size of the tensor (8) must match the existing size (94457) at non-singleton dimension 1.  Target sizes: [4, 8, 99, 61].  Tensor sizes: [4, 94457, 99, 61]
2025-03-11 18:32:43.478749 test begin: paddle.broadcast_to(Tensor([4, 94976, 78, 77],"bool"), list[4,8,78,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 94976, 78, 77],"bool"), list[4,8,78,77,], ) 
 The expanded size of the tensor (8) must match the existing size (94976) at non-singleton dimension 1.  Target sizes: [4, 8, 78, 77].  Tensor sizes: [4, 94976, 78, 77]
2025-03-11 18:32:44.139147 test begin: paddle.broadcast_to(Tensor([4, 95070891, 6],"int32"), tuple(4,1,6,), )

[torch error] paddle.broadcast_to(Tensor([4, 95070891, 6],"int32"), tuple(4,1,6,), ) 
 The expanded size of the tensor (1) must match the existing size (95070891) at non-singleton dimension 1.  Target sizes: [4, 1, 6].  Tensor sizes: [4, 95070891, 6]
2025-03-11 18:32:46.676661 test begin: paddle.broadcast_to(Tensor([4, 950709, 24, 25],"bool"), list[4,8,24,25,], )

[torch error] paddle.broadcast_to(Tensor([4, 950709, 24, 25],"bool"), list[4,8,24,25,], ) 
 The expanded size of the tensor (8) must match the existing size (950709) at non-singleton dimension 1.  Target sizes: [4, 8, 24, 25].  Tensor sizes: [4, 950709, 24, 25]
2025-03-11 18:32:47.334568 test begin: paddle.broadcast_to(Tensor([4, 9507090, 12, 5],"bool"), list[4,8,12,5,], )

[torch error] paddle.broadcast_to(Tensor([4, 9507090, 12, 5],"bool"), list[4,8,12,5,], ) 
 The expanded size of the tensor (8) must match the existing size (9507090) at non-singleton dimension 1.  Target sizes: [4, 8, 12, 5].  Tensor sizes: [4, 9507090, 12, 5]
2025-03-11 18:32:47.756419 test begin: paddle.broadcast_to(Tensor([4, 95421, 98, 61],"bool"), list[4,8,98,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 95421, 98, 61],"bool"), list[4,8,98,61,], ) 
 The expanded size of the tensor (8) must match the existing size (95421) at non-singleton dimension 1.  Target sizes: [4, 8, 98, 61].  Tensor sizes: [4, 95421, 98, 61]
2025-03-11 18:32:48.327098 test begin: paddle.broadcast_to(Tensor([4, 96210, 77, 77],"bool"), list[4,8,77,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 96210, 77, 77],"bool"), list[4,8,77,77,], ) 
 The expanded size of the tensor (8) must match the existing size (96210) at non-singleton dimension 1.  Target sizes: [4, 8, 77, 77].  Tensor sizes: [4, 96210, 77, 77]
2025-03-11 18:32:48.998616 test begin: paddle.broadcast_to(Tensor([4, 963557, 16, 37],"bool"), list[4,8,16,37,], )

[torch error] paddle.broadcast_to(Tensor([4, 963557, 16, 37],"bool"), list[4,8,16,37,], ) 
 The expanded size of the tensor (8) must match the existing size (963557) at non-singleton dimension 1.  Target sizes: [4, 8, 16, 37].  Tensor sizes: [4, 963557, 16, 37]
2025-03-11 18:32:49.435959 test begin: paddle.broadcast_to(Tensor([4, 96405, 97, 61],"bool"), list[4,8,97,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 96405, 97, 61],"bool"), list[4,8,97,61,], ) 
 The expanded size of the tensor (8) must match the existing size (96405) at non-singleton dimension 1.  Target sizes: [4, 8, 97, 61].  Tensor sizes: [4, 96405, 97, 61]
2025-03-11 18:32:49.756745 test begin: paddle.broadcast_to(Tensor([4, 97409, 96, 61],"bool"), list[4,8,96,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 97409, 96, 61],"bool"), list[4,8,96,61,], ) 
 The expanded size of the tensor (8) must match the existing size (97409) at non-singleton dimension 1.  Target sizes: [4, 8, 96, 61].  Tensor sizes: [4, 97409, 96, 61]
2025-03-11 18:32:50.334353 test begin: paddle.broadcast_to(Tensor([4, 97476, 76, 77],"bool"), list[4,8,76,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 97476, 76, 77],"bool"), list[4,8,76,77,], ) 
 The expanded size of the tensor (8) must match the existing size (97476) at non-singleton dimension 1.  Target sizes: [4, 8, 76, 77].  Tensor sizes: [4, 97476, 76, 77]
2025-03-11 18:32:50.761545 test begin: paddle.broadcast_to(Tensor([4, 98435, 95, 61],"bool"), list[4,8,95,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 98435, 95, 61],"bool"), list[4,8,95,61,], ) 
 The expanded size of the tensor (8) must match the existing size (98435) at non-singleton dimension 1.  Target sizes: [4, 8, 95, 61].  Tensor sizes: [4, 98435, 95, 61]
2025-03-11 18:32:51.400825 test begin: paddle.broadcast_to(Tensor([4, 98758, 76, 76],"bool"), list[4,8,76,76,], )

[torch error] paddle.broadcast_to(Tensor([4, 98758, 76, 76],"bool"), list[4,8,76,76,], ) 
 The expanded size of the tensor (8) must match the existing size (98758) at non-singleton dimension 1.  Target sizes: [4, 8, 76, 76].  Tensor sizes: [4, 98758, 76, 76]
2025-03-11 18:32:52.074733 test begin: paddle.broadcast_to(Tensor([4, 98775, 75, 77],"bool"), list[4,8,75,77,], )

[torch error] paddle.broadcast_to(Tensor([4, 98775, 75, 77],"bool"), list[4,8,75,77,], ) 
 The expanded size of the tensor (8) must match the existing size (98775) at non-singleton dimension 1.  Target sizes: [4, 8, 75, 77].  Tensor sizes: [4, 98775, 75, 77]
2025-03-11 18:32:52.751907 test begin: paddle.broadcast_to(Tensor([4, 990322, 24, 24],"bool"), list[4,8,24,24,], )

[torch error] paddle.broadcast_to(Tensor([4, 990322, 24, 24],"bool"), list[4,8,24,24,], ) 
 The expanded size of the tensor (8) must match the existing size (990322) at non-singleton dimension 1.  Target sizes: [4, 8, 24, 24].  Tensor sizes: [4, 990322, 24, 24]
2025-03-11 18:32:53.409857 test begin: paddle.broadcast_to(Tensor([4, 992045, 23, 25],"bool"), list[4,8,23,25,], )

[torch error] paddle.broadcast_to(Tensor([4, 992045, 23, 25],"bool"), list[4,8,23,25,], ) 
 The expanded size of the tensor (8) must match the existing size (992045) at non-singleton dimension 1.  Target sizes: [4, 8, 23, 25].  Tensor sizes: [4, 992045, 23, 25]
2025-03-11 18:32:54.061417 test begin: paddle.broadcast_to(Tensor([4, 99482, 94, 61],"bool"), list[4,8,94,61,], )

[torch error] paddle.broadcast_to(Tensor([4, 99482, 94, 61],"bool"), list[4,8,94,61,], ) 
 The expanded size of the tensor (8) must match the existing size (99482) at non-singleton dimension 1.  Target sizes: [4, 8, 94, 61].  Tensor sizes: [4, 99482, 94, 61]
2025-03-11 18:32:54.722460 test begin: paddle.broadcast_to(Tensor([40029849, 1, 1, 57],"bool"), list[10,8,1,57,], )

[torch error] paddle.broadcast_to(Tensor([40029849, 1, 1, 57],"bool"), list[10,8,1,57,], ) 
 The expanded size of the tensor (10) must match the existing size (40029849) at non-singleton dimension 0.  Target sizes: [10, 8, 1, 57].  Tensor sizes: [40029849, 1, 1, 57]
2025-03-11 18:32:55.380255 test begin: paddle.broadcast_to(Tensor([40029849, 1, 1, 57],"bool"), list[2,8,1,57,], )

[torch error] paddle.broadcast_to(Tensor([40029849, 1, 1, 57],"bool"), list[2,8,1,57,], ) 
 The expanded size of the tensor (2) must match the existing size (40029849) at non-singleton dimension 0.  Target sizes: [2, 8, 1, 57].  Tensor sizes: [40029849, 1, 1, 57]
2025-03-11 18:32:56.057660 test begin: paddle.broadcast_to(Tensor([40029849, 1, 1, 57],"bool"), list[6,8,1,57,], )

[torch error] paddle.broadcast_to(Tensor([40029849, 1, 1, 57],"bool"), list[6,8,1,57,], ) 
 The expanded size of the tensor (6) must match the existing size (40029849) at non-singleton dimension 0.  Target sizes: [6, 8, 1, 57].  Tensor sizes: [40029849, 1, 1, 57]
2025-03-11 18:32:56.714461 test begin: paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[1,57,27216,], )

[torch error] paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[1,57,27216,], ) 
 The expanded size of the tensor (1) must match the existing size (40029849) at non-singleton dimension 0.  Target sizes: [1, 57, 27216].  Tensor sizes: [40029849, 57, 1]
2025-03-11 18:32:57.367387 test begin: paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[1,57,30324,], )

[torch error] paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[1,57,30324,], ) 
 The expanded size of the tensor (1) must match the existing size (40029849) at non-singleton dimension 0.  Target sizes: [1, 57, 30324].  Tensor sizes: [40029849, 57, 1]
2025-03-11 18:32:57.791469 test begin: paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[1,57,33600,], )

[torch error] paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[1,57,33600,], ) 
 The expanded size of the tensor (1) must match the existing size (40029849) at non-singleton dimension 0.  Target sizes: [1, 57, 33600].  Tensor sizes: [40029849, 57, 1]
2025-03-11 18:32:58.354393 test begin: paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[1,57,37044,], )

[torch error] paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[1,57,37044,], ) 
 The expanded size of the tensor (1) must match the existing size (40029849) at non-singleton dimension 0.  Target sizes: [1, 57, 37044].  Tensor sizes: [40029849, 57, 1]
2025-03-11 18:32:59.030942 test begin: paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[1,57,40656,], )

[torch error] paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[1,57,40656,], ) 
 The expanded size of the tensor (1) must match the existing size (40029849) at non-singleton dimension 0.  Target sizes: [1, 57, 40656].  Tensor sizes: [40029849, 57, 1]
2025-03-11 18:32:59.453367 test begin: paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[1,57,56784,], )

[torch error] paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[1,57,56784,], ) 
 The expanded size of the tensor (1) must match the existing size (40029849) at non-singleton dimension 0.  Target sizes: [1, 57, 56784].  Tensor sizes: [40029849, 57, 1]
2025-03-11 18:33:00.023853 test begin: paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[2,57,10164,], )

[torch error] paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[2,57,10164,], ) 
 The expanded size of the tensor (2) must match the existing size (40029849) at non-singleton dimension 0.  Target sizes: [2, 57, 10164].  Tensor sizes: [40029849, 57, 1]
2025-03-11 18:33:00.751781 test begin: paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[2,57,11109,], )

[torch error] paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[2,57,11109,], ) 
 The expanded size of the tensor (2) must match the existing size (40029849) at non-singleton dimension 0.  Target sizes: [2, 57, 11109].  Tensor sizes: [40029849, 57, 1]
2025-03-11 18:33:01.420862 test begin: paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[2,57,12096,], )

[torch error] paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[2,57,12096,], ) 
 The expanded size of the tensor (2) must match the existing size (40029849) at non-singleton dimension 0.  Target sizes: [2, 57, 12096].  Tensor sizes: [40029849, 57, 1]
2025-03-11 18:33:02.093507 test begin: paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[2,57,2100,], )

[torch error] paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[2,57,2100,], ) 
 The expanded size of the tensor (2) must match the existing size (40029849) at non-singleton dimension 0.  Target sizes: [2, 57, 2100].  Tensor sizes: [40029849, 57, 1]
2025-03-11 18:33:02.801238 test begin: paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[2,57,2541,], )

[torch error] paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[2,57,2541,], ) 
 The expanded size of the tensor (2) must match the existing size (40029849) at non-singleton dimension 0.  Target sizes: [2, 57, 2541].  Tensor sizes: [40029849, 57, 1]
2025-03-11 18:33:03.222746 test begin: paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[2,57,3549,], )

[torch error] paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[2,57,3549,], ) 
 The expanded size of the tensor (2) must match the existing size (40029849) at non-singleton dimension 0.  Target sizes: [2, 57, 3549].  Tensor sizes: [40029849, 57, 1]
2025-03-11 18:33:03.523466 test begin: paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[2,57,4725,], )

[torch error] paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[2,57,4725,], ) 
 The expanded size of the tensor (2) must match the existing size (40029849) at non-singleton dimension 0.  Target sizes: [2, 57, 4725].  Tensor sizes: [40029849, 57, 1]
2025-03-11 18:33:03.826690 test begin: paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[2,57,5376,], )

[torch error] paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[2,57,5376,], ) 
 The expanded size of the tensor (2) must match the existing size (40029849) at non-singleton dimension 0.  Target sizes: [2, 57, 5376].  Tensor sizes: [40029849, 57, 1]
2025-03-11 18:33:04.393078 test begin: paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[2,57,6069,], )

[torch error] paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[2,57,6069,], ) 
 The expanded size of the tensor (2) must match the existing size (40029849) at non-singleton dimension 0.  Target sizes: [2, 57, 6069].  Tensor sizes: [40029849, 57, 1]
2025-03-11 18:33:05.090721 test begin: paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[2,57,7581,], )

[torch error] paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[2,57,7581,], ) 
 The expanded size of the tensor (2) must match the existing size (40029849) at non-singleton dimension 0.  Target sizes: [2, 57, 7581].  Tensor sizes: [40029849, 57, 1]
2025-03-11 18:33:05.764031 test begin: paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[2,57,8400,], )

[torch error] paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[2,57,8400,], ) 
 The expanded size of the tensor (2) must match the existing size (40029849) at non-singleton dimension 0.  Target sizes: [2, 57, 8400].  Tensor sizes: [40029849, 57, 1]
2025-03-11 18:33:06.430234 test begin: paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[2,57,9261,], )

[torch error] paddle.broadcast_to(Tensor([40029849, 57, 1],"bool"), list[2,57,9261,], ) 
 The expanded size of the tensor (2) must match the existing size (40029849) at non-singleton dimension 0.  Target sizes: [2, 57, 9261].  Tensor sizes: [40029849, 57, 1]
2025-03-11 18:33:07.114754 test begin: paddle.broadcast_to(Tensor([4002985, 1, 10, 57],"bool"), list[10,8,10,57,], )

[torch error] paddle.broadcast_to(Tensor([4002985, 1, 10, 57],"bool"), list[10,8,10,57,], ) 
 The expanded size of the tensor (10) must match the existing size (4002985) at non-singleton dimension 0.  Target sizes: [10, 8, 10, 57].  Tensor sizes: [4002985, 1, 10, 57]
2025-03-11 18:33:07.797427 test begin: paddle.broadcast_to(Tensor([4002985, 1, 10, 57],"bool"), list[2,8,10,57,], )

[torch error] paddle.broadcast_to(Tensor([4002985, 1, 10, 57],"bool"), list[2,8,10,57,], ) 
 The expanded size of the tensor (2) must match the existing size (4002985) at non-singleton dimension 0.  Target sizes: [2, 8, 10, 57].  Tensor sizes: [4002985, 1, 10, 57]
2025-03-11 18:33:08.480073 test begin: paddle.broadcast_to(Tensor([4002985, 1, 10, 57],"bool"), list[6,8,10,57,], )

[torch error] paddle.broadcast_to(Tensor([4002985, 1, 10, 57],"bool"), list[6,8,10,57,], ) 
 The expanded size of the tensor (6) must match the existing size (4002985) at non-singleton dimension 0.  Target sizes: [6, 8, 10, 57].  Tensor sizes: [4002985, 1, 10, 57]
2025-03-11 18:33:08.902440 test begin: paddle.broadcast_to(Tensor([400299, 1, 100, 57],"bool"), list[30,8,100,57,], )

[torch error] paddle.broadcast_to(Tensor([400299, 1, 100, 57],"bool"), list[30,8,100,57,], ) 
 The expanded size of the tensor (30) must match the existing size (400299) at non-singleton dimension 0.  Target sizes: [30, 8, 100, 57].  Tensor sizes: [400299, 1, 100, 57]
2025-03-11 18:33:09.469931 test begin: paddle.broadcast_to(Tensor([400299, 1, 100, 57],"bool"), list[6,8,100,57,], )

[torch error] paddle.broadcast_to(Tensor([400299, 1, 100, 57],"bool"), list[6,8,100,57,], ) 
 The expanded size of the tensor (6) must match the existing size (400299) at non-singleton dimension 0.  Target sizes: [6, 8, 100, 57].  Tensor sizes: [400299, 1, 100, 57]
2025-03-11 18:33:09.896502 test begin: paddle.broadcast_to(Tensor([400369, 1, 139, 41],"bool"), list[30,8,139,41,], )

[torch error] paddle.broadcast_to(Tensor([400369, 1, 139, 41],"bool"), list[30,8,139,41,], ) 
 The expanded size of the tensor (30) must match the existing size (400369) at non-singleton dimension 0.  Target sizes: [30, 8, 139, 41].  Tensor sizes: [400369, 1, 139, 41]
2025-03-11 18:33:10.316124 test begin: paddle.broadcast_to(Tensor([400439, 1, 74, 77],"bool"), list[4,8,74,77,], )

[torch error] paddle.broadcast_to(Tensor([400439, 1, 74, 77],"bool"), list[4,8,74,77,], ) 
 The expanded size of the tensor (4) must match the existing size (400439) at non-singleton dimension 0.  Target sizes: [4, 8, 74, 77].  Tensor sizes: [400439, 1, 74, 77]
2025-03-11 18:33:10.975161 test begin: paddle.broadcast_to(Tensor([40044, 1, 148, 385],"bool"), list[1,8,148,385,], )

[torch error] paddle.broadcast_to(Tensor([40044, 1, 148, 385],"bool"), list[1,8,148,385,], ) 
 The expanded size of the tensor (1) must match the existing size (40044) at non-singleton dimension 0.  Target sizes: [1, 8, 148, 385].  Tensor sizes: [40044, 1, 148, 385]
2025-03-11 18:33:11.653695 test begin: paddle.broadcast_to(Tensor([400580, 1, 64, 89],"bool"), list[10,8,64,89,], )

[torch error] paddle.broadcast_to(Tensor([400580, 1, 64, 89],"bool"), list[10,8,64,89,], ) 
 The expanded size of the tensor (10) must match the existing size (400580) at non-singleton dimension 0.  Target sizes: [10, 8, 64, 89].  Tensor sizes: [400580, 1, 64, 89]
2025-03-11 18:33:12.280544 test begin: paddle.broadcast_to(Tensor([400580, 1, 64, 89],"bool"), list[9,8,64,89,], )

[torch error] paddle.broadcast_to(Tensor([400580, 1, 64, 89],"bool"), list[9,8,64,89,], ) 
 The expanded size of the tensor (9) must match the existing size (400580) at non-singleton dimension 0.  Target sizes: [9, 8, 64, 89].  Tensor sizes: [400580, 1, 64, 89]
2025-03-11 18:33:12.964090 test begin: paddle.broadcast_to(Tensor([400650, 1, 67, 85],"bool"), list[1,8,67,85,], )

[torch error] paddle.broadcast_to(Tensor([400650, 1, 67, 85],"bool"), list[1,8,67,85,], ) 
 The expanded size of the tensor (1) must match the existing size (400650) at non-singleton dimension 0.  Target sizes: [1, 8, 67, 85].  Tensor sizes: [400650, 1, 67, 85]
2025-03-11 18:33:13.630177 test begin: paddle.broadcast_to(Tensor([400650, 1, 67, 85],"bool"), list[2,8,67,85,], )

[torch error] paddle.broadcast_to(Tensor([400650, 1, 67, 85],"bool"), list[2,8,67,85,], ) 
 The expanded size of the tensor (2) must match the existing size (400650) at non-singleton dimension 0.  Target sizes: [2, 8, 67, 85].  Tensor sizes: [400650, 1, 67, 85]
2025-03-11 18:33:14.276538 test begin: paddle.broadcast_to(Tensor([400721, 1, 78, 73],"bool"), list[1,8,78,73,], )

[torch error] paddle.broadcast_to(Tensor([400721, 1, 78, 73],"bool"), list[1,8,78,73,], ) 
 The expanded size of the tensor (1) must match the existing size (400721) at non-singleton dimension 0.  Target sizes: [1, 8, 78, 73].  Tensor sizes: [400721, 1, 78, 73]
2025-03-11 18:33:14.962957 test begin: paddle.broadcast_to(Tensor([400721, 1, 78, 73],"bool"), list[10,8,78,73,], )

[torch error] paddle.broadcast_to(Tensor([400721, 1, 78, 73],"bool"), list[10,8,78,73,], ) 
 The expanded size of the tensor (10) must match the existing size (400721) at non-singleton dimension 0.  Target sizes: [10, 8, 78, 73].  Tensor sizes: [400721, 1, 78, 73]
2025-03-11 18:33:15.635843 test begin: paddle.broadcast_to(Tensor([40102, 1, 218, 261],"bool"), list[1,8,218,261,], )

[torch error] paddle.broadcast_to(Tensor([40102, 1, 218, 261],"bool"), list[1,8,218,261,], ) 
 The expanded size of the tensor (1) must match the existing size (40102) at non-singleton dimension 0.  Target sizes: [1, 8, 218, 261].  Tensor sizes: [40102, 1, 218, 261]
2025-03-11 18:33:16.158277 test begin: paddle.broadcast_to(Tensor([401214, 1, 47, 121],"bool"), list[2,8,47,121,], )

[torch error] paddle.broadcast_to(Tensor([401214, 1, 47, 121],"bool"), list[2,8,47,121,], ) 
 The expanded size of the tensor (2) must match the existing size (401214) at non-singleton dimension 0.  Target sizes: [2, 8, 47, 121].  Tensor sizes: [401214, 1, 47, 121]
2025-03-11 18:33:16.849238 test begin: paddle.broadcast_to(Tensor([401214, 1, 47, 121],"bool"), list[8,8,47,121,], )

[torch error] paddle.broadcast_to(Tensor([401214, 1, 47, 121],"bool"), list[8,8,47,121,], ) 
 The expanded size of the tensor (8) must match the existing size (401214) at non-singleton dimension 0.  Target sizes: [8, 8, 47, 121].  Tensor sizes: [401214, 1, 47, 121]
2025-03-11 18:33:17.514846 test begin: paddle.broadcast_to(Tensor([401426, 1, 116, 49],"bool"), list[10,8,116,49,], )

[torch error] paddle.broadcast_to(Tensor([401426, 1, 116, 49],"bool"), list[10,8,116,49,], ) 
 The expanded size of the tensor (10) must match the existing size (401426) at non-singleton dimension 0.  Target sizes: [10, 8, 116, 49].  Tensor sizes: [401426, 1, 116, 49]
2025-03-11 18:33:17.950668 test begin: paddle.broadcast_to(Tensor([401992, 1, 44, 129],"bool"), list[10,8,44,129,], )

[torch error] paddle.broadcast_to(Tensor([401992, 1, 44, 129],"bool"), list[10,8,44,129,], ) 
 The expanded size of the tensor (10) must match the existing size (401992) at non-singleton dimension 0.  Target sizes: [10, 8, 44, 129].  Tensor sizes: [401992, 1, 44, 129]
2025-03-11 18:33:18.546831 test begin: paddle.broadcast_to(Tensor([401992, 1, 44, 129],"bool"), list[2,8,44,129,], )

[torch error] paddle.broadcast_to(Tensor([401992, 1, 44, 129],"bool"), list[2,8,44,129,], ) 
 The expanded size of the tensor (2) must match the existing size (401992) at non-singleton dimension 0.  Target sizes: [2, 8, 44, 129].  Tensor sizes: [401992, 1, 44, 129]
2025-03-11 18:33:19.242276 test begin: paddle.broadcast_to(Tensor([402204, 1, 93, 61],"bool"), list[1,8,93,61,], )

[torch error] paddle.broadcast_to(Tensor([402204, 1, 93, 61],"bool"), list[1,8,93,61,], ) 
 The expanded size of the tensor (1) must match the existing size (402204) at non-singleton dimension 0.  Target sizes: [1, 8, 93, 61].  Tensor sizes: [402204, 1, 93, 61]
2025-03-11 18:33:19.764603 test begin: paddle.broadcast_to(Tensor([402204, 1, 93, 61],"bool"), list[4,8,93,61,], )

[torch error] paddle.broadcast_to(Tensor([402204, 1, 93, 61],"bool"), list[4,8,93,61,], ) 
 The expanded size of the tensor (4) must match the existing size (402204) at non-singleton dimension 0.  Target sizes: [4, 8, 93, 61].  Tensor sizes: [402204, 1, 93, 61]
2025-03-11 18:33:20.448144 test begin: paddle.broadcast_to(Tensor([402346, 1, 107, 53],"bool"), list[9,8,107,53,], )

[torch error] paddle.broadcast_to(Tensor([402346, 1, 107, 53],"bool"), list[9,8,107,53,], ) 
 The expanded size of the tensor (9) must match the existing size (402346) at non-singleton dimension 0.  Target sizes: [9, 8, 107, 53].  Tensor sizes: [402346, 1, 107, 53]
2025-03-11 18:33:21.120314 test begin: paddle.broadcast_to(Tensor([4024165, 1, 27, 21],"bool"), list[10,8,27,21,], )

[torch error] paddle.broadcast_to(Tensor([4024165, 1, 27, 21],"bool"), list[10,8,27,21,], ) 
 The expanded size of the tensor (10) must match the existing size (4024165) at non-singleton dimension 0.  Target sizes: [10, 8, 27, 21].  Tensor sizes: [4024165, 1, 27, 21]
2025-03-11 18:33:21.711518 test begin: paddle.broadcast_to(Tensor([4024165, 1, 27, 21],"bool"), list[3,8,27,21,], )

[torch error] paddle.broadcast_to(Tensor([4024165, 1, 27, 21],"bool"), list[3,8,27,21,], ) 
 The expanded size of the tensor (3) must match the existing size (4024165) at non-singleton dimension 0.  Target sizes: [3, 8, 27, 21].  Tensor sizes: [4024165, 1, 27, 21]
2025-03-11 18:33:22.142074 test begin: paddle.broadcast_to(Tensor([4024165, 1, 7, 81],"bool"), list[10,8,7,81,], )

[torch error] paddle.broadcast_to(Tensor([4024165, 1, 7, 81],"bool"), list[10,8,7,81,], ) 
 The expanded size of the tensor (10) must match the existing size (4024165) at non-singleton dimension 0.  Target sizes: [10, 8, 7, 81].  Tensor sizes: [4024165, 1, 7, 81]
2025-03-11 18:33:22.537149 test begin: paddle.broadcast_to(Tensor([4024165, 1, 7, 81],"bool"), list[2,8,7,81,], )

[torch error] paddle.broadcast_to(Tensor([4024165, 1, 7, 81],"bool"), list[2,8,7,81,], ) 
 The expanded size of the tensor (2) must match the existing size (4024165) at non-singleton dimension 0.  Target sizes: [2, 8, 7, 81].  Tensor sizes: [4024165, 1, 7, 81]
2025-03-11 18:33:23.199904 test begin: paddle.broadcast_to(Tensor([402417, 1, 54, 105],"bool"), list[10,8,54,105,], )

[torch error] paddle.broadcast_to(Tensor([402417, 1, 54, 105],"bool"), list[10,8,54,105,], ) 
 The expanded size of the tensor (10) must match the existing size (402417) at non-singleton dimension 0.  Target sizes: [10, 8, 54, 105].  Tensor sizes: [402417, 1, 54, 105]
2025-03-11 18:33:23.731112 test begin: paddle.broadcast_to(Tensor([402417, 1, 54, 105],"bool"), list[3,8,54,105,], )

[torch error] paddle.broadcast_to(Tensor([402417, 1, 54, 105],"bool"), list[3,8,54,105,], ) 
 The expanded size of the tensor (3) must match the existing size (402417) at non-singleton dimension 0.  Target sizes: [3, 8, 54, 105].  Tensor sizes: [402417, 1, 54, 105]
2025-03-11 18:33:24.413689 test begin: paddle.broadcast_to(Tensor([402417, 1, 70, 81],"bool"), list[10,8,70,81,], )

[torch error] paddle.broadcast_to(Tensor([402417, 1, 70, 81],"bool"), list[10,8,70,81,], ) 
 The expanded size of the tensor (10) must match the existing size (402417) at non-singleton dimension 0.  Target sizes: [10, 8, 70, 81].  Tensor sizes: [402417, 1, 70, 81]
2025-03-11 18:33:25.100977 test begin: paddle.broadcast_to(Tensor([402417, 1, 70, 81],"bool"), list[2,8,70,81,], )

[torch error] paddle.broadcast_to(Tensor([402417, 1, 70, 81],"bool"), list[2,8,70,81,], ) 
 The expanded size of the tensor (2) must match the existing size (402417) at non-singleton dimension 0.  Target sizes: [2, 8, 70, 81].  Tensor sizes: [402417, 1, 70, 81]
2025-03-11 18:33:25.621325 test begin: paddle.broadcast_to(Tensor([402559, 1, 52, 109],"bool"), list[1,8,52,109,], )

[torch error] paddle.broadcast_to(Tensor([402559, 1, 52, 109],"bool"), list[1,8,52,109,], ) 
 The expanded size of the tensor (1) must match the existing size (402559) at non-singleton dimension 0.  Target sizes: [1, 8, 52, 109].  Tensor sizes: [402559, 1, 52, 109]
2025-03-11 18:33:26.222759 test begin: paddle.broadcast_to(Tensor([40282, 1, 238, 238],"bool"), list[1,8,238,238,], )

[torch error] paddle.broadcast_to(Tensor([40282, 1, 238, 238],"bool"), list[1,8,238,238,], ) 
 The expanded size of the tensor (1) must match the existing size (40282) at non-singleton dimension 0.  Target sizes: [1, 8, 238, 238].  Tensor sizes: [40282, 1, 238, 238]
2025-03-11 18:33:26.913306 test begin: paddle.broadcast_to(Tensor([40282, 1, 238, 238],"bool"), list[8,8,238,238,], )

[torch error] paddle.broadcast_to(Tensor([40282, 1, 238, 238],"bool"), list[8,8,238,238,], ) 
 The expanded size of the tensor (8) must match the existing size (40282) at non-singleton dimension 0.  Target sizes: [8, 8, 238, 238].  Tensor sizes: [40282, 1, 238, 238]
2025-03-11 18:33:27.330627 test begin: paddle.broadcast_to(Tensor([40287, 1, 217, 261],"bool"), list[1,8,217,261,], )

[torch error] paddle.broadcast_to(Tensor([40287, 1, 217, 261],"bool"), list[1,8,217,261,], ) 
 The expanded size of the tensor (1) must match the existing size (40287) at non-singleton dimension 0.  Target sizes: [1, 8, 217, 261].  Tensor sizes: [40287, 1, 217, 261]
2025-03-11 18:33:27.906669 test begin: paddle.broadcast_to(Tensor([403057, 1, 37, 153],"bool"), list[7,8,37,153,], )

[torch error] paddle.broadcast_to(Tensor([403057, 1, 37, 153],"bool"), list[7,8,37,153,], ) 
 The expanded size of the tensor (7) must match the existing size (403057) at non-singleton dimension 0.  Target sizes: [7, 8, 37, 153].  Tensor sizes: [403057, 1, 37, 153]
2025-03-11 18:33:28.339403 test begin: paddle.broadcast_to(Tensor([40317, 1, 147, 385],"bool"), list[1,8,147,385,], )

[torch error] paddle.broadcast_to(Tensor([40317, 1, 147, 385],"bool"), list[1,8,147,385,], ) 
 The expanded size of the tensor (1) must match the existing size (40317) at non-singleton dimension 0.  Target sizes: [1, 8, 147, 385].  Tensor sizes: [40317, 1, 147, 385]
2025-03-11 18:33:28.918934 test begin: paddle.broadcast_to(Tensor([403270, 1, 82, 69],"bool"), list[1,8,82,69,], )

[torch error] paddle.broadcast_to(Tensor([403270, 1, 82, 69],"bool"), list[1,8,82,69,], ) 
 The expanded size of the tensor (1) must match the existing size (403270) at non-singleton dimension 0.  Target sizes: [1, 8, 82, 69].  Tensor sizes: [403270, 1, 82, 69]
2025-03-11 18:33:29.590100 test begin: paddle.broadcast_to(Tensor([403484, 1, 39, 145],"bool"), list[1,8,39,145,], )

[torch error] paddle.broadcast_to(Tensor([403484, 1, 39, 145],"bool"), list[1,8,39,145,], ) 
 The expanded size of the tensor (1) must match the existing size (403484) at non-singleton dimension 0.  Target sizes: [1, 8, 39, 145].  Tensor sizes: [403484, 1, 39, 145]
2025-03-11 18:33:30.113783 test begin: paddle.broadcast_to(Tensor([403484, 1, 39, 145],"bool"), list[5,8,39,145,], )

[torch error] paddle.broadcast_to(Tensor([403484, 1, 39, 145],"bool"), list[5,8,39,145,], ) 
 The expanded size of the tensor (5) must match the existing size (403484) at non-singleton dimension 0.  Target sizes: [5, 8, 39, 145].  Tensor sizes: [403484, 1, 39, 145]
2025-03-11 18:33:30.635516 test begin: paddle.broadcast_to(Tensor([403484, 1, 87, 65],"bool"), list[1,8,87,65,], )

[torch error] paddle.broadcast_to(Tensor([403484, 1, 87, 65],"bool"), list[1,8,87,65,], ) 
 The expanded size of the tensor (1) must match the existing size (403484) at non-singleton dimension 0.  Target sizes: [1, 8, 87, 65].  Tensor sizes: [403484, 1, 87, 65]
2025-03-11 18:33:31.295307 test begin: paddle.broadcast_to(Tensor([403484, 1, 87, 65],"bool"), list[10,8,87,65,], )

[torch error] paddle.broadcast_to(Tensor([403484, 1, 87, 65],"bool"), list[10,8,87,65,], ) 
 The expanded size of the tensor (10) must match the existing size (403484) at non-singleton dimension 0.  Target sizes: [10, 8, 87, 65].  Tensor sizes: [403484, 1, 87, 65]
2025-03-11 18:33:31.800934 test begin: paddle.broadcast_to(Tensor([403484, 1, 87, 65],"bool"), list[5,8,87,65,], )

[torch error] paddle.broadcast_to(Tensor([403484, 1, 87, 65],"bool"), list[5,8,87,65,], ) 
 The expanded size of the tensor (5) must match the existing size (403484) at non-singleton dimension 0.  Target sizes: [5, 8, 87, 65].  Tensor sizes: [403484, 1, 87, 65]
2025-03-11 18:33:32.434677 test begin: paddle.broadcast_to(Tensor([403699, 1, 36, 157],"bool"), list[1,8,36,157,], )

[torch error] paddle.broadcast_to(Tensor([403699, 1, 36, 157],"bool"), list[1,8,36,157,], ) 
 The expanded size of the tensor (1) must match the existing size (403699) at non-singleton dimension 0.  Target sizes: [1, 8, 36, 157].  Tensor sizes: [403699, 1, 36, 157]
2025-03-11 18:33:33.112513 test begin: paddle.broadcast_to(Tensor([403841, 1, 50, 113],"bool"), list[1,8,50,113,], )

[torch error] paddle.broadcast_to(Tensor([403841, 1, 50, 113],"bool"), list[1,8,50,113,], ) 
 The expanded size of the tensor (1) must match the existing size (403841) at non-singleton dimension 0.  Target sizes: [1, 8, 50, 113].  Tensor sizes: [403841, 1, 50, 113]
2025-03-11 18:33:33.768482 test begin: paddle.broadcast_to(Tensor([403841, 1, 50, 113],"bool"), list[8,8,50,113,], )

[torch error] paddle.broadcast_to(Tensor([403841, 1, 50, 113],"bool"), list[8,8,50,113,], ) 
 The expanded size of the tensor (8) must match the existing size (403841) at non-singleton dimension 0.  Target sizes: [8, 8, 50, 113].  Tensor sizes: [403841, 1, 50, 113]
2025-03-11 18:33:34.447359 test begin: paddle.broadcast_to(Tensor([4038410, 1, 5, 113],"bool"), list[1,8,5,113,], )

[torch error] paddle.broadcast_to(Tensor([4038410, 1, 5, 113],"bool"), list[1,8,5,113,], ) 
 The expanded size of the tensor (1) must match the existing size (4038410) at non-singleton dimension 0.  Target sizes: [1, 8, 5, 113].  Tensor sizes: [4038410, 1, 5, 113]
2025-03-11 18:33:35.121740 test begin: paddle.broadcast_to(Tensor([4038410, 1, 5, 113],"bool"), list[8,8,5,113,], )

[torch error] paddle.broadcast_to(Tensor([4038410, 1, 5, 113],"bool"), list[8,8,5,113,], ) 
 The expanded size of the tensor (8) must match the existing size (4038410) at non-singleton dimension 0.  Target sizes: [8, 8, 5, 113].  Tensor sizes: [4038410, 1, 5, 113]
2025-03-11 18:33:35.791843 test begin: paddle.broadcast_to(Tensor([404342, 1, 99, 57],"bool"), list[10,8,99,57,], )

[torch error] paddle.broadcast_to(Tensor([404342, 1, 99, 57],"bool"), list[10,8,99,57,], ) 
 The expanded size of the tensor (10) must match the existing size (404342) at non-singleton dimension 0.  Target sizes: [10, 8, 99, 57].  Tensor sizes: [404342, 1, 99, 57]
2025-03-11 18:33:36.485331 test begin: paddle.broadcast_to(Tensor([404342, 1, 99, 57],"bool"), list[6,8,99,57,], )

[torch error] paddle.broadcast_to(Tensor([404342, 1, 99, 57],"bool"), list[6,8,99,57,], ) 
 The expanded size of the tensor (6) must match the existing size (404342) at non-singleton dimension 0.  Target sizes: [6, 8, 99, 57].  Tensor sizes: [404342, 1, 99, 57]
2025-03-11 18:33:36.904006 test begin: paddle.broadcast_to(Tensor([404414, 1, 26, 217],"bool"), list[1,8,26,217,], )

[torch error] paddle.broadcast_to(Tensor([404414, 1, 26, 217],"bool"), list[1,8,26,217,], ) 
 The expanded size of the tensor (1) must match the existing size (404414) at non-singleton dimension 0.  Target sizes: [1, 8, 26, 217].  Tensor sizes: [404414, 1, 26, 217]
2025-03-11 18:33:37.221814 test begin: paddle.broadcast_to(Tensor([40473, 1, 216, 261],"bool"), list[1,8,216,261,], )

[torch error] paddle.broadcast_to(Tensor([40473, 1, 216, 261],"bool"), list[1,8,216,261,], ) 
 The expanded size of the tensor (1) must match the existing size (40473) at non-singleton dimension 0.  Target sizes: [1, 8, 216, 261].  Tensor sizes: [40473, 1, 216, 261]
2025-03-11 18:33:37.566614 test begin: paddle.broadcast_to(Tensor([404916, 1, 115, 49],"bool"), list[10,8,115,49,], )

[torch error] paddle.broadcast_to(Tensor([404916, 1, 115, 49],"bool"), list[10,8,115,49,], ) 
 The expanded size of the tensor (10) must match the existing size (404916) at non-singleton dimension 0.  Target sizes: [10, 8, 115, 49].  Tensor sizes: [404916, 1, 115, 49]
2025-03-11 18:33:37.895572 test begin: paddle.broadcast_to(Tensor([404916, 1, 35, 161],"bool"), list[2,8,35,161,], )

[torch error] paddle.broadcast_to(Tensor([404916, 1, 35, 161],"bool"), list[2,8,35,161,], ) 
 The expanded size of the tensor (2) must match the existing size (404916) at non-singleton dimension 0.  Target sizes: [2, 8, 35, 161].  Tensor sizes: [404916, 1, 35, 161]
2025-03-11 18:33:38.223538 test begin: paddle.broadcast_to(Tensor([405564, 1, 58, 97],"bool"), list[1,8,58,97,], )

[torch error] paddle.broadcast_to(Tensor([405564, 1, 58, 97],"bool"), list[1,8,58,97,], ) 
 The expanded size of the tensor (1) must match the existing size (405564) at non-singleton dimension 0.  Target sizes: [1, 8, 58, 97].  Tensor sizes: [405564, 1, 58, 97]
2025-03-11 18:33:38.633748 test begin: paddle.broadcast_to(Tensor([405564, 1, 58, 97],"bool"), list[10,8,58,97,], )

[torch error] paddle.broadcast_to(Tensor([405564, 1, 58, 97],"bool"), list[10,8,58,97,], ) 
 The expanded size of the tensor (10) must match the existing size (405564) at non-singleton dimension 0.  Target sizes: [10, 8, 58, 97].  Tensor sizes: [405564, 1, 58, 97]
2025-03-11 18:33:39.323312 test begin: paddle.broadcast_to(Tensor([405564, 1, 58, 97],"bool"), list[6,8,58,97,], )

[torch error] paddle.broadcast_to(Tensor([405564, 1, 58, 97],"bool"), list[6,8,58,97,], ) 
 The expanded size of the tensor (6) must match the existing size (405564) at non-singleton dimension 0.  Target sizes: [6, 8, 58, 97].  Tensor sizes: [405564, 1, 58, 97]
2025-03-11 18:33:39.999851 test begin: paddle.broadcast_to(Tensor([405636, 1, 75, 75],"bool"), list[1,8,75,75,], )

[torch error] paddle.broadcast_to(Tensor([405636, 1, 75, 75],"bool"), list[1,8,75,75,], ) 
 The expanded size of the tensor (1) must match the existing size (405636) at non-singleton dimension 0.  Target sizes: [1, 8, 75, 75].  Tensor sizes: [405636, 1, 75, 75]
2025-03-11 18:33:40.684493 test begin: paddle.broadcast_to(Tensor([405636, 1, 75, 75],"bool"), list[10,8,75,75,], )

[torch error] paddle.broadcast_to(Tensor([405636, 1, 75, 75],"bool"), list[10,8,75,75,], ) 
 The expanded size of the tensor (10) must match the existing size (405636) at non-singleton dimension 0.  Target sizes: [10, 8, 75, 75].  Tensor sizes: [405636, 1, 75, 75]
2025-03-11 18:33:41.294568 test begin: paddle.broadcast_to(Tensor([405636, 1, 75, 75],"bool"), list[2,8,75,75,], )

[torch error] paddle.broadcast_to(Tensor([405636, 1, 75, 75],"bool"), list[2,8,75,75,], ) 
 The expanded size of the tensor (2) must match the existing size (405636) at non-singleton dimension 0.  Target sizes: [2, 8, 75, 75].  Tensor sizes: [405636, 1, 75, 75]
2025-03-11 18:33:41.785246 test begin: paddle.broadcast_to(Tensor([405636, 1, 75, 75],"bool"), list[3,8,75,75,], )

[torch error] paddle.broadcast_to(Tensor([405636, 1, 75, 75],"bool"), list[3,8,75,75,], ) 
 The expanded size of the tensor (3) must match the existing size (405636) at non-singleton dimension 0.  Target sizes: [3, 8, 75, 75].  Tensor sizes: [405636, 1, 75, 75]
2025-03-11 18:33:42.409735 test begin: paddle.broadcast_to(Tensor([405636, 1, 75, 75],"bool"), list[30,8,75,75,], )

[torch error] paddle.broadcast_to(Tensor([405636, 1, 75, 75],"bool"), list[30,8,75,75,], ) 
 The expanded size of the tensor (30) must match the existing size (405636) at non-singleton dimension 0.  Target sizes: [30, 8, 75, 75].  Tensor sizes: [405636, 1, 75, 75]
2025-03-11 18:33:43.089407 test begin: paddle.broadcast_to(Tensor([405636, 1, 75, 75],"bool"), list[4,8,75,75,], )

[torch error] paddle.broadcast_to(Tensor([405636, 1, 75, 75],"bool"), list[4,8,75,75,], ) 
 The expanded size of the tensor (4) must match the existing size (405636) at non-singleton dimension 0.  Target sizes: [4, 8, 75, 75].  Tensor sizes: [405636, 1, 75, 75]
2025-03-11 18:33:43.761621 test begin: paddle.broadcast_to(Tensor([405636, 1, 75, 75],"bool"), list[5,8,75,75,], )

[torch error] paddle.broadcast_to(Tensor([405636, 1, 75, 75],"bool"), list[5,8,75,75,], ) 
 The expanded size of the tensor (5) must match the existing size (405636) at non-singleton dimension 0.  Target sizes: [5, 8, 75, 75].  Tensor sizes: [405636, 1, 75, 75]
2025-03-11 18:33:44.436878 test begin: paddle.broadcast_to(Tensor([405636, 1, 75, 75],"bool"), list[6,8,75,75,], )

[torch error] paddle.broadcast_to(Tensor([405636, 1, 75, 75],"bool"), list[6,8,75,75,], ) 
 The expanded size of the tensor (6) must match the existing size (405636) at non-singleton dimension 0.  Target sizes: [6, 8, 75, 75].  Tensor sizes: [405636, 1, 75, 75]
2025-03-11 18:33:45.090968 test begin: paddle.broadcast_to(Tensor([405636, 1, 75, 75],"bool"), list[7,8,75,75,], )

[torch error] paddle.broadcast_to(Tensor([405636, 1, 75, 75],"bool"), list[7,8,75,75,], ) 
 The expanded size of the tensor (7) must match the existing size (405636) at non-singleton dimension 0.  Target sizes: [7, 8, 75, 75].  Tensor sizes: [405636, 1, 75, 75]
2025-03-11 18:33:45.502173 test begin: paddle.broadcast_to(Tensor([405636, 1, 75, 75],"bool"), list[8,8,75,75,], )

[torch error] paddle.broadcast_to(Tensor([405636, 1, 75, 75],"bool"), list[8,8,75,75,], ) 
 The expanded size of the tensor (8) must match the existing size (405636) at non-singleton dimension 0.  Target sizes: [8, 8, 75, 75].  Tensor sizes: [405636, 1, 75, 75]
2025-03-11 18:33:46.061768 test begin: paddle.broadcast_to(Tensor([405636, 1, 75, 75],"bool"), list[9,8,75,75,], )

[torch error] paddle.broadcast_to(Tensor([405636, 1, 75, 75],"bool"), list[9,8,75,75,], ) 
 The expanded size of the tensor (9) must match the existing size (405636) at non-singleton dimension 0.  Target sizes: [9, 8, 75, 75].  Tensor sizes: [405636, 1, 75, 75]
2025-03-11 18:33:46.728097 test begin: paddle.broadcast_to(Tensor([405925, 1, 73, 77],"bool"), list[4,8,73,77,], )

[torch error] paddle.broadcast_to(Tensor([405925, 1, 73, 77],"bool"), list[4,8,73,77,], ) 
 The expanded size of the tensor (4) must match the existing size (405925) at non-singleton dimension 0.  Target sizes: [4, 8, 73, 77].  Tensor sizes: [405925, 1, 73, 77]
2025-03-11 18:33:47.404675 test begin: paddle.broadcast_to(Tensor([405925, 1, 77, 73],"bool"), list[1,8,77,73,], )

[torch error] paddle.broadcast_to(Tensor([405925, 1, 77, 73],"bool"), list[1,8,77,73,], ) 
 The expanded size of the tensor (1) must match the existing size (405925) at non-singleton dimension 0.  Target sizes: [1, 8, 77, 73].  Tensor sizes: [405925, 1, 77, 73]
2025-03-11 18:33:47.815907 test begin: paddle.broadcast_to(Tensor([405925, 1, 77, 73],"bool"), list[10,8,77,73,], )

[torch error] paddle.broadcast_to(Tensor([405925, 1, 77, 73],"bool"), list[10,8,77,73,], ) 
 The expanded size of the tensor (10) must match the existing size (405925) at non-singleton dimension 0.  Target sizes: [10, 8, 77, 73].  Tensor sizes: [405925, 1, 77, 73]
2025-03-11 18:33:48.396608 test begin: paddle.broadcast_to(Tensor([40593, 1, 146, 385],"bool"), list[1,8,146,385,], )

[torch error] paddle.broadcast_to(Tensor([40593, 1, 146, 385],"bool"), list[1,8,146,385,], ) 
 The expanded size of the tensor (1) must match the existing size (40593) at non-singleton dimension 0.  Target sizes: [1, 8, 146, 385].  Tensor sizes: [40593, 1, 146, 385]
2025-03-11 18:33:48.808853 test begin: paddle.broadcast_to(Tensor([406142, 1, 106, 53],"bool"), list[9,8,106,53,], )

[torch error] paddle.broadcast_to(Tensor([406142, 1, 106, 53],"bool"), list[9,8,106,53,], ) 
 The expanded size of the tensor (9) must match the existing size (406142) at non-singleton dimension 0.  Target sizes: [9, 8, 106, 53].  Tensor sizes: [406142, 1, 106, 53]
2025-03-11 18:33:49.074795 test begin: paddle.broadcast_to(Tensor([406214, 1, 41, 137],"bool"), list[10,8,41,137,], )

[torch error] paddle.broadcast_to(Tensor([406214, 1, 41, 137],"bool"), list[10,8,41,137,], ) 
 The expanded size of the tensor (10) must match the existing size (406214) at non-singleton dimension 0.  Target sizes: [10, 8, 41, 137].  Tensor sizes: [406214, 1, 41, 137]
2025-03-11 18:33:49.633907 test begin: paddle.broadcast_to(Tensor([40623, 1, 237, 237],"bool"), list[1,8,237,237,], )

[torch error] paddle.broadcast_to(Tensor([40623, 1, 237, 237],"bool"), list[1,8,237,237,], ) 
 The expanded size of the tensor (1) must match the existing size (40623) at non-singleton dimension 0.  Target sizes: [1, 8, 237, 237].  Tensor sizes: [40623, 1, 237, 237]
2025-03-11 18:33:50.314873 test begin: paddle.broadcast_to(Tensor([40623, 1, 237, 237],"bool"), list[8,8,237,237,], )

[torch error] paddle.broadcast_to(Tensor([40623, 1, 237, 237],"bool"), list[8,8,237,237,], ) 
 The expanded size of the tensor (8) must match the existing size (40623) at non-singleton dimension 0.  Target sizes: [8, 8, 237, 237].  Tensor sizes: [40623, 1, 237, 237]
2025-03-11 18:33:50.742984 test begin: paddle.broadcast_to(Tensor([406576, 1, 92, 61],"bool"), list[1,8,92,61,], )

[torch error] paddle.broadcast_to(Tensor([406576, 1, 92, 61],"bool"), list[1,8,92,61,], ) 
 The expanded size of the tensor (1) must match the existing size (406576) at non-singleton dimension 0.  Target sizes: [1, 8, 92, 61].  Tensor sizes: [406576, 1, 92, 61]
2025-03-11 18:33:51.312226 test begin: paddle.broadcast_to(Tensor([406576, 1, 92, 61],"bool"), list[4,8,92,61,], )

[torch error] paddle.broadcast_to(Tensor([406576, 1, 92, 61],"bool"), list[4,8,92,61,], ) 
 The expanded size of the tensor (4) must match the existing size (406576) at non-singleton dimension 0.  Target sizes: [4, 8, 92, 61].  Tensor sizes: [406576, 1, 92, 61]
2025-03-11 18:33:51.736124 test begin: paddle.broadcast_to(Tensor([40662, 1, 215, 261],"bool"), list[1,8,215,261,], )

[torch error] paddle.broadcast_to(Tensor([40662, 1, 215, 261],"bool"), list[1,8,215,261,], ) 
 The expanded size of the tensor (1) must match the existing size (40662) at non-singleton dimension 0.  Target sizes: [1, 8, 215, 261].  Tensor sizes: [40662, 1, 215, 261]
2025-03-11 18:33:51.997331 test begin: paddle.broadcast_to(Tensor([406648, 1, 31, 181],"bool"), list[2,8,31,181,], )

[torch error] paddle.broadcast_to(Tensor([406648, 1, 31, 181],"bool"), list[2,8,31,181,], ) 
 The expanded size of the tensor (2) must match the existing size (406648) at non-singleton dimension 0.  Target sizes: [2, 8, 31, 181].  Tensor sizes: [406648, 1, 31, 181]
2025-03-11 18:33:52.275621 test begin: paddle.broadcast_to(Tensor([4067204, 1, 17, 33],"bool"), list[10,8,17,33,], )

[torch error] paddle.broadcast_to(Tensor([4067204, 1, 17, 33],"bool"), list[10,8,17,33,], ) 
 The expanded size of the tensor (10) must match the existing size (4067204) at non-singleton dimension 0.  Target sizes: [10, 8, 17, 33].  Tensor sizes: [4067204, 1, 17, 33]
2025-03-11 18:33:52.574374 test begin: paddle.broadcast_to(Tensor([4067204, 1, 17, 33],"bool"), list[5,8,17,33,], )

[torch error] paddle.broadcast_to(Tensor([4067204, 1, 17, 33],"bool"), list[5,8,17,33,], ) 
 The expanded size of the tensor (5) must match the existing size (4067204) at non-singleton dimension 0.  Target sizes: [5, 8, 17, 33].  Tensor sizes: [4067204, 1, 17, 33]
2025-03-11 18:33:52.844049 test begin: paddle.broadcast_to(Tensor([4067204, 1, 17, 33],"bool"), list[8,8,17,33,], )

[torch error] paddle.broadcast_to(Tensor([4067204, 1, 17, 33],"bool"), list[8,8,17,33,], ) 
 The expanded size of the tensor (8) must match the existing size (4067204) at non-singleton dimension 0.  Target sizes: [8, 8, 17, 33].  Tensor sizes: [4067204, 1, 17, 33]
2025-03-11 18:33:53.438890 test begin: paddle.broadcast_to(Tensor([4067204, 1, 33, 17],"bool"), list[10,8,33,17,], )

[torch error] paddle.broadcast_to(Tensor([4067204, 1, 33, 17],"bool"), list[10,8,33,17,], ) 
 The expanded size of the tensor (10) must match the existing size (4067204) at non-singleton dimension 0.  Target sizes: [10, 8, 33, 17].  Tensor sizes: [4067204, 1, 33, 17]
2025-03-11 18:33:53.944214 test begin: paddle.broadcast_to(Tensor([4067204, 1, 33, 17],"bool"), list[2,8,33,17,], )

[torch error] paddle.broadcast_to(Tensor([4067204, 1, 33, 17],"bool"), list[2,8,33,17,], ) 
 The expanded size of the tensor (2) must match the existing size (4067204) at non-singleton dimension 0.  Target sizes: [2, 8, 33, 17].  Tensor sizes: [4067204, 1, 33, 17]
2025-03-11 18:33:54.627145 test begin: paddle.broadcast_to(Tensor([406721, 1, 66, 85],"bool"), list[1,8,66,85,], )

[torch error] paddle.broadcast_to(Tensor([406721, 1, 66, 85],"bool"), list[1,8,66,85,], ) 
 The expanded size of the tensor (1) must match the existing size (406721) at non-singleton dimension 0.  Target sizes: [1, 8, 66, 85].  Tensor sizes: [406721, 1, 66, 85]
2025-03-11 18:33:55.312598 test begin: paddle.broadcast_to(Tensor([406721, 1, 66, 85],"bool"), list[2,8,66,85,], )

[torch error] paddle.broadcast_to(Tensor([406721, 1, 66, 85],"bool"), list[2,8,66,85,], ) 
 The expanded size of the tensor (2) must match the existing size (406721) at non-singleton dimension 0.  Target sizes: [2, 8, 66, 85].  Tensor sizes: [406721, 1, 66, 85]
2025-03-11 18:33:55.732231 test begin: paddle.broadcast_to(Tensor([406939, 1, 63, 89],"bool"), list[10,8,63,89,], )

[torch error] paddle.broadcast_to(Tensor([406939, 1, 63, 89],"bool"), list[10,8,63,89,], ) 
 The expanded size of the tensor (10) must match the existing size (406939) at non-singleton dimension 0.  Target sizes: [10, 8, 63, 89].  Tensor sizes: [406939, 1, 63, 89]
2025-03-11 18:33:56.302151 test begin: paddle.broadcast_to(Tensor([406939, 1, 63, 89],"bool"), list[9,8,63,89,], )

[torch error] paddle.broadcast_to(Tensor([406939, 1, 63, 89],"bool"), list[9,8,63,89,], ) 
 The expanded size of the tensor (9) must match the existing size (406939) at non-singleton dimension 0.  Target sizes: [9, 8, 63, 89].  Tensor sizes: [406939, 1, 63, 89]
2025-03-11 18:33:56.973706 test begin: paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[1,56,21504,], )

[torch error] paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[1,56,21504,], ) 
 The expanded size of the tensor (1) must match the existing size (40744668) at non-singleton dimension 0.  Target sizes: [1, 56, 21504].  Tensor sizes: [40744668, 56, 1]
2025-03-11 18:33:57.638707 test begin: paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[1,56,24276,], )

[torch error] paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[1,56,24276,], ) 
 The expanded size of the tensor (1) must match the existing size (40744668) at non-singleton dimension 0.  Target sizes: [1, 56, 24276].  Tensor sizes: [40744668, 56, 1]
2025-03-11 18:33:58.059333 test begin: paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[1,56,33600,], )

[torch error] paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[1,56,33600,], ) 
 The expanded size of the tensor (1) must match the existing size (40744668) at non-singleton dimension 0.  Target sizes: [1, 56, 33600].  Tensor sizes: [40744668, 56, 1]
2025-03-11 18:33:58.634969 test begin: paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[1,56,37044,], )

[torch error] paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[1,56,37044,], ) 
 The expanded size of the tensor (1) must match the existing size (40744668) at non-singleton dimension 0.  Target sizes: [1, 56, 37044].  Tensor sizes: [40744668, 56, 1]
2025-03-11 18:33:59.150967 test begin: paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[1,56,44436,], )

[torch error] paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[1,56,44436,], ) 
 The expanded size of the tensor (1) must match the existing size (40744668) at non-singleton dimension 0.  Target sizes: [1, 56, 44436].  Tensor sizes: [40744668, 56, 1]
2025-03-11 18:33:59.653252 test begin: paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[1,56,48384,], )

[torch error] paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[1,56,48384,], ) 
 The expanded size of the tensor (1) must match the existing size (40744668) at non-singleton dimension 0.  Target sizes: [1, 56, 48384].  Tensor sizes: [40744668, 56, 1]
2025-03-11 18:34:00.066579 test begin: paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[1,56,56784,], )

[torch error] paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[1,56,56784,], ) 
 The expanded size of the tensor (1) must match the existing size (40744668) at non-singleton dimension 0.  Target sizes: [1, 56, 56784].  Tensor sizes: [40744668, 56, 1]
2025-03-11 18:34:00.565432 test begin: paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[1,56,70644,], )

[torch error] paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[1,56,70644,], ) 
 The expanded size of the tensor (1) must match the existing size (40744668) at non-singleton dimension 0.  Target sizes: [1, 56, 70644].  Tensor sizes: [40744668, 56, 1]
2025-03-11 18:34:01.099879 test begin: paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[1,56,75600,], )

[torch error] paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[1,56,75600,], ) 
 The expanded size of the tensor (1) must match the existing size (40744668) at non-singleton dimension 0.  Target sizes: [1, 56, 75600].  Tensor sizes: [40744668, 56, 1]
2025-03-11 18:34:01.508913 test begin: paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[2,56,12096,], )

[torch error] paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[2,56,12096,], ) 
 The expanded size of the tensor (2) must match the existing size (40744668) at non-singleton dimension 0.  Target sizes: [2, 56, 12096].  Tensor sizes: [40744668, 56, 1]
2025-03-11 18:34:01.777188 test begin: paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[2,56,2100,], )

[torch error] paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[2,56,2100,], ) 
 The expanded size of the tensor (2) must match the existing size (40744668) at non-singleton dimension 0.  Target sizes: [2, 56, 2100].  Tensor sizes: [40744668, 56, 1]
2025-03-11 18:34:02.046608 test begin: paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[2,56,2541,], )

[torch error] paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[2,56,2541,], ) 
 The expanded size of the tensor (2) must match the existing size (40744668) at non-singleton dimension 0.  Target sizes: [2, 56, 2541].  Tensor sizes: [40744668, 56, 1]
2025-03-11 18:34:02.436829 test begin: paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[2,56,3549,], )

[torch error] paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[2,56,3549,], ) 
 The expanded size of the tensor (2) must match the existing size (40744668) at non-singleton dimension 0.  Target sizes: [2, 56, 3549].  Tensor sizes: [40744668, 56, 1]
2025-03-11 18:34:02.859371 test begin: paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[2,56,4116,], )

[torch error] paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[2,56,4116,], ) 
 The expanded size of the tensor (2) must match the existing size (40744668) at non-singleton dimension 0.  Target sizes: [2, 56, 4116].  Tensor sizes: [40744668, 56, 1]
2025-03-11 18:34:03.293736 test begin: paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[2,56,4725,], )

[torch error] paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[2,56,4725,], ) 
 The expanded size of the tensor (2) must match the existing size (40744668) at non-singleton dimension 0.  Target sizes: [2, 56, 4725].  Tensor sizes: [40744668, 56, 1]
2025-03-11 18:34:03.707955 test begin: paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[2,56,5376,], )

[torch error] paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[2,56,5376,], ) 
 The expanded size of the tensor (2) must match the existing size (40744668) at non-singleton dimension 0.  Target sizes: [2, 56, 5376].  Tensor sizes: [40744668, 56, 1]
2025-03-11 18:34:03.969586 test begin: paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[2,56,7581,], )

[torch error] paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[2,56,7581,], ) 
 The expanded size of the tensor (2) must match the existing size (40744668) at non-singleton dimension 0.  Target sizes: [2, 56, 7581].  Tensor sizes: [40744668, 56, 1]
2025-03-11 18:34:04.227461 test begin: paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[2,56,8400,], )

[torch error] paddle.broadcast_to(Tensor([40744668, 56, 1],"bool"), list[2,56,8400,], ) 
 The expanded size of the tensor (2) must match the existing size (40744668) at non-singleton dimension 0.  Target sizes: [2, 56, 8400].  Tensor sizes: [40744668, 56, 1]
2025-03-11 18:34:04.486260 test begin: paddle.broadcast_to(Tensor([408176, 1, 86, 65],"bool"), list[1,8,86,65,], )

[torch error] paddle.broadcast_to(Tensor([408176, 1, 86, 65],"bool"), list[1,8,86,65,], ) 
 The expanded size of the tensor (1) must match the existing size (408176) at non-singleton dimension 0.  Target sizes: [1, 8, 86, 65].  Tensor sizes: [408176, 1, 86, 65]
2025-03-11 18:34:04.750608 test begin: paddle.broadcast_to(Tensor([408176, 1, 86, 65],"bool"), list[10,8,86,65,], )

[torch error] paddle.broadcast_to(Tensor([408176, 1, 86, 65],"bool"), list[10,8,86,65,], ) 
 The expanded size of the tensor (10) must match the existing size (408176) at non-singleton dimension 0.  Target sizes: [10, 8, 86, 65].  Tensor sizes: [408176, 1, 86, 65]
2025-03-11 18:34:05.255371 test begin: paddle.broadcast_to(Tensor([408176, 1, 86, 65],"bool"), list[5,8,86,65,], )

[torch error] paddle.broadcast_to(Tensor([408176, 1, 86, 65],"bool"), list[5,8,86,65,], ) 
 The expanded size of the tensor (5) must match the existing size (408176) at non-singleton dimension 0.  Target sizes: [5, 8, 86, 65].  Tensor sizes: [408176, 1, 86, 65]
2025-03-11 18:34:05.671527 test begin: paddle.broadcast_to(Tensor([408249, 1, 69, 81],"bool"), list[10,8,69,81,], )

[torch error] paddle.broadcast_to(Tensor([408249, 1, 69, 81],"bool"), list[10,8,69,81,], ) 
 The expanded size of the tensor (10) must match the existing size (408249) at non-singleton dimension 0.  Target sizes: [10, 8, 69, 81].  Tensor sizes: [408249, 1, 69, 81]
2025-03-11 18:34:05.938840 test begin: paddle.broadcast_to(Tensor([408249, 1, 69, 81],"bool"), list[2,8,69,81,], )

[torch error] paddle.broadcast_to(Tensor([408249, 1, 69, 81],"bool"), list[2,8,69,81,], ) 
 The expanded size of the tensor (2) must match the existing size (408249) at non-singleton dimension 0.  Target sizes: [2, 8, 69, 81].  Tensor sizes: [408249, 1, 69, 81]
2025-03-11 18:34:06.207894 test begin: paddle.broadcast_to(Tensor([408249, 1, 81, 69],"bool"), list[1,8,81,69,], )

[torch error] paddle.broadcast_to(Tensor([408249, 1, 81, 69],"bool"), list[1,8,81,69,], ) 
 The expanded size of the tensor (1) must match the existing size (408249) at non-singleton dimension 0.  Target sizes: [1, 8, 81, 69].  Tensor sizes: [408249, 1, 81, 69]
2025-03-11 18:34:06.479226 test begin: paddle.broadcast_to(Tensor([408468, 1, 114, 49],"bool"), list[10,8,114,49,], )

[torch error] paddle.broadcast_to(Tensor([408468, 1, 114, 49],"bool"), list[10,8,114,49,], ) 
 The expanded size of the tensor (10) must match the existing size (408468) at non-singleton dimension 0.  Target sizes: [10, 8, 114, 49].  Tensor sizes: [408468, 1, 114, 49]
2025-03-11 18:34:06.744740 test begin: paddle.broadcast_to(Tensor([408468, 1, 98, 57],"bool"), list[10,8,98,57,], )

[torch error] paddle.broadcast_to(Tensor([408468, 1, 98, 57],"bool"), list[10,8,98,57,], ) 
 The expanded size of the tensor (10) must match the existing size (408468) at non-singleton dimension 0.  Target sizes: [10, 8, 98, 57].  Tensor sizes: [408468, 1, 98, 57]
2025-03-11 18:34:07.007030 test begin: paddle.broadcast_to(Tensor([408468, 1, 98, 57],"bool"), list[30,8,98,57,], )

[torch error] paddle.broadcast_to(Tensor([408468, 1, 98, 57],"bool"), list[30,8,98,57,], ) 
 The expanded size of the tensor (30) must match the existing size (408468) at non-singleton dimension 0.  Target sizes: [30, 8, 98, 57].  Tensor sizes: [408468, 1, 98, 57]
2025-03-11 18:34:07.263909 test begin: paddle.broadcast_to(Tensor([408468, 1, 98, 57],"bool"), list[6,8,98,57,], )

[torch error] paddle.broadcast_to(Tensor([408468, 1, 98, 57],"bool"), list[6,8,98,57,], ) 
 The expanded size of the tensor (6) must match the existing size (408468) at non-singleton dimension 0.  Target sizes: [6, 8, 98, 57].  Tensor sizes: [408468, 1, 98, 57]
2025-03-11 18:34:07.531744 test begin: paddle.broadcast_to(Tensor([40852, 1, 214, 261],"bool"), list[1,8,214,261,], )

[torch error] paddle.broadcast_to(Tensor([40852, 1, 214, 261],"bool"), list[1,8,214,261,], ) 
 The expanded size of the tensor (1) must match the existing size (40852) at non-singleton dimension 0.  Target sizes: [1, 8, 214, 261].  Tensor sizes: [40852, 1, 214, 261]
2025-03-11 18:34:07.798796 test begin: paddle.broadcast_to(Tensor([40873, 1, 145, 385],"bool"), list[1,8,145,385,], )

[torch error] paddle.broadcast_to(Tensor([40873, 1, 145, 385],"bool"), list[1,8,145,385,], ) 
 The expanded size of the tensor (1) must match the existing size (40873) at non-singleton dimension 0.  Target sizes: [1, 8, 145, 385].  Tensor sizes: [40873, 1, 145, 385]
2025-03-11 18:34:08.407329 test begin: paddle.broadcast_to(Tensor([409128, 1, 33, 169],"bool"), list[1,8,33,169,], )

[torch error] paddle.broadcast_to(Tensor([409128, 1, 33, 169],"bool"), list[1,8,33,169,], ) 
 The expanded size of the tensor (1) must match the existing size (409128) at non-singleton dimension 0.  Target sizes: [1, 8, 33, 169].  Tensor sizes: [409128, 1, 33, 169]
2025-03-11 18:34:09.075066 test begin: paddle.broadcast_to(Tensor([409128, 1, 33, 169],"bool"), list[3,8,33,169,], )

[torch error] paddle.broadcast_to(Tensor([409128, 1, 33, 169],"bool"), list[3,8,33,169,], ) 
 The expanded size of the tensor (3) must match the existing size (409128) at non-singleton dimension 0.  Target sizes: [3, 8, 33, 169].  Tensor sizes: [409128, 1, 33, 169]
2025-03-11 18:34:09.501391 test begin: paddle.broadcast_to(Tensor([40968, 1, 236, 236],"bool"), list[1,8,236,236,], )

[torch error] paddle.broadcast_to(Tensor([40968, 1, 236, 236],"bool"), list[1,8,236,236,], ) 
 The expanded size of the tensor (1) must match the existing size (40968) at non-singleton dimension 0.  Target sizes: [1, 8, 236, 236].  Tensor sizes: [40968, 1, 236, 236]
2025-03-11 18:34:09.760628 test begin: paddle.broadcast_to(Tensor([40968, 1, 236, 236],"bool"), list[8,8,236,236,], )

[torch error] paddle.broadcast_to(Tensor([40968, 1, 236, 236],"bool"), list[8,8,236,236,], ) 
 The expanded size of the tensor (8) must match the existing size (40968) at non-singleton dimension 0.  Target sizes: [8, 8, 236, 236].  Tensor sizes: [40968, 1, 236, 236]
2025-03-11 18:34:10.019805 test begin: paddle.broadcast_to(Tensor([409936, 1, 46, 121],"bool"), list[2,8,46,121,], )

[torch error] paddle.broadcast_to(Tensor([409936, 1, 46, 121],"bool"), list[2,8,46,121,], ) 
 The expanded size of the tensor (2) must match the existing size (409936) at non-singleton dimension 0.  Target sizes: [2, 8, 46, 121].  Tensor sizes: [409936, 1, 46, 121]
2025-03-11 18:34:10.518821 test begin: paddle.broadcast_to(Tensor([409936, 1, 46, 121],"bool"), list[8,8,46,121,], )

[torch error] paddle.broadcast_to(Tensor([409936, 1, 46, 121],"bool"), list[8,8,46,121,], ) 
 The expanded size of the tensor (8) must match the existing size (409936) at non-singleton dimension 0.  Target sizes: [8, 8, 46, 121].  Tensor sizes: [409936, 1, 46, 121]
2025-03-11 18:34:10.938108 test begin: paddle.broadcast_to(Tensor([410010, 1, 105, 53],"bool"), list[9,8,105,53,], )

[torch error] paddle.broadcast_to(Tensor([410010, 1, 105, 53],"bool"), list[9,8,105,53,], ) 
 The expanded size of the tensor (9) must match the existing size (410010) at non-singleton dimension 0.  Target sizes: [9, 8, 105, 53].  Tensor sizes: [410010, 1, 105, 53]
2025-03-11 18:34:11.207123 test begin: paddle.broadcast_to(Tensor([410010, 1, 53, 105],"bool"), list[10,8,53,105,], )

[torch error] paddle.broadcast_to(Tensor([410010, 1, 53, 105],"bool"), list[10,8,53,105,], ) 
 The expanded size of the tensor (10) must match the existing size (410010) at non-singleton dimension 0.  Target sizes: [10, 8, 53, 105].  Tensor sizes: [410010, 1, 53, 105]
2025-03-11 18:34:11.613709 test begin: paddle.broadcast_to(Tensor([410010, 1, 53, 105],"bool"), list[3,8,53,105,], )

[torch error] paddle.broadcast_to(Tensor([410010, 1, 53, 105],"bool"), list[3,8,53,105,], ) 
 The expanded size of the tensor (3) must match the existing size (410010) at non-singleton dimension 0.  Target sizes: [3, 8, 53, 105].  Tensor sizes: [410010, 1, 53, 105]
2025-03-11 18:34:12.188648 test begin: paddle.broadcast_to(Tensor([41043, 1, 213, 261],"bool"), list[1,8,213,261,], )

[torch error] paddle.broadcast_to(Tensor([41043, 1, 213, 261],"bool"), list[1,8,213,261,], ) 
 The expanded size of the tensor (1) must match the existing size (41043) at non-singleton dimension 0.  Target sizes: [1, 8, 213, 261].  Tensor sizes: [41043, 1, 213, 261]
2025-03-11 18:34:12.607643 test begin: paddle.broadcast_to(Tensor([410452, 1, 51, 109],"bool"), list[1,8,51,109,], )

[torch error] paddle.broadcast_to(Tensor([410452, 1, 51, 109],"bool"), list[1,8,51,109,], ) 
 The expanded size of the tensor (1) must match the existing size (410452) at non-singleton dimension 0.  Target sizes: [1, 8, 51, 109].  Tensor sizes: [410452, 1, 51, 109]
2025-03-11 18:34:13.082098 test begin: paddle.broadcast_to(Tensor([411044, 1, 91, 61],"bool"), list[1,8,91,61,], )

[torch error] paddle.broadcast_to(Tensor([411044, 1, 91, 61],"bool"), list[1,8,91,61,], ) 
 The expanded size of the tensor (1) must match the existing size (411044) at non-singleton dimension 0.  Target sizes: [1, 8, 91, 61].  Tensor sizes: [411044, 1, 91, 61]
2025-03-11 18:34:13.507804 test begin: paddle.broadcast_to(Tensor([411044, 1, 91, 61],"bool"), list[4,8,91,61,], )

[torch error] paddle.broadcast_to(Tensor([411044, 1, 91, 61],"bool"), list[4,8,91,61,], ) 
 The expanded size of the tensor (4) must match the existing size (411044) at non-singleton dimension 0.  Target sizes: [4, 8, 91, 61].  Tensor sizes: [411044, 1, 91, 61]
2025-03-11 18:34:13.769424 test begin: paddle.broadcast_to(Tensor([4111174, 1, 15, 37],"bool"), list[10,8,15,37,], )

[torch error] paddle.broadcast_to(Tensor([4111174, 1, 15, 37],"bool"), list[10,8,15,37,], ) 
 The expanded size of the tensor (10) must match the existing size (4111174) at non-singleton dimension 0.  Target sizes: [10, 8, 15, 37].  Tensor sizes: [4111174, 1, 15, 37]
2025-03-11 18:34:14.029592 test begin: paddle.broadcast_to(Tensor([4111174, 1, 15, 37],"bool"), list[4,8,15,37,], )

[torch error] paddle.broadcast_to(Tensor([4111174, 1, 15, 37],"bool"), list[4,8,15,37,], ) 
 The expanded size of the tensor (4) must match the existing size (4111174) at non-singleton dimension 0.  Target sizes: [4, 8, 15, 37].  Tensor sizes: [4111174, 1, 15, 37]
2025-03-11 18:34:14.289425 test begin: paddle.broadcast_to(Tensor([411266, 1, 76, 73],"bool"), list[1,8,76,73,], )

[torch error] paddle.broadcast_to(Tensor([411266, 1, 76, 73],"bool"), list[1,8,76,73,], ) 
 The expanded size of the tensor (1) must match the existing size (411266) at non-singleton dimension 0.  Target sizes: [1, 8, 76, 73].  Tensor sizes: [411266, 1, 76, 73]
2025-03-11 18:34:14.553465 test begin: paddle.broadcast_to(Tensor([411266, 1, 76, 73],"bool"), list[10,8,76,73,], )

[torch error] paddle.broadcast_to(Tensor([411266, 1, 76, 73],"bool"), list[10,8,76,73,], ) 
 The expanded size of the tensor (10) must match the existing size (411266) at non-singleton dimension 0.  Target sizes: [10, 8, 76, 73].  Tensor sizes: [411266, 1, 76, 73]
2025-03-11 18:34:14.816674 test begin: paddle.broadcast_to(Tensor([411340, 1, 43, 129],"bool"), list[10,8,43,129,], )

[torch error] paddle.broadcast_to(Tensor([411340, 1, 43, 129],"bool"), list[10,8,43,129,], ) 
 The expanded size of the tensor (10) must match the existing size (411340) at non-singleton dimension 0.  Target sizes: [10, 8, 43, 129].  Tensor sizes: [411340, 1, 43, 129]
2025-03-11 18:34:15.077742 test begin: paddle.broadcast_to(Tensor([411340, 1, 43, 129],"bool"), list[2,8,43,129,], )

[torch error] paddle.broadcast_to(Tensor([411340, 1, 43, 129],"bool"), list[2,8,43,129,], ) 
 The expanded size of the tensor (2) must match the existing size (411340) at non-singleton dimension 0.  Target sizes: [2, 8, 43, 129].  Tensor sizes: [411340, 1, 43, 129]
2025-03-11 18:34:15.570381 test begin: paddle.broadcast_to(Tensor([411563, 1, 72, 77],"bool"), list[4,8,72,77,], )

[torch error] paddle.broadcast_to(Tensor([411563, 1, 72, 77],"bool"), list[4,8,72,77,], ) 
 The expanded size of the tensor (4) must match the existing size (411563) at non-singleton dimension 0.  Target sizes: [4, 8, 72, 77].  Tensor sizes: [411563, 1, 72, 77]
2025-03-11 18:34:15.991350 test begin: paddle.broadcast_to(Tensor([41157, 1, 144, 385],"bool"), list[1,8,144,385,], )

[torch error] paddle.broadcast_to(Tensor([41157, 1, 144, 385],"bool"), list[1,8,144,385,], ) 
 The expanded size of the tensor (1) must match the existing size (41157) at non-singleton dimension 0.  Target sizes: [1, 8, 144, 385].  Tensor sizes: [41157, 1, 144, 385]
2025-03-11 18:34:16.257172 test begin: paddle.broadcast_to(Tensor([411637, 1, 23, 241],"bool"), list[1,8,23,241,], )

[torch error] paddle.broadcast_to(Tensor([411637, 1, 23, 241],"bool"), list[1,8,23,241,], ) 
 The expanded size of the tensor (1) must match the existing size (411637) at non-singleton dimension 0.  Target sizes: [1, 8, 23, 241].  Tensor sizes: [411637, 1, 23, 241]
2025-03-11 18:34:16.520795 test begin: paddle.broadcast_to(Tensor([412083, 1, 113, 49],"bool"), list[10,8,113,49,], )

[torch error] paddle.broadcast_to(Tensor([412083, 1, 113, 49],"bool"), list[10,8,113,49,], ) 
 The expanded size of the tensor (10) must match the existing size (412083) at non-singleton dimension 0.  Target sizes: [10, 8, 113, 49].  Tensor sizes: [412083, 1, 113, 49]
2025-03-11 18:34:16.779679 test begin: paddle.broadcast_to(Tensor([412083, 1, 49, 113],"bool"), list[1,8,49,113,], )

[torch error] paddle.broadcast_to(Tensor([412083, 1, 49, 113],"bool"), list[1,8,49,113,], ) 
 The expanded size of the tensor (1) must match the existing size (412083) at non-singleton dimension 0.  Target sizes: [1, 8, 49, 113].  Tensor sizes: [412083, 1, 49, 113]
2025-03-11 18:34:17.052002 test begin: paddle.broadcast_to(Tensor([412083, 1, 49, 113],"bool"), list[8,8,49,113,], )

[torch error] paddle.broadcast_to(Tensor([412083, 1, 49, 113],"bool"), list[8,8,49,113,], ) 
 The expanded size of the tensor (8) must match the existing size (412083) at non-singleton dimension 0.  Target sizes: [8, 8, 49, 113].  Tensor sizes: [412083, 1, 49, 113]
2025-03-11 18:34:17.558976 test begin: paddle.broadcast_to(Tensor([41237, 1, 212, 261],"bool"), list[1,8,212,261,], )

[torch error] paddle.broadcast_to(Tensor([41237, 1, 212, 261],"bool"), list[1,8,212,261,], ) 
 The expanded size of the tensor (1) must match the existing size (41237) at non-singleton dimension 0.  Target sizes: [1, 8, 212, 261].  Tensor sizes: [41237, 1, 212, 261]
2025-03-11 18:34:17.981278 test begin: paddle.broadcast_to(Tensor([412679, 1, 57, 97],"bool"), list[1,8,57,97,], )

[torch error] paddle.broadcast_to(Tensor([412679, 1, 57, 97],"bool"), list[1,8,57,97,], ) 
 The expanded size of the tensor (1) must match the existing size (412679) at non-singleton dimension 0.  Target sizes: [1, 8, 57, 97].  Tensor sizes: [412679, 1, 57, 97]
2025-03-11 18:34:18.250456 test begin: paddle.broadcast_to(Tensor([412679, 1, 57, 97],"bool"), list[10,8,57,97,], )

[torch error] paddle.broadcast_to(Tensor([412679, 1, 57, 97],"bool"), list[10,8,57,97,], ) 
 The expanded size of the tensor (10) must match the existing size (412679) at non-singleton dimension 0.  Target sizes: [10, 8, 57, 97].  Tensor sizes: [412679, 1, 57, 97]
2025-03-11 18:34:18.521002 test begin: paddle.broadcast_to(Tensor([412679, 1, 57, 97],"bool"), list[6,8,57,97,], )

[torch error] paddle.broadcast_to(Tensor([412679, 1, 57, 97],"bool"), list[6,8,57,97,], ) 
 The expanded size of the tensor (6) must match the existing size (412679) at non-singleton dimension 0.  Target sizes: [6, 8, 57, 97].  Tensor sizes: [412679, 1, 57, 97]
2025-03-11 18:34:18.789738 test begin: paddle.broadcast_to(Tensor([412679, 1, 97, 57],"bool"), list[10,8,97,57,], )

[torch error] paddle.broadcast_to(Tensor([412679, 1, 97, 57],"bool"), list[10,8,97,57,], ) 
 The expanded size of the tensor (10) must match the existing size (412679) at non-singleton dimension 0.  Target sizes: [10, 8, 97, 57].  Tensor sizes: [412679, 1, 97, 57]
2025-03-11 18:34:19.049028 test begin: paddle.broadcast_to(Tensor([412679, 1, 97, 57],"bool"), list[6,8,97,57,], )

[torch error] paddle.broadcast_to(Tensor([412679, 1, 97, 57],"bool"), list[6,8,97,57,], ) 
 The expanded size of the tensor (6) must match the existing size (412679) at non-singleton dimension 0.  Target sizes: [6, 8, 97, 57].  Tensor sizes: [412679, 1, 97, 57]
2025-03-11 18:34:19.311904 test begin: paddle.broadcast_to(Tensor([412978, 1, 65, 85],"bool"), list[1,8,65,85,], )

[torch error] paddle.broadcast_to(Tensor([412978, 1, 65, 85],"bool"), list[1,8,65,85,], ) 
 The expanded size of the tensor (1) must match the existing size (412978) at non-singleton dimension 0.  Target sizes: [1, 8, 65, 85].  Tensor sizes: [412978, 1, 65, 85]
2025-03-11 18:34:19.575151 test begin: paddle.broadcast_to(Tensor([412978, 1, 65, 85],"bool"), list[2,8,65,85,], )

[torch error] paddle.broadcast_to(Tensor([412978, 1, 65, 85],"bool"), list[2,8,65,85,], ) 
 The expanded size of the tensor (2) must match the existing size (412978) at non-singleton dimension 0.  Target sizes: [2, 8, 65, 85].  Tensor sizes: [412978, 1, 65, 85]
2025-03-11 18:34:19.836207 test begin: paddle.broadcast_to(Tensor([412978, 1, 85, 65],"bool"), list[1,8,85,65,], )

[torch error] paddle.broadcast_to(Tensor([412978, 1, 85, 65],"bool"), list[1,8,85,65,], ) 
 The expanded size of the tensor (1) must match the existing size (412978) at non-singleton dimension 0.  Target sizes: [1, 8, 85, 65].  Tensor sizes: [412978, 1, 85, 65]
2025-03-11 18:34:20.097509 test begin: paddle.broadcast_to(Tensor([412978, 1, 85, 65],"bool"), list[10,8,85,65,], )

[torch error] paddle.broadcast_to(Tensor([412978, 1, 85, 65],"bool"), list[10,8,85,65,], ) 
 The expanded size of the tensor (10) must match the existing size (412978) at non-singleton dimension 0.  Target sizes: [10, 8, 85, 65].  Tensor sizes: [412978, 1, 85, 65]
2025-03-11 18:34:20.600486 test begin: paddle.broadcast_to(Tensor([412978, 1, 85, 65],"bool"), list[5,8,85,65,], )

[torch error] paddle.broadcast_to(Tensor([412978, 1, 85, 65],"bool"), list[5,8,85,65,], ) 
 The expanded size of the tensor (5) must match the existing size (412978) at non-singleton dimension 0.  Target sizes: [5, 8, 85, 65].  Tensor sizes: [412978, 1, 85, 65]
2025-03-11 18:34:21.120565 test begin: paddle.broadcast_to(Tensor([41317, 1, 235, 235],"bool"), list[1,8,235,235,], )

[torch error] paddle.broadcast_to(Tensor([41317, 1, 235, 235],"bool"), list[1,8,235,235,], ) 
 The expanded size of the tensor (1) must match the existing size (41317) at non-singleton dimension 0.  Target sizes: [1, 8, 235, 235].  Tensor sizes: [41317, 1, 235, 235]
2025-03-11 18:34:21.771059 test begin: paddle.broadcast_to(Tensor([41317, 1, 235, 235],"bool"), list[8,8,235,235,], )

[torch error] paddle.broadcast_to(Tensor([41317, 1, 235, 235],"bool"), list[8,8,235,235,], ) 
 The expanded size of the tensor (8) must match the existing size (41317) at non-singleton dimension 0.  Target sizes: [8, 8, 235, 235].  Tensor sizes: [41317, 1, 235, 235]
2025-03-11 18:34:22.247076 test begin: paddle.broadcast_to(Tensor([4133517, 1, 8, 69],"bool"), list[1,8,8,69,], )

[torch error] paddle.broadcast_to(Tensor([4133517, 1, 8, 69],"bool"), list[1,8,8,69,], ) 
 The expanded size of the tensor (1) must match the existing size (4133517) at non-singleton dimension 0.  Target sizes: [1, 8, 8, 69].  Tensor sizes: [4133517, 1, 8, 69]
2025-03-11 18:34:22.855315 test begin: paddle.broadcast_to(Tensor([413352, 1, 80, 69],"bool"), list[1,8,80,69,], )

[torch error] paddle.broadcast_to(Tensor([413352, 1, 80, 69],"bool"), list[1,8,80,69,], ) 
 The expanded size of the tensor (1) must match the existing size (413352) at non-singleton dimension 0.  Target sizes: [1, 8, 80, 69].  Tensor sizes: [413352, 1, 80, 69]
2025-03-11 18:34:23.618549 test begin: paddle.broadcast_to(Tensor([413502, 1, 62, 89],"bool"), list[10,8,62,89,], )

[torch error] paddle.broadcast_to(Tensor([413502, 1, 62, 89],"bool"), list[10,8,62,89,], ) 
 The expanded size of the tensor (10) must match the existing size (413502) at non-singleton dimension 0.  Target sizes: [10, 8, 62, 89].  Tensor sizes: [413502, 1, 62, 89]
2025-03-11 18:34:24.036601 test begin: paddle.broadcast_to(Tensor([413502, 1, 62, 89],"bool"), list[9,8,62,89,], )

[torch error] paddle.broadcast_to(Tensor([413502, 1, 62, 89],"bool"), list[9,8,62,89,], ) 
 The expanded size of the tensor (9) must match the existing size (413502) at non-singleton dimension 0.  Target sizes: [9, 8, 62, 89].  Tensor sizes: [413502, 1, 62, 89]
2025-03-11 18:34:24.537517 test begin: paddle.broadcast_to(Tensor([413952, 1, 104, 53],"bool"), list[9,8,104,53,], )

[torch error] paddle.broadcast_to(Tensor([413952, 1, 104, 53],"bool"), list[9,8,104,53,], ) 
 The expanded size of the tensor (9) must match the existing size (413952) at non-singleton dimension 0.  Target sizes: [9, 8, 104, 53].  Tensor sizes: [413952, 1, 104, 53]
2025-03-11 18:34:24.964659 test begin: paddle.broadcast_to(Tensor([4141019, 1, 19, 29],"bool"), list[10,8,19,29,], )

[torch error] paddle.broadcast_to(Tensor([4141019, 1, 19, 29],"bool"), list[10,8,19,29,], ) 
 The expanded size of the tensor (10) must match the existing size (4141019) at non-singleton dimension 0.  Target sizes: [10, 8, 19, 29].  Tensor sizes: [4141019, 1, 19, 29]
2025-03-11 18:34:25.225593 test begin: paddle.broadcast_to(Tensor([414102, 1, 38, 145],"bool"), list[1,8,38,145,], )

[torch error] paddle.broadcast_to(Tensor([414102, 1, 38, 145],"bool"), list[1,8,38,145,], ) 
 The expanded size of the tensor (1) must match the existing size (414102) at non-singleton dimension 0.  Target sizes: [1, 8, 38, 145].  Tensor sizes: [414102, 1, 38, 145]
2025-03-11 18:34:25.484808 test begin: paddle.broadcast_to(Tensor([414102, 1, 38, 145],"bool"), list[5,8,38,145,], )

[torch error] paddle.broadcast_to(Tensor([414102, 1, 38, 145],"bool"), list[5,8,38,145,], ) 
 The expanded size of the tensor (5) must match the existing size (414102) at non-singleton dimension 0.  Target sizes: [5, 8, 38, 145].  Tensor sizes: [414102, 1, 38, 145]
2025-03-11 18:34:25.746273 test begin: paddle.broadcast_to(Tensor([414253, 1, 36, 153],"bool"), list[7,8,36,153,], )

[torch error] paddle.broadcast_to(Tensor([414253, 1, 36, 153],"bool"), list[7,8,36,153,], ) 
 The expanded size of the tensor (7) must match the existing size (414253) at non-singleton dimension 0.  Target sizes: [7, 8, 36, 153].  Tensor sizes: [414253, 1, 36, 153]
2025-03-11 18:34:26.018161 test begin: paddle.broadcast_to(Tensor([414253, 1, 68, 81],"bool"), list[10,8,68,81,], )

[torch error] paddle.broadcast_to(Tensor([414253, 1, 68, 81],"bool"), list[10,8,68,81,], ) 
 The expanded size of the tensor (10) must match the existing size (414253) at non-singleton dimension 0.  Target sizes: [10, 8, 68, 81].  Tensor sizes: [414253, 1, 68, 81]
2025-03-11 18:34:26.282340 test begin: paddle.broadcast_to(Tensor([414253, 1, 68, 81],"bool"), list[2,8,68,81,], )

[torch error] paddle.broadcast_to(Tensor([414253, 1, 68, 81],"bool"), list[2,8,68,81,], ) 
 The expanded size of the tensor (2) must match the existing size (414253) at non-singleton dimension 0.  Target sizes: [2, 8, 68, 81].  Tensor sizes: [414253, 1, 68, 81]
2025-03-11 18:34:26.553343 test begin: paddle.broadcast_to(Tensor([41432, 1, 211, 261],"bool"), list[1,8,211,261,], )

[torch error] paddle.broadcast_to(Tensor([41432, 1, 211, 261],"bool"), list[1,8,211,261,], ) 
 The expanded size of the tensor (1) must match the existing size (41432) at non-singleton dimension 0.  Target sizes: [1, 8, 211, 261].  Tensor sizes: [41432, 1, 211, 261]
2025-03-11 18:34:26.819294 test begin: paddle.broadcast_to(Tensor([41445, 1, 143, 385],"bool"), list[1,8,143,385,], )

[torch error] paddle.broadcast_to(Tensor([41445, 1, 143, 385],"bool"), list[1,8,143,385,], ) 
 The expanded size of the tensor (1) must match the existing size (41445) at non-singleton dimension 0.  Target sizes: [1, 8, 143, 385].  Tensor sizes: [41445, 1, 143, 385]
2025-03-11 18:34:27.085966 test begin: paddle.broadcast_to(Tensor([4148548, 1, 22, 25],"bool"), list[10,8,22,25,], )

[torch error] paddle.broadcast_to(Tensor([4148548, 1, 22, 25],"bool"), list[10,8,22,25,], ) 
 The expanded size of the tensor (10) must match the existing size (4148548) at non-singleton dimension 0.  Target sizes: [10, 8, 22, 25].  Tensor sizes: [4148548, 1, 22, 25]
2025-03-11 18:34:27.353938 test begin: paddle.broadcast_to(Tensor([4148548, 1, 22, 25],"bool"), list[4,8,22,25,], )

[torch error] paddle.broadcast_to(Tensor([4148548, 1, 22, 25],"bool"), list[4,8,22,25,], ) 
 The expanded size of the tensor (4) must match the existing size (4148548) at non-singleton dimension 0.  Target sizes: [4, 8, 22, 25].  Tensor sizes: [4148548, 1, 22, 25]
2025-03-11 18:34:27.625113 test begin: paddle.broadcast_to(Tensor([4148548, 1, 22, 25],"bool"), list[6,8,22,25,], )

[torch error] paddle.broadcast_to(Tensor([4148548, 1, 22, 25],"bool"), list[6,8,22,25,], ) 
 The expanded size of the tensor (6) must match the existing size (4148548) at non-singleton dimension 0.  Target sizes: [6, 8, 22, 25].  Tensor sizes: [4148548, 1, 22, 25]
2025-03-11 18:34:28.055889 test begin: paddle.broadcast_to(Tensor([41485480, 1, 11, 5],"bool"), list[4,8,11,5,], )

[torch error] paddle.broadcast_to(Tensor([41485480, 1, 11, 5],"bool"), list[4,8,11,5,], ) 
 The expanded size of the tensor (4) must match the existing size (41485480) at non-singleton dimension 0.  Target sizes: [4, 8, 11, 5].  Tensor sizes: [41485480, 1, 11, 5]
2025-03-11 18:34:28.581535 test begin: paddle.broadcast_to(Tensor([41485480, 55, 1],"bool"), list[1,55,30324,], )

[torch error] paddle.broadcast_to(Tensor([41485480, 55, 1],"bool"), list[1,55,30324,], ) 
 The expanded size of the tensor (1) must match the existing size (41485480) at non-singleton dimension 0.  Target sizes: [1, 55, 30324].  Tensor sizes: [41485480, 55, 1]
2025-03-11 18:34:28.971710 test begin: paddle.broadcast_to(Tensor([41485480, 55, 1],"bool"), list[1,55,44436,], )

[torch error] paddle.broadcast_to(Tensor([41485480, 55, 1],"bool"), list[1,55,44436,], ) 
 The expanded size of the tensor (1) must match the existing size (41485480) at non-singleton dimension 0.  Target sizes: [1, 55, 44436].  Tensor sizes: [41485480, 55, 1]
2025-03-11 18:34:29.558065 test begin: paddle.broadcast_to(Tensor([41485480, 55, 1],"bool"), list[1,55,61236,], )

[torch error] paddle.broadcast_to(Tensor([41485480, 55, 1],"bool"), list[1,55,61236,], ) 
 The expanded size of the tensor (1) must match the existing size (41485480) at non-singleton dimension 0.  Target sizes: [1, 55, 61236].  Tensor sizes: [41485480, 55, 1]
2025-03-11 18:34:29.985875 test begin: paddle.broadcast_to(Tensor([41485480, 55, 1],"bool"), list[1,55,70644,], )

[torch error] paddle.broadcast_to(Tensor([41485480, 55, 1],"bool"), list[1,55,70644,], ) 
 The expanded size of the tensor (1) must match the existing size (41485480) at non-singleton dimension 0.  Target sizes: [1, 55, 70644].  Tensor sizes: [41485480, 55, 1]
2025-03-11 18:34:30.508492 test begin: paddle.broadcast_to(Tensor([41485480, 55, 1],"bool"), list[2,55,3024,], )

[torch error] paddle.broadcast_to(Tensor([41485480, 55, 1],"bool"), list[2,55,3024,], ) 
 The expanded size of the tensor (2) must match the existing size (41485480) at non-singleton dimension 0.  Target sizes: [2, 55, 3024].  Tensor sizes: [41485480, 55, 1]
2025-03-11 18:34:31.163837 test begin: paddle.broadcast_to(Tensor([41485480, 55, 1],"bool"), list[2,55,3549,], )

[torch error] paddle.broadcast_to(Tensor([41485480, 55, 1],"bool"), list[2,55,3549,], ) 
 The expanded size of the tensor (2) must match the existing size (41485480) at non-singleton dimension 0.  Target sizes: [2, 55, 3549].  Tensor sizes: [41485480, 55, 1]
2025-03-11 18:34:31.585070 test begin: paddle.broadcast_to(Tensor([41485480, 55, 1],"bool"), list[2,55,4725,], )

[torch error] paddle.broadcast_to(Tensor([41485480, 55, 1],"bool"), list[2,55,4725,], ) 
 The expanded size of the tensor (2) must match the existing size (41485480) at non-singleton dimension 0.  Target sizes: [2, 55, 4725].  Tensor sizes: [41485480, 55, 1]
2025-03-11 18:34:32.154582 test begin: paddle.broadcast_to(Tensor([41485480, 55, 1],"bool"), list[2,55,5376,], )

[torch error] paddle.broadcast_to(Tensor([41485480, 55, 1],"bool"), list[2,55,5376,], ) 
 The expanded size of the tensor (2) must match the existing size (41485480) at non-singleton dimension 0.  Target sizes: [2, 55, 5376].  Tensor sizes: [41485480, 55, 1]
2025-03-11 18:34:32.830285 test begin: paddle.broadcast_to(Tensor([41485480, 55, 1],"bool"), list[2,55,6069,], )

[torch error] paddle.broadcast_to(Tensor([41485480, 55, 1],"bool"), list[2,55,6069,], ) 
 The expanded size of the tensor (2) must match the existing size (41485480) at non-singleton dimension 0.  Target sizes: [2, 55, 6069].  Tensor sizes: [41485480, 55, 1]
2025-03-11 18:34:33.241511 test begin: paddle.broadcast_to(Tensor([41485480, 55, 1],"bool"), list[2,55,6804,], )

[torch error] paddle.broadcast_to(Tensor([41485480, 55, 1],"bool"), list[2,55,6804,], ) 
 The expanded size of the tensor (2) must match the existing size (41485480) at non-singleton dimension 0.  Target sizes: [2, 55, 6804].  Tensor sizes: [41485480, 55, 1]
2025-03-11 18:34:33.607393 test begin: paddle.broadcast_to(Tensor([41485480, 55, 1],"bool"), list[2,55,7581,], )

[torch error] paddle.broadcast_to(Tensor([41485480, 55, 1],"bool"), list[2,55,7581,], ) 
 The expanded size of the tensor (2) must match the existing size (41485480) at non-singleton dimension 0.  Target sizes: [2, 55, 7581].  Tensor sizes: [41485480, 55, 1]
2025-03-11 18:34:34.261844 test begin: paddle.broadcast_to(Tensor([41485480, 55, 1],"bool"), list[2,55,8400,], )

[torch error] paddle.broadcast_to(Tensor([41485480, 55, 1],"bool"), list[2,55,8400,], ) 
 The expanded size of the tensor (2) must match the existing size (41485480) at non-singleton dimension 0.  Target sizes: [2, 55, 8400].  Tensor sizes: [41485480, 55, 1]
2025-03-11 18:34:34.671841 test begin: paddle.broadcast_to(Tensor([415157, 1, 24, 229],"bool"), list[1,8,24,229,], )

[torch error] paddle.broadcast_to(Tensor([415157, 1, 24, 229],"bool"), list[1,8,24,229,], ) 
 The expanded size of the tensor (1) must match the existing size (415157) at non-singleton dimension 0.  Target sizes: [1, 8, 24, 229].  Tensor sizes: [415157, 1, 24, 229]
2025-03-11 18:34:35.233374 test begin: paddle.broadcast_to(Tensor([415233, 1, 35, 157],"bool"), list[1,8,35,157,], )

[torch error] paddle.broadcast_to(Tensor([415233, 1, 35, 157],"bool"), list[1,8,35,157,], ) 
 The expanded size of the tensor (1) must match the existing size (415233) at non-singleton dimension 0.  Target sizes: [1, 8, 35, 157].  Tensor sizes: [415233, 1, 35, 157]
2025-03-11 18:34:35.919529 test begin: paddle.broadcast_to(Tensor([4156105, 1, 9, 61],"bool"), list[1,8,9,61,], )

[torch error] paddle.broadcast_to(Tensor([4156105, 1, 9, 61],"bool"), list[1,8,9,61,], ) 
 The expanded size of the tensor (1) must match the existing size (4156105) at non-singleton dimension 0.  Target sizes: [1, 8, 9, 61].  Tensor sizes: [4156105, 1, 9, 61]
2025-03-11 18:34:36.590123 test begin: paddle.broadcast_to(Tensor([4156105, 1, 9, 61],"bool"), list[4,8,9,61,], )

[torch error] paddle.broadcast_to(Tensor([4156105, 1, 9, 61],"bool"), list[4,8,9,61,], ) 
 The expanded size of the tensor (4) must match the existing size (4156105) at non-singleton dimension 0.  Target sizes: [4, 8, 9, 61].  Tensor sizes: [4156105, 1, 9, 61]
2025-03-11 18:34:37.271941 test begin: paddle.broadcast_to(Tensor([415611, 1, 90, 61],"bool"), list[1,8,90,61,], )

[torch error] paddle.broadcast_to(Tensor([415611, 1, 90, 61],"bool"), list[1,8,90,61,], ) 
 The expanded size of the tensor (1) must match the existing size (415611) at non-singleton dimension 0.  Target sizes: [1, 8, 90, 61].  Tensor sizes: [415611, 1, 90, 61]
2025-03-11 18:34:37.693315 test begin: paddle.broadcast_to(Tensor([415611, 1, 90, 61],"bool"), list[4,8,90,61,], )

[torch error] paddle.broadcast_to(Tensor([415611, 1, 90, 61],"bool"), list[4,8,90,61,], ) 
 The expanded size of the tensor (4) must match the existing size (415611) at non-singleton dimension 0.  Target sizes: [4, 8, 90, 61].  Tensor sizes: [415611, 1, 90, 61]
2025-03-11 18:34:38.297232 test begin: paddle.broadcast_to(Tensor([415762, 1, 112, 49],"bool"), list[10,8,112,49,], )

[torch error] paddle.broadcast_to(Tensor([415762, 1, 112, 49],"bool"), list[10,8,112,49,], ) 
 The expanded size of the tensor (10) must match the existing size (415762) at non-singleton dimension 0.  Target sizes: [10, 8, 112, 49].  Tensor sizes: [415762, 1, 112, 49]
2025-03-11 18:34:38.983129 test begin: paddle.broadcast_to(Tensor([415762, 1, 112, 49],"bool"), list[30,8,112,49,], )

[torch error] paddle.broadcast_to(Tensor([415762, 1, 112, 49],"bool"), list[30,8,112,49,], ) 
 The expanded size of the tensor (30) must match the existing size (415762) at non-singleton dimension 0.  Target sizes: [30, 8, 112, 49].  Tensor sizes: [415762, 1, 112, 49]
2025-03-11 18:34:39.398327 test begin: paddle.broadcast_to(Tensor([416293, 1, 21, 261],"bool"), list[1,8,21,261,], )

[torch error] paddle.broadcast_to(Tensor([416293, 1, 21, 261],"bool"), list[1,8,21,261,], ) 
 The expanded size of the tensor (1) must match the existing size (416293) at non-singleton dimension 0.  Target sizes: [1, 8, 21, 261].  Tensor sizes: [416293, 1, 21, 261]
2025-03-11 18:34:39.971867 test begin: paddle.broadcast_to(Tensor([41630, 1, 210, 261],"bool"), list[1,8,210,261,], )

[torch error] paddle.broadcast_to(Tensor([41630, 1, 210, 261],"bool"), list[1,8,210,261,], ) 
 The expanded size of the tensor (1) must match the existing size (41630) at non-singleton dimension 0.  Target sizes: [1, 8, 210, 261].  Tensor sizes: [41630, 1, 210, 261]
2025-03-11 18:34:40.385442 test begin: paddle.broadcast_to(Tensor([4163689, 1, 4, 137],"bool"), list[10,8,4,137,], )

[torch error] paddle.broadcast_to(Tensor([4163689, 1, 4, 137],"bool"), list[10,8,4,137,], ) 
 The expanded size of the tensor (10) must match the existing size (4163689) at non-singleton dimension 0.  Target sizes: [10, 8, 4, 137].  Tensor sizes: [4163689, 1, 4, 137]
2025-03-11 18:34:40.740791 test begin: paddle.broadcast_to(Tensor([416369, 1, 40, 137],"bool"), list[10,8,40,137,], )

[torch error] paddle.broadcast_to(Tensor([416369, 1, 40, 137],"bool"), list[10,8,40,137,], ) 
 The expanded size of the tensor (10) must match the existing size (416369) at non-singleton dimension 0.  Target sizes: [10, 8, 40, 137].  Tensor sizes: [416369, 1, 40, 137]
2025-03-11 18:34:41.165685 test begin: paddle.broadcast_to(Tensor([416674, 1, 74, 74],"bool"), list[1,8,74,74,], )

[torch error] paddle.broadcast_to(Tensor([416674, 1, 74, 74],"bool"), list[1,8,74,74,], ) 
 The expanded size of the tensor (1) must match the existing size (416674) at non-singleton dimension 0.  Target sizes: [1, 8, 74, 74].  Tensor sizes: [416674, 1, 74, 74]
2025-03-11 18:34:41.533941 test begin: paddle.broadcast_to(Tensor([416674, 1, 74, 74],"bool"), list[10,8,74,74,], )

[torch error] paddle.broadcast_to(Tensor([416674, 1, 74, 74],"bool"), list[10,8,74,74,], ) 
 The expanded size of the tensor (10) must match the existing size (416674) at non-singleton dimension 0.  Target sizes: [10, 8, 74, 74].  Tensor sizes: [416674, 1, 74, 74]
2025-03-11 18:34:41.953049 test begin: paddle.broadcast_to(Tensor([416674, 1, 74, 74],"bool"), list[2,8,74,74,], )

[torch error] paddle.broadcast_to(Tensor([416674, 1, 74, 74],"bool"), list[2,8,74,74,], ) 
 The expanded size of the tensor (2) must match the existing size (416674) at non-singleton dimension 0.  Target sizes: [2, 8, 74, 74].  Tensor sizes: [416674, 1, 74, 74]
2025-03-11 18:34:42.297859 test begin: paddle.broadcast_to(Tensor([416674, 1, 74, 74],"bool"), list[3,8,74,74,], )

[torch error] paddle.broadcast_to(Tensor([416674, 1, 74, 74],"bool"), list[3,8,74,74,], ) 
 The expanded size of the tensor (3) must match the existing size (416674) at non-singleton dimension 0.  Target sizes: [3, 8, 74, 74].  Tensor sizes: [416674, 1, 74, 74]
2025-03-11 18:34:42.982639 test begin: paddle.broadcast_to(Tensor([416674, 1, 74, 74],"bool"), list[30,8,74,74,], )

[torch error] paddle.broadcast_to(Tensor([416674, 1, 74, 74],"bool"), list[30,8,74,74,], ) 
 The expanded size of the tensor (30) must match the existing size (416674) at non-singleton dimension 0.  Target sizes: [30, 8, 74, 74].  Tensor sizes: [416674, 1, 74, 74]
2025-03-11 18:34:43.667534 test begin: paddle.broadcast_to(Tensor([416674, 1, 74, 74],"bool"), list[4,8,74,74,], )

[torch error] paddle.broadcast_to(Tensor([416674, 1, 74, 74],"bool"), list[4,8,74,74,], ) 
 The expanded size of the tensor (4) must match the existing size (416674) at non-singleton dimension 0.  Target sizes: [4, 8, 74, 74].  Tensor sizes: [416674, 1, 74, 74]
2025-03-11 18:34:44.083606 test begin: paddle.broadcast_to(Tensor([416674, 1, 74, 74],"bool"), list[5,8,74,74,], )

[torch error] paddle.broadcast_to(Tensor([416674, 1, 74, 74],"bool"), list[5,8,74,74,], ) 
 The expanded size of the tensor (5) must match the existing size (416674) at non-singleton dimension 0.  Target sizes: [5, 8, 74, 74].  Tensor sizes: [416674, 1, 74, 74]
2025-03-11 18:34:44.647162 test begin: paddle.broadcast_to(Tensor([416674, 1, 74, 74],"bool"), list[6,8,74,74,], )

[torch error] paddle.broadcast_to(Tensor([416674, 1, 74, 74],"bool"), list[6,8,74,74,], ) 
 The expanded size of the tensor (6) must match the existing size (416674) at non-singleton dimension 0.  Target sizes: [6, 8, 74, 74].  Tensor sizes: [416674, 1, 74, 74]
2025-03-11 18:34:45.330109 test begin: paddle.broadcast_to(Tensor([416674, 1, 74, 74],"bool"), list[7,8,74,74,], )

[torch error] paddle.broadcast_to(Tensor([416674, 1, 74, 74],"bool"), list[7,8,74,74,], ) 
 The expanded size of the tensor (7) must match the existing size (416674) at non-singleton dimension 0.  Target sizes: [7, 8, 74, 74].  Tensor sizes: [416674, 1, 74, 74]
2025-03-11 18:34:45.750533 test begin: paddle.broadcast_to(Tensor([416674, 1, 74, 74],"bool"), list[8,8,74,74,], )

[torch error] paddle.broadcast_to(Tensor([416674, 1, 74, 74],"bool"), list[8,8,74,74,], ) 
 The expanded size of the tensor (8) must match the existing size (416674) at non-singleton dimension 0.  Target sizes: [8, 8, 74, 74].  Tensor sizes: [416674, 1, 74, 74]
2025-03-11 18:34:46.337958 test begin: paddle.broadcast_to(Tensor([416674, 1, 74, 74],"bool"), list[9,8,74,74,], )

[torch error] paddle.broadcast_to(Tensor([416674, 1, 74, 74],"bool"), list[9,8,74,74,], ) 
 The expanded size of the tensor (9) must match the existing size (416674) at non-singleton dimension 0.  Target sizes: [9, 8, 74, 74].  Tensor sizes: [416674, 1, 74, 74]
2025-03-11 18:34:46.774057 test begin: paddle.broadcast_to(Tensor([41671, 1, 234, 234],"bool"), list[1,8,234,234,], )

[torch error] paddle.broadcast_to(Tensor([41671, 1, 234, 234],"bool"), list[1,8,234,234,], ) 
 The expanded size of the tensor (1) must match the existing size (41671) at non-singleton dimension 0.  Target sizes: [1, 8, 234, 234].  Tensor sizes: [41671, 1, 234, 234]
2025-03-11 18:34:47.139300 test begin: paddle.broadcast_to(Tensor([41671, 1, 234, 234],"bool"), list[8,8,234,234,], )

[torch error] paddle.broadcast_to(Tensor([41671, 1, 234, 234],"bool"), list[8,8,234,234,], ) 
 The expanded size of the tensor (8) must match the existing size (41671) at non-singleton dimension 0.  Target sizes: [8, 8, 234, 234].  Tensor sizes: [41671, 1, 234, 234]
2025-03-11 18:34:47.474701 test begin: paddle.broadcast_to(Tensor([416750, 1, 75, 73],"bool"), list[1,8,75,73,], )

[torch error] paddle.broadcast_to(Tensor([416750, 1, 75, 73],"bool"), list[1,8,75,73,], ) 
 The expanded size of the tensor (1) must match the existing size (416750) at non-singleton dimension 0.  Target sizes: [1, 8, 75, 73].  Tensor sizes: [416750, 1, 75, 73]
2025-03-11 18:34:47.882697 test begin: paddle.broadcast_to(Tensor([416750, 1, 75, 73],"bool"), list[10,8,75,73,], )

[torch error] paddle.broadcast_to(Tensor([416750, 1, 75, 73],"bool"), list[10,8,75,73,], ) 
 The expanded size of the tensor (10) must match the existing size (416750) at non-singleton dimension 0.  Target sizes: [10, 8, 75, 73].  Tensor sizes: [416750, 1, 75, 73]
2025-03-11 18:34:48.575652 test begin: paddle.broadcast_to(Tensor([416826, 1, 34, 161],"bool"), list[2,8,34,161,], )

[torch error] paddle.broadcast_to(Tensor([416826, 1, 34, 161],"bool"), list[2,8,34,161,], ) 
 The expanded size of the tensor (2) must match the existing size (416826) at non-singleton dimension 0.  Target sizes: [2, 8, 34, 161].  Tensor sizes: [416826, 1, 34, 161]
2025-03-11 18:34:49.077074 test begin: paddle.broadcast_to(Tensor([416978, 1, 96, 57],"bool"), list[10,8,96,57,], )

[torch error] paddle.broadcast_to(Tensor([416978, 1, 96, 57],"bool"), list[10,8,96,57,], ) 
 The expanded size of the tensor (10) must match the existing size (416978) at non-singleton dimension 0.  Target sizes: [10, 8, 96, 57].  Tensor sizes: [416978, 1, 96, 57]
2025-03-11 18:34:49.761397 test begin: paddle.broadcast_to(Tensor([416978, 1, 96, 57],"bool"), list[6,8,96,57,], )

[torch error] paddle.broadcast_to(Tensor([416978, 1, 96, 57],"bool"), list[6,8,96,57,], ) 
 The expanded size of the tensor (6) must match the existing size (416978) at non-singleton dimension 0.  Target sizes: [6, 8, 96, 57].  Tensor sizes: [416978, 1, 96, 57]
2025-03-11 18:34:50.413162 test begin: paddle.broadcast_to(Tensor([417359, 1, 71, 77],"bool"), list[4,8,71,77,], )

[torch error] paddle.broadcast_to(Tensor([417359, 1, 71, 77],"bool"), list[4,8,71,77,], ) 
 The expanded size of the tensor (4) must match the existing size (417359) at non-singleton dimension 0.  Target sizes: [4, 8, 71, 77].  Tensor sizes: [417359, 1, 71, 77]
2025-03-11 18:34:50.829219 test begin: paddle.broadcast_to(Tensor([41736, 1, 142, 385],"bool"), list[1,8,142,385,], )

[torch error] paddle.broadcast_to(Tensor([41736, 1, 142, 385],"bool"), list[1,8,142,385,], ) 
 The expanded size of the tensor (1) must match the existing size (41736) at non-singleton dimension 0.  Target sizes: [1, 8, 142, 385].  Tensor sizes: [41736, 1, 142, 385]
2025-03-11 18:34:51.422285 test begin: paddle.broadcast_to(Tensor([4178941, 1, 26, 21],"bool"), list[10,8,26,21,], )

[torch error] paddle.broadcast_to(Tensor([4178941, 1, 26, 21],"bool"), list[10,8,26,21,], ) 
 The expanded size of the tensor (10) must match the existing size (4178941) at non-singleton dimension 0.  Target sizes: [10, 8, 26, 21].  Tensor sizes: [4178941, 1, 26, 21]
2025-03-11 18:34:52.098361 test begin: paddle.broadcast_to(Tensor([4178941, 1, 26, 21],"bool"), list[3,8,26,21,], )

[torch error] paddle.broadcast_to(Tensor([4178941, 1, 26, 21],"bool"), list[3,8,26,21,], ) 
 The expanded size of the tensor (3) must match the existing size (4178941) at non-singleton dimension 0.  Target sizes: [3, 8, 26, 21].  Tensor sizes: [4178941, 1, 26, 21]
2025-03-11 18:34:52.527730 test begin: paddle.broadcast_to(Tensor([4178941, 1, 42, 13],"bool"), list[30,8,42,13,], )

[torch error] paddle.broadcast_to(Tensor([4178941, 1, 42, 13],"bool"), list[30,8,42,13,], ) 
 The expanded size of the tensor (30) must match the existing size (4178941) at non-singleton dimension 0.  Target sizes: [30, 8, 42, 13].  Tensor sizes: [4178941, 1, 42, 13]
2025-03-11 18:34:53.118418 test begin: paddle.broadcast_to(Tensor([417895, 1, 52, 105],"bool"), list[10,8,52,105,], )

[torch error] paddle.broadcast_to(Tensor([417895, 1, 52, 105],"bool"), list[10,8,52,105,], ) 
 The expanded size of the tensor (10) must match the existing size (417895) at non-singleton dimension 0.  Target sizes: [10, 8, 52, 105].  Tensor sizes: [417895, 1, 52, 105]
2025-03-11 18:34:53.632904 test begin: paddle.broadcast_to(Tensor([417895, 1, 52, 105],"bool"), list[3,8,52,105,], )

[torch error] paddle.broadcast_to(Tensor([417895, 1, 52, 105],"bool"), list[3,8,52,105,], ) 
 The expanded size of the tensor (3) must match the existing size (417895) at non-singleton dimension 0.  Target sizes: [3, 8, 52, 105].  Tensor sizes: [417895, 1, 52, 105]
2025-03-11 18:34:54.293972 test begin: paddle.broadcast_to(Tensor([417895, 1, 84, 65],"bool"), list[1,8,84,65,], )

[torch error] paddle.broadcast_to(Tensor([417895, 1, 84, 65],"bool"), list[1,8,84,65,], ) 
 The expanded size of the tensor (1) must match the existing size (417895) at non-singleton dimension 0.  Target sizes: [1, 8, 84, 65].  Tensor sizes: [417895, 1, 84, 65]
2025-03-11 18:34:54.797626 test begin: paddle.broadcast_to(Tensor([417895, 1, 84, 65],"bool"), list[10,8,84,65,], )

[torch error] paddle.broadcast_to(Tensor([417895, 1, 84, 65],"bool"), list[10,8,84,65,], ) 
 The expanded size of the tensor (10) must match the existing size (417895) at non-singleton dimension 0.  Target sizes: [10, 8, 84, 65].  Tensor sizes: [417895, 1, 84, 65]
2025-03-11 18:34:55.482132 test begin: paddle.broadcast_to(Tensor([417895, 1, 84, 65],"bool"), list[5,8,84,65,], )

[torch error] paddle.broadcast_to(Tensor([417895, 1, 84, 65],"bool"), list[5,8,84,65,], ) 
 The expanded size of the tensor (5) must match the existing size (417895) at non-singleton dimension 0.  Target sizes: [5, 8, 84, 65].  Tensor sizes: [417895, 1, 84, 65]
2025-03-11 18:34:56.148572 test begin: paddle.broadcast_to(Tensor([417971, 1, 103, 53],"bool"), list[9,8,103,53,], )

[torch error] paddle.broadcast_to(Tensor([417971, 1, 103, 53],"bool"), list[9,8,103,53,], ) 
 The expanded size of the tensor (9) must match the existing size (417971) at non-singleton dimension 0.  Target sizes: [9, 8, 103, 53].  Tensor sizes: [417971, 1, 103, 53]
2025-03-11 18:34:56.821777 test begin: paddle.broadcast_to(Tensor([41829, 1, 209, 261],"bool"), list[1,8,209,261,], )

[torch error] paddle.broadcast_to(Tensor([41829, 1, 209, 261],"bool"), list[1,8,209,261,], ) 
 The expanded size of the tensor (1) must match the existing size (41829) at non-singleton dimension 0.  Target sizes: [1, 8, 209, 261].  Tensor sizes: [41829, 1, 209, 261]
2025-03-11 18:34:57.502973 test begin: paddle.broadcast_to(Tensor([418584, 1, 79, 69],"bool"), list[1,8,79,69,], )

[torch error] paddle.broadcast_to(Tensor([418584, 1, 79, 69],"bool"), list[1,8,79,69,], ) 
 The expanded size of the tensor (1) must match the existing size (418584) at non-singleton dimension 0.  Target sizes: [1, 8, 79, 69].  Tensor sizes: [418584, 1, 79, 69]
2025-03-11 18:34:57.928813 test begin: paddle.broadcast_to(Tensor([4186609, 1, 5, 109],"bool"), list[1,8,5,109,], )

[torch error] paddle.broadcast_to(Tensor([4186609, 1, 5, 109],"bool"), list[1,8,5,109,], ) 
 The expanded size of the tensor (1) must match the existing size (4186609) at non-singleton dimension 0.  Target sizes: [1, 8, 5, 109].  Tensor sizes: [4186609, 1, 5, 109]
2025-03-11 18:34:58.187552 test begin: paddle.broadcast_to(Tensor([418661, 1, 50, 109],"bool"), list[1,8,50,109,], )

[torch error] paddle.broadcast_to(Tensor([418661, 1, 50, 109],"bool"), list[1,8,50,109,], ) 
 The expanded size of the tensor (1) must match the existing size (418661) at non-singleton dimension 0.  Target sizes: [1, 8, 50, 109].  Tensor sizes: [418661, 1, 50, 109]
2025-03-11 18:34:58.762629 test begin: paddle.broadcast_to(Tensor([419046, 1, 45, 121],"bool"), list[2,8,45,121,], )

[torch error] paddle.broadcast_to(Tensor([419046, 1, 45, 121],"bool"), list[2,8,45,121,], ) 
 The expanded size of the tensor (2) must match the existing size (419046) at non-singleton dimension 0.  Target sizes: [2, 8, 45, 121].  Tensor sizes: [419046, 1, 45, 121]
2025-03-11 18:34:59.441758 test begin: paddle.broadcast_to(Tensor([419046, 1, 45, 121],"bool"), list[8,8,45,121,], )

[torch error] paddle.broadcast_to(Tensor([419046, 1, 45, 121],"bool"), list[8,8,45,121,], ) 
 The expanded size of the tensor (8) must match the existing size (419046) at non-singleton dimension 0.  Target sizes: [8, 8, 45, 121].  Tensor sizes: [419046, 1, 45, 121]
2025-03-11 18:34:59.859361 test begin: paddle.broadcast_to(Tensor([4194305, 1, 32, 17],"bool"), list[10,8,32,17,], )

[torch error] paddle.broadcast_to(Tensor([4194305, 1, 32, 17],"bool"), list[10,8,32,17,], ) 
 The expanded size of the tensor (10) must match the existing size (4194305) at non-singleton dimension 0.  Target sizes: [10, 8, 32, 17].  Tensor sizes: [4194305, 1, 32, 17]
2025-03-11 18:35:00.118298 test begin: paddle.broadcast_to(Tensor([4194305, 1, 32, 17],"bool"), list[2,8,32,17,], )

[torch error] paddle.broadcast_to(Tensor([4194305, 1, 32, 17],"bool"), list[2,8,32,17,], ) 
 The expanded size of the tensor (2) must match the existing size (4194305) at non-singleton dimension 0.  Target sizes: [2, 8, 32, 17].  Tensor sizes: [4194305, 1, 32, 17]
2025-03-11 18:35:00.508171 test begin: paddle.broadcast_to(Tensor([419431, 1, 64, 85],"bool"), list[1,8,64,85,], )

[torch error] paddle.broadcast_to(Tensor([419431, 1, 64, 85],"bool"), list[1,8,64,85,], ) 
 The expanded size of the tensor (1) must match the existing size (419431) at non-singleton dimension 0.  Target sizes: [1, 8, 64, 85].  Tensor sizes: [419431, 1, 64, 85]
2025-03-11 18:35:00.917420 test begin: paddle.broadcast_to(Tensor([419431, 1, 64, 85],"bool"), list[2,8,64,85,], )

[torch error] paddle.broadcast_to(Tensor([419431, 1, 64, 85],"bool"), list[2,8,64,85,], ) 
 The expanded size of the tensor (2) must match the existing size (419431) at non-singleton dimension 0.  Target sizes: [2, 8, 64, 85].  Tensor sizes: [419431, 1, 64, 85]
2025-03-11 18:35:01.477042 test begin: paddle.broadcast_to(Tensor([419508, 1, 111, 49],"bool"), list[10,8,111,49,], )

[torch error] paddle.broadcast_to(Tensor([419508, 1, 111, 49],"bool"), list[10,8,111,49,], ) 
 The expanded size of the tensor (10) must match the existing size (419508) at non-singleton dimension 0.  Target sizes: [10, 8, 111, 49].  Tensor sizes: [419508, 1, 111, 49]
2025-03-11 18:35:01.900348 test begin: paddle.broadcast_to(Tensor([420049, 1, 56, 97],"bool"), list[1,8,56,97,], )

[torch error] paddle.broadcast_to(Tensor([420049, 1, 56, 97],"bool"), list[1,8,56,97,], ) 
 The expanded size of the tensor (1) must match the existing size (420049) at non-singleton dimension 0.  Target sizes: [1, 8, 56, 97].  Tensor sizes: [420049, 1, 56, 97]
2025-03-11 18:35:02.485351 test begin: paddle.broadcast_to(Tensor([420049, 1, 56, 97],"bool"), list[10,8,56,97,], )

[torch error] paddle.broadcast_to(Tensor([420049, 1, 56, 97],"bool"), list[10,8,56,97,], ) 
 The expanded size of the tensor (10) must match the existing size (420049) at non-singleton dimension 0.  Target sizes: [10, 8, 56, 97].  Tensor sizes: [420049, 1, 56, 97]
2025-03-11 18:35:02.897227 test begin: paddle.broadcast_to(Tensor([420049, 1, 56, 97],"bool"), list[6,8,56,97,], )

[torch error] paddle.broadcast_to(Tensor([420049, 1, 56, 97],"bool"), list[6,8,56,97,], ) 
 The expanded size of the tensor (6) must match the existing size (420049) at non-singleton dimension 0.  Target sizes: [6, 8, 56, 97].  Tensor sizes: [420049, 1, 56, 97]
2025-03-11 18:35:03.156552 test begin: paddle.broadcast_to(Tensor([4202029, 1, 3, 181],"bool"), list[2,8,3,181,], )

[torch error] paddle.broadcast_to(Tensor([4202029, 1, 3, 181],"bool"), list[2,8,3,181,], ) 
 The expanded size of the tensor (2) must match the existing size (4202029) at non-singleton dimension 0.  Target sizes: [2, 8, 3, 181].  Tensor sizes: [4202029, 1, 3, 181]
2025-03-11 18:35:03.541311 test begin: paddle.broadcast_to(Tensor([420203, 1, 30, 181],"bool"), list[2,8,30,181,], )

[torch error] paddle.broadcast_to(Tensor([420203, 1, 30, 181],"bool"), list[2,8,30,181,], ) 
 The expanded size of the tensor (2) must match the existing size (420203) at non-singleton dimension 0.  Target sizes: [2, 8, 30, 181].  Tensor sizes: [420203, 1, 30, 181]
2025-03-11 18:35:03.964320 test begin: paddle.broadcast_to(Tensor([420281, 1, 61, 89],"bool"), list[10,8,61,89,], )

[torch error] paddle.broadcast_to(Tensor([420281, 1, 61, 89],"bool"), list[10,8,61,89,], ) 
 The expanded size of the tensor (10) must match the existing size (420281) at non-singleton dimension 0.  Target sizes: [10, 8, 61, 89].  Tensor sizes: [420281, 1, 61, 89]
2025-03-11 18:35:04.522830 test begin: paddle.broadcast_to(Tensor([420281, 1, 61, 89],"bool"), list[9,8,61,89,], )

[torch error] paddle.broadcast_to(Tensor([420281, 1, 61, 89],"bool"), list[9,8,61,89,], ) 
 The expanded size of the tensor (9) must match the existing size (420281) at non-singleton dimension 0.  Target sizes: [9, 8, 61, 89].  Tensor sizes: [420281, 1, 61, 89]
2025-03-11 18:35:05.157893 test begin: paddle.broadcast_to(Tensor([420281, 1, 89, 61],"bool"), list[1,8,89,61,], )

[torch error] paddle.broadcast_to(Tensor([420281, 1, 89, 61],"bool"), list[1,8,89,61,], ) 
 The expanded size of the tensor (1) must match the existing size (420281) at non-singleton dimension 0.  Target sizes: [1, 8, 89, 61].  Tensor sizes: [420281, 1, 89, 61]
2025-03-11 18:35:05.809243 test begin: paddle.broadcast_to(Tensor([420281, 1, 89, 61],"bool"), list[4,8,89,61,], )

[torch error] paddle.broadcast_to(Tensor([420281, 1, 89, 61],"bool"), list[4,8,89,61,], ) 
 The expanded size of the tensor (4) must match the existing size (420281) at non-singleton dimension 0.  Target sizes: [4, 8, 89, 61].  Tensor sizes: [420281, 1, 89, 61]
2025-03-11 18:35:06.454928 test begin: paddle.broadcast_to(Tensor([42029, 1, 233, 233],"bool"), list[1,8,233,233,], )

[torch error] paddle.broadcast_to(Tensor([42029, 1, 233, 233],"bool"), list[1,8,233,233,], ) 
 The expanded size of the tensor (1) must match the existing size (42029) at non-singleton dimension 0.  Target sizes: [1, 8, 233, 233].  Tensor sizes: [42029, 1, 233, 233]
2025-03-11 18:35:07.097241 test begin: paddle.broadcast_to(Tensor([42029, 1, 233, 233],"bool"), list[8,8,233,233,], )

[torch error] paddle.broadcast_to(Tensor([42029, 1, 233, 233],"bool"), list[8,8,233,233,], ) 
 The expanded size of the tensor (8) must match the existing size (42029) at non-singleton dimension 0.  Target sizes: [8, 8, 233, 233].  Tensor sizes: [42029, 1, 233, 233]
2025-03-11 18:35:07.785653 test begin: paddle.broadcast_to(Tensor([42030, 1, 208, 261],"bool"), list[1,8,208,261,], )

[torch error] paddle.broadcast_to(Tensor([42030, 1, 208, 261],"bool"), list[1,8,208,261,], ) 
 The expanded size of the tensor (1) must match the existing size (42030) at non-singleton dimension 0.  Target sizes: [1, 8, 208, 261].  Tensor sizes: [42030, 1, 208, 261]
2025-03-11 18:35:08.435246 test begin: paddle.broadcast_to(Tensor([42032, 1, 141, 385],"bool"), list[1,8,141,385,], )

[torch error] paddle.broadcast_to(Tensor([42032, 1, 141, 385],"bool"), list[1,8,141,385,], ) 
 The expanded size of the tensor (1) must match the existing size (42032) at non-singleton dimension 0.  Target sizes: [1, 8, 141, 385].  Tensor sizes: [42032, 1, 141, 385]
2025-03-11 18:35:09.115813 test begin: paddle.broadcast_to(Tensor([420436, 1, 67, 81],"bool"), list[10,8,67,81,], )

[torch error] paddle.broadcast_to(Tensor([420436, 1, 67, 81],"bool"), list[10,8,67,81,], ) 
 The expanded size of the tensor (10) must match the existing size (420436) at non-singleton dimension 0.  Target sizes: [10, 8, 67, 81].  Tensor sizes: [420436, 1, 67, 81]
2025-03-11 18:35:09.783493 test begin: paddle.broadcast_to(Tensor([420436, 1, 67, 81],"bool"), list[2,8,67,81,], )

[torch error] paddle.broadcast_to(Tensor([420436, 1, 67, 81],"bool"), list[2,8,67,81,], ) 
 The expanded size of the tensor (2) must match the existing size (420436) at non-singleton dimension 0.  Target sizes: [2, 8, 67, 81].  Tensor sizes: [420436, 1, 67, 81]
2025-03-11 18:35:10.270039 test begin: paddle.broadcast_to(Tensor([420591, 1, 25, 217],"bool"), list[1,8,25,217,], )

[torch error] paddle.broadcast_to(Tensor([420591, 1, 25, 217],"bool"), list[1,8,25,217,], ) 
 The expanded size of the tensor (1) must match the existing size (420591) at non-singleton dimension 0.  Target sizes: [1, 8, 25, 217].  Tensor sizes: [420591, 1, 25, 217]
2025-03-11 18:35:10.849037 test begin: paddle.broadcast_to(Tensor([420668, 1, 48, 113],"bool"), list[1,8,48,113,], )

[torch error] paddle.broadcast_to(Tensor([420668, 1, 48, 113],"bool"), list[1,8,48,113,], ) 
 The expanded size of the tensor (1) must match the existing size (420668) at non-singleton dimension 0.  Target sizes: [1, 8, 48, 113].  Tensor sizes: [420668, 1, 48, 113]
2025-03-11 18:35:11.516085 test begin: paddle.broadcast_to(Tensor([420668, 1, 48, 113],"bool"), list[8,8,48,113,], )

[torch error] paddle.broadcast_to(Tensor([420668, 1, 48, 113],"bool"), list[8,8,48,113,], ) 
 The expanded size of the tensor (8) must match the existing size (420668) at non-singleton dimension 0.  Target sizes: [8, 8, 48, 113].  Tensor sizes: [420668, 1, 48, 113]
2025-03-11 18:35:12.217285 test begin: paddle.broadcast_to(Tensor([42107523, 17, 1, 6],"float16"), list[5,17,0,6,], )

[torch error] paddle.broadcast_to(Tensor([42107523, 17, 1, 6],"float16"), list[5,17,0,6,], ) 
 The expanded size of the tensor (5) must match the existing size (42107523) at non-singleton dimension 0.  Target sizes: [5, 17, 0, 6].  Tensor sizes: [42107523, 17, 1, 6]
2025-03-11 18:35:14.423862 test begin: paddle.broadcast_to(Tensor([42107523, 17, 6],"float16"), list[0,5,17,6,], )

[torch error] paddle.broadcast_to(Tensor([42107523, 17, 6],"float16"), list[0,5,17,6,], ) 
 The expanded size of the tensor (5) must match the existing size (42107523) at non-singleton dimension 1.  Target sizes: [0, 5, 17, 6].  Tensor sizes: [42107523, 17, 6]
2025-03-11 18:35:15.994635 test begin: paddle.broadcast_to(Tensor([42107523, 17, 6],"float16"), list[5,17,6,], )

[torch error] paddle.broadcast_to(Tensor([42107523, 17, 6],"float16"), list[5,17,6,], ) 
 The expanded size of the tensor (5) must match the existing size (42107523) at non-singleton dimension 0.  Target sizes: [5, 17, 6].  Tensor sizes: [42107523, 17, 6]
2025-03-11 18:35:16.982232 test begin: paddle.broadcast_to(Tensor([421134, 1, 42, 129],"bool"), list[10,8,42,129,], )

[torch error] paddle.broadcast_to(Tensor([421134, 1, 42, 129],"bool"), list[10,8,42,129,], ) 
 The expanded size of the tensor (10) must match the existing size (421134) at non-singleton dimension 0.  Target sizes: [10, 8, 42, 129].  Tensor sizes: [421134, 1, 42, 129]
2025-03-11 18:35:17.666413 test begin: paddle.broadcast_to(Tensor([421134, 1, 42, 129],"bool"), list[2,8,42,129,], )

[torch error] paddle.broadcast_to(Tensor([421134, 1, 42, 129],"bool"), list[2,8,42,129,], ) 
 The expanded size of the tensor (2) must match the existing size (421134) at non-singleton dimension 0.  Target sizes: [2, 8, 42, 129].  Tensor sizes: [421134, 1, 42, 129]
2025-03-11 18:35:18.086306 test begin: paddle.broadcast_to(Tensor([421367, 1, 95, 57],"bool"), list[10,8,95,57,], )

[torch error] paddle.broadcast_to(Tensor([421367, 1, 95, 57],"bool"), list[10,8,95,57,], ) 
 The expanded size of the tensor (10) must match the existing size (421367) at non-singleton dimension 0.  Target sizes: [10, 8, 95, 57].  Tensor sizes: [421367, 1, 95, 57]
2025-03-11 18:35:18.674564 test begin: paddle.broadcast_to(Tensor([421367, 1, 95, 57],"bool"), list[30,8,95,57,], )

[torch error] paddle.broadcast_to(Tensor([421367, 1, 95, 57],"bool"), list[30,8,95,57,], ) 
 The expanded size of the tensor (30) must match the existing size (421367) at non-singleton dimension 0.  Target sizes: [30, 8, 95, 57].  Tensor sizes: [421367, 1, 95, 57]
2025-03-11 18:35:19.338609 test begin: paddle.broadcast_to(Tensor([421367, 1, 95, 57],"bool"), list[6,8,95,57,], )

[torch error] paddle.broadcast_to(Tensor([421367, 1, 95, 57],"bool"), list[6,8,95,57,], ) 
 The expanded size of the tensor (6) must match the existing size (421367) at non-singleton dimension 0.  Target sizes: [6, 8, 95, 57].  Tensor sizes: [421367, 1, 95, 57]
2025-03-11 18:35:20.018733 test begin: paddle.broadcast_to(Tensor([421913, 1, 32, 169],"bool"), list[1,8,32,169,], )

[torch error] paddle.broadcast_to(Tensor([421913, 1, 32, 169],"bool"), list[1,8,32,169,], ) 
 The expanded size of the tensor (1) must match the existing size (421913) at non-singleton dimension 0.  Target sizes: [1, 8, 32, 169].  Tensor sizes: [421913, 1, 32, 169]
2025-03-11 18:35:20.681982 test begin: paddle.broadcast_to(Tensor([421913, 1, 32, 169],"bool"), list[3,8,32,169,], )

[torch error] paddle.broadcast_to(Tensor([421913, 1, 32, 169],"bool"), list[3,8,32,169,], ) 
 The expanded size of the tensor (3) must match the existing size (421913) at non-singleton dimension 0.  Target sizes: [3, 8, 32, 169].  Tensor sizes: [421913, 1, 32, 169]
2025-03-11 18:35:21.372267 test begin: paddle.broadcast_to(Tensor([422069, 1, 102, 53],"bool"), list[9,8,102,53,], )

[torch error] paddle.broadcast_to(Tensor([422069, 1, 102, 53],"bool"), list[9,8,102,53,], ) 
 The expanded size of the tensor (9) must match the existing size (422069) at non-singleton dimension 0.  Target sizes: [9, 8, 102, 53].  Tensor sizes: [422069, 1, 102, 53]
2025-03-11 18:35:22.052534 test begin: paddle.broadcast_to(Tensor([42233, 1, 207, 261],"bool"), list[1,8,207,261,], )

[torch error] paddle.broadcast_to(Tensor([42233, 1, 207, 261],"bool"), list[1,8,207,261,], ) 
 The expanded size of the tensor (1) must match the existing size (42233) at non-singleton dimension 0.  Target sizes: [1, 8, 207, 261].  Tensor sizes: [42233, 1, 207, 261]
2025-03-11 18:35:22.702409 test begin: paddle.broadcast_to(Tensor([422381, 1, 74, 73],"bool"), list[1,8,74,73,], )

[torch error] paddle.broadcast_to(Tensor([422381, 1, 74, 73],"bool"), list[1,8,74,73,], ) 
 The expanded size of the tensor (1) must match the existing size (422381) at non-singleton dimension 0.  Target sizes: [1, 8, 74, 73].  Tensor sizes: [422381, 1, 74, 73]
2025-03-11 18:35:23.356650 test begin: paddle.broadcast_to(Tensor([422381, 1, 74, 73],"bool"), list[10,8,74,73,], )

[torch error] paddle.broadcast_to(Tensor([422381, 1, 74, 73],"bool"), list[10,8,74,73,], ) 
 The expanded size of the tensor (10) must match the existing size (422381) at non-singleton dimension 0.  Target sizes: [10, 8, 74, 73].  Tensor sizes: [422381, 1, 74, 73]
2025-03-11 18:35:23.781185 test begin: paddle.broadcast_to(Tensor([4225373, 1, 12, 45],"bool"), list[6,8,12,45,], )

[torch error] paddle.broadcast_to(Tensor([4225373, 1, 12, 45],"bool"), list[6,8,12,45,], ) 
 The expanded size of the tensor (6) must match the existing size (4225373) at non-singleton dimension 0.  Target sizes: [6, 8, 12, 45].  Tensor sizes: [4225373, 1, 12, 45]
2025-03-11 18:35:24.361872 test begin: paddle.broadcast_to(Tensor([42253730, 1, 6, 9],"bool"), list[10,8,6,9,], )

[torch error] paddle.broadcast_to(Tensor([42253730, 1, 6, 9],"bool"), list[10,8,6,9,], ) 
 The expanded size of the tensor (10) must match the existing size (42253730) at non-singleton dimension 0.  Target sizes: [10, 8, 6, 9].  Tensor sizes: [42253730, 1, 6, 9]
2025-03-11 18:35:24.780113 test begin: paddle.broadcast_to(Tensor([42253730, 1, 6, 9],"bool"), list[2,8,6,9,], )

[torch error] paddle.broadcast_to(Tensor([42253730, 1, 6, 9],"bool"), list[2,8,6,9,], ) 
 The expanded size of the tensor (2) must match the existing size (42253730) at non-singleton dimension 0.  Target sizes: [2, 8, 6, 9].  Tensor sizes: [42253730, 1, 6, 9]
2025-03-11 18:35:25.049244 test begin: paddle.broadcast_to(Tensor([42253730, 1, 6, 9],"bool"), list[3,8,6,9,], )

[torch error] paddle.broadcast_to(Tensor([42253730, 1, 6, 9],"bool"), list[3,8,6,9,], ) 
 The expanded size of the tensor (3) must match the existing size (42253730) at non-singleton dimension 0.  Target sizes: [3, 8, 6, 9].  Tensor sizes: [42253730, 1, 6, 9]
2025-03-11 18:35:25.309820 test begin: paddle.broadcast_to(Tensor([42253730, 54, 1],"bool"), list[1,54,27216,], )

[torch error] paddle.broadcast_to(Tensor([42253730, 54, 1],"bool"), list[1,54,27216,], ) 
 The expanded size of the tensor (1) must match the existing size (42253730) at non-singleton dimension 0.  Target sizes: [1, 54, 27216].  Tensor sizes: [42253730, 54, 1]
2025-03-11 18:35:25.892417 test begin: paddle.broadcast_to(Tensor([42253730, 54, 1],"bool"), list[1,54,48384,], )

[torch error] paddle.broadcast_to(Tensor([42253730, 54, 1],"bool"), list[1,54,48384,], ) 
 The expanded size of the tensor (1) must match the existing size (42253730) at non-singleton dimension 0.  Target sizes: [1, 54, 48384].  Tensor sizes: [42253730, 54, 1]
2025-03-11 18:35:26.383434 test begin: paddle.broadcast_to(Tensor([42253730, 54, 1],"bool"), list[1,54,52500,], )

[torch error] paddle.broadcast_to(Tensor([42253730, 54, 1],"bool"), list[1,54,52500,], ) 
 The expanded size of the tensor (1) must match the existing size (42253730) at non-singleton dimension 0.  Target sizes: [1, 54, 52500].  Tensor sizes: [42253730, 54, 1]
2025-03-11 18:35:26.810976 test begin: paddle.broadcast_to(Tensor([42253730, 54, 1],"bool"), list[1,54,61236,], )

[torch error] paddle.broadcast_to(Tensor([42253730, 54, 1],"bool"), list[1,54,61236,], ) 
 The expanded size of the tensor (1) must match the existing size (42253730) at non-singleton dimension 0.  Target sizes: [1, 54, 61236].  Tensor sizes: [42253730, 54, 1]
2025-03-11 18:35:27.086246 test begin: paddle.broadcast_to(Tensor([42253730, 54, 1],"bool"), list[1,54,65856,], )

[torch error] paddle.broadcast_to(Tensor([42253730, 54, 1],"bool"), list[1,54,65856,], ) 
 The expanded size of the tensor (1) must match the existing size (42253730) at non-singleton dimension 0.  Target sizes: [1, 54, 65856].  Tensor sizes: [42253730, 54, 1]
2025-03-11 18:35:27.671688 test begin: paddle.broadcast_to(Tensor([42253730, 54, 1],"bool"), list[1,54,75600,], )

[torch error] paddle.broadcast_to(Tensor([42253730, 54, 1],"bool"), list[1,54,75600,], ) 
 The expanded size of the tensor (1) must match the existing size (42253730) at non-singleton dimension 0.  Target sizes: [1, 54, 75600].  Tensor sizes: [42253730, 54, 1]
2025-03-11 18:35:28.087934 test begin: paddle.broadcast_to(Tensor([42253730, 54, 1],"bool"), list[2,54,12096,], )

[torch error] paddle.broadcast_to(Tensor([42253730, 54, 1],"bool"), list[2,54,12096,], ) 
 The expanded size of the tensor (2) must match the existing size (42253730) at non-singleton dimension 0.  Target sizes: [2, 54, 12096].  Tensor sizes: [42253730, 54, 1]
2025-03-11 18:35:28.349787 test begin: paddle.broadcast_to(Tensor([42253730, 54, 1],"bool"), list[2,54,2100,], )

[torch error] paddle.broadcast_to(Tensor([42253730, 54, 1],"bool"), list[2,54,2100,], ) 
 The expanded size of the tensor (2) must match the existing size (42253730) at non-singleton dimension 0.  Target sizes: [2, 54, 2100].  Tensor sizes: [42253730, 54, 1]
2025-03-11 18:35:28.945117 test begin: paddle.broadcast_to(Tensor([42253730, 54, 1],"bool"), list[2,54,2541,], )

[torch error] paddle.broadcast_to(Tensor([42253730, 54, 1],"bool"), list[2,54,2541,], ) 
 The expanded size of the tensor (2) must match the existing size (42253730) at non-singleton dimension 0.  Target sizes: [2, 54, 2541].  Tensor sizes: [42253730, 54, 1]
2025-03-11 18:35:29.435939 test begin: paddle.broadcast_to(Tensor([42253730, 54, 1],"bool"), list[2,54,3024,], )

[torch error] paddle.broadcast_to(Tensor([42253730, 54, 1],"bool"), list[2,54,3024,], ) 
 The expanded size of the tensor (2) must match the existing size (42253730) at non-singleton dimension 0.  Target sizes: [2, 54, 3024].  Tensor sizes: [42253730, 54, 1]
2025-03-11 18:35:29.863863 test begin: paddle.broadcast_to(Tensor([42253730, 54, 1],"bool"), list[2,54,3549,], )

[torch error] paddle.broadcast_to(Tensor([42253730, 54, 1],"bool"), list[2,54,3549,], ) 
 The expanded size of the tensor (2) must match the existing size (42253730) at non-singleton dimension 0.  Target sizes: [2, 54, 3549].  Tensor sizes: [42253730, 54, 1]
2025-03-11 18:35:30.137322 test begin: paddle.broadcast_to(Tensor([42253730, 54, 1],"bool"), list[2,54,4116,], )

[torch error] paddle.broadcast_to(Tensor([42253730, 54, 1],"bool"), list[2,54,4116,], ) 
 The expanded size of the tensor (2) must match the existing size (42253730) at non-singleton dimension 0.  Target sizes: [2, 54, 4116].  Tensor sizes: [42253730, 54, 1]
2025-03-11 18:35:30.403867 test begin: paddle.broadcast_to(Tensor([42253730, 54, 1],"bool"), list[2,54,5376,], )

[torch error] paddle.broadcast_to(Tensor([42253730, 54, 1],"bool"), list[2,54,5376,], ) 
 The expanded size of the tensor (2) must match the existing size (42253730) at non-singleton dimension 0.  Target sizes: [2, 54, 5376].  Tensor sizes: [42253730, 54, 1]
2025-03-11 18:35:30.672961 test begin: paddle.broadcast_to(Tensor([42253730, 54, 1],"bool"), list[2,54,6804,], )

[torch error] paddle.broadcast_to(Tensor([42253730, 54, 1],"bool"), list[2,54,6804,], ) 
 The expanded size of the tensor (2) must match the existing size (42253730) at non-singleton dimension 0.  Target sizes: [2, 54, 6804].  Tensor sizes: [42253730, 54, 1]
2025-03-11 18:35:30.936452 test begin: paddle.broadcast_to(Tensor([42253730, 54, 1],"bool"), list[2,54,7581,], )

[torch error] paddle.broadcast_to(Tensor([42253730, 54, 1],"bool"), list[2,54,7581,], ) 
 The expanded size of the tensor (2) must match the existing size (42253730) at non-singleton dimension 0.  Target sizes: [2, 54, 7581].  Tensor sizes: [42253730, 54, 1]
2025-03-11 18:35:31.482763 test begin: paddle.broadcast_to(Tensor([42253730, 54, 1],"bool"), list[2,54,8400,], )

[torch error] paddle.broadcast_to(Tensor([42253730, 54, 1],"bool"), list[2,54,8400,], ) 
 The expanded size of the tensor (2) must match the existing size (42253730) at non-singleton dimension 0.  Target sizes: [2, 54, 8400].  Tensor sizes: [42253730, 54, 1]
2025-03-11 18:35:31.877871 test begin: paddle.broadcast_to(Tensor([422929, 1, 83, 65],"bool"), list[1,8,83,65,], )

[torch error] paddle.broadcast_to(Tensor([422929, 1, 83, 65],"bool"), list[1,8,83,65,], ) 
 The expanded size of the tensor (1) must match the existing size (422929) at non-singleton dimension 0.  Target sizes: [1, 8, 83, 65].  Tensor sizes: [422929, 1, 83, 65]
2025-03-11 18:35:32.138566 test begin: paddle.broadcast_to(Tensor([422929, 1, 83, 65],"bool"), list[10,8,83,65,], )

[torch error] paddle.broadcast_to(Tensor([422929, 1, 83, 65],"bool"), list[10,8,83,65,], ) 
 The expanded size of the tensor (10) must match the existing size (422929) at non-singleton dimension 0.  Target sizes: [10, 8, 83, 65].  Tensor sizes: [422929, 1, 83, 65]
2025-03-11 18:35:32.400155 test begin: paddle.broadcast_to(Tensor([422929, 1, 83, 65],"bool"), list[5,8,83,65,], )

[torch error] paddle.broadcast_to(Tensor([422929, 1, 83, 65],"bool"), list[5,8,83,65,], ) 
 The expanded size of the tensor (5) must match the existing size (422929) at non-singleton dimension 0.  Target sizes: [5, 8, 83, 65].  Tensor sizes: [422929, 1, 83, 65]
2025-03-11 18:35:32.671665 test begin: paddle.broadcast_to(Tensor([4233213, 1, 11, 49],"bool"), list[10,8,11,49,], )

[torch error] paddle.broadcast_to(Tensor([4233213, 1, 11, 49],"bool"), list[10,8,11,49,], ) 
 The expanded size of the tensor (10) must match the existing size (4233213) at non-singleton dimension 0.  Target sizes: [10, 8, 11, 49].  Tensor sizes: [4233213, 1, 11, 49]
2025-03-11 18:35:32.963524 test begin: paddle.broadcast_to(Tensor([4233213, 1, 11, 49],"bool"), list[7,8,11,49,], )

[torch error] paddle.broadcast_to(Tensor([4233213, 1, 11, 49],"bool"), list[7,8,11,49,], ) 
 The expanded size of the tensor (7) must match the existing size (4233213) at non-singleton dimension 0.  Target sizes: [7, 8, 11, 49].  Tensor sizes: [4233213, 1, 11, 49]
2025-03-11 18:35:33.302972 test begin: paddle.broadcast_to(Tensor([4233213, 1, 7, 77],"bool"), list[4,8,7,77,], )

[torch error] paddle.broadcast_to(Tensor([4233213, 1, 7, 77],"bool"), list[4,8,7,77,], ) 
 The expanded size of the tensor (4) must match the existing size (4233213) at non-singleton dimension 0.  Target sizes: [4, 8, 7, 77].  Tensor sizes: [4233213, 1, 7, 77]
2025-03-11 18:35:33.736895 test begin: paddle.broadcast_to(Tensor([423322, 1, 110, 49],"bool"), list[10,8,110,49,], )

[torch error] paddle.broadcast_to(Tensor([423322, 1, 110, 49],"bool"), list[10,8,110,49,], ) 
 The expanded size of the tensor (10) must match the existing size (423322) at non-singleton dimension 0.  Target sizes: [10, 8, 110, 49].  Tensor sizes: [423322, 1, 110, 49]
2025-03-11 18:35:34.320310 test begin: paddle.broadcast_to(Tensor([423322, 1, 14, 385],"bool"), list[1,8,14,385,], )

[torch error] paddle.broadcast_to(Tensor([423322, 1, 14, 385],"bool"), list[1,8,14,385,], ) 
 The expanded size of the tensor (1) must match the existing size (423322) at non-singleton dimension 0.  Target sizes: [1, 8, 14, 385].  Tensor sizes: [423322, 1, 14, 385]
2025-03-11 18:35:34.995864 test begin: paddle.broadcast_to(Tensor([423322, 1, 70, 77],"bool"), list[4,8,70,77,], )

[torch error] paddle.broadcast_to(Tensor([423322, 1, 70, 77],"bool"), list[4,8,70,77,], ) 
 The expanded size of the tensor (4) must match the existing size (423322) at non-singleton dimension 0.  Target sizes: [4, 8, 70, 77].  Tensor sizes: [423322, 1, 70, 77]
2025-03-11 18:35:35.665790 test begin: paddle.broadcast_to(Tensor([42333, 1, 140, 385],"bool"), list[1,8,140,385,], )

[torch error] paddle.broadcast_to(Tensor([42333, 1, 140, 385],"bool"), list[1,8,140,385,], ) 
 The expanded size of the tensor (1) must match the existing size (42333) at non-singleton dimension 0.  Target sizes: [1, 8, 140, 385].  Tensor sizes: [42333, 1, 140, 385]
2025-03-11 18:35:36.338128 test begin: paddle.broadcast_to(Tensor([42392, 1, 232, 232],"bool"), list[1,8,232,232,], )

[torch error] paddle.broadcast_to(Tensor([42392, 1, 232, 232],"bool"), list[1,8,232,232,], ) 
 The expanded size of the tensor (1) must match the existing size (42392) at non-singleton dimension 0.  Target sizes: [1, 8, 232, 232].  Tensor sizes: [42392, 1, 232, 232]
2025-03-11 18:35:37.054671 test begin: paddle.broadcast_to(Tensor([42392, 1, 232, 232],"bool"), list[8,8,232,232,], )

[torch error] paddle.broadcast_to(Tensor([42392, 1, 232, 232],"bool"), list[8,8,232,232,], ) 
 The expanded size of the tensor (8) must match the existing size (42392) at non-singleton dimension 0.  Target sizes: [8, 8, 232, 232].  Tensor sizes: [42392, 1, 232, 232]
2025-03-11 18:35:37.576982 test begin: paddle.broadcast_to(Tensor([423951, 1, 78, 69],"bool"), list[1,8,78,69,], )

[torch error] paddle.broadcast_to(Tensor([423951, 1, 78, 69],"bool"), list[1,8,78,69,], ) 
 The expanded size of the tensor (1) must match the existing size (423951) at non-singleton dimension 0.  Target sizes: [1, 8, 78, 69].  Tensor sizes: [423951, 1, 78, 69]
2025-03-11 18:35:38.271126 test begin: paddle.broadcast_to(Tensor([42438, 1, 206, 261],"bool"), list[1,8,206,261,], )

[torch error] paddle.broadcast_to(Tensor([42438, 1, 206, 261],"bool"), list[1,8,206,261,], ) 
 The expanded size of the tensor (1) must match the existing size (42438) at non-singleton dimension 0.  Target sizes: [1, 8, 206, 261].  Tensor sizes: [42438, 1, 206, 261]
2025-03-11 18:35:38.713543 test begin: paddle.broadcast_to(Tensor([424424, 5376, 1],"bool"), list[1,5376,4,], )

[torch error] paddle.broadcast_to(Tensor([424424, 5376, 1],"bool"), list[1,5376,4,], ) 
 The expanded size of the tensor (1) must match the existing size (424424) at non-singleton dimension 0.  Target sizes: [1, 5376, 4].  Tensor sizes: [424424, 5376, 1]
2025-03-11 18:35:39.290692 test begin: paddle.broadcast_to(Tensor([424424, 5376, 1],"bool"), list[4,5376,256,], )

[torch error] paddle.broadcast_to(Tensor([424424, 5376, 1],"bool"), list[4,5376,256,], ) 
 The expanded size of the tensor (4) must match the existing size (424424) at non-singleton dimension 0.  Target sizes: [4, 5376, 256].  Tensor sizes: [424424, 5376, 1]
2025-03-11 18:35:40.015572 test begin: paddle.broadcast_to(Tensor([424424, 7, 768],"int64"), tuple(8,7,768,), )

[torch error] paddle.broadcast_to(Tensor([424424, 7, 768],"int64"), tuple(8,7,768,), ) 
 The expanded size of the tensor (8) must match the existing size (424424) at non-singleton dimension 0.  Target sizes: [8, 7, 768].  Tensor sizes: [424424, 7, 768]
2025-03-11 18:35:58.680159 test begin: paddle.broadcast_to(Tensor([425057, 1, 88, 61],"bool"), list[1,8,88,61,], )

[torch error] paddle.broadcast_to(Tensor([425057, 1, 88, 61],"bool"), list[1,8,88,61,], ) 
 The expanded size of the tensor (1) must match the existing size (425057) at non-singleton dimension 0.  Target sizes: [1, 8, 88, 61].  Tensor sizes: [425057, 1, 88, 61]
2025-03-11 18:36:00.179842 test begin: paddle.broadcast_to(Tensor([425057, 1, 88, 61],"bool"), list[4,8,88,61,], )

[torch error] paddle.broadcast_to(Tensor([425057, 1, 88, 61],"bool"), list[4,8,88,61,], ) 
 The expanded size of the tensor (4) must match the existing size (425057) at non-singleton dimension 0.  Target sizes: [4, 8, 88, 61].  Tensor sizes: [425057, 1, 88, 61]
2025-03-11 18:36:00.623785 test begin: paddle.broadcast_to(Tensor([425294, 1, 37, 145],"bool"), list[1,8,37,145,], )

[torch error] paddle.broadcast_to(Tensor([425294, 1, 37, 145],"bool"), list[1,8,37,145,], ) 
 The expanded size of the tensor (1) must match the existing size (425294) at non-singleton dimension 0.  Target sizes: [1, 8, 37, 145].  Tensor sizes: [425294, 1, 37, 145]
2025-03-11 18:36:01.187335 test begin: paddle.broadcast_to(Tensor([425294, 1, 37, 145],"bool"), list[5,8,37,145,], )

[torch error] paddle.broadcast_to(Tensor([425294, 1, 37, 145],"bool"), list[5,8,37,145,], ) 
 The expanded size of the tensor (5) must match the existing size (425294) at non-singleton dimension 0.  Target sizes: [5, 8, 37, 145].  Tensor sizes: [425294, 1, 37, 145]
2025-03-11 18:36:01.615165 test begin: paddle.broadcast_to(Tensor([425850, 1, 94, 57],"bool"), list[10,8,94,57,], )

[torch error] paddle.broadcast_to(Tensor([425850, 1, 94, 57],"bool"), list[10,8,94,57,], ) 
 The expanded size of the tensor (10) must match the existing size (425850) at non-singleton dimension 0.  Target sizes: [10, 8, 94, 57].  Tensor sizes: [425850, 1, 94, 57]
2025-03-11 18:36:01.926909 test begin: paddle.broadcast_to(Tensor([425850, 1, 94, 57],"bool"), list[6,8,94,57,], )

[torch error] paddle.broadcast_to(Tensor([425850, 1, 94, 57],"bool"), list[6,8,94,57,], ) 
 The expanded size of the tensor (6) must match the existing size (425850) at non-singleton dimension 0.  Target sizes: [6, 8, 94, 57].  Tensor sizes: [425850, 1, 94, 57]
2025-03-11 18:36:02.239164 test begin: paddle.broadcast_to(Tensor([426089, 1, 35, 153],"bool"), list[7,8,35,153,], )

[torch error] paddle.broadcast_to(Tensor([426089, 1, 35, 153],"bool"), list[7,8,35,153,], ) 
 The expanded size of the tensor (7) must match the existing size (426089) at non-singleton dimension 0.  Target sizes: [7, 8, 35, 153].  Tensor sizes: [426089, 1, 35, 153]
2025-03-11 18:36:02.580590 test begin: paddle.broadcast_to(Tensor([426089, 1, 51, 105],"bool"), list[10,8,51,105,], )

[torch error] paddle.broadcast_to(Tensor([426089, 1, 51, 105],"bool"), list[10,8,51,105,], ) 
 The expanded size of the tensor (10) must match the existing size (426089) at non-singleton dimension 0.  Target sizes: [10, 8, 51, 105].  Tensor sizes: [426089, 1, 51, 105]
2025-03-11 18:36:02.895709 test begin: paddle.broadcast_to(Tensor([426089, 1, 51, 105],"bool"), list[3,8,51,105,], )

[torch error] paddle.broadcast_to(Tensor([426089, 1, 51, 105],"bool"), list[3,8,51,105,], ) 
 The expanded size of the tensor (3) must match the existing size (426089) at non-singleton dimension 0.  Target sizes: [3, 8, 51, 105].  Tensor sizes: [426089, 1, 51, 105]
2025-03-11 18:36:03.479667 test begin: paddle.broadcast_to(Tensor([426089, 1, 63, 85],"bool"), list[1,8,63,85,], )

[torch error] paddle.broadcast_to(Tensor([426089, 1, 63, 85],"bool"), list[1,8,63,85,], ) 
 The expanded size of the tensor (1) must match the existing size (426089) at non-singleton dimension 0.  Target sizes: [1, 8, 63, 85].  Tensor sizes: [426089, 1, 63, 85]
2025-03-11 18:36:04.182009 test begin: paddle.broadcast_to(Tensor([426089, 1, 63, 85],"bool"), list[2,8,63,85,], )

[torch error] paddle.broadcast_to(Tensor([426089, 1, 63, 85],"bool"), list[2,8,63,85,], ) 
 The expanded size of the tensor (2) must match the existing size (426089) at non-singleton dimension 0.  Target sizes: [2, 8, 63, 85].  Tensor sizes: [426089, 1, 63, 85]
2025-03-11 18:36:04.885115 test begin: paddle.broadcast_to(Tensor([426248, 1, 101, 53],"bool"), list[9,8,101,53,], )

[torch error] paddle.broadcast_to(Tensor([426248, 1, 101, 53],"bool"), list[9,8,101,53,], ) 
 The expanded size of the tensor (9) must match the existing size (426248) at non-singleton dimension 0.  Target sizes: [9, 8, 101, 53].  Tensor sizes: [426248, 1, 101, 53]
2025-03-11 18:36:05.582543 test begin: paddle.broadcast_to(Tensor([42637, 1, 139, 385],"bool"), list[1,8,139,385,], )

[torch error] paddle.broadcast_to(Tensor([42637, 1, 139, 385],"bool"), list[1,8,139,385,], ) 
 The expanded size of the tensor (1) must match the existing size (42637) at non-singleton dimension 0.  Target sizes: [1, 8, 139, 385].  Tensor sizes: [42637, 1, 139, 385]
2025-03-11 18:36:06.265661 test begin: paddle.broadcast_to(Tensor([42645, 1, 205, 261],"bool"), list[1,8,205,261,], )

[torch error] paddle.broadcast_to(Tensor([42645, 1, 205, 261],"bool"), list[1,8,205,261,], ) 
 The expanded size of the tensor (1) must match the existing size (42645) at non-singleton dimension 0.  Target sizes: [1, 8, 205, 261].  Tensor sizes: [42645, 1, 205, 261]
2025-03-11 18:36:06.954819 test begin: paddle.broadcast_to(Tensor([426806, 1, 66, 81],"bool"), list[10,8,66,81,], )

[torch error] paddle.broadcast_to(Tensor([426806, 1, 66, 81],"bool"), list[10,8,66,81,], ) 
 The expanded size of the tensor (10) must match the existing size (426806) at non-singleton dimension 0.  Target sizes: [10, 8, 66, 81].  Tensor sizes: [426806, 1, 66, 81]
2025-03-11 18:36:07.398103 test begin: paddle.broadcast_to(Tensor([426806, 1, 66, 81],"bool"), list[2,8,66,81,], )

[torch error] paddle.broadcast_to(Tensor([426806, 1, 66, 81],"bool"), list[2,8,66,81,], ) 
 The expanded size of the tensor (2) must match the existing size (426806) at non-singleton dimension 0.  Target sizes: [2, 8, 66, 81].  Tensor sizes: [426806, 1, 66, 81]
2025-03-11 18:36:07.718109 test begin: paddle.broadcast_to(Tensor([427045, 1, 39, 137],"bool"), list[10,8,39,137,], )

[torch error] paddle.broadcast_to(Tensor([427045, 1, 39, 137],"bool"), list[10,8,39,137,], ) 
 The expanded size of the tensor (10) must match the existing size (427045) at non-singleton dimension 0.  Target sizes: [10, 8, 39, 137].  Tensor sizes: [427045, 1, 39, 137]
2025-03-11 18:36:08.301912 test begin: paddle.broadcast_to(Tensor([427205, 1, 109, 49],"bool"), list[10,8,109,49,], )

[torch error] paddle.broadcast_to(Tensor([427205, 1, 109, 49],"bool"), list[10,8,109,49,], ) 
 The expanded size of the tensor (10) must match the existing size (427205) at non-singleton dimension 0.  Target sizes: [10, 8, 109, 49].  Tensor sizes: [427205, 1, 109, 49]
2025-03-11 18:36:08.995423 test begin: paddle.broadcast_to(Tensor([427205, 1, 49, 109],"bool"), list[1,8,49,109,], )

[torch error] paddle.broadcast_to(Tensor([427205, 1, 49, 109],"bool"), list[1,8,49,109,], ) 
 The expanded size of the tensor (1) must match the existing size (427205) at non-singleton dimension 0.  Target sizes: [1, 8, 49, 109].  Tensor sizes: [427205, 1, 49, 109]
2025-03-11 18:36:09.413771 test begin: paddle.broadcast_to(Tensor([427285, 1, 60, 89],"bool"), list[10,8,60,89,], )

[torch error] paddle.broadcast_to(Tensor([427285, 1, 60, 89],"bool"), list[10,8,60,89,], ) 
 The expanded size of the tensor (10) must match the existing size (427285) at non-singleton dimension 0.  Target sizes: [10, 8, 60, 89].  Tensor sizes: [427285, 1, 60, 89]
2025-03-11 18:36:10.016706 test begin: paddle.broadcast_to(Tensor([427285, 1, 60, 89],"bool"), list[9,8,60,89,], )

[torch error] paddle.broadcast_to(Tensor([427285, 1, 60, 89],"bool"), list[9,8,60,89,], ) 
 The expanded size of the tensor (9) must match the existing size (427285) at non-singleton dimension 0.  Target sizes: [9, 8, 60, 89].  Tensor sizes: [427285, 1, 60, 89]
2025-03-11 18:36:10.700619 test begin: paddle.broadcast_to(Tensor([4272850, 1, 6, 89],"bool"), list[10,8,6,89,], )

[torch error] paddle.broadcast_to(Tensor([4272850, 1, 6, 89],"bool"), list[10,8,6,89,], ) 
 The expanded size of the tensor (10) must match the existing size (4272850) at non-singleton dimension 0.  Target sizes: [10, 8, 6, 89].  Tensor sizes: [4272850, 1, 6, 89]
2025-03-11 18:36:11.380622 test begin: paddle.broadcast_to(Tensor([4272850, 1, 6, 89],"bool"), list[9,8,6,89,], )

[torch error] paddle.broadcast_to(Tensor([4272850, 1, 6, 89],"bool"), list[9,8,6,89,], ) 
 The expanded size of the tensor (9) must match the existing size (4272850) at non-singleton dimension 0.  Target sizes: [9, 8, 6, 89].  Tensor sizes: [4272850, 1, 6, 89]
2025-03-11 18:36:12.060484 test begin: paddle.broadcast_to(Tensor([427445, 1, 34, 157],"bool"), list[1,8,34,157,], )

[torch error] paddle.broadcast_to(Tensor([427445, 1, 34, 157],"bool"), list[1,8,34,157,], ) 
 The expanded size of the tensor (1) must match the existing size (427445) at non-singleton dimension 0.  Target sizes: [1, 8, 34, 157].  Tensor sizes: [427445, 1, 34, 157]
2025-03-11 18:36:12.775446 test begin: paddle.broadcast_to(Tensor([42760, 1, 231, 231],"bool"), list[1,8,231,231,], )

[torch error] paddle.broadcast_to(Tensor([42760, 1, 231, 231],"bool"), list[1,8,231,231,], ) 
 The expanded size of the tensor (1) must match the existing size (42760) at non-singleton dimension 0.  Target sizes: [1, 8, 231, 231].  Tensor sizes: [42760, 1, 231, 231]
2025-03-11 18:36:13.481419 test begin: paddle.broadcast_to(Tensor([42760, 1, 231, 231],"bool"), list[8,8,231,231,], )

[torch error] paddle.broadcast_to(Tensor([42760, 1, 231, 231],"bool"), list[8,8,231,231,], ) 
 The expanded size of the tensor (8) must match the existing size (42760) at non-singleton dimension 0.  Target sizes: [8, 8, 231, 231].  Tensor sizes: [42760, 1, 231, 231]
2025-03-11 18:36:13.914865 test begin: paddle.broadcast_to(Tensor([427686, 1, 55, 97],"bool"), list[1,8,55,97,], )

[torch error] paddle.broadcast_to(Tensor([427686, 1, 55, 97],"bool"), list[1,8,55,97,], ) 
 The expanded size of the tensor (1) must match the existing size (427686) at non-singleton dimension 0.  Target sizes: [1, 8, 55, 97].  Tensor sizes: [427686, 1, 55, 97]
2025-03-11 18:36:14.501403 test begin: paddle.broadcast_to(Tensor([427686, 1, 55, 97],"bool"), list[10,8,55,97,], )

[torch error] paddle.broadcast_to(Tensor([427686, 1, 55, 97],"bool"), list[10,8,55,97,], ) 
 The expanded size of the tensor (10) must match the existing size (427686) at non-singleton dimension 0.  Target sizes: [10, 8, 55, 97].  Tensor sizes: [427686, 1, 55, 97]
2025-03-11 18:36:15.194447 test begin: paddle.broadcast_to(Tensor([427686, 1, 55, 97],"bool"), list[6,8,55,97,], )

[torch error] paddle.broadcast_to(Tensor([427686, 1, 55, 97],"bool"), list[6,8,55,97,], ) 
 The expanded size of the tensor (6) must match the existing size (427686) at non-singleton dimension 0.  Target sizes: [6, 8, 55, 97].  Tensor sizes: [427686, 1, 55, 97]
2025-03-11 18:36:15.922681 test begin: paddle.broadcast_to(Tensor([4280866, 1, 13, 41],"bool"), list[10,8,13,41,], )

[torch error] paddle.broadcast_to(Tensor([4280866, 1, 13, 41],"bool"), list[10,8,13,41,], ) 
 The expanded size of the tensor (10) must match the existing size (4280866) at non-singleton dimension 0.  Target sizes: [10, 8, 13, 41].  Tensor sizes: [4280866, 1, 13, 41]
2025-03-11 18:36:16.354204 test begin: paddle.broadcast_to(Tensor([4280866, 1, 13, 41],"bool"), list[2,8,13,41,], )

[torch error] paddle.broadcast_to(Tensor([4280866, 1, 13, 41],"bool"), list[2,8,13,41,], ) 
 The expanded size of the tensor (2) must match the existing size (4280866) at non-singleton dimension 0.  Target sizes: [2, 8, 13, 41].  Tensor sizes: [4280866, 1, 13, 41]
2025-03-11 18:36:16.678819 test begin: paddle.broadcast_to(Tensor([428087, 1, 82, 65],"bool"), list[1,8,82,65,], )

[torch error] paddle.broadcast_to(Tensor([428087, 1, 82, 65],"bool"), list[1,8,82,65,], ) 
 The expanded size of the tensor (1) must match the existing size (428087) at non-singleton dimension 0.  Target sizes: [1, 8, 82, 65].  Tensor sizes: [428087, 1, 82, 65]
2025-03-11 18:36:17.259110 test begin: paddle.broadcast_to(Tensor([428087, 1, 82, 65],"bool"), list[10,8,82,65,], )

[torch error] paddle.broadcast_to(Tensor([428087, 1, 82, 65],"bool"), list[10,8,82,65,], ) 
 The expanded size of the tensor (10) must match the existing size (428087) at non-singleton dimension 0.  Target sizes: [10, 8, 82, 65].  Tensor sizes: [428087, 1, 82, 65]
2025-03-11 18:36:17.682332 test begin: paddle.broadcast_to(Tensor([428087, 1, 82, 65],"bool"), list[5,8,82,65,], )

[torch error] paddle.broadcast_to(Tensor([428087, 1, 82, 65],"bool"), list[5,8,82,65,], ) 
 The expanded size of the tensor (5) must match the existing size (428087) at non-singleton dimension 0.  Target sizes: [5, 8, 82, 65].  Tensor sizes: [428087, 1, 82, 65]
2025-03-11 18:36:18.263525 test begin: paddle.broadcast_to(Tensor([428167, 1, 73, 73],"bool"), list[1,8,73,73,], )

[torch error] paddle.broadcast_to(Tensor([428167, 1, 73, 73],"bool"), list[1,8,73,73,], ) 
 The expanded size of the tensor (1) must match the existing size (428167) at non-singleton dimension 0.  Target sizes: [1, 8, 73, 73].  Tensor sizes: [428167, 1, 73, 73]
2025-03-11 18:36:18.947009 test begin: paddle.broadcast_to(Tensor([428167, 1, 73, 73],"bool"), list[10,8,73,73,], )

[torch error] paddle.broadcast_to(Tensor([428167, 1, 73, 73],"bool"), list[10,8,73,73,], ) 
 The expanded size of the tensor (10) must match the existing size (428167) at non-singleton dimension 0.  Target sizes: [10, 8, 73, 73].  Tensor sizes: [428167, 1, 73, 73]
2025-03-11 18:36:19.366710 test begin: paddle.broadcast_to(Tensor([428167, 1, 73, 73],"bool"), list[2,8,73,73,], )

[torch error] paddle.broadcast_to(Tensor([428167, 1, 73, 73],"bool"), list[2,8,73,73,], ) 
 The expanded size of the tensor (2) must match the existing size (428167) at non-singleton dimension 0.  Target sizes: [2, 8, 73, 73].  Tensor sizes: [428167, 1, 73, 73]
2025-03-11 18:36:19.936647 test begin: paddle.broadcast_to(Tensor([428167, 1, 73, 73],"bool"), list[3,8,73,73,], )

[torch error] paddle.broadcast_to(Tensor([428167, 1, 73, 73],"bool"), list[3,8,73,73,], ) 
 The expanded size of the tensor (3) must match the existing size (428167) at non-singleton dimension 0.  Target sizes: [3, 8, 73, 73].  Tensor sizes: [428167, 1, 73, 73]
2025-03-11 18:36:20.415294 test begin: paddle.broadcast_to(Tensor([428167, 1, 73, 73],"bool"), list[30,8,73,73,], )

[torch error] paddle.broadcast_to(Tensor([428167, 1, 73, 73],"bool"), list[30,8,73,73,], ) 
 The expanded size of the tensor (30) must match the existing size (428167) at non-singleton dimension 0.  Target sizes: [30, 8, 73, 73].  Tensor sizes: [428167, 1, 73, 73]
2025-03-11 18:36:21.003941 test begin: paddle.broadcast_to(Tensor([428167, 1, 73, 73],"bool"), list[4,8,73,73,], )

[torch error] paddle.broadcast_to(Tensor([428167, 1, 73, 73],"bool"), list[4,8,73,73,], ) 
 The expanded size of the tensor (4) must match the existing size (428167) at non-singleton dimension 0.  Target sizes: [4, 8, 73, 73].  Tensor sizes: [428167, 1, 73, 73]
2025-03-11 18:36:21.699002 test begin: paddle.broadcast_to(Tensor([428167, 1, 73, 73],"bool"), list[5,8,73,73,], )

[torch error] paddle.broadcast_to(Tensor([428167, 1, 73, 73],"bool"), list[5,8,73,73,], ) 
 The expanded size of the tensor (5) must match the existing size (428167) at non-singleton dimension 0.  Target sizes: [5, 8, 73, 73].  Tensor sizes: [428167, 1, 73, 73]
2025-03-11 18:36:22.396039 test begin: paddle.broadcast_to(Tensor([428167, 1, 73, 73],"bool"), list[6,8,73,73,], )

[torch error] paddle.broadcast_to(Tensor([428167, 1, 73, 73],"bool"), list[6,8,73,73,], ) 
 The expanded size of the tensor (6) must match the existing size (428167) at non-singleton dimension 0.  Target sizes: [6, 8, 73, 73].  Tensor sizes: [428167, 1, 73, 73]
2025-03-11 18:36:23.088693 test begin: paddle.broadcast_to(Tensor([428167, 1, 73, 73],"bool"), list[7,8,73,73,], )

[torch error] paddle.broadcast_to(Tensor([428167, 1, 73, 73],"bool"), list[7,8,73,73,], ) 
 The expanded size of the tensor (7) must match the existing size (428167) at non-singleton dimension 0.  Target sizes: [7, 8, 73, 73].  Tensor sizes: [428167, 1, 73, 73]
2025-03-11 18:36:23.778123 test begin: paddle.broadcast_to(Tensor([428167, 1, 73, 73],"bool"), list[8,8,73,73,], )

[torch error] paddle.broadcast_to(Tensor([428167, 1, 73, 73],"bool"), list[8,8,73,73,], ) 
 The expanded size of the tensor (8) must match the existing size (428167) at non-singleton dimension 0.  Target sizes: [8, 8, 73, 73].  Tensor sizes: [428167, 1, 73, 73]
2025-03-11 18:36:24.228148 test begin: paddle.broadcast_to(Tensor([428167, 1, 73, 73],"bool"), list[9,8,73,73,], )

[torch error] paddle.broadcast_to(Tensor([428167, 1, 73, 73],"bool"), list[9,8,73,73,], ) 
 The expanded size of the tensor (9) must match the existing size (428167) at non-singleton dimension 0.  Target sizes: [9, 8, 73, 73].  Tensor sizes: [428167, 1, 73, 73]
2025-03-11 18:36:24.824919 test begin: paddle.broadcast_to(Tensor([42854, 1, 204, 261],"bool"), list[1,8,204,261,], )

[torch error] paddle.broadcast_to(Tensor([42854, 1, 204, 261],"bool"), list[1,8,204,261,], ) 
 The expanded size of the tensor (1) must match the existing size (42854) at non-singleton dimension 0.  Target sizes: [1, 8, 204, 261].  Tensor sizes: [42854, 1, 204, 261]
2025-03-11 18:36:25.524235 test begin: paddle.broadcast_to(Tensor([428570, 1, 44, 121],"bool"), list[2,8,44,121,], )

[torch error] paddle.broadcast_to(Tensor([428570, 1, 44, 121],"bool"), list[2,8,44,121,], ) 
 The expanded size of the tensor (2) must match the existing size (428570) at non-singleton dimension 0.  Target sizes: [2, 8, 44, 121].  Tensor sizes: [428570, 1, 44, 121]
2025-03-11 18:36:26.223778 test begin: paddle.broadcast_to(Tensor([428570, 1, 44, 121],"bool"), list[8,8,44,121,], )

[torch error] paddle.broadcast_to(Tensor([428570, 1, 44, 121],"bool"), list[8,8,44,121,], ) 
 The expanded size of the tensor (8) must match the existing size (428570) at non-singleton dimension 0.  Target sizes: [8, 8, 44, 121].  Tensor sizes: [428570, 1, 44, 121]
2025-03-11 18:36:26.918917 test begin: paddle.broadcast_to(Tensor([429457, 1, 33, 161],"bool"), list[2,8,33,161,], )

[torch error] paddle.broadcast_to(Tensor([429457, 1, 33, 161],"bool"), list[2,8,33,161,], ) 
 The expanded size of the tensor (2) must match the existing size (429457) at non-singleton dimension 0.  Target sizes: [2, 8, 33, 161].  Tensor sizes: [429457, 1, 33, 161]
2025-03-11 18:36:27.350272 test begin: paddle.broadcast_to(Tensor([429457, 1, 69, 77],"bool"), list[4,8,69,77,], )

[torch error] paddle.broadcast_to(Tensor([429457, 1, 69, 77],"bool"), list[4,8,69,77,], ) 
 The expanded size of the tensor (4) must match the existing size (429457) at non-singleton dimension 0.  Target sizes: [4, 8, 69, 77].  Tensor sizes: [429457, 1, 69, 77]
2025-03-11 18:36:27.944488 test begin: paddle.broadcast_to(Tensor([429457, 1, 77, 69],"bool"), list[1,8,77,69,], )

[torch error] paddle.broadcast_to(Tensor([429457, 1, 77, 69],"bool"), list[1,8,77,69,], ) 
 The expanded size of the tensor (1) must match the existing size (429457) at non-singleton dimension 0.  Target sizes: [1, 8, 77, 69].  Tensor sizes: [429457, 1, 77, 69]
2025-03-11 18:36:28.456062 test begin: paddle.broadcast_to(Tensor([42946, 1, 138, 385],"bool"), list[1,8,138,385,], )

[torch error] paddle.broadcast_to(Tensor([42946, 1, 138, 385],"bool"), list[1,8,138,385,], ) 
 The expanded size of the tensor (1) must match the existing size (42946) at non-singleton dimension 0.  Target sizes: [1, 8, 138, 385].  Tensor sizes: [42946, 1, 138, 385]
2025-03-11 18:36:29.143697 test begin: paddle.broadcast_to(Tensor([4294967297, 1],"float16"), list[300,40,], )

[torch error] paddle.broadcast_to(Tensor([4294967297, 1],"float16"), list[300,40,], ) 
 The expanded size of the tensor (300) must match the existing size (4294967297) at non-singleton dimension 0.  Target sizes: [300, 40].  Tensor sizes: [4294967297, 1]
2025-03-11 18:36:33.100392 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,10,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,10,], ) 
 The expanded size of the tensor (10) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 10].  Tensor sizes: [4294967297]
2025-03-11 18:36:35.692891 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,100,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,100,], ) 
 The expanded size of the tensor (100) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 100].  Tensor sizes: [4294967297]
2025-03-11 18:36:38.340131 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,101,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,101,], ) 
 The expanded size of the tensor (101) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 101].  Tensor sizes: [4294967297]
2025-03-11 18:36:39.926848 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,102,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,102,], ) 
 The expanded size of the tensor (102) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 102].  Tensor sizes: [4294967297]
2025-03-11 18:36:41.054738 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,103,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,103,], ) 
 The expanded size of the tensor (103) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 103].  Tensor sizes: [4294967297]
2025-03-11 18:36:42.511466 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,11,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,11,], ) 
 The expanded size of the tensor (11) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 11].  Tensor sizes: [4294967297]
2025-03-11 18:36:45.073111 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,12,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,12,], ) 
 The expanded size of the tensor (12) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 12].  Tensor sizes: [4294967297]
2025-03-11 18:36:46.710533 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,13,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,13,], ) 
 The expanded size of the tensor (13) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 13].  Tensor sizes: [4294967297]
2025-03-11 18:36:48.892717 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,14,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,14,], ) 
 The expanded size of the tensor (14) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 14].  Tensor sizes: [4294967297]
2025-03-11 18:36:50.898410 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,15,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,15,], ) 
 The expanded size of the tensor (15) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 15].  Tensor sizes: [4294967297]
2025-03-11 18:36:52.861802 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,16,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,16,], ) 
 The expanded size of the tensor (16) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 16].  Tensor sizes: [4294967297]
2025-03-11 18:36:54.815065 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,17,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,17,], ) 
 The expanded size of the tensor (17) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 17].  Tensor sizes: [4294967297]
2025-03-11 18:36:57.416751 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,18,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,18,], ) 
 The expanded size of the tensor (18) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 18].  Tensor sizes: [4294967297]
2025-03-11 18:37:00.013676 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,19,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,19,], ) 
 The expanded size of the tensor (19) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 19].  Tensor sizes: [4294967297]
2025-03-11 18:37:02.601981 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,20,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,20,], ) 
 The expanded size of the tensor (20) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 20].  Tensor sizes: [4294967297]
2025-03-11 18:37:05.213435 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,21,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,21,], ) 
 The expanded size of the tensor (21) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 21].  Tensor sizes: [4294967297]
2025-03-11 18:37:07.781266 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,22,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,22,], ) 
 The expanded size of the tensor (22) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 22].  Tensor sizes: [4294967297]
2025-03-11 18:37:09.385954 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,23,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,23,], ) 
 The expanded size of the tensor (23) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 23].  Tensor sizes: [4294967297]
2025-03-11 18:37:11.572968 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,24,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,24,], ) 
 The expanded size of the tensor (24) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 24].  Tensor sizes: [4294967297]
2025-03-11 18:37:14.159497 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,25,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,25,], ) 
 The expanded size of the tensor (25) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 25].  Tensor sizes: [4294967297]
2025-03-11 18:37:16.732166 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,26,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,26,], ) 
 The expanded size of the tensor (26) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 26].  Tensor sizes: [4294967297]
2025-03-11 18:37:19.286485 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,27,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,27,], ) 
 The expanded size of the tensor (27) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 27].  Tensor sizes: [4294967297]
2025-03-11 18:37:21.858828 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,28,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,28,], ) 
 The expanded size of the tensor (28) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 28].  Tensor sizes: [4294967297]
2025-03-11 18:37:24.421509 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,29,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,29,], ) 
 The expanded size of the tensor (29) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 29].  Tensor sizes: [4294967297]
2025-03-11 18:37:26.971554 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,30,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,30,], ) 
 The expanded size of the tensor (30) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 30].  Tensor sizes: [4294967297]
2025-03-11 18:37:28.540397 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,31,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,31,], ) 
 The expanded size of the tensor (31) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 31].  Tensor sizes: [4294967297]
2025-03-11 18:37:30.758526 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,32,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,32,], ) 
 The expanded size of the tensor (32) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 32].  Tensor sizes: [4294967297]
2025-03-11 18:37:32.827843 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,33,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,33,], ) 
 The expanded size of the tensor (33) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 33].  Tensor sizes: [4294967297]
2025-03-11 18:37:35.395538 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,34,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,34,], ) 
 The expanded size of the tensor (34) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 34].  Tensor sizes: [4294967297]
2025-03-11 18:37:37.040040 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,35,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,35,], ) 
 The expanded size of the tensor (35) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 35].  Tensor sizes: [4294967297]
2025-03-11 18:37:38.216885 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,36,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,36,], ) 
 The expanded size of the tensor (36) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 36].  Tensor sizes: [4294967297]
2025-03-11 18:37:39.541952 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,37,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,37,], ) 
 The expanded size of the tensor (37) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 37].  Tensor sizes: [4294967297]
2025-03-11 18:37:42.018350 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,38,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,38,], ) 
 The expanded size of the tensor (38) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 38].  Tensor sizes: [4294967297]
2025-03-11 18:37:44.472182 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,39,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,39,], ) 
 The expanded size of the tensor (39) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 39].  Tensor sizes: [4294967297]
2025-03-11 18:37:45.984621 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,40,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,40,], ) 
 The expanded size of the tensor (40) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 40].  Tensor sizes: [4294967297]
2025-03-11 18:37:47.135449 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,41,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,41,], ) 
 The expanded size of the tensor (41) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 41].  Tensor sizes: [4294967297]
2025-03-11 18:37:48.472121 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,42,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,42,], ) 
 The expanded size of the tensor (42) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 42].  Tensor sizes: [4294967297]
2025-03-11 18:37:50.911457 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,43,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,43,], ) 
 The expanded size of the tensor (43) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 43].  Tensor sizes: [4294967297]
2025-03-11 18:37:52.489255 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,44,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,44,], ) 
 The expanded size of the tensor (44) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 44].  Tensor sizes: [4294967297]
2025-03-11 18:37:53.633264 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,45,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,45,], ) 
 The expanded size of the tensor (45) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 45].  Tensor sizes: [4294967297]
2025-03-11 18:37:54.770097 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,46,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,46,], ) 
 The expanded size of the tensor (46) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 46].  Tensor sizes: [4294967297]
2025-03-11 18:37:56.932735 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,47,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,47,], ) 
 The expanded size of the tensor (47) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 47].  Tensor sizes: [4294967297]
2025-03-11 18:37:58.463132 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,48,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,48,], ) 
 The expanded size of the tensor (48) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 48].  Tensor sizes: [4294967297]
2025-03-11 18:37:59.665075 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,49,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,49,], ) 
 The expanded size of the tensor (49) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 49].  Tensor sizes: [4294967297]
2025-03-11 18:38:00.809739 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,5,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,5,], ) 
 The expanded size of the tensor (5) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 5].  Tensor sizes: [4294967297]
2025-03-11 18:38:01.951409 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,50,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,50,], ) 
 The expanded size of the tensor (50) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 50].  Tensor sizes: [4294967297]
2025-03-11 18:38:03.104333 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,51,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,51,], ) 
 The expanded size of the tensor (51) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 51].  Tensor sizes: [4294967297]
2025-03-11 18:38:04.432481 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,52,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,52,], ) 
 The expanded size of the tensor (52) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 52].  Tensor sizes: [4294967297]
2025-03-11 18:38:06.908471 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,53,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,53,], ) 
 The expanded size of the tensor (53) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 53].  Tensor sizes: [4294967297]
2025-03-11 18:38:09.417434 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,54,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,54,], ) 
 The expanded size of the tensor (54) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 54].  Tensor sizes: [4294967297]
2025-03-11 18:38:11.383195 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,55,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,55,], ) 
 The expanded size of the tensor (55) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 55].  Tensor sizes: [4294967297]
2025-03-11 18:38:12.951753 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,56,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,56,], ) 
 The expanded size of the tensor (56) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 56].  Tensor sizes: [4294967297]
2025-03-11 18:38:15.147714 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,57,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,57,], ) 
 The expanded size of the tensor (57) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 57].  Tensor sizes: [4294967297]
2025-03-11 18:38:16.850449 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,58,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,58,], ) 
 The expanded size of the tensor (58) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 58].  Tensor sizes: [4294967297]
2025-03-11 18:38:18.067922 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,59,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,59,], ) 
 The expanded size of the tensor (59) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 59].  Tensor sizes: [4294967297]
2025-03-11 18:38:20.343800 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,6,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,6,], ) 
 The expanded size of the tensor (6) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 6].  Tensor sizes: [4294967297]
2025-03-11 18:38:22.087362 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,60,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,60,], ) 
 The expanded size of the tensor (60) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 60].  Tensor sizes: [4294967297]
2025-03-11 18:38:23.493372 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,61,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,61,], ) 
 The expanded size of the tensor (61) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 61].  Tensor sizes: [4294967297]
2025-03-11 18:38:26.113377 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,62,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,62,], ) 
 The expanded size of the tensor (62) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 62].  Tensor sizes: [4294967297]
2025-03-11 18:38:27.764326 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,63,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,63,], ) 
 The expanded size of the tensor (63) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 63].  Tensor sizes: [4294967297]
2025-03-11 18:38:29.567785 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,64,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,64,], ) 
 The expanded size of the tensor (64) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 64].  Tensor sizes: [4294967297]
2025-03-11 18:38:31.268490 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,65,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,65,], ) 
 The expanded size of the tensor (65) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 65].  Tensor sizes: [4294967297]
2025-03-11 18:38:33.889848 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,66,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,66,], ) 
 The expanded size of the tensor (66) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 66].  Tensor sizes: [4294967297]
2025-03-11 18:38:35.797174 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,67,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,67,], ) 
 The expanded size of the tensor (67) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 67].  Tensor sizes: [4294967297]
2025-03-11 18:38:37.349048 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,68,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,68,], ) 
 The expanded size of the tensor (68) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 68].  Tensor sizes: [4294967297]
2025-03-11 18:38:38.669075 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,69,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,69,], ) 
 The expanded size of the tensor (69) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 69].  Tensor sizes: [4294967297]
2025-03-11 18:38:41.249187 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,7,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,7,], ) 
 The expanded size of the tensor (7) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 7].  Tensor sizes: [4294967297]
2025-03-11 18:38:42.910502 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,70,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,70,], ) 
 The expanded size of the tensor (70) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 70].  Tensor sizes: [4294967297]
2025-03-11 18:38:44.124578 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,71,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,71,], ) 
 The expanded size of the tensor (71) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 71].  Tensor sizes: [4294967297]
2025-03-11 18:38:45.384218 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,72,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,72,], ) 
 The expanded size of the tensor (72) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 72].  Tensor sizes: [4294967297]
2025-03-11 18:38:46.547538 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,73,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,73,], ) 
 The expanded size of the tensor (73) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 73].  Tensor sizes: [4294967297]
2025-03-11 18:38:48.801016 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,74,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,74,], ) 
 The expanded size of the tensor (74) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 74].  Tensor sizes: [4294967297]
2025-03-11 18:38:50.331835 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,75,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,75,], ) 
 The expanded size of the tensor (75) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 75].  Tensor sizes: [4294967297]
2025-03-11 18:38:51.455673 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,76,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,76,], ) 
 The expanded size of the tensor (76) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 76].  Tensor sizes: [4294967297]
2025-03-11 18:38:53.642802 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,77,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,77,], ) 
 The expanded size of the tensor (77) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 77].  Tensor sizes: [4294967297]
2025-03-11 18:38:56.134642 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,78,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,78,], ) 
 The expanded size of the tensor (78) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 78].  Tensor sizes: [4294967297]
2025-03-11 18:38:58.611950 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,79,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,79,], ) 
 The expanded size of the tensor (79) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 79].  Tensor sizes: [4294967297]
2025-03-11 18:39:00.733241 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,8,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,8,], ) 
 The expanded size of the tensor (8) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 8].  Tensor sizes: [4294967297]
2025-03-11 18:39:03.093405 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,80,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,80,], ) 
 The expanded size of the tensor (80) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 80].  Tensor sizes: [4294967297]
2025-03-11 18:39:05.584732 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,81,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,81,], ) 
 The expanded size of the tensor (81) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 81].  Tensor sizes: [4294967297]
2025-03-11 18:39:08.080355 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,82,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,82,], ) 
 The expanded size of the tensor (82) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 82].  Tensor sizes: [4294967297]
2025-03-11 18:39:10.582550 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,83,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,83,], ) 
 The expanded size of the tensor (83) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 83].  Tensor sizes: [4294967297]
2025-03-11 18:39:13.086134 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,84,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,84,], ) 
 The expanded size of the tensor (84) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 84].  Tensor sizes: [4294967297]
2025-03-11 18:39:15.538641 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,85,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,85,], ) 
 The expanded size of the tensor (85) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 85].  Tensor sizes: [4294967297]
2025-03-11 18:39:17.054369 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,86,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,86,], ) 
 The expanded size of the tensor (86) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 86].  Tensor sizes: [4294967297]
2025-03-11 18:39:18.395307 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,87,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,87,], ) 
 The expanded size of the tensor (87) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 87].  Tensor sizes: [4294967297]
2025-03-11 18:39:19.912700 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,88,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,88,], ) 
 The expanded size of the tensor (88) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 88].  Tensor sizes: [4294967297]
2025-03-11 18:39:21.095963 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,89,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,89,], ) 
 The expanded size of the tensor (89) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 89].  Tensor sizes: [4294967297]
2025-03-11 18:39:23.218595 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,9,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,9,], ) 
 The expanded size of the tensor (9) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 9].  Tensor sizes: [4294967297]
2025-03-11 18:39:25.723651 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,90,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,90,], ) 
 The expanded size of the tensor (90) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 90].  Tensor sizes: [4294967297]
2025-03-11 18:39:28.287110 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,91,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,91,], ) 
 The expanded size of the tensor (91) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 91].  Tensor sizes: [4294967297]
2025-03-11 18:39:29.890900 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,92,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,92,], ) 
 The expanded size of the tensor (92) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 92].  Tensor sizes: [4294967297]
2025-03-11 18:39:31.764150 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,93,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,93,], ) 
 The expanded size of the tensor (93) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 93].  Tensor sizes: [4294967297]
2025-03-11 18:39:33.660705 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,94,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,94,], ) 
 The expanded size of the tensor (94) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 94].  Tensor sizes: [4294967297]
2025-03-11 18:39:35.956042 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,95,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,95,], ) 
 The expanded size of the tensor (95) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 95].  Tensor sizes: [4294967297]
2025-03-11 18:39:38.509200 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,96,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,96,], ) 
 The expanded size of the tensor (96) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 96].  Tensor sizes: [4294967297]
2025-03-11 18:39:41.118856 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,97,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,97,], ) 
 The expanded size of the tensor (97) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 97].  Tensor sizes: [4294967297]
2025-03-11 18:39:43.244380 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,98,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,98,], ) 
 The expanded size of the tensor (98) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 98].  Tensor sizes: [4294967297]
2025-03-11 18:39:45.130560 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,99,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,1,99,], ) 
 The expanded size of the tensor (99) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 1, 99].  Tensor sizes: [4294967297]
2025-03-11 18:39:47.518805 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,2048,2048,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,2048,2048,], ) 
 The expanded size of the tensor (2048) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 2048, 2048].  Tensor sizes: [4294967297]
2025-03-11 18:39:50.102869 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,4,4,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,4,4,], ) 
 The expanded size of the tensor (4) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 4, 4].  Tensor sizes: [4294967297]
2025-03-11 18:39:52.324606 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,58,58,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,1,58,58,], ) 
 The expanded size of the tensor (58) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 1, 58, 58].  Tensor sizes: [4294967297]
2025-03-11 18:39:55.060111 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,123904,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,123904,], ) 
 The expanded size of the tensor (123904) must match the existing size (4294967297) at non-singleton dimension 1.  Target sizes: [1, 123904].  Tensor sizes: [4294967297]
2025-03-11 18:39:57.737292 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,135424,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,135424,], ) 
 The expanded size of the tensor (135424) must match the existing size (4294967297) at non-singleton dimension 1.  Target sizes: [1, 135424].  Tensor sizes: [4294967297]
2025-03-11 18:39:59.879559 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,147456,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,147456,], ) 
 The expanded size of the tensor (147456) must match the existing size (4294967297) at non-singleton dimension 1.  Target sizes: [1, 147456].  Tensor sizes: [4294967297]
2025-03-11 18:40:01.612663 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,2,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,2,], ) 
 The expanded size of the tensor (2) must match the existing size (4294967297) at non-singleton dimension 1.  Target sizes: [1, 2].  Tensor sizes: [4294967297]
2025-03-11 18:40:03.022175 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,4,4,2,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,4,4,2,], ) 
 The expanded size of the tensor (2) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [1, 4, 4, 2].  Tensor sizes: [4294967297]
2025-03-11 18:40:05.694974 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,4,4,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,4,4,], ) 
 The expanded size of the tensor (4) must match the existing size (4294967297) at non-singleton dimension 2.  Target sizes: [1, 4, 4].  Tensor sizes: [4294967297]
2025-03-11 18:40:08.382036 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,4,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,4,], ) 
 The expanded size of the tensor (4) must match the existing size (4294967297) at non-singleton dimension 1.  Target sizes: [1, 4].  Tensor sizes: [4294967297]
2025-03-11 18:40:10.974271 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[1,], ) 
 The expanded size of the tensor (1) must match the existing size (4294967297) at non-singleton dimension 0.  Target sizes: [1].  Tensor sizes: [4294967297]
2025-03-11 18:40:12.596167 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[10,10,5,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[10,10,5,], ) 
 The expanded size of the tensor (5) must match the existing size (4294967297) at non-singleton dimension 2.  Target sizes: [10, 10, 5].  Tensor sizes: [4294967297]
2025-03-11 18:40:13.802176 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[100,100,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[100,100,], ) 
 The expanded size of the tensor (100) must match the existing size (4294967297) at non-singleton dimension 1.  Target sizes: [100, 100].  Tensor sizes: [4294967297]
2025-03-11 18:40:15.027464 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[100,123904,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[100,123904,], ) 
 The expanded size of the tensor (123904) must match the existing size (4294967297) at non-singleton dimension 1.  Target sizes: [100, 123904].  Tensor sizes: [4294967297]
2025-03-11 18:40:17.225152 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[100,135424,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[100,135424,], ) 
 The expanded size of the tensor (135424) must match the existing size (4294967297) at non-singleton dimension 1.  Target sizes: [100, 135424].  Tensor sizes: [4294967297]
2025-03-11 18:40:19.073597 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[100,147456,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[100,147456,], ) 
 The expanded size of the tensor (147456) must match the existing size (4294967297) at non-singleton dimension 1.  Target sizes: [100, 147456].  Tensor sizes: [4294967297]
2025-03-11 18:40:21.398504 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[13,1,10,10,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[13,1,10,10,], ) 
 The expanded size of the tensor (10) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [13, 1, 10, 10].  Tensor sizes: [4294967297]
2025-03-11 18:40:23.976386 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[13,1,1007,1007,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[13,1,1007,1007,], ) 
 The expanded size of the tensor (1007) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [13, 1, 1007, 1007].  Tensor sizes: [4294967297]
2025-03-11 18:40:26.504746 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[13,1,3,10,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[13,1,3,10,], ) 
 The expanded size of the tensor (10) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [13, 1, 3, 10].  Tensor sizes: [4294967297]
2025-03-11 18:40:28.174845 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[13,1,7,7,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[13,1,7,7,], ) 
 The expanded size of the tensor (7) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [13, 1, 7, 7].  Tensor sizes: [4294967297]
2025-03-11 18:40:29.694719 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[168,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[168,], ) 
 The expanded size of the tensor (168) must match the existing size (4294967297) at non-singleton dimension 0.  Target sizes: [168].  Tensor sizes: [4294967297]
2025-03-11 18:40:32.252656 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[2,1,1,100,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[2,1,1,100,], ) 
 The expanded size of the tensor (100) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [2, 1, 1, 100].  Tensor sizes: [4294967297]
2025-03-11 18:40:33.835477 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[2,1,1,101,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[2,1,1,101,], ) 
 The expanded size of the tensor (101) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [2, 1, 1, 101].  Tensor sizes: [4294967297]
2025-03-11 18:40:35.565573 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[2,1,1,102,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[2,1,1,102,], ) 
 The expanded size of the tensor (102) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [2, 1, 1, 102].  Tensor sizes: [4294967297]
2025-03-11 18:40:37.124586 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[2,1,1,103,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[2,1,1,103,], ) 
 The expanded size of the tensor (103) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [2, 1, 1, 103].  Tensor sizes: [4294967297]
2025-03-11 18:40:39.326935 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[2,1,1,104,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[2,1,1,104,], ) 
 The expanded size of the tensor (104) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [2, 1, 1, 104].  Tensor sizes: [4294967297]
2025-03-11 18:40:41.270705 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[2,1,1,105,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[2,1,1,105,], ) 
 The expanded size of the tensor (105) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [2, 1, 1, 105].  Tensor sizes: [4294967297]
2025-03-11 18:40:43.209627 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[2,1,1,106,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[2,1,1,106,], ) 
 The expanded size of the tensor (106) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [2, 1, 1, 106].  Tensor sizes: [4294967297]
2025-03-11 18:40:45.025042 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[2,1,1,107,], )

[torch error] paddle.broadcast_to(Tensor([4294967297],"float16"), list[2,1,1,107,], ) 
 The expanded size of the tensor (107) must match the existing size (4294967297) at non-singleton dimension 3.  Target sizes: [2, 1, 1, 107].  Tensor sizes: [4294967297]
2025-03-11 18:40:47.478394 test begin: paddle.broadcast_to(Tensor([4294967297],"float16"), list[2,1,1,108,], )

2025-03-11 18:40:57.056564 test begin: paddle.reshape(Tensor([1, 9, 812572, 312],"float32"), shape=list[-1,200,312,], )

W0311 18:42:01.167541 108318 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 18:42:01.168937 108318 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.reshape(Tensor([1, 9, 812572, 312],"float32"), shape=list[-1,200,312,], ) 
 shape '[-1, 200, 312]' is invalid for input of size 2281702176
2025-03-11 18:42:03.200661 test begin: paddle.reshape(Tensor([1, 9, 833956, 304],"float32"), shape=list[-1,200,304,], )

[torch error] paddle.reshape(Tensor([1, 9, 833956, 304],"float32"), shape=list[-1,200,304,], ) 
 shape '[-1, 200, 304]' is invalid for input of size 2281703616
2025-03-11 18:42:05.341680 test begin: paddle.reshape(Tensor([1, 9, 932068, 272],"float32"), shape=list[-1,200,272,], )

[torch error] paddle.reshape(Tensor([1, 9, 932068, 272],"float32"), shape=list[-1,200,272,], ) 
 shape '[-1, 200, 272]' is invalid for input of size 2281702464
2025-03-11 18:42:06.839266 test begin: paddle.reshape(Tensor([1, 91268056, 5, 5],"float32"), shape=list[1,2,24,5,5,], )

[torch error] paddle.reshape(Tensor([1, 91268056, 5, 5],"float32"), shape=list[1,2,24,5,5,], ) 
 shape '[1, 2, 24, 5, 5]' is invalid for input of size 2281701400
2025-03-11 18:42:09.100472 test begin: paddle.reshape(Tensor([1, 95070891, 12, 2],"float32"), shape=tuple(-1,2,), )

[Pass] paddle.reshape(Tensor([1, 95070891, 12, 2],"float32"), shape=tuple(-1,2,), )
2025-03-11 18:44:40.084958 test begin: paddle.reshape(Tensor([1, 96, 3, 7922575],"float32"), shape=list[1,2,48,3,3,], )

[torch error] paddle.reshape(Tensor([1, 96, 3, 7922575],"float32"), shape=list[1,2,48,3,3,], ) 
 shape '[1, 2, 48, 3, 3]' is invalid for input of size 2281701600
2025-03-11 18:44:43.596918 test begin: paddle.reshape(Tensor([1, 96, 7922575, 3],"float32"), shape=list[1,2,48,3,3,], )

[torch error] paddle.reshape(Tensor([1, 96, 7922575, 3],"float32"), shape=list[1,2,48,3,3,], ) 
 shape '[1, 2, 48, 3, 3]' is invalid for input of size 2281701600
2025-03-11 18:44:45.491652 test begin: paddle.reshape(Tensor([1, 990322, 2304],"float32"), list[256,256,3,3,], )

[torch error] paddle.reshape(Tensor([1, 990322, 2304],"float32"), list[256,256,3,3,], ) 
 shape '[256, 256, 3, 3]' is invalid for input of size 2281701888
2025-03-11 18:44:47.943220 test begin: paddle.reshape(Tensor([10, 1, 228170138],"float32"), list[10,10,], )

[torch error] paddle.reshape(Tensor([10, 1, 228170138],"float32"), list[10,10,], ) 
 shape '[10, 10]' is invalid for input of size 2281701380
2025-03-11 18:44:50.152194 test begin: paddle.reshape(Tensor([10, 10, 10, 2281702],"float32"), list[10,100,20,], )

[torch error] paddle.reshape(Tensor([10, 10, 10, 2281702],"float32"), list[10,100,20,], ) 
 shape '[10, 100, 20]' is invalid for input of size 2281702000
2025-03-11 18:44:52.325706 test begin: paddle.reshape(Tensor([10, 10, 1140851, 20],"float32"), list[10,100,20,], )

[torch error] paddle.reshape(Tensor([10, 10, 1140851, 20],"float32"), list[10,100,20,], ) 
 shape '[10, 100, 20]' is invalid for input of size 2281702000
2025-03-11 18:44:54.499345 test begin: paddle.reshape(Tensor([10, 1140851, 10, 20],"float32"), list[10,100,20,], )

[torch error] paddle.reshape(Tensor([10, 1140851, 10, 20],"float32"), list[10,100,20,], ) 
 shape '[10, 100, 20]' is invalid for input of size 2281702000
2025-03-11 18:44:56.694397 test begin: paddle.reshape(Tensor([10, 228170138],"float32"), list[-1,1,], )

[Pass] paddle.reshape(Tensor([10, 228170138],"float32"), list[-1,1,], )
2025-03-11 18:47:56.978156 test begin: paddle.reshape(Tensor([10, 228170138],"float32"), list[2,25,], )

[torch error] paddle.reshape(Tensor([10, 228170138],"float32"), list[2,25,], ) 
 shape '[2, 25]' is invalid for input of size 2281701380
2025-03-11 18:48:01.251116 test begin: paddle.reshape(Tensor([10, 228170138],"int64"), list[2,25,], )

[torch error] paddle.reshape(Tensor([10, 228170138],"int64"), list[2,25,], ) 
 shape '[2, 25]' is invalid for input of size 2281701380
2025-03-11 18:48:59.338409 test begin: paddle.reshape(Tensor([10, 22817014, 10],"float32"), list[10,10,], )

[torch error] paddle.reshape(Tensor([10, 22817014, 10],"float32"), list[10,10,], ) 
 shape '[10, 10]' is invalid for input of size 2281701400
2025-03-11 18:49:03.566204 test begin: paddle.reshape(Tensor([10, 2328267, 7, 14],"float32"), list[10,4,14,7,], )

[torch error] paddle.reshape(Tensor([10, 2328267, 7, 14],"float32"), list[10,4,14,7,], ) 
 shape '[10, 4, 14, 7]' is invalid for input of size 2281701660
2025-03-11 18:49:05.728586 test begin: paddle.reshape(Tensor([10, 2507365, 13, 7],"float32"), list[10,4,7,13,], )

[torch error] paddle.reshape(Tensor([10, 2507365, 13, 7],"float32"), list[10,4,7,13,], ) 
 shape '[10, 4, 7, 13]' is invalid for input of size 2281702150
2025-03-11 18:49:07.694460 test begin: paddle.reshape(Tensor([10, 4, 13, 4387888],"float32"), list[10,4,7,13,], )

[torch error] paddle.reshape(Tensor([10, 4, 13, 4387888],"float32"), list[10,4,7,13,], ) 
 shape '[10, 4, 7, 13]' is invalid for input of size 2281701760
2025-03-11 18:49:10.140402 test begin: paddle.reshape(Tensor([10, 4, 4074467, 14],"float32"), list[10,4,14,7,], )

[torch error] paddle.reshape(Tensor([10, 4, 4074467, 14],"float32"), list[10,4,14,7,], ) 
 shape '[10, 4, 14, 7]' is invalid for input of size 2281701520
2025-03-11 18:49:12.326562 test begin: paddle.reshape(Tensor([10, 4, 7, 8148934],"float32"), list[10,4,14,7,], )

[torch error] paddle.reshape(Tensor([10, 4, 7, 8148934],"float32"), list[10,4,14,7,], ) 
 shape '[10, 4, 14, 7]' is invalid for input of size 2281701520
2025-03-11 18:49:14.099531 test begin: paddle.reshape(Tensor([10, 4, 8148934, 7],"float32"), list[10,4,7,13,], )

[torch error] paddle.reshape(Tensor([10, 4, 8148934, 7],"float32"), list[10,4,7,13,], ) 
 shape '[10, 4, 7, 13]' is invalid for input of size 2281701520
2025-03-11 18:49:16.177343 test begin: paddle.reshape(Tensor([10, 429496730],"float16"), list[10,20,], )

[torch error] paddle.reshape(Tensor([10, 429496730],"float16"), list[10,20,], ) 
 shape '[10, 20]' is invalid for input of size 4294967300
2025-03-11 18:50:33.712901 test begin: paddle.reshape(Tensor([100, 141282, 304],"float16"), shape=tuple(100,-1,), )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f3280d814f0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-11 19:00:43.617132 test begin: paddle.reshape(Tensor([100, 200, 114086],"float32"), shape=tuple(100,-1,), )

W0311 19:02:08.402580 129929 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 19:02:08.403815 129929 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.reshape(Tensor([100, 200, 114086],"float32"), shape=tuple(100,-1,), )
2025-03-11 19:04:21.999097 test begin: paddle.reshape(Tensor([100, 200, 214749],"float16"), shape=tuple(100,-1,), )

2025-03-11 19:13:43.939616 test begin: paddle.reshape(Tensor([100, 22817014],"float32"), shape=list[4,-1,256,], )

W0311 19:14:48.239310 146081 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 19:14:48.240767 146081 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.reshape(Tensor([100, 22817014],"float32"), shape=list[4,-1,256,], ) 
 shape '[4, -1, 256]' is invalid for input of size 2281701400
2025-03-11 19:14:50.215518 test begin: paddle.reshape(Tensor([100, 42949673],"float16"), shape=list[4,-1,256,], )

[torch error] paddle.reshape(Tensor([100, 42949673],"float16"), shape=list[4,-1,256,], ) 
 shape '[4, -1, 256]' is invalid for input of size 4294967300
2025-03-11 19:16:11.805832 test begin: paddle.reshape(Tensor([100, 75056, 304],"float32"), shape=tuple(100,-1,), )

[Pass] paddle.reshape(Tensor([100, 75056, 304],"float32"), shape=tuple(100,-1,), )
2025-03-11 19:19:03.950011 test begin: paddle.reshape(Tensor([1000, 2281702],"float32"), list[-1,1,4,], )

[Pass] paddle.reshape(Tensor([1000, 2281702],"float32"), list[-1,1,4,], )
2025-03-11 19:21:41.011595 test begin: paddle.reshape(Tensor([1000, 2281702],"int64"), tuple(-1,1,), )

[Pass] paddle.reshape(Tensor([1000, 2281702],"int64"), tuple(-1,1,), )
2025-03-11 19:25:07.256345 test begin: paddle.reshape(Tensor([1000, 3, 760568],"float32"), list[-1,4,], )

[Pass] paddle.reshape(Tensor([1000, 3, 760568],"float32"), list[-1,4,], )
2025-03-11 19:28:03.900730 test begin: paddle.reshape(Tensor([1000, 570426, 4],"float32"), list[-1,4,], )

[Pass] paddle.reshape(Tensor([1000, 570426, 4],"float32"), list[-1,4,], )
2025-03-11 19:31:01.601901 test begin: paddle.reshape(Tensor([100357, 232, 2, 7, 7],"float32"), shape=list[2,464,7,7,], )

[torch error] paddle.reshape(Tensor([100357, 232, 2, 7, 7],"float32"), shape=list[2,464,7,7,], ) 
 shape '[2, 464, 7, 7]' is invalid for input of size 2281716752
2025-03-11 19:31:05.650980 test begin: paddle.reshape(Tensor([100357, 464, 7, 7],"float32"), shape=list[2,2,232,7,7,], )

[torch error] paddle.reshape(Tensor([100357, 464, 7, 7],"float32"), shape=list[2,2,232,7,7,], ) 
 shape '[2, 2, 232, 7, 7]' is invalid for input of size 2281716752
2025-03-11 19:31:07.794964 test begin: paddle.reshape(Tensor([10044, 227171],"float32"), shape=tuple(-1,4,), )

[Pass] paddle.reshape(Tensor([10044, 227171],"float32"), shape=tuple(-1,4,), )
2025-03-11 19:33:49.553471 test begin: paddle.reshape(Tensor([1008, 2263593],"float32"), list[-1,1,4,], )

[Pass] paddle.reshape(Tensor([1008, 2263593],"float32"), list[-1,1,4,], )
2025-03-11 19:36:25.495299 test begin: paddle.reshape(Tensor([1008, 2263593],"float32"), shape=tuple(-1,4,), )

[Pass] paddle.reshape(Tensor([1008, 2263593],"float32"), shape=tuple(-1,4,), )
2025-03-11 19:39:06.440967 test begin: paddle.reshape(Tensor([1008, 3, 754531],"float32"), list[-1,4,], )

[Pass] paddle.reshape(Tensor([1008, 3, 754531],"float32"), list[-1,4,], )
2025-03-11 19:42:05.728691 test begin: paddle.reshape(Tensor([1008, 565899, 4],"float32"), list[-1,4,], )

[Pass] paddle.reshape(Tensor([1008, 565899, 4],"float32"), list[-1,4,], )
2025-03-11 19:45:09.724860 test begin: paddle.reshape(Tensor([10186167, 4, 7, 8],"float32"), list[13,4,7,1,-1,], )

[torch error] paddle.reshape(Tensor([10186167, 4, 7, 8],"float32"), list[13,4,7,1,-1,], ) 
 shape '[13, 4, 7, 1, -1]' is invalid for input of size 2281701408
2025-03-11 19:45:13.696814 test begin: paddle.reshape(Tensor([10186167, 4, 7, 8],"float32"), list[52,4,7,1,-1,], )

[torch error] paddle.reshape(Tensor([10186167, 4, 7, 8],"float32"), list[52,4,7,1,-1,], ) 
 shape '[52, 4, 7, 1, -1]' is invalid for input of size 2281701408
2025-03-11 19:45:15.186133 test begin: paddle.reshape(Tensor([10186167, 4, 7, 8],"float32"), tuple(52,-1,8,), )

[torch error] paddle.reshape(Tensor([10186167, 4, 7, 8],"float32"), tuple(52,-1,8,), ) 
 shape '[52, -1, 8]' is invalid for input of size 2281701408
2025-03-11 19:45:16.550362 test begin: paddle.reshape(Tensor([10186167, 7, 32],"float32"), list[13,-1,4,8,], )

[torch error] paddle.reshape(Tensor([10186167, 7, 32],"float32"), list[13,-1,4,8,], ) 
 shape '[13, -1, 4, 8]' is invalid for input of size 2281701408
2025-03-11 19:45:18.076374 test begin: paddle.reshape(Tensor([10186167, 7, 32],"float32"), list[13,7,4,-1,], )

[torch error] paddle.reshape(Tensor([10186167, 7, 32],"float32"), list[13,7,4,-1,], ) 
 shape '[13, 7, 4, -1]' is invalid for input of size 2281701408
2025-03-11 19:45:19.855076 test begin: paddle.reshape(Tensor([10186167, 7, 32],"float32"), list[13,7,4,8,], )

[torch error] paddle.reshape(Tensor([10186167, 7, 32],"float32"), list[13,7,4,8,], ) 
 shape '[13, 7, 4, 8]' is invalid for input of size 2281701408
2025-03-11 19:45:21.324366 test begin: paddle.reshape(Tensor([10186167, 7, 32],"float32"), list[13,7,8,4,], )

[torch error] paddle.reshape(Tensor([10186167, 7, 32],"float32"), list[13,7,8,4,], ) 
 shape '[13, 7, 8, 4]' is invalid for input of size 2281701408
2025-03-11 19:45:23.084074 test begin: paddle.reshape(Tensor([10186167, 7, 32],"float32"), list[52,7,4,-1,], )

[torch error] paddle.reshape(Tensor([10186167, 7, 32],"float32"), list[52,7,4,-1,], ) 
 shape '[52, 7, 4, -1]' is invalid for input of size 2281701408
2025-03-11 19:45:24.316399 test begin: paddle.reshape(Tensor([10186167, 7, 32],"float32"), shape=list[7,7,4,8,], )

[torch error] paddle.reshape(Tensor([10186167, 7, 32],"float32"), shape=list[7,7,4,8,], ) 
 shape '[7, 7, 4, 8]' is invalid for input of size 2281701408
2025-03-11 19:45:25.522930 test begin: paddle.reshape(Tensor([10186167, 7, 32],"float32"), tuple(2,7,13,32,), )

[torch error] paddle.reshape(Tensor([10186167, 7, 32],"float32"), tuple(2,7,13,32,), ) 
 shape '[2, 7, 13, 32]' is invalid for input of size 2281701408
2025-03-11 19:45:26.994994 test begin: paddle.reshape(Tensor([10186167, 7, 4, 8],"float32"), list[13,7,-1,], )

[torch error] paddle.reshape(Tensor([10186167, 7, 4, 8],"float32"), list[13,7,-1,], ) 
 shape '[13, 7, -1]' is invalid for input of size 2281701408
2025-03-11 19:45:28.724830 test begin: paddle.reshape(Tensor([10186167, 7, 4, 8],"float32"), list[52,7,-1,], )

[torch error] paddle.reshape(Tensor([10186167, 7, 4, 8],"float32"), list[52,7,-1,], ) 
 shape '[52, 7, -1]' is invalid for input of size 2281701408
2025-03-11 19:45:30.193848 test begin: paddle.reshape(Tensor([10186167, 7, 4, 8],"float32"), tuple(13,1,7,32,), )

[torch error] paddle.reshape(Tensor([10186167, 7, 4, 8],"float32"), tuple(13,1,7,32,), ) 
 shape '[13, 1, 7, 32]' is invalid for input of size 2281701408
2025-03-11 19:45:31.973566 test begin: paddle.reshape(Tensor([10186167, 7, 4, 8],"float32"), tuple(13,7,32,), )

[torch error] paddle.reshape(Tensor([10186167, 7, 4, 8],"float32"), tuple(13,7,32,), ) 
 shape '[13, 7, 32]' is invalid for input of size 2281701408
2025-03-11 19:45:33.505520 test begin: paddle.reshape(Tensor([1024, 1, 2228225],"float32"), shape=list[-1,1000,], )

[torch error] paddle.reshape(Tensor([1024, 1, 2228225],"float32"), shape=list[-1,1000,], ) 
 shape '[-1, 1000]' is invalid for input of size 2281702400
2025-03-11 19:45:35.330000 test begin: paddle.reshape(Tensor([1024, 1, 2228225],"float32"), shape=list[-1,32,128,], )

[torch error] paddle.reshape(Tensor([1024, 1, 2228225],"float32"), shape=list[-1,32,128,], ) 
 shape '[-1, 32, 128]' is invalid for input of size 2281702400
2025-03-11 19:45:36.624366 test begin: paddle.reshape(Tensor([1024, 1, 4194305],"float16"), shape=list[-1,1000,], )

[torch error] paddle.reshape(Tensor([1024, 1, 4194305],"float16"), shape=list[-1,1000,], ) 
 shape '[-1, 1000]' is invalid for input of size 4294968320
2025-03-11 19:45:40.309850 test begin: paddle.reshape(Tensor([1024, 17409, 128],"float32"), shape=list[-1,32,128,], )

[Pass] paddle.reshape(Tensor([1024, 17409, 128],"float32"), shape=list[-1,32,128,], )
2025-03-11 19:48:20.684017 test begin: paddle.reshape(Tensor([1024, 2229, 1000],"float32"), shape=list[-1,1000,], )

[Pass] paddle.reshape(Tensor([1024, 2229, 1000],"float32"), shape=list[-1,1000,], )
2025-03-11 19:51:18.069101 test begin: paddle.reshape(Tensor([1024, 4195, 1000],"float16"), shape=list[-1,1000,], )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7fd0e3ecfd30>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-11 20:01:29.252992 test begin: paddle.reshape(Tensor([104, 151831, 272],"float16"), shape=tuple(104,-1,), )

W0311 20:03:17.136615  7970 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 20:03:17.137800  7970 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7efe0cff4bb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-11 20:11:36.881139 test begin: paddle.reshape(Tensor([104, 200, 109698],"float32"), shape=tuple(104,-1,), )

W0311 20:13:07.100450 12497 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 20:13:07.101662 12497 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.reshape(Tensor([104, 200, 109698],"float32"), shape=tuple(104,-1,), )
2025-03-11 20:15:32.520799 test begin: paddle.reshape(Tensor([104, 200, 206489],"float16"), shape=tuple(104,-1,), )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7fb5f9d22940>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_ptr_inplace<c10::SafePyObject, std::allocator<c10::SafePyObject>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741695933 (unix time) try "date -d @1741695933" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x3071) received by PID 12401 (TID 0x7fb736949700) from PID 12401 ***]

2025-03-11 20:26:16.741859 test begin: paddle.reshape(Tensor([104, 2742430, 8],"float32"), tuple(-1,8,), )

W0311 20:27:44.258816 20441 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 20:27:44.260020 20441 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.reshape(Tensor([104, 2742430, 8],"float32"), tuple(-1,8,), )
2025-03-11 20:30:06.749363 test begin: paddle.reshape(Tensor([104, 2742430, 8],"float32"), tuple(2,-1,7,8,), )

[torch error] paddle.reshape(Tensor([104, 2742430, 8],"float32"), tuple(2,-1,7,8,), ) 
 shape '[2, -1, 7, 8]' is invalid for input of size 2281701760
2025-03-11 20:30:10.843953 test begin: paddle.reshape(Tensor([104, 7, 3134206],"float32"), tuple(-1,8,), )

[Pass] paddle.reshape(Tensor([104, 7, 3134206],"float32"), tuple(-1,8,), )
2025-03-11 20:33:00.103562 test begin: paddle.reshape(Tensor([104, 7, 3134206],"float32"), tuple(2,-1,7,8,), )

[Pass] paddle.reshape(Tensor([104, 7, 3134206],"float32"), tuple(2,-1,7,8,), )
2025-03-11 20:35:43.915645 test begin: paddle.reshape(Tensor([104, 80660, 272],"float32"), shape=tuple(104,-1,), )

[Pass] paddle.reshape(Tensor([104, 80660, 272],"float32"), shape=tuple(104,-1,), )
2025-03-11 20:38:30.701134 test begin: paddle.reshape(Tensor([1043, 25, 288, 304],"float32"), shape=list[-1,288,304,], )

[Pass] paddle.reshape(Tensor([1043, 25, 288, 304],"float32"), shape=list[-1,288,304,], )
2025-03-11 20:42:12.636708 test begin: paddle.reshape(Tensor([1048577, 1, 64, 32, 2],"float16"), list[1,1,64,64,], )

[torch error] paddle.reshape(Tensor([1048577, 1, 64, 32, 2],"float16"), list[1,1,64,64,], ) 
 shape '[1, 1, 64, 64]' is invalid for input of size 4294971392
2025-03-11 20:43:35.793776 test begin: paddle.reshape(Tensor([10513, 6, 68096],"float16"), shape=tuple(-1,224,304,), )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f735c0ec7f0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-11 20:53:45.264422 test begin: paddle.reshape(Tensor([105269, 100, 136, 3],"float16"), shape=tuple(8,-1,1,), )

W0311 20:55:24.871053 34471 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 20:55:24.872251 34471 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7ffa45a6cbb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-11 21:03:54.136487 test begin: paddle.reshape(Tensor([10611, 2, 336, 320],"float32"), shape=list[-1,336,320,], )

W0311 21:05:18.609931 39973 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 21:05:18.611119 39973 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.reshape(Tensor([10611, 2, 336, 320],"float32"), shape=list[-1,336,320,], )
2025-03-11 21:07:41.339945 test begin: paddle.reshape(Tensor([10700, 56, 56, 128],"float16"), list[128,56,56,128,], )

[torch error] paddle.reshape(Tensor([10700, 56, 56, 128],"float16"), list[128,56,56,128,], ) 
 shape '[128, 56, 56, 128]' is invalid for input of size 4295065600
2025-03-11 21:09:03.639413 test begin: paddle.reshape(Tensor([1073741825, 4],"float16"), list[1,-1,4,], )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7ffa64232670>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-11 21:19:12.701872 test begin: paddle.reshape(Tensor([107374183, 40],"float16"), shape=list[-1,20,2,], )

W0311 21:21:10.526755 48340 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 21:21:10.528173 48340 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f6d54ff4b80>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741699754 (unix time) try "date -d @1741699754" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xbc75) received by PID 48245 (TID 0x7f6d0134a700) from PID 48245 ***]

2025-03-11 21:30:01.247362 test begin: paddle.reshape(Tensor([10737419, 1, 1, 1, 400],"float16"), shape=tuple(8,-1,), )

W0311 21:31:46.972456 54476 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 21:31:46.974514 54476 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f4d2e42dc10>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-11 21:40:10.946695 test begin: paddle.reshape(Tensor([10737419, 4, 100],"float16"), list[-1,400,], )

W0311 21:41:50.865146 59899 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 21:41:50.866351 59899 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f43805b3d60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-11 21:50:18.784609 test begin: paddle.reshape(Tensor([10737419, 400],"float16"), list[-1,8,400,], )

W0311 21:51:48.468286 65217 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 21:51:48.469444 65217 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.reshape(Tensor([10737419, 400],"float16"), list[-1,8,400,], ) 
 shape '[-1, 8, 400]' is invalid for input of size 4294967600
2025-03-11 21:51:50.418843 test begin: paddle.reshape(Tensor([108, 146207, 272],"float16"), shape=tuple(108,-1,), )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7ff65acc9820>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-11 22:01:58.788809 test begin: paddle.reshape(Tensor([108, 200, 105635],"float32"), shape=tuple(108,-1,), )

W0311 22:03:23.747705 71298 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 22:03:23.749466 71298 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.reshape(Tensor([108, 200, 105635],"float32"), shape=tuple(108,-1,), )
2025-03-11 22:05:59.406847 test begin: paddle.reshape(Tensor([108, 200, 198842],"float16"), shape=tuple(108,-1,), )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f93cde06460>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-11 22:16:09.642886 test begin: paddle.reshape(Tensor([108, 77673, 272],"float32"), shape=tuple(108,-1,), )

W0311 22:17:55.787771 78755 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 22:17:55.789891 78755 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.reshape(Tensor([108, 77673, 272],"float32"), shape=tuple(108,-1,), )
2025-03-11 22:21:00.162823 test begin: paddle.reshape(Tensor([10969719, 4, 13, 4],"float32"), list[13,4,4,13,], )

[torch error] paddle.reshape(Tensor([10969719, 4, 13, 4],"float32"), list[13,4,4,13,], ) 
 shape '[13, 4, 4, 13]' is invalid for input of size 2281701552
2025-03-11 22:21:04.804175 test begin: paddle.reshape(Tensor([11, 1, 207427399],"float32"), shape=tuple(-1,), )

[Pass] paddle.reshape(Tensor([11, 1, 207427399],"float32"), shape=tuple(-1,), )
2025-03-11 22:23:58.294563 test begin: paddle.reshape(Tensor([11, 207427399, 1],"float32"), shape=tuple(-1,), )

[Pass] paddle.reshape(Tensor([11, 207427399, 1],"float32"), shape=tuple(-1,), )
2025-03-11 22:27:16.083208 test begin: paddle.reshape(Tensor([11, 2116607, 7, 14],"float32"), list[11,4,14,7,], )

[torch error] paddle.reshape(Tensor([11, 2116607, 7, 14],"float32"), list[11,4,14,7,], ) 
 shape '[11, 4, 14, 7]' is invalid for input of size 2281702346
2025-03-11 22:27:20.553529 test begin: paddle.reshape(Tensor([11, 2279422, 13, 7],"float32"), list[11,4,7,13,], )

[torch error] paddle.reshape(Tensor([11, 2279422, 13, 7],"float32"), list[11,4,7,13,], ) 
 shape '[11, 4, 7, 13]' is invalid for input of size 2281701422
2025-03-11 22:27:22.330399 test begin: paddle.reshape(Tensor([11, 4, 13, 3988989],"float32"), list[11,4,7,13,], )

[torch error] paddle.reshape(Tensor([11, 4, 13, 3988989],"float32"), list[11,4,7,13,], ) 
 shape '[11, 4, 7, 13]' is invalid for input of size 2281701708
2025-03-11 22:27:23.852804 test begin: paddle.reshape(Tensor([11, 4, 3704061, 14],"float32"), list[11,4,14,7,], )

[torch error] paddle.reshape(Tensor([11, 4, 3704061, 14],"float32"), list[11,4,14,7,], ) 
 shape '[11, 4, 14, 7]' is invalid for input of size 2281701576
2025-03-11 22:27:26.308870 test begin: paddle.reshape(Tensor([11, 4, 7, 7408122],"float32"), list[11,4,14,7,], )

[torch error] paddle.reshape(Tensor([11, 4, 7, 7408122],"float32"), list[11,4,14,7,], ) 
 shape '[11, 4, 14, 7]' is invalid for input of size 2281701576
2025-03-11 22:27:28.138602 test begin: paddle.reshape(Tensor([11, 4, 7408122, 7],"float32"), list[11,4,7,13,], )

[torch error] paddle.reshape(Tensor([11, 4, 7408122, 7],"float32"), list[11,4,7,13,], ) 
 shape '[11, 4, 7, 13]' is invalid for input of size 2281701576
2025-03-11 22:27:29.925064 test begin: paddle.reshape(Tensor([1114113, 2, 64, 16],"float32"), shape=list[13,2,-1,4,16,], )

[Pass] paddle.reshape(Tensor([1114113, 2, 64, 16],"float32"), shape=list[13,2,-1,4,16,], )
2025-03-11 22:30:31.817883 test begin: paddle.reshape(Tensor([1114113, 2, 64, 16],"float32"), shape=list[13,2,4,16,16,], )

[torch error] paddle.reshape(Tensor([1114113, 2, 64, 16],"float32"), shape=list[13,2,4,16,16,], ) 
 shape '[13, 2, 4, 16, 16]' is invalid for input of size 2281703424
2025-03-11 22:30:36.435521 test begin: paddle.reshape(Tensor([1114113, 2048, 1, 1],"float32"), list[-1,3,2048,], )

[Pass] paddle.reshape(Tensor([1114113, 2048, 1, 1],"float32"), list[-1,3,2048,], )
2025-03-11 22:33:49.367437 test begin: paddle.reshape(Tensor([1114113, 2048, 1, 1],"float32"), list[-1,8,2048,], )

[torch error] paddle.reshape(Tensor([1114113, 2048, 1, 1],"float32"), list[-1,8,2048,], ) 
 shape '[-1, 8, 2048]' is invalid for input of size 2281703424
2025-03-11 22:33:54.002778 test begin: paddle.reshape(Tensor([1114113, 2048, 1, 1],"float32"), list[240,2048,], )

[torch error] paddle.reshape(Tensor([1114113, 2048, 1, 1],"float32"), list[240,2048,], ) 
 shape '[240, 2048]' is invalid for input of size 2281703424
2025-03-11 22:33:55.972888 test begin: paddle.reshape(Tensor([1114113, 2048, 1, 1],"float32"), shape=list[-1,2048,], )

[Pass] paddle.reshape(Tensor([1114113, 2048, 1, 1],"float32"), shape=list[-1,2048,], )
2025-03-11 22:37:06.613778 test begin: paddle.reshape(Tensor([1114113, 2048],"float32"), list[1,2048,1,1,], )

[torch error] paddle.reshape(Tensor([1114113, 2048],"float32"), list[1,2048,1,1,], ) 
 shape '[1, 2048, 1, 1]' is invalid for input of size 2281703424
2025-03-11 22:37:11.055336 test begin: paddle.reshape(Tensor([1114113, 2048],"float32"), list[8,2048,1,1,], )

[torch error] paddle.reshape(Tensor([1114113, 2048],"float32"), list[8,2048,1,1,], ) 
 shape '[8, 2048, 1, 1]' is invalid for input of size 2281703424
2025-03-11 22:37:13.562138 test begin: paddle.reshape(Tensor([1114113, 2048],"float32"), shape=list[-1,2048,], )

[Pass] paddle.reshape(Tensor([1114113, 2048],"float32"), shape=list[-1,2048,], )
2025-03-11 22:40:34.483688 test begin: paddle.reshape(Tensor([1114113, 4, 512],"float32"), shape=list[-1,512,], )

[Pass] paddle.reshape(Tensor([1114113, 4, 512],"float32"), shape=list[-1,512,], )
2025-03-11 22:44:10.596927 test begin: paddle.reshape(Tensor([11142, 2, 320, 320],"float32"), shape=list[-1,320,320,], )

[Pass] paddle.reshape(Tensor([11142, 2, 320, 320],"float32"), shape=list[-1,320,320,], )
2025-03-11 22:48:02.007054 test begin: paddle.reshape(Tensor([112, 140986, 272],"float16"), shape=tuple(112,-1,), )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f46a69554f0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-11 22:58:14.068740 test begin: paddle.reshape(Tensor([112, 200, 101862],"float32"), shape=tuple(112,-1,), )

W0311 22:59:54.626380 100477 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 22:59:54.628171 100477 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.reshape(Tensor([112, 200, 101862],"float32"), shape=tuple(112,-1,), )
2025-03-11 23:03:03.527588 test begin: paddle.reshape(Tensor([112, 200, 191740],"float16"), shape=tuple(112,-1,), )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7fb5fdede940>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-11 23:13:16.613452 test begin: paddle.reshape(Tensor([112, 20372334],"int64"), list[14,2,4,2,], name="Categorical_sample", )

W0311 23:14:36.808215 108372 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0311 23:14:36.809249 108372 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.reshape(Tensor([112, 20372334],"int64"), list[14,2,4,2,], name="Categorical_sample", ) 
 shape '[14, 2, 4, 2]' is invalid for input of size 2281701408
2025-03-11 23:14:38.767941 test begin: paddle.reshape(Tensor([112, 74899, 272],"float32"), shape=tuple(112,-1,), )

[Pass] paddle.reshape(Tensor([112, 74899, 272],"float32"), shape=tuple(112,-1,), )
2025-03-11 23:19:35.981334 test begin: paddle.reshape(Tensor([11369, 28, 28, 256],"float32"), list[12,28,28,256,], )

[torch error] paddle.reshape(Tensor([11369, 28, 28, 256],"float32"), list[12,28,28,256,], ) 
 shape '[12, 28, 28, 256]' is invalid for input of size 2281803776
2025-03-11 23:19:40.684800 test begin: paddle.reshape(Tensor([11369, 28, 28, 256],"float32"), list[16,28,28,256,], )

[torch error] paddle.reshape(Tensor([11369, 28, 28, 256],"float32"), list[16,28,28,256,], ) 
 shape '[16, 28, 28, 256]' is invalid for input of size 2281803776
2025-03-11 23:19:42.463783 test begin: paddle.reshape(Tensor([11369, 28, 28, 256],"float32"), list[60,28,28,256,], )

[torch error] paddle.reshape(Tensor([11369, 28, 28, 256],"float32"), list[60,28,28,256,], ) 
 shape '[60, 28, 28, 256]' is invalid for input of size 2281803776
2025-03-11 23:19:44.410708 test begin: paddle.reshape(Tensor([11369, 28, 28, 256],"float32"), list[64,28,28,256,], )

[torch error] paddle.reshape(Tensor([11369, 28, 28, 256],"float32"), list[64,28,28,256,], ) 
 shape '[64, 28, 28, 256]' is invalid for input of size 2281803776
2025-03-11 23:19:46.105447 test begin: paddle.reshape(Tensor([114085069, 20],"bool"), list[-1,20,1,], )

[Pass] paddle.reshape(Tensor([114085069, 20],"bool"), list[-1,20,1,], )
2025-03-11 23:21:25.650854 test begin: paddle.reshape(Tensor([114085069, 20],"float32"), list[4,5,4,], )

[torch error] paddle.reshape(Tensor([114085069, 20],"float32"), list[4,5,4,], ) 
 shape '[4, 5, 4]' is invalid for input of size 2281701380
2025-03-11 23:21:29.786073 test begin: paddle.reshape(Tensor([114085069, 4, 5],"float32"), list[1,-1,], )

[Pass] paddle.reshape(Tensor([114085069, 4, 5],"float32"), list[1,-1,], )
2025-03-11 23:25:18.131733 test begin: paddle.reshape(Tensor([114085069, 4, 5],"float32"), list[2,-1,], )

[Pass] paddle.reshape(Tensor([114085069, 4, 5],"float32"), list[2,-1,], )
2025-03-11 23:28:20.714258 test begin: paddle.reshape(Tensor([114085069, 4, 5],"float32"), list[3,-1,], )

[torch error] paddle.reshape(Tensor([114085069, 4, 5],"float32"), list[3,-1,], ) 
 shape '[3, -1]' is invalid for input of size 2281701380
2025-03-11 23:28:24.878977 test begin: paddle.reshape(Tensor([114085069, 4, 5],"float32"), list[4,-1,], )

[Pass] paddle.reshape(Tensor([114085069, 4, 5],"float32"), list[4,-1,], )
2025-03-11 23:31:23.918500 test begin: paddle.reshape(Tensor([114085069, 4, 5],"float32"), list[5,-1,], )

[Pass] paddle.reshape(Tensor([114085069, 4, 5],"float32"), list[5,-1,], )
2025-03-11 23:34:27.296641 test begin: paddle.reshape(Tensor([114085069, 5, 4],"float32"), tuple(4,-1,), )

[Pass] paddle.reshape(Tensor([114085069, 5, 4],"float32"), tuple(4,-1,), )
2025-03-11 23:37:28.352463 test begin: paddle.reshape(Tensor([1140850690, 1, 2],"float32"), shape=list[-1,2,], )

[Pass] paddle.reshape(Tensor([1140850690, 1, 2],"float32"), shape=list[-1,2,], )
2025-03-11 23:40:25.818289 test begin: paddle.reshape(Tensor([1140850690, 2],"float32"), shape=list[1,20,2,], )

[torch error] paddle.reshape(Tensor([1140850690, 2],"float32"), shape=list[1,20,2,], ) 
 shape '[1, 20, 2]' is invalid for input of size 2281701380
2025-03-11 23:40:29.782515 test begin: paddle.reshape(Tensor([1140850690, 2],"float32"), shape=list[3200,1,2,], )

[torch error] paddle.reshape(Tensor([1140850690, 2],"float32"), shape=list[3200,1,2,], ) 
 shape '[3200, 1, 2]' is invalid for input of size 2281701380
2025-03-11 23:40:31.448693 test begin: paddle.reshape(Tensor([1140850690, 2],"float32"), shape=list[40,], )

[torch error] paddle.reshape(Tensor([1140850690, 2],"float32"), shape=list[40,], ) 
 shape '[40]' is invalid for input of size 2281701380
2025-03-11 23:40:32.546978 test begin: paddle.reshape(Tensor([1140850690, 2],"float32"), shape=list[5,-1,], )

[Pass] paddle.reshape(Tensor([1140850690, 2],"float32"), shape=list[5,-1,], )
2025-03-11 23:43:20.249578 test begin: paddle.reshape(Tensor([1140850690, 2],"float32"), tuple(2,4,), )

[torch error] paddle.reshape(Tensor([1140850690, 2],"float32"), tuple(2,4,), ) 
 shape '[2, 4]' is invalid for input of size 2281701380
2025-03-11 23:43:23.059620 test begin: paddle.reshape(Tensor([1140850690, 2],"int64"), list[14,2,4,2,], name="Categorical_sample", )

[torch error] paddle.reshape(Tensor([1140850690, 2],"int64"), list[14,2,4,2,], name="Categorical_sample", ) 
 shape '[14, 2, 4, 2]' is invalid for input of size 2281701380
2025-03-11 23:43:32.147683 test begin: paddle.reshape(Tensor([11408507, 20, 10],"float32"), list[-1,10,], )

[Pass] paddle.reshape(Tensor([11408507, 20, 10],"float32"), list[-1,10,], )
2025-03-11 23:46:41.880033 test begin: paddle.reshape(Tensor([1140851, 10, 10, 20],"float32"), list[10,100,20,], )

[torch error] paddle.reshape(Tensor([1140851, 10, 10, 20],"float32"), list[10,100,20,], ) 
 shape '[10, 100, 20]' is invalid for input of size 2281702000
2025-03-11 23:46:44.310700 test begin: paddle.reshape(Tensor([114086, 100, 100, 2],"float32"), tuple(-1,2,), )

[Pass] paddle.reshape(Tensor([114086, 100, 100, 2],"float32"), tuple(-1,2,), )
2025-03-11 23:49:45.244375 test begin: paddle.reshape(Tensor([11523745, 198],"float32"), list[6,-1,], )

[Pass] paddle.reshape(Tensor([11523745, 198],"float32"), list[6,-1,], )
2025-03-11 23:52:43.230032 test begin: paddle.reshape(Tensor([116, 136124, 272],"float16"), shape=tuple(116,-1,), )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f8af415bbe0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 00:02:56.900649 test begin: paddle.reshape(Tensor([116, 200, 185128],"float16"), shape=tuple(116,-1,), )

W0312 00:04:41.827435 134075 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 00:04:41.828869 134075 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f969a42dbb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 00:13:06.002892 test begin: paddle.reshape(Tensor([116, 200, 98350],"float32"), shape=tuple(116,-1,), )

W0312 00:14:43.231132 139714 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 00:14:43.232993 139714 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.reshape(Tensor([116, 200, 98350],"float32"), shape=tuple(116,-1,), )
2025-03-12 00:17:57.779890 test begin: paddle.reshape(Tensor([116, 72316, 272],"float32"), shape=tuple(116,-1,), )

[Pass] paddle.reshape(Tensor([116, 72316, 272],"float32"), shape=tuple(116,-1,), )
2025-03-12 00:20:55.954888 test begin: paddle.reshape(Tensor([11606, 3, 256, 256],"float32"), list[-1,196608,], )

[Pass] paddle.reshape(Tensor([11606, 3, 256, 256],"float32"), list[-1,196608,], )
2025-03-12 00:24:45.812521 test begin: paddle.reshape(Tensor([11641334, 28, 7],"int32"), tuple(-1,7,), )

[Pass] paddle.reshape(Tensor([11641334, 28, 7],"int32"), tuple(-1,7,), )
2025-03-12 00:27:45.559159 test begin: paddle.reshape(Tensor([1164134, 40, 49],"float32"), list[1960,], )

[torch error] paddle.reshape(Tensor([1164134, 40, 49],"float32"), list[1960,], ) 
 shape '[1960]' is invalid for input of size 2281702640
2025-03-12 00:27:50.441309 test begin: paddle.reshape(Tensor([1165, 35, 200, 280],"float32"), shape=list[-1,200,280,], )

[Pass] paddle.reshape(Tensor([1165, 35, 200, 280],"float32"), shape=list[-1,200,280,], )
2025-03-12 00:31:43.817186 test begin: paddle.reshape(Tensor([116509, 256, 12, 12],"float16"), shape=tuple(4,256,-1,), )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7fb9ef9da8e0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 00:42:04.108928 test begin: paddle.reshape(Tensor([1170, 22, 264, 336],"float32"), shape=list[-1,264,336,], )

W0312 00:43:37.230125 155771 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 00:43:37.231562 155771 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.reshape(Tensor([1170, 22, 264, 336],"float32"), shape=list[-1,264,336,], )
2025-03-12 00:46:59.389089 test begin: paddle.reshape(Tensor([11751, 2, 328, 296],"float32"), shape=list[-1,328,296,], )

[Pass] paddle.reshape(Tensor([11751, 2, 328, 296],"float32"), shape=list[-1,328,296,], )
2025-03-12 00:50:55.133643 test begin: paddle.reshape(Tensor([11847, 1, 192612],"float32"), shape=list[-1,192612,], )

[Pass] paddle.reshape(Tensor([11847, 1, 192612],"float32"), shape=list[-1,192612,], )
2025-03-12 00:54:44.365399 test begin: paddle.reshape(Tensor([11883862, 192],"float32"), list[1,64,3,], )

[torch error] paddle.reshape(Tensor([11883862, 192],"float32"), list[1,64,3,], ) 
 shape '[1, 64, 3]' is invalid for input of size 2281701504
2025-03-12 00:54:48.664334 test begin: paddle.reshape(Tensor([11883862, 192],"float32"), list[128,64,3,], )

[torch error] paddle.reshape(Tensor([11883862, 192],"float32"), list[128,64,3,], ) 
 shape '[128, 64, 3]' is invalid for input of size 2281701504
2025-03-12 00:54:52.191384 test begin: paddle.reshape(Tensor([11883862, 192],"float32"), list[64,64,3,], )

[torch error] paddle.reshape(Tensor([11883862, 192],"float32"), list[64,64,3,], ) 
 shape '[64, 64, 3]' is invalid for input of size 2281701504
2025-03-12 00:54:54.634425 test begin: paddle.reshape(Tensor([11883862, 64, 3],"float32"), tuple(1,-1,), )

[Pass] paddle.reshape(Tensor([11883862, 64, 3],"float32"), tuple(1,-1,), )
2025-03-12 00:58:43.622932 test begin: paddle.reshape(Tensor([11883862, 64, 3],"float32"), tuple(128,-1,), )

[Pass] paddle.reshape(Tensor([11883862, 64, 3],"float32"), tuple(128,-1,), )
2025-03-12 01:02:35.431628 test begin: paddle.reshape(Tensor([1188387, 20, 96],"float32"), list[1,20,4,-1,], )

[Pass] paddle.reshape(Tensor([1188387, 20, 96],"float32"), list[1,20,4,-1,], )
2025-03-12 01:06:10.092406 test begin: paddle.reshape(Tensor([119305, 100, 120, 3],"float16"), shape=tuple(4,-1,1,), )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7eff50d36760>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 01:16:22.783583 test begin: paddle.reshape(Tensor([12, 10923, 34, 512],"float32"), list[-1,512,], )

W0312 01:18:00.838773 10883 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 01:18:00.840153 10883 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.reshape(Tensor([12, 10923, 34, 512],"float32"), list[-1,512,], )
2025-03-12 01:21:47.130370 test begin: paddle.reshape(Tensor([12, 1177349, 304],"float16"), shape=tuple(12,-1,), )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7fe0637433a0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 01:31:55.922643 test begin: paddle.reshape(Tensor([12, 138512, 19, 34, 4],"float16"), list[-1,4,], )

W0312 01:33:35.095727 19188 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 01:33:35.096877 19188 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f5bb7bf2bb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 01:42:05.411170 test begin: paddle.reshape(Tensor([12, 14, 14, 970112],"float32"), list[12,14,14,512,], )

W0312 01:43:25.846086 23870 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 01:43:25.847282 23870 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.reshape(Tensor([12, 14, 14, 970112],"float32"), list[12,14,14,512,], ) 
 shape '[12, 14, 14, 512]' is invalid for input of size 2281703424
2025-03-12 01:43:27.941758 test begin: paddle.reshape(Tensor([12, 14, 26527, 512],"float32"), list[12,14,14,512,], )

[torch error] paddle.reshape(Tensor([12, 14, 26527, 512],"float32"), list[12,14,14,512,], ) 
 shape '[12, 14, 14, 512]' is invalid for input of size 2281746432
2025-03-12 01:43:29.968768 test begin: paddle.reshape(Tensor([12, 147169, 19, 34, 2],"float32"), list[-1,2,], )

[Pass] paddle.reshape(Tensor([12, 147169, 19, 34, 2],"float32"), list[-1,2,], )
2025-03-12 01:46:09.833263 test begin: paddle.reshape(Tensor([12, 19, 19546, 512],"float32"), list[-1,512,], )

[Pass] paddle.reshape(Tensor([12, 19, 19546, 512],"float32"), list[-1,512,], )
2025-03-12 01:49:03.280184 test begin: paddle.reshape(Tensor([12, 19, 34, 294338],"float32"), list[-1,512,], )

[torch error] paddle.reshape(Tensor([12, 19, 34, 294338],"float32"), list[-1,512,], ) 
 shape '[-1, 512]' is invalid for input of size 2281708176
2025-03-12 01:49:07.366371 test begin: paddle.reshape(Tensor([12, 19, 34, 554047],"float16"), list[-1,512,], )

[torch error] paddle.reshape(Tensor([12, 19, 34, 554047],"float16"), list[-1,512,], ) 
 shape '[-1, 512]' is invalid for input of size 4294972344
2025-03-12 01:50:34.019561 test begin: paddle.reshape(Tensor([12, 19, 36793, 512],"float16"), list[-1,512,], )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f99f17442e0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 02:00:42.740545 test begin: paddle.reshape(Tensor([12, 14, 14, 970112],"float32"), list[12,14,14,512,], )

W0312 02:01:48.494380 32555 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 02:01:48.495468 32555 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.reshape(Tensor([12, 14, 14, 970112],"float32"), list[12,14,14,512,], ) 
 shape '[12, 14, 14, 512]' is invalid for input of size 2281703424
2025-03-12 02:01:50.756733 test begin: paddle.reshape(Tensor([12, 190141782],"float32"), list[15,4,], )

[torch error] paddle.reshape(Tensor([12, 190141782],"float32"), list[15,4,], ) 
 shape '[15, 4]' is invalid for input of size 2281701384
2025-03-12 02:01:52.973730 test begin: paddle.reshape(Tensor([12, 190141782],"float32"), shape=list[-1,102,], )

[torch error] paddle.reshape(Tensor([12, 190141782],"float32"), shape=list[-1,102,], ) 
 shape '[-1, 102]' is invalid for input of size 2281701384
2025-03-12 02:01:56.058937 test begin: paddle.reshape(Tensor([12, 190141782],"float32"), shape=list[-1,3,], )

[Pass] paddle.reshape(Tensor([12, 190141782],"float32"), shape=list[-1,3,], )
2025-03-12 02:04:53.365621 test begin: paddle.reshape(Tensor([12, 190141782],"int64"), list[12,], )

[torch error] paddle.reshape(Tensor([12, 190141782],"int64"), list[12,], ) 
 shape '[12]' is invalid for input of size 2281701384
2025-03-12 02:06:05.731005 test begin: paddle.reshape(Tensor([12, 190141782],"int64"), list[15,4,], )

[torch error] paddle.reshape(Tensor([12, 190141782],"int64"), list[15,4,], ) 
 shape '[15, 4]' is invalid for input of size 2281701384
2025-03-12 02:06:09.658047 test begin: paddle.reshape(Tensor([12, 200, 1789570],"float16"), shape=tuple(12,-1,), )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7fcd78da1520>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741716974 (unix time) try "date -d @1741716974" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x7f2b) received by PID 32555 (TID 0x7fcd2ddc2700) from PID 32555 ***]

2025-03-12 02:17:00.971916 test begin: paddle.reshape(Tensor([12, 200, 950709],"float32"), shape=tuple(12,-1,), )

W0312 02:18:39.036620 40471 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 02:18:39.038713 40471 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.reshape(Tensor([12, 200, 950709],"float32"), shape=tuple(12,-1,), )
2025-03-12 02:21:23.524996 test begin: paddle.reshape(Tensor([12, 20561, 34, 512],"float16"), list[-1,512,], )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f997d8e4e80>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 02:31:33.434648 test begin: paddle.reshape(Tensor([12, 26527, 14, 512],"float32"), list[12,14,14,512,], )

W0312 02:32:45.493191 46992 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 02:32:45.494856 46992 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.reshape(Tensor([12, 26527, 14, 512],"float32"), list[12,14,14,512,], ) 
 shape '[12, 14, 14, 512]' is invalid for input of size 2281746432
2025-03-12 02:32:47.578189 test begin: paddle.reshape(Tensor([12, 26527, 28, 256],"float32"), list[12,28,28,256,], )

[torch error] paddle.reshape(Tensor([12, 26527, 28, 256],"float32"), list[12,28,28,256,], ) 
 shape '[12, 28, 28, 256]' is invalid for input of size 2281746432
2025-03-12 02:32:49.058007 test begin: paddle.reshape(Tensor([12, 26527, 56, 128],"float32"), list[12,56,56,128,], )

[torch error] paddle.reshape(Tensor([12, 26527, 56, 128],"float32"), list[12,56,56,128,], ) 
 shape '[12, 56, 56, 128]' is invalid for input of size 2281746432
2025-03-12 02:32:51.485053 test begin: paddle.reshape(Tensor([12, 277024, 19, 34, 2],"float16"), list[-1,2,], )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f647c42ebe0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 02:43:04.265573 test begin: paddle.reshape(Tensor([12, 28, 26527, 256],"float32"), list[12,28,28,256,], )

W0312 02:44:19.492719 52297 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 02:44:19.493877 52297 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.reshape(Tensor([12, 28, 26527, 256],"float32"), list[12,28,28,256,], ) 
 shape '[12, 28, 28, 256]' is invalid for input of size 2281746432
2025-03-12 02:44:21.354091 test begin: paddle.reshape(Tensor([12, 28, 28, 242528],"float32"), list[12,28,28,256,], )

[torch error] paddle.reshape(Tensor([12, 28, 28, 242528],"float32"), list[12,28,28,256,], ) 
 shape '[12, 28, 28, 256]' is invalid for input of size 2281703424
2025-03-12 02:44:23.243794 test begin: paddle.reshape(Tensor([12, 32, 15, 396129],"float32"), shape=list[12,32,-1,], )

[Pass] paddle.reshape(Tensor([12, 32, 15, 396129],"float32"), shape=list[12,32,-1,], )
2025-03-12 02:46:55.307508 test begin: paddle.reshape(Tensor([12, 32, 396129, 15],"float32"), shape=list[12,32,-1,], )

[Pass] paddle.reshape(Tensor([12, 32, 396129, 15],"float32"), shape=list[12,32,-1,], )
2025-03-12 02:49:51.677779 test begin: paddle.reshape(Tensor([12, 357913942],"float16"), shape=list[-1,3,], )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7fdec32d2e80>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 03:00:01.369339 test begin: paddle.reshape(Tensor([12, 4, 1315861, 34, 2],"float16"), list[-1,2,], )

W0312 03:01:54.686933 60686 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 03:01:54.688249 60686 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f86805b3bb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 03:10:08.908791 test begin: paddle.reshape(Tensor([12, 4, 19, 1177349, 4],"float16"), list[-1,4,], )

W0312 03:12:09.608073 65518 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 03:12:09.609227 65518 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f40edbf3bb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 03:20:20.129612 test begin: paddle.reshape(Tensor([12, 4, 19, 1250933, 2],"float32"), list[-1,2,], )

W0312 03:21:45.439839 70319 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 03:21:45.441083 70319 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.reshape(Tensor([12, 4, 19, 1250933, 2],"float32"), list[-1,2,], )
2025-03-12 03:24:07.683098 test begin: paddle.reshape(Tensor([12, 4, 19, 2354697, 2],"float16"), list[-1,2,], )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f35b97379d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 03:34:15.657350 test begin: paddle.reshape(Tensor([12, 4, 19, 34, 138512],"float16"), list[-1,2,], )

W0312 03:35:54.579000 77083 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 03:35:54.583451 77083 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f1189bf2bb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 03:44:23.890530 test begin: paddle.reshape(Tensor([12, 4, 19, 34, 138512],"float16"), list[-1,4,], )

W0312 03:46:13.387428 82180 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 03:46:13.388636 82180 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f9e805b3bb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 03:54:32.186524 test begin: paddle.reshape(Tensor([12, 4, 19, 34, 138512],"float16"), list[-1,4,], )

W0312 03:56:12.080591 87567 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 03:56:12.081796 87567 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f18faff5bb0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 04:04:39.029743 test begin: paddle.reshape(Tensor([12, 4, 19, 34, 73585],"float32"), list[-1,2,], )

W0312 04:06:03.483912 91939 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 04:06:03.485484 91939 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.reshape(Tensor([12, 4, 19, 34, 73585],"float32"), list[-1,2,], )
2025-03-12 04:08:28.917502 test begin: paddle.reshape(Tensor([12, 4, 657931, 34, 4],"float16"), list[-1,4,], )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f09580969d0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 04:18:38.925445 test begin: paddle.reshape(Tensor([12, 4, 19, 34, 73585],"float32"), list[-1,2,], )

W0312 04:20:05.635337 97909 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 04:20:05.636435 97909 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.reshape(Tensor([12, 4, 19, 34, 73585],"float32"), list[-1,2,], )
2025-03-12 04:23:01.749527 test begin: paddle.reshape(Tensor([12, 4, 699051, 34, 2],"float32"), list[-1,2,], )

[Pass] paddle.reshape(Tensor([12, 4, 699051, 34, 2],"float32"), list[-1,2,], )
2025-03-12 04:26:08.682152 test begin: paddle.reshape(Tensor([12, 56, 26527, 128],"float32"), list[12,56,56,128,], )

[torch error] paddle.reshape(Tensor([12, 56, 26527, 128],"float32"), list[12,56,56,128,], ) 
 shape '[12, 56, 56, 128]' is invalid for input of size 2281746432
2025-03-12 04:26:12.846037 test begin: paddle.reshape(Tensor([12, 56, 56, 60632],"float32"), list[12,56,56,128,], )

[torch error] paddle.reshape(Tensor([12, 56, 56, 60632],"float32"), list[12,56,56,128,], ) 
 shape '[12, 56, 56, 128]' is invalid for input of size 2281703424
2025-03-12 04:26:14.932843 test begin: paddle.reshape(Tensor([12, 625467, 304],"float32"), shape=tuple(12,-1,), )

[Pass] paddle.reshape(Tensor([12, 625467, 304],"float32"), shape=tuple(12,-1,), )
2025-03-12 04:29:27.779981 test begin: paddle.reshape(Tensor([12, 845075, 15, 15],"float32"), shape=list[12,32,-1,], )

[torch error] paddle.reshape(Tensor([12, 845075, 15, 15],"float32"), shape=list[12,32,-1,], ) 
 shape '[12, 32, -1]' is invalid for input of size 2281702500
2025-03-12 04:29:32.304345 test begin: paddle.reshape(Tensor([120, 131587, 272],"float16"), shape=tuple(120,-1,), )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f1f30efe760>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_ptr_inplace<c10::SafePyObject, std::allocator<c10::SafePyObject>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741725575 (unix time) try "date -d @1741725575" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x17e1d) received by PID 97821 (TID 0x7f1ed3f48700) from PID 97821 ***]

2025-03-12 04:40:16.921934 test begin: paddle.reshape(Tensor([12, 4, 699051, 34, 2],"float32"), list[-1,2,], )

W0312 04:41:53.440181 107373 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 04:41:53.441412 107373 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.reshape(Tensor([12, 4, 699051, 34, 2],"float32"), list[-1,2,], )
2025-03-12 04:44:42.703912 test begin: paddle.reshape(Tensor([120, 200, 178957],"float16"), shape=tuple(120,-1,), )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f5fff9338b0>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_ptr_inplace<c10::SafePyObject, std::allocator<c10::SafePyObject>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741726487 (unix time) try "date -d @1741726487" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1a2b0) received by PID 107184 (TID 0x7f5fb5f48700) from PID 107184 ***]

2025-03-12 04:55:30.575812 test begin: paddle.reshape(Tensor([120, 200, 95071],"float32"), shape=tuple(120,-1,), )

W0312 04:57:06.437666 113629 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 04:57:06.438913 113629 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.reshape(Tensor([120, 200, 95071],"float32"), shape=tuple(120,-1,), )
2025-03-12 05:00:25.433126 test begin: paddle.reshape(Tensor([120, 69906, 272],"float32"), shape=tuple(120,-1,), )

[Pass] paddle.reshape(Tensor([120, 69906, 272],"float32"), shape=tuple(120,-1,), )
2025-03-12 05:04:01.327719 test begin: paddle.reshape(Tensor([1201, 25, 264, 288],"float32"), shape=list[-1,264,288,], )

[Pass] paddle.reshape(Tensor([1201, 25, 264, 288],"float32"), shape=list[-1,264,288,], )
2025-03-12 05:08:01.162603 test begin: paddle.reshape(Tensor([121264, 12, 2, 28, 28],"float32"), shape=list[2,24,28,28,], )

[torch error] paddle.reshape(Tensor([121264, 12, 2, 28, 28],"float32"), shape=list[2,24,28,28,], ) 
 shape '[2, 24, 28, 28]' is invalid for input of size 2281703424
2025-03-12 05:08:05.479011 test begin: paddle.reshape(Tensor([121264, 24, 28, 28],"float32"), shape=list[2,2,12,28,28,], )

[torch error] paddle.reshape(Tensor([121264, 24, 28, 28],"float32"), shape=list[2,2,12,28,28,], ) 
 shape '[2, 2, 12, 28, 28]' is invalid for input of size 2281703424
2025-03-12 05:08:07.499165 test begin: paddle.reshape(Tensor([121264, 48, 2, 14, 14],"float32"), shape=list[2,96,14,14,], )

[torch error] paddle.reshape(Tensor([121264, 48, 2, 14, 14],"float32"), shape=list[2,96,14,14,], ) 
 shape '[2, 96, 14, 14]' is invalid for input of size 2281703424
2025-03-12 05:08:09.032201 test begin: paddle.reshape(Tensor([121264, 96, 14, 14],"float32"), shape=list[2,2,48,14,14,], )

[torch error] paddle.reshape(Tensor([121264, 96, 14, 14],"float32"), shape=list[2,2,48,14,14,], ) 
 shape '[2, 2, 48, 14, 14]' is invalid for input of size 2281703424
2025-03-12 05:08:10.933523 test begin: paddle.reshape(Tensor([124, 127342, 272],"float16"), shape=tuple(124,-1,), )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f2142880820>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 05:18:24.672301 test begin: paddle.reshape(Tensor([120, 200, 95071],"float32"), shape=tuple(120,-1,), )

W0312 05:20:04.639573 122935 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 05:20:04.641070 122935 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.reshape(Tensor([120, 200, 95071],"float32"), shape=tuple(120,-1,), )
2025-03-12 05:23:36.417646 test begin: paddle.reshape(Tensor([120, 69906, 272],"float32"), shape=tuple(120,-1,), )

[Pass] paddle.reshape(Tensor([120, 69906, 272],"float32"), shape=tuple(120,-1,), )
2025-03-12 05:26:53.608511 test begin: paddle.reshape(Tensor([124, 14, 14, 93882],"float32"), list[124,14,14,384,], )

[torch error] paddle.reshape(Tensor([124, 14, 14, 93882],"float32"), list[124,14,14,384,], ) 
 shape '[124, 14, 14, 384]' is invalid for input of size 2281708128
2025-03-12 05:26:58.196599 test begin: paddle.reshape(Tensor([124, 14, 14, 93882],"float32"), list[124,14,14,768,], )

[torch error] paddle.reshape(Tensor([124, 14, 14, 93882],"float32"), list[124,14,14,768,], ) 
 shape '[124, 14, 14, 768]' is invalid for input of size 2281708128
2025-03-12 05:27:00.015377 test begin: paddle.reshape(Tensor([124, 14, 1712, 768],"float32"), list[124,14,14,768,], )

[torch error] paddle.reshape(Tensor([124, 14, 1712, 768],"float32"), list[124,14,14,768,], ) 
 shape '[124, 14, 14, 768]' is invalid for input of size 2282520576
2025-03-12 05:27:02.571134 test begin: paddle.reshape(Tensor([124, 14, 3423, 384],"float32"), list[124,14,14,384,], )

[torch error] paddle.reshape(Tensor([124, 14, 3423, 384],"float32"), list[124,14,14,384,], ) 
 shape '[124, 14, 14, 384]' is invalid for input of size 2281853952
2025-03-12 05:27:05.278044 test begin: paddle.reshape(Tensor([124, 1712, 14, 768],"float32"), list[124,14,14,768,], )

[torch error] paddle.reshape(Tensor([124, 1712, 14, 768],"float32"), list[124,14,14,768,], ) 
 shape '[124, 14, 14, 768]' is invalid for input of size 2282520576
2025-03-12 05:27:07.872255 test begin: paddle.reshape(Tensor([124, 1712, 28, 384],"float32"), list[124,28,28,384,], )

[torch error] paddle.reshape(Tensor([124, 1712, 28, 384],"float32"), list[124,28,28,384,], ) 
 shape '[124, 28, 28, 384]' is invalid for input of size 2282520576
2025-03-12 05:27:09.755985 test begin: paddle.reshape(Tensor([124, 1712, 56, 192],"float32"), list[124,56,56,192,], )

[torch error] paddle.reshape(Tensor([124, 1712, 56, 192],"float32"), list[124,56,56,192,], ) 
 shape '[124, 56, 56, 192]' is invalid for input of size 2282520576
2025-03-12 05:27:12.279475 test begin: paddle.reshape(Tensor([124, 18400818],"float32"), shape=list[-1,102,], )

[torch error] paddle.reshape(Tensor([124, 18400818],"float32"), shape=list[-1,102,], ) 
 shape '[-1, 102]' is invalid for input of size 2281701432
2025-03-12 05:27:14.211536 test begin: paddle.reshape(Tensor([124, 200, 173185],"float16"), shape=tuple(124,-1,), )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f268ed5b790>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741729035 (unix time) try "date -d @1741729035" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1dfe6) received by PID 122854 (TID 0x7f24ec71a700) from PID 122854 ***]

2025-03-12 05:38:03.422827 test begin: paddle.reshape(Tensor([120, 200, 95071],"float32"), shape=tuple(120,-1,), )

W0312 05:39:43.001550 130725 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 05:39:43.002738 130725 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.reshape(Tensor([120, 200, 95071],"float32"), shape=tuple(120,-1,), )
2025-03-12 05:42:32.573262 test begin: paddle.reshape(Tensor([120, 69906, 272],"float32"), shape=tuple(120,-1,), )

[Pass] paddle.reshape(Tensor([120, 69906, 272],"float32"), shape=tuple(120,-1,), )
2025-03-12 05:45:45.175155 test begin: paddle.reshape(Tensor([124, 200, 92005],"float32"), shape=tuple(124,-1,), )

[Pass] paddle.reshape(Tensor([124, 200, 92005],"float32"), shape=tuple(124,-1,), )
2025-03-12 05:49:50.106686 test begin: paddle.reshape(Tensor([124, 28, 1712, 384],"float32"), list[124,28,28,384,], )

[torch error] paddle.reshape(Tensor([124, 28, 1712, 384],"float32"), list[124,28,28,384,], ) 
 shape '[124, 28, 28, 384]' is invalid for input of size 2282520576
2025-03-12 05:49:54.299494 test begin: paddle.reshape(Tensor([124, 28, 28, 23471],"float32"), list[124,28,28,192,], )

[torch error] paddle.reshape(Tensor([124, 28, 28, 23471],"float32"), list[124,28,28,192,], ) 
 shape '[124, 28, 28, 192]' is invalid for input of size 2281756736
2025-03-12 05:49:57.304442 test begin: paddle.reshape(Tensor([124, 28, 28, 23471],"float32"), list[124,28,28,384,], )

[torch error] paddle.reshape(Tensor([124, 28, 28, 23471],"float32"), list[124,28,28,384,], ) 
 shape '[124, 28, 28, 384]' is invalid for input of size 2281756736
2025-03-12 05:49:59.951638 test begin: paddle.reshape(Tensor([124, 28, 3423, 192],"float32"), list[124,28,28,192,], )

[torch error] paddle.reshape(Tensor([124, 28, 3423, 192],"float32"), list[124,28,28,192,], ) 
 shape '[124, 28, 28, 192]' is invalid for input of size 2281853952
2025-03-12 05:50:01.657542 test begin: paddle.reshape(Tensor([124, 3423, 14, 384],"float32"), list[124,14,14,384,], )

[torch error] paddle.reshape(Tensor([124, 3423, 14, 384],"float32"), list[124,14,14,384,], ) 
 shape '[124, 14, 14, 384]' is invalid for input of size 2281853952
2025-03-12 05:50:04.024729 test begin: paddle.reshape(Tensor([124, 3423, 28, 192],"float32"), list[124,28,28,192,], )

[torch error] paddle.reshape(Tensor([124, 3423, 28, 192],"float32"), list[124,28,28,192,], ) 
 shape '[124, 28, 28, 192]' is invalid for input of size 2281853952
2025-03-12 05:50:05.731725 test begin: paddle.reshape(Tensor([124, 3423, 56, 96],"float32"), list[124,56,56,96,], )

[torch error] paddle.reshape(Tensor([124, 3423, 56, 96],"float32"), list[124,56,56,96,], ) 
 shape '[124, 56, 56, 96]' is invalid for input of size 2281853952
2025-03-12 05:50:07.637996 test begin: paddle.reshape(Tensor([124, 56, 1712, 192],"float32"), list[124,56,56,192,], )

[torch error] paddle.reshape(Tensor([124, 56, 1712, 192],"float32"), list[124,56,56,192,], ) 
 shape '[124, 56, 56, 192]' is invalid for input of size 2282520576
2025-03-12 05:50:09.347497 test begin: paddle.reshape(Tensor([124, 56, 3423, 96],"float32"), list[124,56,56,96,], )

[torch error] paddle.reshape(Tensor([124, 56, 3423, 96],"float32"), list[124,56,56,96,], ) 
 shape '[124, 56, 56, 96]' is invalid for input of size 2281853952
2025-03-12 05:50:10.760509 test begin: paddle.reshape(Tensor([124, 56, 56, 5868],"float32"), list[124,56,56,192,], )

[torch error] paddle.reshape(Tensor([124, 56, 56, 5868],"float32"), list[124,56,56,192,], ) 
 shape '[124, 56, 56, 192]' is invalid for input of size 2281853952
2025-03-12 05:50:12.467869 test begin: paddle.reshape(Tensor([124, 56, 56, 5868],"float32"), list[124,56,56,96,], )

[torch error] paddle.reshape(Tensor([124, 56, 56, 5868],"float32"), list[124,56,56,96,], ) 
 shape '[124, 56, 56, 96]' is invalid for input of size 2281853952
2025-03-12 05:50:13.897328 test begin: paddle.reshape(Tensor([124, 67651, 272],"float32"), shape=tuple(124,-1,), )

[Pass] paddle.reshape(Tensor([124, 67651, 272],"float32"), shape=tuple(124,-1,), )
2025-03-12 05:53:08.854903 test begin: paddle.reshape(Tensor([1245, 22, 248, 336],"float32"), shape=list[-1,248,336,], )

[Pass] paddle.reshape(Tensor([1245, 22, 248, 336],"float32"), shape=list[-1,248,336,], )
2025-03-12 05:56:35.847020 test begin: paddle.reshape(Tensor([126761188, 18],"float32"), list[3,2,3,3,], )

[torch error] paddle.reshape(Tensor([126761188, 18],"float32"), list[3,2,3,3,], ) 
 shape '[3, 2, 3, 3]' is invalid for input of size 2281701384
2025-03-12 05:56:40.313066 test begin: paddle.reshape(Tensor([126761188, 2, 3, 3],"float32"), tuple(3,-1,), )

[Pass] paddle.reshape(Tensor([126761188, 2, 3, 3],"float32"), tuple(3,-1,), )
2025-03-12 05:59:31.403842 test begin: paddle.reshape(Tensor([1267612, 10, 15, 12],"float32"), shape=tuple(1,-1,4,), )

[Pass] paddle.reshape(Tensor([1267612, 10, 15, 12],"float32"), shape=tuple(1,-1,4,), )
2025-03-12 06:02:24.277246 test begin: paddle.reshape(Tensor([1273271, 56, 32],"float32"), shape=list[7,56,4,8,], )

[torch error] paddle.reshape(Tensor([1273271, 56, 32],"float32"), shape=list[7,56,4,8,], ) 
 shape '[7, 56, 4, 8]' is invalid for input of size 2281701632
2025-03-12 06:02:26.869287 test begin: paddle.reshape(Tensor([128, 1, 17825793],"float32"), shape=list[-1,102,], )

[torch error] paddle.reshape(Tensor([128, 1, 17825793],"float32"), shape=list[-1,102,], ) 
 shape '[-1, 102]' is invalid for input of size 2281701504
2025-03-12 06:02:28.666490 test begin: paddle.reshape(Tensor([128, 1, 17825793],"float32"), shape=list[-1,159,], )

[torch error] paddle.reshape(Tensor([128, 1, 17825793],"float32"), shape=list[-1,159,], ) 
 shape '[-1, 159]' is invalid for input of size 2281701504
2025-03-12 06:02:30.966597 test begin: paddle.reshape(Tensor([128, 1, 17825793],"float32"), shape=list[-1,192612,], )

[torch error] paddle.reshape(Tensor([128, 1, 17825793],"float32"), shape=list[-1,192612,], ) 
 shape '[-1, 192612]' is invalid for input of size 2281701504
2025-03-12 06:02:33.031932 test begin: paddle.reshape(Tensor([128, 1, 17825793],"float32"), shape=list[-1,2,], )

[Pass] paddle.reshape(Tensor([128, 1, 17825793],"float32"), shape=list[-1,2,], )
2025-03-12 06:05:48.398709 test begin: paddle.reshape(Tensor([128, 1, 17825793],"float32"), shape=list[128,-1,], )

[Pass] paddle.reshape(Tensor([128, 1, 17825793],"float32"), shape=list[128,-1,], )
2025-03-12 06:09:05.403627 test begin: paddle.reshape(Tensor([128, 1, 33554433],"float16"), shape=list[-1,192612,], )

[torch error] paddle.reshape(Tensor([128, 1, 33554433],"float16"), shape=list[-1,192612,], ) 
 shape '[-1, 192612]' is invalid for input of size 4294967424
2025-03-12 06:10:36.859987 test begin: paddle.reshape(Tensor([128, 1114113, 4, 4],"float32"), shape=list[-1,800,], )

[torch error] paddle.reshape(Tensor([128, 1114113, 4, 4],"float32"), shape=list[-1,800,], ) 
 shape '[-1, 800]' is invalid for input of size 2281703424
2025-03-12 06:10:40.886748 test begin: paddle.reshape(Tensor([128, 112112, 159],"float32"), shape=list[-1,159,], )

[Pass] paddle.reshape(Tensor([128, 112112, 159],"float32"), shape=list[-1,159,], )
2025-03-12 06:13:55.702245 test begin: paddle.reshape(Tensor([128, 116509, 288],"float16"), shape=tuple(128,-1,), )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f32eedd64f0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 06:24:05.502738 test begin: paddle.reshape(Tensor([120, 200, 95071],"float32"), shape=tuple(120,-1,), )

W0312 06:25:41.211206 149435 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 06:25:41.212711 149435 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.reshape(Tensor([120, 200, 95071],"float32"), shape=tuple(120,-1,), )
2025-03-12 06:28:56.670249 test begin: paddle.reshape(Tensor([124, 200, 92005],"float32"), shape=tuple(124,-1,), )

[Pass] paddle.reshape(Tensor([124, 200, 92005],"float32"), shape=tuple(124,-1,), )
2025-03-12 06:32:13.623194 test begin: paddle.reshape(Tensor([124, 28, 1712, 384],"float32"), list[124,28,28,384,], )

[torch error] paddle.reshape(Tensor([124, 28, 1712, 384],"float32"), list[124,28,28,384,], ) 
 shape '[124, 28, 28, 384]' is invalid for input of size 2282520576
2025-03-12 06:32:17.611776 test begin: paddle.reshape(Tensor([128, 1, 33554433],"float16"), shape=list[-1,192612,], )

[torch error] paddle.reshape(Tensor([128, 1, 33554433],"float16"), shape=list[-1,192612,], ) 
 shape '[-1, 192612]' is invalid for input of size 4294967424
2025-03-12 06:33:38.297374 test begin: paddle.reshape(Tensor([128, 128, 139265],"float32"), list[-1,768,], )

[torch error] paddle.reshape(Tensor([128, 128, 139265],"float32"), list[-1,768,], ) 
 shape '[-1, 768]' is invalid for input of size 2281717760
2025-03-12 06:33:41.406825 test begin: paddle.reshape(Tensor([128, 128, 262145],"float16"), list[-1,768,], )

[torch error] paddle.reshape(Tensor([128, 128, 262145],"float16"), list[-1,768,], ) 
 shape '[-1, 768]' is invalid for input of size 4294983680
2025-03-12 06:33:42.769137 test begin: paddle.reshape(Tensor([128, 1280, 1, 13927],"float32"), list[-1,16,1280,], )

[Pass] paddle.reshape(Tensor([128, 1280, 1, 13927],"float32"), list[-1,16,1280,], )
2025-03-12 06:36:41.461056 test begin: paddle.reshape(Tensor([128, 1280, 13927, 1],"float32"), list[-1,16,1280,], )

[Pass] paddle.reshape(Tensor([128, 1280, 13927, 1],"float32"), list[-1,16,1280,], )
2025-03-12 06:39:25.185174 test begin: paddle.reshape(Tensor([128, 14, 14, 171197],"float16"), list[128,14,14,384,], )

[torch error] paddle.reshape(Tensor([128, 14, 14, 171197],"float16"), list[128,14,14,384,], ) 
 shape '[128, 14, 14, 384]' is invalid for input of size 4294990336
2025-03-12 06:39:29.468325 test begin: paddle.reshape(Tensor([128, 14, 14, 171197],"float16"), list[128,14,14,512,], )

[torch error] paddle.reshape(Tensor([128, 14, 14, 171197],"float16"), list[128,14,14,512,], ) 
 shape '[128, 14, 14, 512]' is invalid for input of size 4294990336
2025-03-12 06:39:31.623050 test begin: paddle.reshape(Tensor([128, 14, 14, 90948],"float32"), list[128,14,14,384,], )

[torch error] paddle.reshape(Tensor([128, 14, 14, 90948],"float32"), list[128,14,14,384,], ) 
 shape '[128, 14, 14, 384]' is invalid for input of size 2281703424
2025-03-12 06:39:35.613875 test begin: paddle.reshape(Tensor([128, 14, 14, 90948],"float32"), list[128,14,14,768,], )

[torch error] paddle.reshape(Tensor([128, 14, 14, 90948],"float32"), list[128,14,14,768,], ) 
 shape '[128, 14, 14, 768]' is invalid for input of size 2281703424
2025-03-12 06:39:37.743184 test begin: paddle.reshape(Tensor([128, 14, 1658, 768],"float32"), list[128,14,14,768,], )

[torch error] paddle.reshape(Tensor([128, 14, 1658, 768],"float32"), list[128,14,14,768,], ) 
 shape '[128, 14, 14, 768]' is invalid for input of size 2281832448
2025-03-12 06:39:39.724551 test begin: paddle.reshape(Tensor([128, 14, 3316, 384],"float32"), list[128,14,14,384,], )

[torch error] paddle.reshape(Tensor([128, 14, 3316, 384],"float32"), list[128,14,14,384,], ) 
 shape '[128, 14, 14, 384]' is invalid for input of size 2281832448
2025-03-12 06:39:41.691643 test begin: paddle.reshape(Tensor([128, 14, 4682, 512],"float16"), list[128,14,14,512,], )

[torch error] paddle.reshape(Tensor([128, 14, 4682, 512],"float16"), list[128,14,14,512,], ) 
 shape '[128, 14, 14, 512]' is invalid for input of size 4295753728
2025-03-12 06:39:44.052927 test begin: paddle.reshape(Tensor([128, 14, 6242, 384],"float16"), list[128,14,14,384,], )

[torch error] paddle.reshape(Tensor([128, 14, 6242, 384],"float16"), list[128,14,14,384,], ) 
 shape '[128, 14, 14, 384]' is invalid for input of size 4295294976
2025-03-12 06:39:46.395116 test begin: paddle.reshape(Tensor([128, 1658, 14, 768],"float32"), list[128,14,14,768,], )

[torch error] paddle.reshape(Tensor([128, 1658, 14, 768],"float32"), list[128,14,14,768,], ) 
 shape '[128, 14, 14, 768]' is invalid for input of size 2281832448
2025-03-12 06:39:48.367064 test begin: paddle.reshape(Tensor([128, 1658, 28, 384],"float32"), list[128,28,28,384,], )

[torch error] paddle.reshape(Tensor([128, 1658, 28, 384],"float32"), list[128,28,28,384,], ) 
 shape '[128, 28, 28, 384]' is invalid for input of size 2281832448
2025-03-12 06:39:50.405630 test begin: paddle.reshape(Tensor([128, 1658, 56, 192],"float32"), list[128,56,56,192,], )

[torch error] paddle.reshape(Tensor([128, 1658, 56, 192],"float32"), list[128,56,56,192,], ) 
 shape '[128, 56, 56, 192]' is invalid for input of size 2281832448
2025-03-12 06:39:52.375931 test begin: paddle.reshape(Tensor([128, 174763, 102],"float32"), shape=list[-1,102,], )

[Pass] paddle.reshape(Tensor([128, 174763, 102],"float32"), shape=list[-1,102,], )
2025-03-12 06:43:35.432604 test begin: paddle.reshape(Tensor([128, 175, 192612],"float16"), shape=list[-1,192612,], )

[torch error] paddle.reshape(Tensor([128, 175, 192612],"float16"), shape=list[-1,192612,], ) 
 cannot reshape array of size 4300000000 into shape (128,175,192612)
2025-03-12 06:43:35.434267 test begin: paddle.reshape(Tensor([128, 17825793, 1, 1],"float32"), list[-1,16,1280,], )

[torch error] paddle.reshape(Tensor([128, 17825793, 1, 1],"float32"), list[-1,16,1280,], ) 
 shape '[-1, 16, 1280]' is invalid for input of size 2281701504
2025-03-12 06:43:39.525858 test begin: paddle.reshape(Tensor([128, 17825793, 1, 1],"float32"), list[-1,8,2048,], )

[torch error] paddle.reshape(Tensor([128, 17825793, 1, 1],"float32"), list[-1,8,2048,], ) 
 shape '[-1, 8, 2048]' is invalid for input of size 2281701504
2025-03-12 06:43:41.647061 test begin: paddle.reshape(Tensor([128, 17825793, 1],"float32"), tuple(128,-1,), )

[Pass] paddle.reshape(Tensor([128, 17825793, 1],"float32"), tuple(128,-1,), )
2025-03-12 06:46:26.180773 test begin: paddle.reshape(Tensor([128, 17825793],"float32"), list[128,64,3,], )

[torch error] paddle.reshape(Tensor([128, 17825793],"float32"), list[128,64,3,], ) 
 shape '[128, 64, 3]' is invalid for input of size 2281701504
2025-03-12 06:46:30.160147 test begin: paddle.reshape(Tensor([128, 17825793],"float32"), list[128,80,1,], )

[torch error] paddle.reshape(Tensor([128, 17825793],"float32"), list[128,80,1,], ) 
 shape '[128, 80, 1]' is invalid for input of size 2281701504
2025-03-12 06:46:32.383424 test begin: paddle.reshape(Tensor([128, 17825793],"float32"), shape=list[-1,1000,], )

[torch error] paddle.reshape(Tensor([128, 17825793],"float32"), shape=list[-1,1000,], ) 
 shape '[-1, 1000]' is invalid for input of size 2281701504
2025-03-12 06:46:33.994566 test begin: paddle.reshape(Tensor([128, 200, 167773],"float16"), shape=tuple(128,-1,), )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f7f1d9434f0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 06:56:44.239562 test begin: paddle.reshape(Tensor([128, 128, 139265],"float32"), list[-1,768,], )

W0312 06:58:02.024772 163248 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 06:58:02.025914 163248 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.reshape(Tensor([128, 128, 139265],"float32"), list[-1,768,], ) 
 shape '[-1, 768]' is invalid for input of size 2281717760
2025-03-12 06:58:03.891704 test begin: paddle.reshape(Tensor([128, 1280, 13927, 1],"float32"), list[-1,16,1280,], )

[Pass] paddle.reshape(Tensor([128, 1280, 13927, 1],"float32"), list[-1,16,1280,], )
2025-03-12 07:01:30.385892 test begin: paddle.reshape(Tensor([128, 14, 14, 171197],"float16"), list[128,14,14,512,], )

[torch error] paddle.reshape(Tensor([128, 14, 14, 171197],"float16"), list[128,14,14,512,], ) 
 shape '[128, 14, 14, 512]' is invalid for input of size 4294990336
2025-03-12 07:02:47.722760 test begin: paddle.reshape(Tensor([128, 200, 89129],"float32"), shape=tuple(128,-1,), )

[Pass] paddle.reshape(Tensor([128, 200, 89129],"float32"), shape=tuple(128,-1,), )
2025-03-12 07:05:45.990119 test begin: paddle.reshape(Tensor([128, 2048, 1, 16385],"float16"), list[-1,8,2048,], )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f45cfc557c0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 07:15:57.443430 test begin: paddle.reshape(Tensor([128, 1280, 13927, 1],"float32"), list[-1,16,1280,], )

W0312 07:17:32.795492  8337 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 07:17:32.796702  8337 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.reshape(Tensor([128, 1280, 13927, 1],"float32"), list[-1,16,1280,], )
2025-03-12 07:21:04.179276 test begin: paddle.reshape(Tensor([128, 14, 14, 171197],"float16"), list[128,14,14,512,], )

[torch error] paddle.reshape(Tensor([128, 14, 14, 171197],"float16"), list[128,14,14,512,], ) 
 shape '[128, 14, 14, 512]' is invalid for input of size 4294990336
2025-03-12 07:22:49.142081 test begin: paddle.reshape(Tensor([128, 200, 89129],"float32"), shape=tuple(128,-1,), )

[Pass] paddle.reshape(Tensor([128, 200, 89129],"float32"), shape=tuple(128,-1,), )
2025-03-12 07:26:26.936363 test begin: paddle.reshape(Tensor([128, 2048, 1, 8705],"float32"), list[-1,8,2048,], )

[Pass] paddle.reshape(Tensor([128, 2048, 1, 8705],"float32"), list[-1,8,2048,], )
2025-03-12 07:30:30.135170 test begin: paddle.reshape(Tensor([128, 2048, 16385, 1],"float16"), list[-1,8,2048,], )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f3aaef45b50>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 07:40:41.885460 test begin: paddle.reshape(Tensor([128, 200, 89129],"float32"), shape=tuple(128,-1,), )

W0312 07:42:33.421731 17912 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 07:42:33.423192 17912 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.reshape(Tensor([128, 200, 89129],"float32"), shape=tuple(128,-1,), )
2025-03-12 07:45:03.985928 test begin: paddle.reshape(Tensor([128, 2048, 1, 8705],"float32"), list[-1,8,2048,], )

[Pass] paddle.reshape(Tensor([128, 2048, 1, 8705],"float32"), list[-1,8,2048,], )
2025-03-12 07:48:47.049747 test begin: paddle.reshape(Tensor([128, 2048, 8705, 1],"float32"), list[-1,8,2048,], )

[Pass] paddle.reshape(Tensor([128, 2048, 8705, 1],"float32"), list[-1,8,2048,], )
2025-03-12 07:52:39.326506 test begin: paddle.reshape(Tensor([128, 22737, 784],"float32"), shape=list[128,-1,], )

[Pass] paddle.reshape(Tensor([128, 22737, 784],"float32"), shape=list[128,-1,], )
2025-03-12 07:56:12.439389 test begin: paddle.reshape(Tensor([128, 23211, 768],"float32"), list[-1,768,], )

[Pass] paddle.reshape(Tensor([128, 23211, 768],"float32"), list[-1,768,], )
2025-03-12 07:59:33.894098 test begin: paddle.reshape(Tensor([128, 28, 1658, 384],"float32"), list[128,28,28,384,], )

[torch error] paddle.reshape(Tensor([128, 28, 1658, 384],"float32"), list[128,28,28,384,], ) 
 shape '[128, 28, 28, 384]' is invalid for input of size 2281832448
2025-03-12 07:59:38.275966 test begin: paddle.reshape(Tensor([128, 28, 28, 22737],"float32"), list[128,28,28,192,], )

[torch error] paddle.reshape(Tensor([128, 28, 28, 22737],"float32"), list[128,28,28,192,], ) 
 shape '[128, 28, 28, 192]' is invalid for input of size 2281703424
2025-03-12 07:59:41.023999 test begin: paddle.reshape(Tensor([128, 28, 28, 22737],"float32"), list[128,28,28,384,], )

[torch error] paddle.reshape(Tensor([128, 28, 28, 22737],"float32"), list[128,28,28,384,], ) 
 shape '[128, 28, 28, 384]' is invalid for input of size 2281703424
2025-03-12 07:59:42.820224 test begin: paddle.reshape(Tensor([128, 28, 28, 42800],"float16"), list[128,28,28,192,], )

[torch error] paddle.reshape(Tensor([128, 28, 28, 42800],"float16"), list[128,28,28,192,], ) 
 shape '[128, 28, 28, 192]' is invalid for input of size 4295065600
2025-03-12 08:01:05.515927 test begin: paddle.reshape(Tensor([128, 28, 28, 42800],"float16"), list[128,28,28,256,], )

[torch error] paddle.reshape(Tensor([128, 28, 28, 42800],"float16"), list[128,28,28,256,], ) 
 shape '[128, 28, 28, 256]' is invalid for input of size 4295065600
2025-03-12 08:01:07.457140 test begin: paddle.reshape(Tensor([128, 28, 3316, 192],"float32"), list[128,28,28,192,], )

[torch error] paddle.reshape(Tensor([128, 28, 3316, 192],"float32"), list[128,28,28,192,], ) 
 shape '[128, 28, 28, 192]' is invalid for input of size 2281832448
2025-03-12 08:01:11.458644 test begin: paddle.reshape(Tensor([128, 28, 4682, 256],"float16"), list[128,28,28,256,], )

[torch error] paddle.reshape(Tensor([128, 28, 4682, 256],"float16"), list[128,28,28,256,], ) 
 shape '[128, 28, 28, 256]' is invalid for input of size 4295753728
2025-03-12 08:01:13.083208 test begin: paddle.reshape(Tensor([128, 28, 6242, 192],"float16"), list[128,28,28,192,], )

[torch error] paddle.reshape(Tensor([128, 28, 6242, 192],"float16"), list[128,28,28,192,], ) 
 shape '[128, 28, 28, 192]' is invalid for input of size 4295294976
2025-03-12 08:01:14.934858 test begin: paddle.reshape(Tensor([128, 3316, 14, 384],"float32"), list[128,14,14,384,], )

[torch error] paddle.reshape(Tensor([128, 3316, 14, 384],"float32"), list[128,14,14,384,], ) 
 shape '[128, 14, 14, 384]' is invalid for input of size 2281832448
2025-03-12 08:01:18.122719 test begin: paddle.reshape(Tensor([128, 3316, 28, 192],"float32"), list[128,28,28,192,], )

[torch error] paddle.reshape(Tensor([128, 3316, 28, 192],"float32"), list[128,28,28,192,], ) 
 shape '[128, 28, 28, 192]' is invalid for input of size 2281832448
2025-03-12 08:01:19.737034 test begin: paddle.reshape(Tensor([128, 3316, 56, 96],"float32"), list[128,56,56,96,], )

[torch error] paddle.reshape(Tensor([128, 3316, 56, 96],"float32"), list[128,56,56,96,], ) 
 shape '[128, 56, 56, 96]' is invalid for input of size 2281832448
2025-03-12 08:01:21.515430 test begin: paddle.reshape(Tensor([128, 33554433, 1, 1],"float16"), list[-1,8,2048,], )

[torch error] paddle.reshape(Tensor([128, 33554433, 1, 1],"float16"), list[-1,8,2048,], ) 
 shape '[-1, 8, 2048]' is invalid for input of size 4294967424
2025-03-12 08:01:23.610875 test begin: paddle.reshape(Tensor([128, 33554433],"float16"), shape=list[-1,1000,], )

[torch error] paddle.reshape(Tensor([128, 33554433],"float16"), shape=list[-1,1000,], ) 
 shape '[-1, 1000]' is invalid for input of size 4294967424
2025-03-12 08:01:26.101278 test begin: paddle.reshape(Tensor([128, 43691, 768],"float16"), list[-1,768,], )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7fcb719334f0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 08:11:37.364478 test begin: paddle.reshape(Tensor([128, 200, 89129],"float32"), shape=tuple(128,-1,), )

W0312 08:13:06.328150 31125 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 08:13:06.329334 31125 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.reshape(Tensor([128, 200, 89129],"float32"), shape=tuple(128,-1,), )
2025-03-12 08:16:20.430059 test begin: paddle.reshape(Tensor([128, 22737, 784],"float32"), shape=list[128,-1,], )

[Pass] paddle.reshape(Tensor([128, 22737, 784],"float32"), shape=list[128,-1,], )
2025-03-12 08:20:15.681761 test begin: paddle.reshape(Tensor([128, 23211, 768],"float32"), list[-1,768,], )

[Pass] paddle.reshape(Tensor([128, 23211, 768],"float32"), list[-1,768,], )
2025-03-12 08:23:39.547292 test begin: paddle.reshape(Tensor([128, 33554433, 1, 1],"float16"), list[-1,8,2048,], )

[torch error] paddle.reshape(Tensor([128, 33554433, 1, 1],"float16"), list[-1,8,2048,], ) 
 shape '[-1, 8, 2048]' is invalid for input of size 4294967424
2025-03-12 08:25:08.783735 test begin: paddle.reshape(Tensor([128, 4682, 14, 512],"float16"), list[128,14,14,512,], )

[torch error] paddle.reshape(Tensor([128, 4682, 14, 512],"float16"), list[128,14,14,512,], ) 
 shape '[128, 14, 14, 512]' is invalid for input of size 4295753728
2025-03-12 08:25:11.767246 test begin: paddle.reshape(Tensor([128, 4682, 28, 256],"float16"), list[128,28,28,256,], )

[torch error] paddle.reshape(Tensor([128, 4682, 28, 256],"float16"), list[128,28,28,256,], ) 
 shape '[128, 28, 28, 256]' is invalid for input of size 4295753728
2025-03-12 08:25:13.855166 test begin: paddle.reshape(Tensor([128, 4682, 56, 128],"float16"), list[128,56,56,128,], )

[torch error] paddle.reshape(Tensor([128, 4682, 56, 128],"float16"), list[128,56,56,128,], ) 
 shape '[128, 56, 56, 128]' is invalid for input of size 4295753728
2025-03-12 08:25:15.326039 test begin: paddle.reshape(Tensor([128, 50, 4, 89129],"float32"), shape=list[-1,800,], )

[Pass] paddle.reshape(Tensor([128, 50, 4, 89129],"float32"), shape=list[-1,800,], )
2025-03-12 08:28:45.765494 test begin: paddle.reshape(Tensor([128, 50, 89129, 4],"float32"), shape=list[-1,800,], )

[Pass] paddle.reshape(Tensor([128, 50, 89129, 4],"float32"), shape=list[-1,800,], )
2025-03-12 08:32:20.600033 test begin: paddle.reshape(Tensor([128, 56, 1658, 192],"float32"), list[128,56,56,192,], )

[torch error] paddle.reshape(Tensor([128, 56, 1658, 192],"float32"), list[128,56,56,192,], ) 
 shape '[128, 56, 56, 192]' is invalid for input of size 2281832448
2025-03-12 08:32:25.573461 test begin: paddle.reshape(Tensor([128, 56, 3316, 96],"float32"), list[128,56,56,96,], )

[torch error] paddle.reshape(Tensor([128, 56, 3316, 96],"float32"), list[128,56,56,96,], ) 
 shape '[128, 56, 56, 96]' is invalid for input of size 2281832448
2025-03-12 08:32:27.717327 test begin: paddle.reshape(Tensor([128, 56, 4682, 128],"float16"), list[128,56,56,128,], )

[torch error] paddle.reshape(Tensor([128, 56, 4682, 128],"float16"), list[128,56,56,128,], ) 
 shape '[128, 56, 56, 128]' is invalid for input of size 4295753728
2025-03-12 08:32:31.698822 test begin: paddle.reshape(Tensor([128, 56, 56, 10700],"float16"), list[128,56,56,128,], )

[torch error] paddle.reshape(Tensor([128, 56, 56, 10700],"float16"), list[128,56,56,128,], ) 
 shape '[128, 56, 56, 128]' is invalid for input of size 4295065600
2025-03-12 08:32:34.058865 test begin: paddle.reshape(Tensor([128, 56, 56, 10700],"float16"), list[128,56,56,96,], )

[torch error] paddle.reshape(Tensor([128, 56, 56, 10700],"float16"), list[128,56,56,96,], ) 
 shape '[128, 56, 56, 96]' is invalid for input of size 4295065600
2025-03-12 08:32:36.418189 test begin: paddle.reshape(Tensor([128, 56, 56, 5685],"float32"), list[128,56,56,192,], )

[torch error] paddle.reshape(Tensor([128, 56, 56, 5685],"float32"), list[128,56,56,192,], ) 
 shape '[128, 56, 56, 192]' is invalid for input of size 2282004480
2025-03-12 08:32:39.157733 test begin: paddle.reshape(Tensor([128, 56, 56, 5685],"float32"), list[128,56,56,96,], )

[torch error] paddle.reshape(Tensor([128, 56, 56, 5685],"float32"), list[128,56,56,96,], ) 
 shape '[128, 56, 56, 96]' is invalid for input of size 2282004480
2025-03-12 08:32:41.697412 test begin: paddle.reshape(Tensor([128, 56, 6242, 96],"float16"), list[128,56,56,96,], )

[torch error] paddle.reshape(Tensor([128, 56, 6242, 96],"float16"), list[128,56,56,96,], ) 
 shape '[128, 56, 56, 96]' is invalid for input of size 4295294976
2025-03-12 08:32:44.069908 test begin: paddle.reshape(Tensor([128, 5941931, 3],"float32"), tuple(128,-1,), )

[Pass] paddle.reshape(Tensor([128, 5941931, 3],"float32"), tuple(128,-1,), )
2025-03-12 08:36:02.671646 test begin: paddle.reshape(Tensor([128, 61896, 288],"float32"), shape=tuple(128,-1,), )

[Pass] paddle.reshape(Tensor([128, 61896, 288],"float32"), shape=tuple(128,-1,), )
2025-03-12 08:39:09.713941 test begin: paddle.reshape(Tensor([128, 6242, 14, 384],"float16"), list[128,14,14,384,], )

[torch error] paddle.reshape(Tensor([128, 6242, 14, 384],"float16"), list[128,14,14,384,], ) 
 shape '[128, 14, 14, 384]' is invalid for input of size 4295294976
2025-03-12 08:39:13.933352 test begin: paddle.reshape(Tensor([128, 6242, 28, 192],"float16"), list[128,28,28,192,], )

[torch error] paddle.reshape(Tensor([128, 6242, 28, 192],"float16"), list[128,28,28,192,], ) 
 shape '[128, 28, 28, 192]' is invalid for input of size 4295294976
2025-03-12 08:39:16.183559 test begin: paddle.reshape(Tensor([128, 6242, 56, 96],"float16"), list[128,56,56,96,], )

[torch error] paddle.reshape(Tensor([128, 6242, 56, 96],"float16"), list[128,56,56,96,], ) 
 shape '[128, 56, 56, 96]' is invalid for input of size 4295294976
2025-03-12 08:39:17.669758 test begin: paddle.reshape(Tensor([128, 64, 278529],"float32"), tuple(128,-1,), )

[Pass] paddle.reshape(Tensor([128, 64, 278529],"float32"), tuple(128,-1,), )
2025-03-12 08:42:29.024550 test begin: paddle.reshape(Tensor([128, 80, 222823],"float32"), tuple(128,-1,), )

[Pass] paddle.reshape(Tensor([128, 80, 222823],"float32"), tuple(128,-1,), )
2025-03-12 08:45:41.627019 test begin: paddle.reshape(Tensor([128, 8912897, 2],"float32"), shape=list[-1,2,], )

[Pass] paddle.reshape(Tensor([128, 8912897, 2],"float32"), shape=list[-1,2,], )
2025-03-12 08:48:39.818035 test begin: paddle.reshape(Tensor([128, 93, 192612],"float32"), shape=list[-1,192612,], )

[Pass] paddle.reshape(Tensor([128, 93, 192612],"float32"), shape=list[-1,192612,], )
2025-03-12 08:51:49.925292 test begin: paddle.reshape(Tensor([1287, 21, 264, 320],"float32"), shape=list[-1,264,320,], )

[Pass] paddle.reshape(Tensor([1287, 21, 264, 320],"float32"), shape=list[-1,264,320,], )
2025-03-12 08:54:43.864108 test begin: paddle.reshape(Tensor([1290, 96, 96, 192],"float32"), list[28,96,96,192,], )

[torch error] paddle.reshape(Tensor([1290, 96, 96, 192],"float32"), list[28,96,96,192,], ) 
 shape '[28, 96, 96, 192]' is invalid for input of size 2282618880
2025-03-12 08:54:48.186479 test begin: paddle.reshape(Tensor([1290, 96, 96, 192],"float32"), list[32,96,96,192,], )

[torch error] paddle.reshape(Tensor([1290, 96, 96, 192],"float32"), list[32,96,96,192,], ) 
 shape '[32, 96, 96, 192]' is invalid for input of size 2282618880
2025-03-12 08:54:50.613642 test begin: paddle.reshape(Tensor([1290, 96, 96, 192],"float32"), list[4,96,96,192,], )

[torch error] paddle.reshape(Tensor([1290, 96, 96, 192],"float32"), list[4,96,96,192,], ) 
 shape '[4, 96, 96, 192]' is invalid for input of size 2282618880
2025-03-12 08:54:52.633702 test begin: paddle.reshape(Tensor([12986, 19, 34, 512],"float16"), list[-1,512,], )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f7dc2b9d4f0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 09:05:02.416544 test begin: paddle.reshape(Tensor([128, 22737, 784],"float32"), shape=list[128,-1,], )

W0312 09:06:27.300735 55396 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 09:06:27.301868 55396 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.reshape(Tensor([128, 22737, 784],"float32"), shape=list[128,-1,], )
2025-03-12 09:08:43.020705 test begin: paddle.reshape(Tensor([128, 23211, 768],"float32"), list[-1,768,], )

[Pass] paddle.reshape(Tensor([128, 23211, 768],"float32"), list[-1,768,], )
2025-03-12 09:11:41.444236 test begin: paddle.reshape(Tensor([128, 4682, 14, 512],"float16"), list[128,14,14,512,], )

[torch error] paddle.reshape(Tensor([128, 4682, 14, 512],"float16"), list[128,14,14,512,], ) 
 shape '[128, 14, 14, 512]' is invalid for input of size 4295753728
2025-03-12 09:13:02.192195 test begin: paddle.reshape(Tensor([128, 50, 89129, 4],"float32"), shape=list[-1,800,], )

[Pass] paddle.reshape(Tensor([128, 50, 89129, 4],"float32"), shape=list[-1,800,], )
2025-03-12 09:16:10.438178 test begin: paddle.reshape(Tensor([128, 56, 1658, 192],"float32"), list[128,56,56,192,], )

[torch error] paddle.reshape(Tensor([128, 56, 1658, 192],"float32"), list[128,56,56,192,], ) 
 shape '[128, 56, 56, 192]' is invalid for input of size 2281832448
2025-03-12 09:16:14.435493 test begin: paddle.reshape(Tensor([128, 61896, 288],"float32"), shape=tuple(128,-1,), )

[Pass] paddle.reshape(Tensor([128, 61896, 288],"float32"), shape=tuple(128,-1,), )
2025-03-12 09:19:07.117873 test begin: paddle.reshape(Tensor([128, 6242, 14, 384],"float16"), list[128,14,14,384,], )

[torch error] paddle.reshape(Tensor([128, 6242, 14, 384],"float16"), list[128,14,14,384,], ) 
 shape '[128, 14, 14, 384]' is invalid for input of size 4295294976
2025-03-12 09:19:10.998575 test begin: paddle.reshape(Tensor([128, 80, 222823],"float32"), tuple(128,-1,), )

[Pass] paddle.reshape(Tensor([128, 80, 222823],"float32"), tuple(128,-1,), )
2025-03-12 09:22:13.096717 test begin: paddle.reshape(Tensor([128, 8912897, 2],"float32"), shape=list[-1,2,], )

[Pass] paddle.reshape(Tensor([128, 8912897, 2],"float32"), shape=list[-1,2,], )
2025-03-12 09:25:24.647662 test begin: paddle.reshape(Tensor([128, 93, 192612],"float32"), shape=list[-1,192612,], )

[Pass] paddle.reshape(Tensor([128, 93, 192612],"float32"), shape=list[-1,192612,], )
2025-03-12 09:28:26.069823 test begin: paddle.reshape(Tensor([1290, 96, 96, 192],"float32"), list[28,96,96,192,], )

[torch error] paddle.reshape(Tensor([1290, 96, 96, 192],"float32"), list[28,96,96,192,], ) 
 shape '[28, 96, 96, 192]' is invalid for input of size 2282618880
2025-03-12 09:28:30.110288 test begin: paddle.reshape(Tensor([12988, 175678],"bool"), list[-1,32,1,], )

[torch error] paddle.reshape(Tensor([12988, 175678],"bool"), list[-1,32,1,], ) 
 shape '[-1, 32, 1]' is invalid for input of size 2281705864
2025-03-12 09:29:30.704664 test begin: paddle.reshape(Tensor([12993, 112, 2, 28, 28],"float32"), shape=list[2,224,28,28,], )

[torch error] paddle.reshape(Tensor([12993, 112, 2, 28, 28],"float32"), shape=list[2,224,28,28,], ) 
 shape '[2, 224, 28, 28]' is invalid for input of size 2281778688
2025-03-12 09:29:34.166708 test begin: paddle.reshape(Tensor([12993, 224, 28, 28],"float32"), shape=list[2,2,112,28,28,], )

[torch error] paddle.reshape(Tensor([12993, 224, 28, 28],"float32"), shape=list[2,2,112,28,28,], ) 
 shape '[2, 2, 112, 28, 28]' is invalid for input of size 2281778688
2025-03-12 09:29:36.344443 test begin: paddle.reshape(Tensor([13, 1, 1, 175515491],"float32"), list[13,7,1,], )

[torch error] paddle.reshape(Tensor([13, 1, 1, 175515491],"float32"), list[13,7,1,], ) 
 shape '[13, 7, 1]' is invalid for input of size 2281701383
2025-03-12 09:29:38.430010 test begin: paddle.reshape(Tensor([13, 1, 175515491, 1],"float32"), list[13,7,1,], )

[torch error] paddle.reshape(Tensor([13, 1, 175515491, 1],"float32"), list[13,7,1,], ) 
 shape '[13, 7, 1]' is invalid for input of size 2281701383
2025-03-12 09:29:40.436700 test begin: paddle.reshape(Tensor([13, 1, 175515491],"float32"), list[13,-1,32,], )

[torch error] paddle.reshape(Tensor([13, 1, 175515491],"float32"), list[13,-1,32,], ) 
 shape '[13, -1, 32]' is invalid for input of size 2281701383
2025-03-12 09:29:42.472785 test begin: paddle.reshape(Tensor([13, 1, 175515491],"float32"), list[13,1,1,1,], )

[torch error] paddle.reshape(Tensor([13, 1, 175515491],"float32"), list[13,1,1,1,], ) 
 shape '[13, 1, 1, 1]' is invalid for input of size 2281701383
2025-03-12 09:29:44.509094 test begin: paddle.reshape(Tensor([13, 1, 175515491],"int64"), shape=list[13,1,-1,4,], )

[torch error] paddle.reshape(Tensor([13, 1, 175515491],"int64"), shape=list[13,1,-1,4,], ) 
 shape '[13, 1, -1, 4]' is invalid for input of size 2281701383
2025-03-12 09:30:41.027191 test begin: paddle.reshape(Tensor([13, 1, 25073642, 7],"float32"), list[13,7,1,], )

[torch error] paddle.reshape(Tensor([13, 1, 25073642, 7],"float32"), list[13,7,1,], ) 
 shape '[13, 7, 1]' is invalid for input of size 2281701422
2025-03-12 09:30:44.929611 test begin: paddle.reshape(Tensor([13, 1, 2742430, 64],"float32"), shape=list[13,-1,64,], )

[Pass] paddle.reshape(Tensor([13, 1, 2742430, 64],"float32"), shape=list[13,-1,64,], )
2025-03-12 09:33:41.020924 test begin: paddle.reshape(Tensor([13, 1, 7, 25073642],"float32"), list[13,7,1,], )

[torch error] paddle.reshape(Tensor([13, 1, 7, 25073642],"float32"), list[13,7,1,], ) 
 shape '[13, 7, 1]' is invalid for input of size 2281701422
2025-03-12 09:33:45.062985 test begin: paddle.reshape(Tensor([13, 1, 8, 21939437],"float32"), shape=list[13,-1,64,], )

[torch error] paddle.reshape(Tensor([13, 1, 8, 21939437],"float32"), shape=list[13,-1,64,], ) 
 shape '[13, -1, 64]' is invalid for input of size 2281701448
2025-03-12 09:33:48.626881 test begin: paddle.reshape(Tensor([13, 1044736, 21, 8],"float32"), tuple(52,-1,8,), )

[Pass] paddle.reshape(Tensor([13, 1044736, 21, 8],"float32"), tuple(52,-1,8,), )
2025-03-12 09:36:36.368619 test begin: paddle.reshape(Tensor([13, 10969719, 16],"float32"), tuple(13,2,4,4,), )

[torch error] paddle.reshape(Tensor([13, 10969719, 16],"float32"), tuple(13,2,4,4,), ) 
 shape '[13, 2, 4, 4]' is invalid for input of size 2281701552
2025-03-12 09:36:40.541774 test begin: paddle.reshape(Tensor([13, 10969719, 16],"float32"), tuple(13,4,4,4,), )

[torch error] paddle.reshape(Tensor([13, 10969719, 16],"float32"), tuple(13,4,4,4,), ) 
 shape '[13, 4, 4, 4]' is invalid for input of size 2281701552
2025-03-12 09:36:42.582793 test begin: paddle.reshape(Tensor([13, 10969719, 16],"float32"), tuple(13,7,4,4,), )

[torch error] paddle.reshape(Tensor([13, 10969719, 16],"float32"), tuple(13,7,4,4,), ) 
 shape '[13, 7, 4, 4]' is invalid for input of size 2281701552
2025-03-12 09:36:44.630259 test begin: paddle.reshape(Tensor([13, 10969719, 2, 8],"float32"), list[13,4,8,2,], )

[torch error] paddle.reshape(Tensor([13, 10969719, 2, 8],"float32"), list[13,4,8,2,], ) 
 shape '[13, 4, 8, 2]' is invalid for input of size 2281701552
2025-03-12 09:36:46.645438 test begin: paddle.reshape(Tensor([13, 1371215, 128],"float32"), list[13,-1,32,], )

[Pass] paddle.reshape(Tensor([13, 1371215, 128],"float32"), list[13,-1,32,], )
2025-03-12 09:39:27.535638 test begin: paddle.reshape(Tensor([13, 14, 12536821],"float32"), tuple(13,2,7,-1,), )

[Pass] paddle.reshape(Tensor([13, 14, 12536821],"float32"), tuple(13,2,7,-1,), )
2025-03-12 09:42:10.037717 test begin: paddle.reshape(Tensor([13, 14626291, 6, 2],"float32"), list[13,4,2,6,], )

[torch error] paddle.reshape(Tensor([13, 14626291, 6, 2],"float32"), list[13,4,2,6,], ) 
 shape '[13, 4, 2, 6]' is invalid for input of size 2281701396
2025-03-12 09:42:14.385429 test begin: paddle.reshape(Tensor([13, 16, 2, 5484860],"float32"), shape=list[13,-1,32,], )

[Pass] paddle.reshape(Tensor([13, 16, 2, 5484860],"float32"), shape=list[13,-1,32,], )
2025-03-12 09:45:07.696037 test begin: paddle.reshape(Tensor([13, 16, 685608, 16],"float32"), shape=list[13,-1,32,], )

[Pass] paddle.reshape(Tensor([13, 16, 685608, 16],"float32"), shape=list[13,-1,32,], )
2025-03-12 09:48:05.423203 test begin: paddle.reshape(Tensor([13, 171402, 64, 16],"float32"), shape=list[13,2,-1,4,16,], )

[Pass] paddle.reshape(Tensor([13, 171402, 64, 16],"float32"), shape=list[13,2,-1,4,16,], )
2025-03-12 09:50:48.108093 test begin: paddle.reshape(Tensor([13, 171402, 64, 16],"float32"), shape=list[13,2,4,16,16,], )

[torch error] paddle.reshape(Tensor([13, 171402, 64, 16],"float32"), shape=list[13,2,4,16,16,], ) 
 shape '[13, 2, 4, 16, 16]' is invalid for input of size 2281703424
2025-03-12 09:50:52.176549 test begin: paddle.reshape(Tensor([13, 175515491, 1],"float32"), list[13,1,1,1,], )

[torch error] paddle.reshape(Tensor([13, 175515491, 1],"float32"), list[13,1,1,1,], ) 
 shape '[13, 1, 1, 1]' is invalid for input of size 2281701383
2025-03-12 09:50:54.528687 test begin: paddle.reshape(Tensor([13, 175515491, 1],"float32"), list[13,5,1,1,], )

[torch error] paddle.reshape(Tensor([13, 175515491, 1],"float32"), list[13,5,1,1,], ) 
 shape '[13, 5, 1, 1]' is invalid for input of size 2281701383
2025-03-12 09:50:56.587391 test begin: paddle.reshape(Tensor([13, 175515491],"int64"), list[13,], )

[torch error] paddle.reshape(Tensor([13, 175515491],"int64"), list[13,], ) 
 shape '[13]' is invalid for input of size 2281701383
2025-03-12 09:51:04.276529 test begin: paddle.reshape(Tensor([13, 1790975, 7, 14],"float32"), list[13,4,14,7,], )

[torch error] paddle.reshape(Tensor([13, 1790975, 7, 14],"float32"), list[13,4,14,7,], ) 
 shape '[13, 4, 14, 7]' is invalid for input of size 2281702150
2025-03-12 09:51:06.087229 test begin: paddle.reshape(Tensor([13, 1828287, 96],"float32"), list[13,7,4,-1,], )

[torch error] paddle.reshape(Tensor([13, 1828287, 96],"float32"), list[13,7,4,-1,], ) 
 shape '[13, 7, 4, -1]' is invalid for input of size 2281702176
2025-03-12 09:51:08.105245 test begin: paddle.reshape(Tensor([13, 1928742, 13, 7],"float32"), list[13,4,7,13,], )

[torch error] paddle.reshape(Tensor([13, 1928742, 13, 7],"float32"), list[13,4,7,13,], ) 
 shape '[13, 4, 7, 13]' is invalid for input of size 2281701786
2025-03-12 09:51:10.134848 test begin: paddle.reshape(Tensor([13, 2, 1371215, 64],"float32"), shape=list[13,-1,64,], )

[Pass] paddle.reshape(Tensor([13, 2, 1371215, 64],"float32"), shape=list[13,-1,64,], )
2025-03-12 09:54:06.086536 test begin: paddle.reshape(Tensor([13, 2, 2742430, 4, 8],"float32"), tuple(13,2,7,32,), )

[torch error] paddle.reshape(Tensor([13, 2, 2742430, 4, 8],"float32"), tuple(13,2,7,32,), ) 
 shape '[13, 2, 7, 32]' is invalid for input of size 2281701760
2025-03-12 09:54:10.132926 test begin: paddle.reshape(Tensor([13, 2, 32, 2742430],"float32"), shape=list[13,2,-1,4,16,], )

[Pass] paddle.reshape(Tensor([13, 2, 32, 2742430],"float32"), shape=list[13,2,-1,4,16,], )
2025-03-12 09:57:11.681386 test begin: paddle.reshape(Tensor([13, 2, 5484860, 16],"float32"), shape=list[13,2,-1,4,16,], )

[Pass] paddle.reshape(Tensor([13, 2, 5484860, 16],"float32"), shape=list[13,2,-1,4,16,], )
2025-03-12 09:59:59.494867 test begin: paddle.reshape(Tensor([13, 2, 5484860, 16],"float32"), shape=list[13,2,4,16,16,], )

[torch error] paddle.reshape(Tensor([13, 2, 5484860, 16],"float32"), shape=list[13,2,4,16,16,], ) 
 shape '[13, 2, 4, 16, 16]' is invalid for input of size 2281701760
2025-03-12 10:00:03.486281 test begin: paddle.reshape(Tensor([13, 2, 64, 1371215],"float32"), shape=list[13,2,-1,4,16,], )

[Pass] paddle.reshape(Tensor([13, 2, 64, 1371215],"float32"), shape=list[13,2,-1,4,16,], )
2025-03-12 10:02:50.158358 test begin: paddle.reshape(Tensor([13, 2, 64, 1371215],"float32"), shape=list[13,2,4,16,16,], )

[torch error] paddle.reshape(Tensor([13, 2, 64, 1371215],"float32"), shape=list[13,2,4,16,16,], ) 
 shape '[13, 2, 4, 16, 16]' is invalid for input of size 2281701760
2025-03-12 10:02:54.151809 test begin: paddle.reshape(Tensor([13, 2, 7, 1567103, 8],"float32"), tuple(13,2,7,32,), )

[torch error] paddle.reshape(Tensor([13, 2, 7, 1567103, 8],"float32"), tuple(13,2,7,32,), ) 
 shape '[13, 2, 7, 32]' is invalid for input of size 2281701968
2025-03-12 10:02:55.895455 test begin: paddle.reshape(Tensor([13, 2, 7, 4, 3134206],"float32"), tuple(13,2,7,32,), )

[torch error] paddle.reshape(Tensor([13, 2, 7, 4, 3134206],"float32"), tuple(13,2,7,32,), ) 
 shape '[13, 2, 7, 32]' is invalid for input of size 2281701968
2025-03-12 10:02:57.857418 test begin: paddle.reshape(Tensor([13, 2, 8, 10969719],"float32"), shape=list[13,-1,64,], )

[torch error] paddle.reshape(Tensor([13, 2, 8, 10969719],"float32"), shape=list[13,-1,64,], ) 
 shape '[13, -1, 64]' is invalid for input of size 2281701552
2025-03-12 10:03:00.222406 test begin: paddle.reshape(Tensor([13, 2, 87757746],"float32"), shape=list[13,2,4,16,], )

[torch error] paddle.reshape(Tensor([13, 2, 87757746],"float32"), shape=list[13,2,4,16,], ) 
 shape '[13, 2, 4, 16]' is invalid for input of size 2281701396
2025-03-12 10:03:01.923298 test begin: paddle.reshape(Tensor([13, 2, 87757746],"float32"), tuple(13,2,4,4,), )

[torch error] paddle.reshape(Tensor([13, 2, 87757746],"float32"), tuple(13,2,4,4,), ) 
 shape '[13, 2, 4, 4]' is invalid for input of size 2281701396
2025-03-12 10:03:03.890935 test begin: paddle.reshape(Tensor([13, 2, 87757746],"int64"), shape=list[13,2,-1,4,], )

[torch error] paddle.reshape(Tensor([13, 2, 87757746],"int64"), shape=list[13,2,-1,4,], ) 
 shape '[13, 2, -1, 4]' is invalid for input of size 2281701396
2025-03-12 10:03:11.738012 test begin: paddle.reshape(Tensor([13, 21, 1044736, 8],"float32"), tuple(13,21,32,), )

[torch error] paddle.reshape(Tensor([13, 21, 1044736, 8],"float32"), tuple(13,21,32,), ) 
 shape '[13, 21, 32]' is invalid for input of size 2281703424
2025-03-12 10:03:15.980607 test begin: paddle.reshape(Tensor([13, 21, 4, 2089471],"float32"), tuple(13,21,32,), )

[torch error] paddle.reshape(Tensor([13, 21, 4, 2089471],"float32"), tuple(13,21,32,), ) 
 shape '[13, 21, 32]' is invalid for input of size 2281702332
2025-03-12 10:03:18.008003 test begin: paddle.reshape(Tensor([13, 21, 8357881],"float32"), list[13,21,4,8,], )

[torch error] paddle.reshape(Tensor([13, 21, 8357881],"float32"), list[13,21,4,8,], ) 
 shape '[13, 21, 4, 8]' is invalid for input of size 2281701513
2025-03-12 10:03:19.478159 test begin: paddle.reshape(Tensor([13, 21, 8357881],"float32"), tuple(13,-1,4,8,), )

[torch error] paddle.reshape(Tensor([13, 21, 8357881],"float32"), tuple(13,-1,4,8,), ) 
 shape '[13, -1, 4, 8]' is invalid for input of size 2281701513
2025-03-12 10:03:21.812875 test begin: paddle.reshape(Tensor([13, 21, 8357881],"float32"), tuple(13,21,4,8,), )

[torch error] paddle.reshape(Tensor([13, 21, 8357881],"float32"), tuple(13,21,4,8,), ) 
 shape '[13, 21, 4, 8]' is invalid for input of size 2281701513
2025-03-12 10:03:23.866870 test begin: paddle.reshape(Tensor([13, 21939437, 2, 4],"float32"), list[13,4,4,2,], )

[torch error] paddle.reshape(Tensor([13, 21939437, 2, 4],"float32"), list[13,4,4,2,], ) 
 shape '[13, 4, 4, 2]' is invalid for input of size 2281701448
2025-03-12 10:03:25.570930 test begin: paddle.reshape(Tensor([13, 25073642, 1, 7],"float32"), list[13,7,1,], )

[torch error] paddle.reshape(Tensor([13, 25073642, 1, 7],"float32"), list[13,7,1,], ) 
 shape '[13, 7, 1]' is invalid for input of size 2281701422
2025-03-12 10:03:27.912244 test begin: paddle.reshape(Tensor([13, 25073642, 7, 1],"float32"), list[13,7,1,], )

[torch error] paddle.reshape(Tensor([13, 25073642, 7, 1],"float32"), list[13,7,1,], ) 
 shape '[13, 7, 1]' is invalid for input of size 2281701422
2025-03-12 10:03:29.975042 test begin: paddle.reshape(Tensor([13, 25073642, 7],"int32"), tuple(-1,7,), )

[Pass] paddle.reshape(Tensor([13, 25073642, 7],"int32"), tuple(-1,7,), )
2025-03-12 10:06:21.990184 test begin: paddle.reshape(Tensor([13, 2742430, 64],"float32"), shape=list[13,2,4,16,], )

[torch error] paddle.reshape(Tensor([13, 2742430, 64],"float32"), shape=list[13,2,4,16,], ) 
 shape '[13, 2, 4, 16]' is invalid for input of size 2281701760
2025-03-12 10:06:25.978242 test begin: paddle.reshape(Tensor([13, 2742430, 64],"int64"), shape=list[13,2,-1,4,], )

[Pass] paddle.reshape(Tensor([13, 2742430, 64],"int64"), shape=list[13,2,-1,4,], )
2025-03-12 10:10:09.150954 test begin: paddle.reshape(Tensor([13, 28, 6268411],"int32"), tuple(-1,7,), )

[Pass] paddle.reshape(Tensor([13, 28, 6268411],"int32"), tuple(-1,7,), )
2025-03-12 10:12:20.403889 test begin: paddle.reshape(Tensor([13, 29252582, 3, 2],"float32"), list[13,4,2,3,], )

[torch error] paddle.reshape(Tensor([13, 29252582, 3, 2],"float32"), list[13,4,2,3,], ) 
 shape '[13, 4, 2, 3]' is invalid for input of size 2281701396
2025-03-12 10:12:24.359409 test begin: paddle.reshape(Tensor([13, 2925259, 4, 15],"float32"), list[13,4,15,4,], )

[torch error] paddle.reshape(Tensor([13, 2925259, 4, 15],"float32"), list[13,4,15,4,], ) 
 shape '[13, 4, 15, 4]' is invalid for input of size 2281702020
2025-03-12 10:12:25.758087 test begin: paddle.reshape(Tensor([13, 3, 1828287, 32],"float32"), tuple(13,-1,32,), )

[Pass] paddle.reshape(Tensor([13, 3, 1828287, 32],"float32"), tuple(13,-1,32,), )
2025-03-12 10:15:22.970759 test begin: paddle.reshape(Tensor([13, 3, 7, 8357881],"float32"), tuple(13,-1,32,), )

[torch error] paddle.reshape(Tensor([13, 3, 7, 8357881],"float32"), tuple(13,-1,32,), ) 
 shape '[13, -1, 32]' is invalid for input of size 2281701513
2025-03-12 10:15:26.996338 test begin: paddle.reshape(Tensor([13, 3134206, 7, 8],"float32"), list[13,4,7,1,-1,], )

[Pass] paddle.reshape(Tensor([13, 3134206, 7, 8],"float32"), list[13,4,7,1,-1,], )
2025-03-12 10:18:29.573825 test begin: paddle.reshape(Tensor([13, 3134206, 7, 8],"float32"), tuple(52,-1,8,), )

[torch error] paddle.reshape(Tensor([13, 3134206, 7, 8],"float32"), tuple(52,-1,8,), ) 
 shape '[52, -1, 8]' is invalid for input of size 2281701968
2025-03-12 10:18:33.582165 test begin: paddle.reshape(Tensor([13, 32, 2, 2742430],"float32"), shape=list[13,-1,32,], )

[Pass] paddle.reshape(Tensor([13, 32, 2, 2742430],"float32"), shape=list[13,-1,32,], )
2025-03-12 10:21:25.541222 test begin: paddle.reshape(Tensor([13, 32, 342804, 16],"float32"), shape=list[13,-1,32,], )

[Pass] paddle.reshape(Tensor([13, 32, 342804, 16],"float32"), shape=list[13,-1,32,], )
2025-03-12 10:24:23.290195 test begin: paddle.reshape(Tensor([13, 3375298, 13, 4],"float32"), list[13,4,4,13,], )

[torch error] paddle.reshape(Tensor([13, 3375298, 13, 4],"float32"), list[13,4,4,13,], ) 
 shape '[13, 4, 4, 13]' is invalid for input of size 2281701448
2025-03-12 10:24:27.322479 test begin: paddle.reshape(Tensor([13, 342804, 32, 16],"float32"), shape=list[13,2,-1,4,16,], )

[Pass] paddle.reshape(Tensor([13, 342804, 32, 16],"float32"), shape=list[13,2,-1,4,16,], )
2025-03-12 10:27:21.901739 test begin: paddle.reshape(Tensor([13, 342804, 8, 64],"float32"), shape=list[13,-1,64,], )

[Pass] paddle.reshape(Tensor([13, 342804, 8, 64],"float32"), shape=list[13,-1,64,], )
2025-03-12 10:30:49.864545 test begin: paddle.reshape(Tensor([13, 4, 10969719, 4],"float32"), list[13,4,4,13,], )

[torch error] paddle.reshape(Tensor([13, 4, 10969719, 4],"float32"), list[13,4,4,13,], ) 
 shape '[13, 4, 4, 13]' is invalid for input of size 2281701552
2025-03-12 10:30:54.170062 test begin: paddle.reshape(Tensor([13, 4, 10969719, 4],"float32"), list[13,4,4,2,], )

[torch error] paddle.reshape(Tensor([13, 4, 10969719, 4],"float32"), list[13,4,4,2,], ) 
 shape '[13, 4, 4, 2]' is invalid for input of size 2281701552
2025-03-12 10:30:55.760001 test begin: paddle.reshape(Tensor([13, 4, 10969719, 4],"float32"), list[13,4,4,7,], )

[torch error] paddle.reshape(Tensor([13, 4, 10969719, 4],"float32"), list[13,4,4,7,], ) 
 shape '[13, 4, 4, 7]' is invalid for input of size 2281701552
2025-03-12 10:30:58.152194 test begin: paddle.reshape(Tensor([13, 4, 1096972, 5, 1, 8],"float32"), list[13,4,5,5,-1,], )

[torch error] paddle.reshape(Tensor([13, 4, 1096972, 5, 1, 8],"float32"), list[13,4,5,5,-1,], ) 
 shape '[13, 4, 5, 5, -1]' is invalid for input of size 2281701760
2025-03-12 10:30:59.549154 test begin: paddle.reshape(Tensor([13, 4, 13, 3375298],"float32"), list[13,4,4,13,], )

[torch error] paddle.reshape(Tensor([13, 4, 13, 3375298],"float32"), list[13,4,4,13,], ) 
 shape '[13, 4, 4, 13]' is invalid for input of size 2281701448
2025-03-12 10:31:01.912226 test begin: paddle.reshape(Tensor([13, 4, 13, 3375298],"float32"), list[13,4,7,13,], )

[torch error] paddle.reshape(Tensor([13, 4, 13, 3375298],"float32"), list[13,4,7,13,], ) 
 shape '[13, 4, 7, 13]' is invalid for input of size 2281701448
2025-03-12 10:31:03.332004 test begin: paddle.reshape(Tensor([13, 4, 1371215, 32],"float32"), shape=list[13,32,-1,], )

[Pass] paddle.reshape(Tensor([13, 4, 1371215, 32],"float32"), shape=list[13,32,-1,], )
2025-03-12 10:34:57.129618 test begin: paddle.reshape(Tensor([13, 4, 2, 21939437],"float32"), list[13,4,4,2,], )

[torch error] paddle.reshape(Tensor([13, 4, 2, 21939437],"float32"), list[13,4,4,2,], ) 
 shape '[13, 4, 4, 2]' is invalid for input of size 2281701448
2025-03-12 10:35:01.489325 test begin: paddle.reshape(Tensor([13, 4, 2, 21939437],"float32"), list[13,4,8,2,], )

[torch error] paddle.reshape(Tensor([13, 4, 2, 21939437],"float32"), list[13,4,8,2,], ) 
 shape '[13, 4, 8, 2]' is invalid for input of size 2281701448
2025-03-12 10:35:03.566008 test begin: paddle.reshape(Tensor([13, 4, 21, 2089471],"float32"), tuple(52,-1,8,), )

[torch error] paddle.reshape(Tensor([13, 4, 21, 2089471],"float32"), tuple(52,-1,8,), ) 
 shape '[52, -1, 8]' is invalid for input of size 2281702332
2025-03-12 10:35:05.611022 test begin: paddle.reshape(Tensor([13, 4, 21939437, 2],"float32"), list[13,4,2,3,], )

[torch error] paddle.reshape(Tensor([13, 4, 21939437, 2],"float32"), list[13,4,2,3,], ) 
 shape '[13, 4, 2, 3]' is invalid for input of size 2281701448
2025-03-12 10:35:07.659597 test begin: paddle.reshape(Tensor([13, 4, 21939437, 2],"float32"), list[13,4,2,6,], )

[torch error] paddle.reshape(Tensor([13, 4, 21939437, 2],"float32"), list[13,4,2,6,], ) 
 shape '[13, 4, 2, 6]' is invalid for input of size 2281701448
2025-03-12 10:35:09.090737 test begin: paddle.reshape(Tensor([13, 4, 2925259, 15],"float32"), list[13,4,15,4,], )

[torch error] paddle.reshape(Tensor([13, 4, 2925259, 15],"float32"), list[13,4,15,4,], ) 
 shape '[13, 4, 15, 4]' is invalid for input of size 2281702020
2025-03-12 10:35:11.027428 test begin: paddle.reshape(Tensor([13, 4, 3, 14626291],"float32"), list[13,4,2,3,], )

[torch error] paddle.reshape(Tensor([13, 4, 3, 14626291],"float32"), list[13,4,2,3,], ) 
 shape '[13, 4, 2, 3]' is invalid for input of size 2281701396
2025-03-12 10:35:13.050700 test begin: paddle.reshape(Tensor([13, 4, 3134206, 14],"float32"), list[13,4,14,7,], )

[torch error] paddle.reshape(Tensor([13, 4, 3134206, 14],"float32"), list[13,4,14,7,], ) 
 shape '[13, 4, 14, 7]' is invalid for input of size 2281701968
2025-03-12 10:35:15.004764 test begin: paddle.reshape(Tensor([13, 4, 4, 10969719],"float32"), list[13,4,15,4,], )

[torch error] paddle.reshape(Tensor([13, 4, 4, 10969719],"float32"), list[13,4,15,4,], ) 
 shape '[13, 4, 15, 4]' is invalid for input of size 2281701552
2025-03-12 10:35:17.059018 test begin: paddle.reshape(Tensor([13, 4, 4, 10969719],"float32"), list[13,4,8,4,], )

[torch error] paddle.reshape(Tensor([13, 4, 4, 10969719],"float32"), list[13,4,8,4,], ) 
 shape '[13, 4, 8, 4]' is invalid for input of size 2281701552
2025-03-12 10:35:19.009148 test begin: paddle.reshape(Tensor([13, 4, 43878873],"float32"), tuple(13,4,4,4,), )

[torch error] paddle.reshape(Tensor([13, 4, 43878873],"float32"), tuple(13,4,4,4,), ) 
 shape '[13, 4, 4, 4]' is invalid for input of size 2281701396
2025-03-12 10:35:21.011485 test begin: paddle.reshape(Tensor([13, 4, 5, 1, 8775775],"float32"), list[13,4,5,-1,], )

[Pass] paddle.reshape(Tensor([13, 4, 5, 1, 8775775],"float32"), list[13,4,5,-1,], )
2025-03-12 10:38:26.146254 test begin: paddle.reshape(Tensor([13, 4, 5, 1096972, 1, 8],"float32"), list[13,4,5,5,-1,], )

[torch error] paddle.reshape(Tensor([13, 4, 5, 1096972, 1, 8],"float32"), list[13,4,5,5,-1,], ) 
 shape '[13, 4, 5, 5, -1]' is invalid for input of size 2281701760
2025-03-12 10:38:30.127361 test begin: paddle.reshape(Tensor([13, 4, 5, 1096972, 8],"float32"), list[13,4,5,-1,], )

[Pass] paddle.reshape(Tensor([13, 4, 5, 1096972, 8],"float32"), list[13,4,5,-1,], )
2025-03-12 10:41:21.171806 test begin: paddle.reshape(Tensor([13, 4, 5, 5, 1, 1755155],"float32"), list[13,4,5,5,-1,], )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 86, in func_timeout
    thread.join(timeout)
  File "/usr/lib/python3.9/threading.py", line 1064, in join
    self._wait_for_tstate_lock(timeout=max(timeout, 0))
  File "/usr/lib/python3.9/threading.py", line 1080, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt
2025-03-12 10:48:21.252305 test begin: paddle.nn.functional.grid_sample(Tensor([56, 3, 6790778, 2],"float32"), Tensor([56, 2, 2, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, )

W0312 10:49:40.165410 104201 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 10:49:40.166556 104201 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741747781 (unix time) try "date -d @1741747781" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x19644) received by PID 104004 (TID 0x7f57da7c3700) from PID 104004 ***]

2025-03-12 10:50:27.371884 test begin: paddle.nn.functional.grid_sample(Tensor([56, 3, 6790778, 2],"float32"), Tensor([56, 2, 6790778, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, )

W0312 10:52:23.667630 105737 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 10:52:23.669227 105737 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741747945 (unix time) try "date -d @1741747945" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x19c92) received by PID 105618 (TID 0x7f65907c3700) from PID 105618 ***]

2025-03-12 10:53:07.144800 test begin: paddle.nn.functional.grid_sample(Tensor([56, 3, 848848, 16],"float32"), Tensor([56, 16, 16, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, )

W0312 10:54:23.112847 107213 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 10:54:23.114063 107213 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741748063 (unix time) try "date -d @1741748063" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1a264) received by PID 107108 (TID 0x7fa04b4f4700) from PID 107108 ***]

2025-03-12 10:54:28.484107 test begin: paddle.nn.functional.grid_sample(Tensor([56, 3, 848848, 16],"float32"), Tensor([56, 16, 848848, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, )

W0312 10:56:18.350091 108064 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 10:56:18.351526 108064 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741748180 (unix time) try "date -d @1741748180" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1a5b2) received by PID 107954 (TID 0x7f27c7abb700) from PID 107954 ***]

2025-03-12 10:56:24.968176 test begin: paddle.nn.functional.grid_sample(Tensor([56, 39790, 32, 32],"float32"), Tensor([56, 32, 32, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, )

W0312 10:58:01.202891 109143 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 10:58:01.204586 109143 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741748282 (unix time) try "date -d @1741748282" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1a9f2) received by PID 109042 (TID 0x7f1e5534a700) from PID 109042 ***]

2025-03-12 10:58:07.641575 test begin: paddle.nn.functional.grid_sample(Tensor([56, 39790, 32, 32],"float32"), Tensor([56, 39790, 32, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, )

W0312 10:59:13.616374 109977 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 10:59:13.617429 109977 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.nn.functional.grid_sample(Tensor([56, 39790, 32, 32],"float32"), Tensor([56, 39790, 32, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, ) 
 CUDA out of memory. Tried to allocate 10569.29 GiB. GPU 0 has a total capacity of 79.18 GiB of which 69.16 GiB is free. Process 55346 has 10.03 GiB memory in use. Of the allocated memory 9.03 GiB is allocated by PyTorch, and 3.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-12 10:59:15.494686 test begin: paddle.nn.functional.grid_sample(Tensor([56, 9948, 64, 64],"float32"), Tensor([56, 64, 64, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741748375 (unix time) try "date -d @1741748375" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1ad99) received by PID 109977 (TID 0x7ff64d935700) from PID 109977 ***]

2025-03-12 11:00:17.572591 test begin: paddle.nn.functional.grid_sample(Tensor([56, 9948, 64, 64],"float32"), Tensor([56, 9948, 64, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, )

W0312 11:01:24.657127 111212 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 11:01:24.658231 111212 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.nn.functional.grid_sample(Tensor([56, 9948, 64, 64],"float32"), Tensor([56, 9948, 64, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, ) 
 CUDA out of memory. Tried to allocate 1321.29 GiB. GPU 0 has a total capacity of 79.18 GiB of which 69.42 GiB is free. Process 6169 has 9.76 GiB memory in use. Of the allocated memory 8.77 GiB is allocated by PyTorch, and 3.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-12 11:01:26.775398 test begin: paddle.nn.functional.grid_sample(Tensor([5821, 4, 280, 350],"float32"), Tensor([1, 280, 350, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, )

[torch error] paddle.nn.functional.grid_sample(Tensor([5821, 4, 280, 350],"float32"), Tensor([1, 280, 350, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, ) 
 grid_sampler(): expected grid and input to have same batch size, but got input with sizes [5821, 4, 280, 350] and grid with sizes [1, 280, 350, 2]
2025-03-12 11:01:28.252374 test begin: paddle.nn.functional.grid_sample(Tensor([5821, 4, 280, 350],"float32"), Tensor([1, 298, 364, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, )

[torch error] paddle.nn.functional.grid_sample(Tensor([5821, 4, 280, 350],"float32"), Tensor([1, 298, 364, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, ) 
 grid_sampler(): expected grid and input to have same batch size, but got input with sizes [5821, 4, 280, 350] and grid with sizes [1, 298, 364, 2]
2025-03-12 11:01:30.470626 test begin: paddle.nn.functional.grid_sample(Tensor([5821, 4, 280, 350],"float32"), Tensor([1, 368, 416, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, )

[torch error] paddle.nn.functional.grid_sample(Tensor([5821, 4, 280, 350],"float32"), Tensor([1, 368, 416, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, ) 
 grid_sampler(): expected grid and input to have same batch size, but got input with sizes [5821, 4, 280, 350] and grid with sizes [1, 368, 416, 2]
2025-03-12 11:01:32.905759 test begin: paddle.nn.functional.grid_sample(Tensor([5821, 4, 280, 350],"float32"), Tensor([5821, 280, 350, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741748523 (unix time) try "date -d @1741748523" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1b26c) received by PID 111212 (TID 0x7f12427c3700) from PID 111212 ***]

2025-03-12 11:02:09.242790 test begin: paddle.nn.functional.grid_sample(Tensor([5821, 4, 280, 350],"float32"), Tensor([5821, 298, 364, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, )

W0312 11:03:42.214294 112318 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 11:03:42.215481 112318 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741748623 (unix time) try "date -d @1741748623" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1b653) received by PID 112211 (TID 0x7f557d744700) from PID 112211 ***]

2025-03-12 11:04:28.660355 test begin: paddle.nn.functional.grid_sample(Tensor([5821, 4, 280, 350],"float32"), Tensor([5821, 368, 416, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, )

W0312 11:06:22.121194 113801 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 11:06:22.122470 113801 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741748784 (unix time) try "date -d @1741748784" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1bc19) received by PID 113689 (TID 0x7faa20949700) from PID 113689 ***]

2025-03-12 11:07:07.284646 test begin: paddle.nn.functional.grid_sample(Tensor([61896, 1, 192, 192],"float32"), Tensor([1, 1, 12544, 2],"float32"), align_corners=False, )

W0312 11:08:18.148118 115076 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 11:08:18.149286 115076 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.nn.functional.grid_sample(Tensor([61896, 1, 192, 192],"float32"), Tensor([1, 1, 12544, 2],"float32"), align_corners=False, ) 
 grid_sampler(): expected grid and input to have same batch size, but got input with sizes [61896, 1, 192, 192] and grid with sizes [1, 1, 12544, 2]
2025-03-12 11:08:20.245814 test begin: paddle.nn.functional.grid_sample(Tensor([61896, 1, 192, 192],"float32"), Tensor([61896, 1, 12544, 2],"float32"), align_corners=False, )

[accuracy error] paddle.nn.functional.grid_sample(Tensor([61896, 1, 192, 192],"float32"), Tensor([61896, 1, 12544, 2],"float32"), align_corners=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 44016245 / 776423424 (5.67%)
Max absolute difference: 0.96928257
Max relative difference: 7893854.5
 x: array([[[[ 0.176651,  0.123384, -0.075706, ..., -0.212378, -0.185817,
          -0.120427]]],
...
 y: array([[[[ 0.176651,  0.123384, -0.075706, ..., -0.212378, -0.185817,
          -0.120427]]],
...
2025-03-12 11:09:18.957160 test begin: paddle.nn.functional.grid_sample(Tensor([67395, 1, 184, 184],"float32"), Tensor([1, 1, 12544, 2],"float32"), align_corners=False, )

[torch error] paddle.nn.functional.grid_sample(Tensor([67395, 1, 184, 184],"float32"), Tensor([1, 1, 12544, 2],"float32"), align_corners=False, ) 
 grid_sampler(): expected grid and input to have same batch size, but got input with sizes [67395, 1, 184, 184] and grid with sizes [1, 1, 12544, 2]
2025-03-12 11:09:23.039964 test begin: paddle.nn.functional.grid_sample(Tensor([67395, 1, 184, 184],"float32"), Tensor([1, 1, 37632, 2],"float32"), align_corners=False, )

[torch error] paddle.nn.functional.grid_sample(Tensor([67395, 1, 184, 184],"float32"), Tensor([1, 1, 37632, 2],"float32"), align_corners=False, ) 
 grid_sampler(): expected grid and input to have same batch size, but got input with sizes [67395, 1, 184, 184] and grid with sizes [1, 1, 37632, 2]
2025-03-12 11:09:25.735333 test begin: paddle.nn.functional.grid_sample(Tensor([67395, 1, 184, 184],"float32"), Tensor([67395, 1, 12544, 2],"float32"), align_corners=False, )

[accuracy error] paddle.nn.functional.grid_sample(Tensor([67395, 1, 184, 184],"float32"), Tensor([67395, 1, 12544, 2],"float32"), align_corners=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 47922694 / 845402880 (5.67%)
Max absolute difference: 0.97413635
Max relative difference: 12147870.
 x: array([[[[-0.046991, -0.207008, -0.38845 , ...,  0.058028, -0.261237,
          -0.291256]]],
...
 y: array([[[[-0.046991, -0.207008, -0.38845 , ...,  0.058028, -0.261237,
          -0.291256]]],
...
2025-03-12 11:10:36.953087 test begin: paddle.nn.functional.grid_sample(Tensor([67395, 1, 184, 184],"float32"), Tensor([67395, 1, 37632, 2],"float32"), align_corners=False, )

[torch error] paddle.nn.functional.grid_sample(Tensor([67395, 1, 184, 184],"float32"), Tensor([67395, 1, 37632, 2],"float32"), align_corners=False, ) 
 cannot reshape array of size 4300000000 into shape (67395,1,37632,2)
2025-03-12 11:10:39.515818 test begin: paddle.nn.functional.grid_sample(Tensor([727584, 4, 28, 28],"float32"), Tensor([1, 28, 28, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )

[torch error] paddle.nn.functional.grid_sample(Tensor([727584, 4, 28, 28],"float32"), Tensor([1, 28, 28, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, ) 
 grid_sampler(): expected grid and input to have same batch size, but got input with sizes [727584, 4, 28, 28] and grid with sizes [1, 28, 28, 2]
2025-03-12 11:10:41.609874 test begin: paddle.nn.functional.grid_sample(Tensor([727584, 4, 28, 28],"float32"), Tensor([1, 34, 34, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, )

[torch error] paddle.nn.functional.grid_sample(Tensor([727584, 4, 28, 28],"float32"), Tensor([1, 34, 34, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, ) 
 grid_sampler(): expected grid and input to have same batch size, but got input with sizes [727584, 4, 28, 28] and grid with sizes [1, 34, 34, 2]
2025-03-12 11:10:43.934680 test begin: paddle.nn.functional.grid_sample(Tensor([727584, 4, 28, 28],"float32"), Tensor([727584, 28, 28, 2],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741749072 (unix time) try "date -d @1741749072" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1c184) received by PID 115076 (TID 0x7fadd134a700) from PID 115076 ***]

2025-03-12 11:11:58.372286 test begin: paddle.nn.functional.grid_sample(Tensor([727584, 4, 28, 28],"float32"), Tensor([727584, 34, 34, 2],"float32"), mode="nearest", padding_mode="zeros", align_corners=False, )

W0312 11:14:52.191848 117981 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 11:14:52.195191 117981 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741749296 (unix time) try "date -d @1741749296" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1cc6a) received by PID 117866 (TID 0x7fe723f48700) from PID 117866 ***]

2025-03-12 11:15:47.092282 test begin: paddle.nn.functional.grid_sample(Tensor([73661, 1, 176, 176],"float32"), Tensor([1, 1, 12544, 2],"float32"), align_corners=False, )

W0312 11:17:13.284934 119796 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 11:17:13.286188 119796 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.nn.functional.grid_sample(Tensor([73661, 1, 176, 176],"float32"), Tensor([1, 1, 12544, 2],"float32"), align_corners=False, ) 
 grid_sampler(): expected grid and input to have same batch size, but got input with sizes [73661, 1, 176, 176] and grid with sizes [1, 1, 12544, 2]
2025-03-12 11:17:16.331415 test begin: paddle.nn.functional.grid_sample(Tensor([73661, 1, 176, 176],"float32"), Tensor([1, 1, 37632, 2],"float32"), align_corners=False, )

[torch error] paddle.nn.functional.grid_sample(Tensor([73661, 1, 176, 176],"float32"), Tensor([1, 1, 37632, 2],"float32"), align_corners=False, ) 
 grid_sampler(): expected grid and input to have same batch size, but got input with sizes [73661, 1, 176, 176] and grid with sizes [1, 1, 37632, 2]
2025-03-12 11:17:22.619304 test begin: paddle.nn.functional.grid_sample(Tensor([73661, 1, 176, 176],"float32"), Tensor([73661, 1, 12544, 2],"float32"), align_corners=False, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741749478 (unix time) try "date -d @1741749478" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1d3f4) received by PID 119796 (TID 0x7f58f36f8700) from PID 119796 ***]

2025-03-12 11:18:45.678316 test begin: paddle.nn.functional.grid_sample(Tensor([73661, 1, 176, 176],"float32"), Tensor([73661, 1, 37632, 2],"float32"), align_corners=False, )

W0312 11:20:16.427829 121391 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 11:20:16.429620 121391 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.nn.functional.grid_sample(Tensor([73661, 1, 176, 176],"float32"), Tensor([73661, 1, 37632, 2],"float32"), align_corners=False, ) 
 cannot reshape array of size 4300000000 into shape (73661,1,37632,2)
2025-03-12 11:20:19.209546 test begin: paddle.nn.functional.grid_sample(Tensor([742742, 3, 32, 32],"float32"), Tensor([56, 32, 32, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, )

[torch error] paddle.nn.functional.grid_sample(Tensor([742742, 3, 32, 32],"float32"), Tensor([56, 32, 32, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, ) 
 grid_sampler(): expected grid and input to have same batch size, but got input with sizes [742742, 3, 32, 32] and grid with sizes [56, 32, 32, 2]
2025-03-12 11:20:23.116348 test begin: paddle.nn.functional.grid_sample(Tensor([742742, 3, 32, 32],"float32"), Tensor([742742, 32, 32, 2],"float32"), mode="bilinear", padding_mode="border", align_corners=True, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741749661 (unix time) try "date -d @1741749661" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1da2f) received by PID 121391 (TID 0x7fb951abb700) from PID 121391 ***]

2025-03-12 11:21:50.290607 test begin: paddle.nn.functional.grid_sample(x=Tensor([16, 64, 80, 94, 311],"float32"), grid=Tensor([16, 280, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )

W0312 11:24:12.417876 123082 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 11:24:12.422000 123082 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741749854 (unix time) try "date -d @1741749854" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1e04f) received by PID 122959 (TID 0x7f7e5334a700) from PID 122959 ***]

2025-03-12 11:25:00.790820 test begin: paddle.nn.functional.grid_sample(x=Tensor([16, 64, 80, 94, 311],"float32"), grid=Tensor([4, 280, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )

W0312 11:26:15.229674 124737 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 11:26:15.230841 124737 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.nn.functional.grid_sample(x=Tensor([16, 64, 80, 94, 311],"float32"), grid=Tensor([4, 280, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, ) 
 grid_sampler(): expected grid and input to have same batch size, but got input with sizes [16, 64, 80, 94, 311] and grid with sizes [4, 280, 376, 25, 3]
2025-03-12 11:26:17.764619 test begin: paddle.nn.functional.grid_sample(x=Tensor([4, 244, 80, 94, 311],"float32"), grid=Tensor([4, 244, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )

[Pass] paddle.nn.functional.grid_sample(x=Tensor([4, 244, 80, 94, 311],"float32"), grid=Tensor([4, 244, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )
2025-03-12 11:30:52.094776 test begin: paddle.nn.functional.grid_sample(x=Tensor([4, 244, 80, 94, 311],"float32"), grid=Tensor([4, 280, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )

[Pass] paddle.nn.functional.grid_sample(x=Tensor([4, 244, 80, 94, 311],"float32"), grid=Tensor([4, 280, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )
2025-03-12 11:34:59.171323 test begin: paddle.nn.functional.grid_sample(x=Tensor([4, 64, 305, 94, 311],"float32"), grid=Tensor([4, 280, 305, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )

[Pass] paddle.nn.functional.grid_sample(x=Tensor([4, 64, 305, 94, 311],"float32"), grid=Tensor([4, 280, 305, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )
2025-03-12 11:36:59.176329 test begin: paddle.nn.functional.grid_sample(x=Tensor([4, 64, 305, 94, 311],"float32"), grid=Tensor([4, 280, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )

[Pass] paddle.nn.functional.grid_sample(x=Tensor([4, 64, 305, 94, 311],"float32"), grid=Tensor([4, 280, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )
2025-03-12 11:39:16.827557 test begin: paddle.nn.functional.grid_sample(x=Tensor([4, 64, 80, 359, 311],"float32"), grid=Tensor([4, 280, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )

[Pass] paddle.nn.functional.grid_sample(x=Tensor([4, 64, 80, 359, 311],"float32"), grid=Tensor([4, 280, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )
2025-03-12 11:41:43.942603 test begin: paddle.nn.functional.grid_sample(x=Tensor([4, 64, 80, 359, 311],"float32"), grid=Tensor([4, 280, 376, 359, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )

2025-03-12 11:41:54.397863 test begin: paddle.nn.functional.grid_sample(x=Tensor([4, 64, 80, 94, 1186],"float32"), grid=Tensor([4, 280, 376, 25, 1186],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )

[torch error] paddle.nn.functional.grid_sample(x=Tensor([4, 64, 80, 94, 1186],"float32"), grid=Tensor([4, 280, 376, 25, 1186],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, ) 
 cannot reshape array of size 4300000000 into shape (4,280,376,25,1186)
2025-03-12 11:41:57.647221 test begin: paddle.nn.functional.grid_sample(x=Tensor([4, 64, 80, 94, 1186],"float32"), grid=Tensor([4, 280, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )

[Pass] paddle.nn.functional.grid_sample(x=Tensor([4, 64, 80, 94, 1186],"float32"), grid=Tensor([4, 280, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )
2025-03-12 11:44:11.628394 test begin: paddle.nn.functional.grid_sample(x=Tensor([4, 64, 80, 94, 311],"float32"), grid=Tensor([289, 280, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )

[torch error] paddle.nn.functional.grid_sample(x=Tensor([4, 64, 80, 94, 311],"float32"), grid=Tensor([289, 280, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, ) 
 grid_sampler(): expected grid and input to have same batch size, but got input with sizes [4, 64, 80, 94, 311] and grid with sizes [289, 280, 376, 25, 3]
2025-03-12 11:44:17.443895 test begin: paddle.nn.functional.grid_sample(x=Tensor([4, 64, 80, 94, 311],"float32"), grid=Tensor([4, 20228, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )

[torch error] paddle.nn.functional.grid_sample(x=Tensor([4, 64, 80, 94, 311],"float32"), grid=Tensor([4, 20228, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, ) 
 CUDA out of memory. Tried to allocate 181.34 GiB. GPU 0 has a total capacity of 79.18 GiB of which 66.86 GiB is free. Process 137095 has 12.32 GiB memory in use. Of the allocated memory 10.73 GiB is allocated by PyTorch, and 1.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-12 11:44:21.921849 test begin: paddle.nn.functional.grid_sample(x=Tensor([4, 64, 80, 94, 311],"float32"), grid=Tensor([4, 280, 27164, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )

[torch error] paddle.nn.functional.grid_sample(x=Tensor([4, 64, 80, 94, 311],"float32"), grid=Tensor([4, 280, 27164, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, ) 
 CUDA out of memory. Tried to allocate 181.34 GiB. GPU 0 has a total capacity of 79.18 GiB of which 66.86 GiB is free. Process 137095 has 12.32 GiB memory in use. Of the allocated memory 10.73 GiB is allocated by PyTorch, and 1.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-12 11:44:26.475203 test begin: paddle.nn.functional.grid_sample(x=Tensor([4, 64, 80, 94, 311],"float32"), grid=Tensor([4, 280, 376, 1807, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )

[torch error] paddle.nn.functional.grid_sample(x=Tensor([4, 64, 80, 94, 311],"float32"), grid=Tensor([4, 280, 376, 1807, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, ) 
 CUDA out of memory. Tried to allocate 181.43 GiB. GPU 0 has a total capacity of 79.18 GiB of which 66.86 GiB is free. Process 137095 has 12.32 GiB memory in use. Of the allocated memory 10.73 GiB is allocated by PyTorch, and 1.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-12 11:44:29.955964 test begin: paddle.nn.functional.grid_sample(x=Tensor([4, 64, 80, 94, 311],"float32"), grid=Tensor([4, 280, 376, 25, 217],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )

[torch error] paddle.nn.functional.grid_sample(x=Tensor([4, 64, 80, 94, 311],"float32"), grid=Tensor([4, 280, 376, 25, 217],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, ) 
 grid_sampler(): expected grid to have size 3 in last dimension, but got grid with sizes [4, 280, 376, 25, 217]
2025-03-12 11:44:32.510249 test begin: paddle.nn.functional.hardshrink(Tensor([2281701379],"float32"), -1, None, )

[Pass] paddle.nn.functional.hardshrink(Tensor([2281701379],"float32"), -1, None, )
2025-03-12 11:48:15.325303 test begin: paddle.nn.functional.hardshrink(Tensor([2281701379],"float32"), 0.5, None, )

[accuracy error] paddle.nn.functional.hardshrink(Tensor([2281701379],"float32"), 0.5, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 62 / 2281701379 (2.72e-06%)
Max absolute difference: 0.5
Max relative difference: inf
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)
 y: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)
2025-03-12 11:50:02.608138 test begin: paddle.nn.functional.hardshrink(Tensor([4294967297],"float16"), -1, None, )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f80ac34a6a0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 12:00:18.567898 test begin: paddle.nn.functional.hardshrink(Tensor([4294967297],"float16"), 0, None, )

W0312 12:02:00.203189 141530 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 12:02:00.204404 141530 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7fd7285b3b80>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   THPVariable_subclass_dealloc(_object*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741752620 (unix time) try "date -d @1741752620" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2287c) received by PID 141436 (TID 0x7fd6f0949700) from PID 141436 ***]

2025-03-12 12:11:06.887610 test begin: paddle.nn.functional.hardshrink(Tensor([4294967297],"float16"), 0.5, None, )

W0312 12:13:02.331423 146961 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 12:13:02.332623 146961 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7fd6bf5dfbe0>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 12:21:20.387757 test begin: paddle.nn.functional.hardshrink(x=Tensor([2281701379],"float32"), )

W0312 12:22:52.342399 151830 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 12:22:52.343631 151830 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.nn.functional.hardshrink(x=Tensor([2281701379],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 68 / 2281701379 (2.98e-06%)
Max absolute difference: 0.5
Max relative difference: inf
 x: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)
 y: array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)
2025-03-12 12:24:18.780235 test begin: paddle.nn.functional.hardshrink(x=Tensor([4294967297],"float16"), )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f63eb8d4790>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 12:34:30.649883 test begin: paddle.nn.functional.hardshrink(x=Tensor([4294967297],"float16"), threshold=-1, )

W0312 12:36:17.076606 158146 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 12:36:17.077749 158146 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f01bbbf2bb0>,)) (kwargs={}) timed out after 600.000000 seconds.

terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741754671 (unix time) try "date -d @1741754671" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x26962) received by PID 158050 (TID 0x7f015df48700) from PID 158050 ***]

2025-03-12 12:45:18.205087 test begin: paddle.nn.functional.hardshrink(x=Tensor([4294967297],"float16"), threshold=0, )

W0312 12:47:06.778337 163255 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 12:47:06.779524 163255 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f7c93a6dd60>,)) (kwargs={}) timed out after 600.000000 seconds.

2025-03-12 12:55:28.060655 test begin: paddle.nn.functional.hardsigmoid(Tensor([1, 1024, 1, 2228225],"float32"), name=None, )

W0312 12:56:55.258741  5067 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0312 12:56:55.259989  5067 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.nn.functional.hardsigmoid(Tensor([1, 1024, 1, 2228225],"float32"), name=None, )
2025-03-12 12:59:12.475902 test begin: paddle.nn.functional.hardsigmoid(Tensor([1, 1024, 2228225, 1],"float32"), name=None, )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([1, 1024, 2228225, 1],"float32"), name=None, )
2025-03-12 13:02:00.626725 test begin: paddle.nn.functional.hardsigmoid(Tensor([1, 120, 1, 19014179],"float32"), name=None, )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([1, 120, 1, 19014179],"float32"), name=None, )
2025-03-12 13:05:00.441977 test begin: paddle.nn.functional.hardsigmoid(Tensor([1, 120, 19014179, 1],"float32"), name=None, )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([1, 120, 19014179, 1],"float32"), name=None, )
2025-03-12 13:08:03.050116 test begin: paddle.nn.functional.hardsigmoid(Tensor([1, 144, 1, 15845149],"float32"), name=None, )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([1, 144, 1, 15845149],"float32"), name=None, )
2025-03-12 13:11:10.169885 test begin: paddle.nn.functional.hardsigmoid(Tensor([1, 144, 15845149, 1],"float32"), name=None, )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([1, 144, 15845149, 1],"float32"), name=None, )
2025-03-12 13:14:16.030772 test begin: paddle.nn.functional.hardsigmoid(Tensor([1, 16, 1, 142606337],"float32"), name=None, )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([1, 16, 1, 142606337],"float32"), name=None, )
2025-03-12 13:17:21.773188 test begin: paddle.nn.functional.hardsigmoid(Tensor([1, 16, 142606337, 1],"float32"), name=None, )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([1, 16, 142606337, 1],"float32"), name=None, )
2025-03-12 13:20:30.308808 test begin: paddle.nn.functional.hardsigmoid(Tensor([1, 192, 1, 11883862],"float32"), )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([1, 192, 1, 11883862],"float32"), )
2025-03-12 13:23:38.467035 test begin: paddle.nn.functional.hardsigmoid(Tensor([1, 192, 1, 11883862],"float32"), name=None, )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([1, 192, 1, 11883862],"float32"), name=None, )
2025-03-12 13:26:47.436258 test begin: paddle.nn.functional.hardsigmoid(Tensor([1, 192, 11883862, 1],"float32"), )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([1, 192, 11883862, 1],"float32"), )
2025-03-12 13:29:39.899764 test begin: paddle.nn.functional.hardsigmoid(Tensor([1, 192, 11883862, 1],"float32"), name=None, )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([1, 192, 11883862, 1],"float32"), name=None, )
2025-03-12 13:32:38.414770 test begin: paddle.nn.functional.hardsigmoid(Tensor([1, 2281701379, 1, 1],"float32"), )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([1, 2281701379, 1, 1],"float32"), )
2025-03-12 13:35:21.564384 test begin: paddle.nn.functional.hardsigmoid(Tensor([1, 2281701379, 1, 1],"float32"), name=None, )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([1, 2281701379, 1, 1],"float32"), name=None, )
2025-03-12 13:38:37.337339 test begin: paddle.nn.functional.hardsigmoid(Tensor([1, 384, 1, 5941931],"float32"), )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([1, 384, 1, 5941931],"float32"), )
2025-03-12 13:41:24.365944 test begin: paddle.nn.functional.hardsigmoid(Tensor([1, 384, 5941931, 1],"float32"), )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([1, 384, 5941931, 1],"float32"), )
2025-03-12 13:44:25.991943 test begin: paddle.nn.functional.hardsigmoid(Tensor([1, 48, 1, 47535446],"float32"), )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([1, 48, 1, 47535446],"float32"), )
2025-03-12 13:47:23.392242 test begin: paddle.nn.functional.hardsigmoid(Tensor([1, 48, 47535446, 1],"float32"), )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([1, 48, 47535446, 1],"float32"), )
2025-03-12 13:50:47.969893 test begin: paddle.nn.functional.hardsigmoid(Tensor([1, 768, 1, 2970966],"float32"), )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([1, 768, 1, 2970966],"float32"), )
2025-03-12 13:53:51.791067 test begin: paddle.nn.functional.hardsigmoid(Tensor([1, 768, 2970966, 1],"float32"), )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([1, 768, 2970966, 1],"float32"), )
2025-03-12 13:56:53.195395 test begin: paddle.nn.functional.hardsigmoid(Tensor([1, 96, 1, 23767723],"float32"), )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([1, 96, 1, 23767723],"float32"), )
2025-03-12 13:59:34.565633 test begin: paddle.nn.functional.hardsigmoid(Tensor([1, 96, 23767723, 1],"float32"), )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([1, 96, 23767723, 1],"float32"), )
2025-03-12 14:02:16.962745 test begin: paddle.nn.functional.hardsigmoid(Tensor([1140850690, 2],"float32"), name=None, )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 86, in func_timeout
    thread.join(timeout)
  File "/usr/lib/python3.9/threading.py", line 1064, in join
    self._wait_for_tstate_lock(timeout=max(timeout, 0))
  File "/usr/lib/python3.9/threading.py", line 1080, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt
2025-03-13 10:09:46.538811 test begin: paddle.nn.functional.hardsigmoid(Tensor([11883862, 192, 1, 1],"float32"), )

W0313 10:11:15.009080 11018 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0313 10:11:15.010535 11018 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.nn.functional.hardsigmoid(Tensor([11883862, 192, 1, 1],"float32"), )
2025-03-13 10:14:04.421526 test begin: paddle.nn.functional.hardsigmoid(Tensor([11883862, 192, 1, 1],"float32"), name=None, )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([11883862, 192, 1, 1],"float32"), name=None, )
2025-03-13 10:17:11.088058 test begin: paddle.nn.functional.hardsigmoid(Tensor([142606337, 16, 1, 1],"float32"), name=None, )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([142606337, 16, 1, 1],"float32"), name=None, )
2025-03-13 10:20:21.036330 test begin: paddle.nn.functional.hardsigmoid(Tensor([1431655766, 3],"float16"), name=None, )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([1431655766, 3],"float16"), name=None, )
2025-03-13 10:39:28.772839 test begin: paddle.nn.functional.hardsigmoid(Tensor([15845149, 144, 1, 1],"float32"), name=None, )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([15845149, 144, 1, 1],"float32"), name=None, )
2025-03-13 10:42:30.544414 test begin: paddle.nn.functional.hardsigmoid(Tensor([19014179, 120, 1, 1],"float32"), name=None, )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([19014179, 120, 1, 1],"float32"), name=None, )
2025-03-13 10:45:43.175007 test begin: paddle.nn.functional.hardsigmoid(Tensor([2, 1140850690],"float32"), name=None, )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([2, 1140850690],"float32"), name=None, )
2025-03-13 10:48:53.464816 test begin: paddle.nn.functional.hardsigmoid(Tensor([2, 2147483649],"float16"), name=None, )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([2, 2147483649],"float16"), name=None, )
2025-03-13 11:06:02.754859 test begin: paddle.nn.functional.hardsigmoid(Tensor([2147483649, 2],"float16"), name=None, )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([2147483649, 2],"float16"), name=None, )
2025-03-13 11:22:41.655019 test begin: paddle.nn.functional.hardsigmoid(Tensor([2228225, 1024, 1, 1],"float32"), name=None, )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([2228225, 1024, 1, 1],"float32"), name=None, )
2025-03-13 11:25:46.032413 test begin: paddle.nn.functional.hardsigmoid(Tensor([23767723, 96, 1, 1],"float32"), )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([23767723, 96, 1, 1],"float32"), )
2025-03-13 11:28:44.592405 test begin: paddle.nn.functional.hardsigmoid(Tensor([285213, 200, 40],"float32"), )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([285213, 200, 40],"float32"), )
2025-03-13 11:31:39.493210 test begin: paddle.nn.functional.hardsigmoid(Tensor([2970966, 768, 1, 1],"float32"), )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([2970966, 768, 1, 1],"float32"), )
2025-03-13 11:34:29.311114 test begin: paddle.nn.functional.hardsigmoid(Tensor([3, 1431655766],"float16"), name=None, )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([3, 1431655766],"float16"), name=None, )
2025-03-13 11:51:10.604711 test begin: paddle.nn.functional.hardsigmoid(Tensor([30, 1901418, 40],"float32"), )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([30, 1901418, 40],"float32"), )
2025-03-13 11:54:15.751265 test begin: paddle.nn.functional.hardsigmoid(Tensor([30, 200, 380284],"float32"), )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([30, 200, 380284],"float32"), )
2025-03-13 11:57:03.498194 test begin: paddle.nn.functional.hardsigmoid(Tensor([300, 7605672],"float32"), )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([300, 7605672],"float32"), )
2025-03-13 12:00:01.133520 test begin: paddle.nn.functional.hardsigmoid(Tensor([47535446, 48, 1, 1],"float32"), )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([47535446, 48, 1, 1],"float32"), )
2025-03-13 12:03:28.620659 test begin: paddle.nn.functional.hardsigmoid(Tensor([557057, 4096],"float32"), )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([557057, 4096],"float32"), )
2025-03-13 12:06:53.623970 test begin: paddle.nn.functional.hardsigmoid(Tensor([5941931, 384, 1, 1],"float32"), )

[Pass] paddle.nn.functional.hardsigmoid(Tensor([5941931, 384, 1, 1],"float32"), )
2025-03-13 12:10:20.350313 test begin: paddle.nn.functional.hardswish(Tensor([1, 1, 10, 228170138],"float32"), )

[Pass] paddle.nn.functional.hardswish(Tensor([1, 1, 10, 228170138],"float32"), )
2025-03-13 12:13:50.506955 test begin: paddle.nn.functional.hardswish(Tensor([1, 1, 11, 207427399],"float32"), )

[Pass] paddle.nn.functional.hardswish(Tensor([1, 1, 11, 207427399],"float32"), )
2025-03-13 12:16:59.758445 test begin: paddle.nn.functional.hardswish(Tensor([1, 1, 13, 175515491],"float32"), )

[Pass] paddle.nn.functional.hardswish(Tensor([1, 1, 13, 175515491],"float32"), )
2025-03-13 12:20:16.820427 test begin: paddle.nn.functional.hardswish(Tensor([1, 1, 14, 162978670],"float32"), )

[Pass] paddle.nn.functional.hardswish(Tensor([1, 1, 14, 162978670],"float32"), )
2025-03-13 12:23:25.260927 test begin: paddle.nn.functional.hardswish(Tensor([1, 1, 15, 152113426],"float32"), )

[Pass] paddle.nn.functional.hardswish(Tensor([1, 1, 15, 152113426],"float32"), )
2025-03-13 12:26:42.708090 test begin: paddle.nn.functional.hardswish(Tensor([1, 1, 152113426, 15],"float32"), )

[Pass] paddle.nn.functional.hardswish(Tensor([1, 1, 152113426, 15],"float32"), )
2025-03-13 12:29:58.006686 test begin: paddle.nn.functional.hardswish(Tensor([1, 1, 162978670, 14],"float32"), )

[Pass] paddle.nn.functional.hardswish(Tensor([1, 1, 162978670, 14],"float32"), )
2025-03-13 12:32:59.025712 test begin: paddle.nn.functional.hardswish(Tensor([1, 1, 175515491, 13],"float32"), )

[Pass] paddle.nn.functional.hardswish(Tensor([1, 1, 175515491, 13],"float32"), )
2025-03-13 12:36:02.765276 test begin: paddle.nn.functional.hardswish(Tensor([1, 1, 207427399, 11],"float32"), )

[Pass] paddle.nn.functional.hardswish(Tensor([1, 1, 207427399, 11],"float32"), )
2025-03-13 12:39:10.099869 test begin: paddle.nn.functional.hardswish(Tensor([1, 1, 228170138, 10],"float32"), )

[Pass] paddle.nn.functional.hardswish(Tensor([1, 1, 228170138, 10],"float32"), )
2025-03-13 12:42:48.461320 test begin: paddle.nn.functional.hardswish(Tensor([1, 10140896, 15, 15],"float32"), )

[Pass] paddle.nn.functional.hardswish(Tensor([1, 10140896, 15, 15],"float32"), )
2025-03-13 12:47:10.289189 test begin: paddle.nn.functional.hardswish(Tensor([1, 1024, 101283, 22],"float32"), None, )

[Pass] paddle.nn.functional.hardswish(Tensor([1, 1024, 101283, 22],"float32"), None, )
2025-03-13 12:50:19.246037 test begin: paddle.nn.functional.hardswish(Tensor([1, 1024, 106106, 21],"float32"), None, )

[Pass] paddle.nn.functional.hardswish(Tensor([1, 1024, 106106, 21],"float32"), None, )
2025-03-13 12:53:50.613457 test begin: paddle.nn.functional.hardswish(Tensor([1, 1024, 111412, 20],"float32"), None, )

[Pass] paddle.nn.functional.hardswish(Tensor([1, 1024, 111412, 20],"float32"), None, )
2025-03-13 12:57:11.888311 test begin: paddle.nn.functional.hardswish(Tensor([1, 1024, 117275, 19],"float32"), None, )

[Pass] paddle.nn.functional.hardswish(Tensor([1, 1024, 117275, 19],"float32"), None, )
2025-03-13 13:01:23.097164 test begin: paddle.nn.functional.hardswish(Tensor([1, 1024, 123791, 18],"float32"), None, )

[Pass] paddle.nn.functional.hardswish(Tensor([1, 1024, 123791, 18],"float32"), None, )
2025-03-13 13:05:12.900632 test begin: paddle.nn.functional.hardswish(Tensor([1, 1024, 18, 123791],"float32"), None, )

[Pass] paddle.nn.functional.hardswish(Tensor([1, 1024, 18, 123791],"float32"), None, )
2025-03-13 13:08:55.063737 test begin: paddle.nn.functional.hardswish(Tensor([1, 1024, 19, 117275],"float32"), None, )

[Pass] paddle.nn.functional.hardswish(Tensor([1, 1024, 19, 117275],"float32"), None, )
2025-03-13 13:12:22.162337 test begin: paddle.nn.functional.hardswish(Tensor([1, 1024, 20, 111412],"float32"), None, )

[Pass] paddle.nn.functional.hardswish(Tensor([1, 1024, 20, 111412],"float32"), None, )
2025-03-13 13:15:53.427807 test begin: paddle.nn.functional.hardswish(Tensor([1, 1024, 21, 106106],"float32"), None, )

[Pass] paddle.nn.functional.hardswish(Tensor([1, 1024, 21, 106106],"float32"), None, )
2025-03-13 13:19:26.805987 test begin: paddle.nn.functional.hardswish(Tensor([1, 1024, 22, 101283],"float32"), None, )

[Pass] paddle.nn.functional.hardswish(Tensor([1, 1024, 22, 101283],"float32"), None, )
2025-03-13 13:23:16.520157 test begin: paddle.nn.functional.hardswish(Tensor([1, 11641334, 14, 14],"float32"), )

[Pass] paddle.nn.functional.hardswish(Tensor([1, 11641334, 14, 14],"float32"), )
2025-03-13 13:27:05.377635 test begin: paddle.nn.functional.hardswish(Tensor([1, 13501192, 13, 13],"float32"), )

[Pass] paddle.nn.functional.hardswish(Tensor([1, 13501192, 13, 13],"float32"), )
2025-03-13 13:30:23.599501 test begin: paddle.nn.functional.hardswish(Tensor([1, 18857037, 11, 11],"float32"), )

[Pass] paddle.nn.functional.hardswish(Tensor([1, 18857037, 11, 11],"float32"), )
2025-03-13 13:33:49.974407 test begin: paddle.nn.functional.hardswish(Tensor([1, 2281701379],"float32"), None, )

[Pass] paddle.nn.functional.hardswish(Tensor([1, 2281701379],"float32"), None, )
2025-03-13 13:37:11.498167 test begin: paddle.nn.functional.hardswish(Tensor([1, 22817014, 10, 10],"float32"), )

[Pass] paddle.nn.functional.hardswish(Tensor([1, 22817014, 10, 10],"float32"), )
2025-03-13 13:40:30.390763 test begin: paddle.nn.functional.hardswish(Tensor([1, 4714260, 22, 22],"float32"), None, )

[Pass] paddle.nn.functional.hardswish(Tensor([1, 4714260, 22, 22],"float32"), None, )
2025-03-13 13:43:54.264161 test begin: paddle.nn.functional.hardswish(Tensor([1, 5173927, 21, 21],"float32"), None, )

[Pass] paddle.nn.functional.hardswish(Tensor([1, 5173927, 21, 21],"float32"), None, )
2025-03-13 13:47:02.684787 test begin: paddle.nn.functional.hardswish(Tensor([1, 5704254, 20, 20],"float32"), None, )

[Pass] paddle.nn.functional.hardswish(Tensor([1, 5704254, 20, 20],"float32"), None, )
2025-03-13 13:50:10.927754 test begin: paddle.nn.functional.hardswish(Tensor([1, 6320503, 19, 19],"float32"), None, )

[Pass] paddle.nn.functional.hardswish(Tensor([1, 6320503, 19, 19],"float32"), None, )
2025-03-13 13:53:29.152814 test begin: paddle.nn.functional.hardswish(Tensor([1, 7042289, 18, 18],"float32"), None, )

[Pass] paddle.nn.functional.hardswish(Tensor([1, 7042289, 18, 18],"float32"), None, )
2025-03-13 13:56:36.298811 test begin: paddle.nn.functional.hardswish(Tensor([10140896, 1, 15, 15],"float32"), )

[Pass] paddle.nn.functional.hardswish(Tensor([10140896, 1, 15, 15],"float32"), )
2025-03-13 13:59:30.685546 test begin: paddle.nn.functional.hardswish(Tensor([1073741825, 4],"float16"), None, )

[Pass] paddle.nn.functional.hardswish(Tensor([1073741825, 4],"float16"), None, )
2025-03-13 14:16:13.232013 test begin: paddle.nn.functional.hardswish(Tensor([11641334, 1, 14, 14],"float32"), )

[Pass] paddle.nn.functional.hardswish(Tensor([11641334, 1, 14, 14],"float32"), )
2025-03-13 14:19:43.786076 test begin: paddle.nn.functional.hardswish(Tensor([13501192, 1, 13, 13],"float32"), )

[Pass] paddle.nn.functional.hardswish(Tensor([13501192, 1, 13, 13],"float32"), )
2025-03-13 14:22:43.492146 test begin: paddle.nn.functional.hardswish(Tensor([1782580, 1280],"float32"), None, )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 86, in func_timeout
    thread.join(timeout)
  File "/usr/lib/python3.9/threading.py", line 1064, in join
    self._wait_for_tstate_lock(timeout=max(timeout, 0))
  File "/usr/lib/python3.9/threading.py", line 1080, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt
2025-03-14 09:07:43.714660 test begin: paddle.nn.functional.hardswish(Tensor([18857037, 1, 11, 11],"float32"), )

W0314 09:09:11.896095 129675 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0314 09:09:11.897279 129675 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.nn.functional.hardswish(Tensor([18857037, 1, 11, 11],"float32"), )
2025-03-14 09:11:50.917891 test begin: paddle.nn.functional.hardswish(Tensor([2, 1140850690],"float32"), None, )

[Pass] paddle.nn.functional.hardswish(Tensor([2, 1140850690],"float32"), None, )
2025-03-14 09:14:37.978645 test begin: paddle.nn.functional.hardswish(Tensor([2, 300, 3802836],"float32"), )

[Pass] paddle.nn.functional.hardswish(Tensor([2, 300, 3802836],"float32"), )
2025-03-14 09:17:33.671615 test begin: paddle.nn.functional.hardswish(Tensor([2, 557057, 2048],"float32"), )

[Pass] paddle.nn.functional.hardswish(Tensor([2, 557057, 2048],"float32"), )
2025-03-14 09:20:56.520892 test begin: paddle.nn.functional.hardswish(Tensor([2228225, 1024],"float32"), None, )

[Pass] paddle.nn.functional.hardswish(Tensor([2228225, 1024],"float32"), None, )
2025-03-14 09:24:20.443385 test begin: paddle.nn.functional.hardswish(Tensor([22817014, 1, 10, 10],"float32"), )

[Pass] paddle.nn.functional.hardswish(Tensor([22817014, 1, 10, 10],"float32"), )
2025-03-14 09:28:00.061354 test begin: paddle.nn.functional.hardswish(Tensor([253522376, 3, 3],"float32"), None, )

[Pass] paddle.nn.functional.hardswish(Tensor([253522376, 3, 3],"float32"), None, )
2025-03-14 09:31:23.730557 test begin: paddle.nn.functional.hardswish(Tensor([285213, 200, 40],"float32"), )

[Pass] paddle.nn.functional.hardswish(Tensor([285213, 200, 40],"float32"), )
2025-03-14 09:34:36.788829 test begin: paddle.nn.functional.hardswish(Tensor([3, 1431655766],"float16"), None, )

[Pass] paddle.nn.functional.hardswish(Tensor([3, 1431655766],"float16"), None, )
2025-03-14 09:53:07.429653 test begin: paddle.nn.functional.hardswish(Tensor([3, 253522376, 3],"float32"), None, )

[Pass] paddle.nn.functional.hardswish(Tensor([3, 253522376, 3],"float32"), None, )
2025-03-14 09:57:31.486276 test begin: paddle.nn.functional.hardswish(Tensor([3, 3, 253522376],"float32"), None, )

[Pass] paddle.nn.functional.hardswish(Tensor([3, 3, 253522376],"float32"), None, )
2025-03-14 10:01:42.702287 test begin: paddle.nn.functional.hardswish(Tensor([3, 3, 477218589],"float16"), None, )

[Pass] paddle.nn.functional.hardswish(Tensor([3, 3, 477218589],"float16"), None, )
2025-03-14 10:18:57.140323 test begin: paddle.nn.functional.hardswish(Tensor([3, 477218589, 3],"float16"), None, )

[Pass] paddle.nn.functional.hardswish(Tensor([3, 477218589, 3],"float16"), None, )
2025-03-14 10:36:06.690227 test begin: paddle.nn.functional.hardswish(Tensor([30, 1901418, 40],"float32"), )

[Pass] paddle.nn.functional.hardswish(Tensor([30, 1901418, 40],"float32"), )
2025-03-14 10:39:02.617774 test begin: paddle.nn.functional.hardswish(Tensor([30, 200, 380284],"float32"), )

[Pass] paddle.nn.functional.hardswish(Tensor([30, 200, 380284],"float32"), )
2025-03-14 10:42:11.488678 test begin: paddle.nn.functional.hardswish(Tensor([3714, 300, 2048],"float32"), )

[Pass] paddle.nn.functional.hardswish(Tensor([3714, 300, 2048],"float32"), )
2025-03-14 10:45:37.502190 test begin: paddle.nn.functional.hardswish(Tensor([4604, 1024, 22, 22],"float32"), None, )

[Pass] paddle.nn.functional.hardswish(Tensor([4604, 1024, 22, 22],"float32"), None, )
2025-03-14 10:49:30.505766 test begin: paddle.nn.functional.hardswish(Tensor([477218589, 3, 3],"float16"), None, )

[Pass] paddle.nn.functional.hardswish(Tensor([477218589, 3, 3],"float16"), None, )
2025-03-14 11:06:19.070833 test begin: paddle.nn.functional.hardswish(Tensor([5053, 1024, 21, 21],"float32"), None, )

[Pass] paddle.nn.functional.hardswish(Tensor([5053, 1024, 21, 21],"float32"), None, )
2025-03-14 11:09:26.643261 test begin: paddle.nn.functional.hardswish(Tensor([5571, 1024, 20, 20],"float32"), None, )

[Pass] paddle.nn.functional.hardswish(Tensor([5571, 1024, 20, 20],"float32"), None, )
2025-03-14 11:12:13.211249 test begin: paddle.nn.functional.hardswish(Tensor([6173, 1024, 19, 19],"float32"), None, )

[Pass] paddle.nn.functional.hardswish(Tensor([6173, 1024, 19, 19],"float32"), None, )
2025-03-14 11:15:10.264568 test begin: paddle.nn.functional.hardswish(Tensor([6878, 1024, 18, 18],"float32"), None, )

[Pass] paddle.nn.functional.hardswish(Tensor([6878, 1024, 18, 18],"float32"), None, )
2025-03-14 11:18:09.238292 test begin: paddle.nn.functional.hardswish(x=Tensor([1073741825, 4],"float16"), )

[Pass] paddle.nn.functional.hardswish(x=Tensor([1073741825, 4],"float16"), )
2025-03-14 11:34:44.721971 test begin: paddle.nn.functional.hardswish(x=Tensor([2, 1140850690],"float32"), )

[Pass] paddle.nn.functional.hardswish(x=Tensor([2, 1140850690],"float32"), )
2025-03-14 11:37:52.420102 test begin: paddle.nn.functional.hardswish(x=Tensor([2, 2147483649],"float16"), )

[Pass] paddle.nn.functional.hardswish(x=Tensor([2, 2147483649],"float16"), )
2025-03-14 11:54:42.434288 test begin: paddle.nn.functional.hardswish(x=Tensor([4, 2, 536870913],"float16"), )

[Pass] paddle.nn.functional.hardswish(x=Tensor([4, 2, 536870913],"float16"), )
2025-03-14 12:11:42.025322 test begin: paddle.nn.functional.hardswish(x=Tensor([4, 268435457, 4],"float16"), )

[Pass] paddle.nn.functional.hardswish(x=Tensor([4, 268435457, 4],"float16"), )
2025-03-14 12:28:12.616255 test begin: paddle.nn.functional.hardswish(x=Tensor([536870913, 2, 4],"float16"), )

[Pass] paddle.nn.functional.hardswish(x=Tensor([536870913, 2, 4],"float16"), )
2025-03-14 12:44:39.608892 test begin: paddle.nn.functional.hardswish(x=Tensor([570425345, 4],"float32"), )

[Pass] paddle.nn.functional.hardswish(x=Tensor([570425345, 4],"float32"), )
2025-03-14 12:47:44.045371 test begin: paddle.nn.functional.hardtanh(Tensor([10, 20, 11408507],"float32"), -1.0, 1.0, )

[Pass] paddle.nn.functional.hardtanh(Tensor([10, 20, 11408507],"float32"), -1.0, 1.0, )
2025-03-14 12:50:41.060440 test begin: paddle.nn.functional.hardtanh(Tensor([10, 228170138, 1],"float32"), -1.0, 1.0, )

[Pass] paddle.nn.functional.hardtanh(Tensor([10, 228170138, 1],"float32"), -1.0, 1.0, )
2025-03-14 12:53:40.683779 test begin: paddle.nn.functional.hardtanh(Tensor([114085069, 20, 1],"float32"), -1.0, 1.0, )

[Pass] paddle.nn.functional.hardtanh(Tensor([114085069, 20, 1],"float32"), -1.0, 1.0, )
2025-03-14 12:56:25.620063 test begin: paddle.nn.functional.hardtanh(Tensor([2281701379],"float32"), -1.0, 1.0, None, )

[Pass] paddle.nn.functional.hardtanh(Tensor([2281701379],"float32"), -1.0, 1.0, None, )
2025-03-14 12:59:29.345011 test begin: paddle.nn.functional.hardtanh(Tensor([3, 3, 477218589],"float16"), -3.2, -3.2, None, )

[Pass] paddle.nn.functional.hardtanh(Tensor([3, 3, 477218589],"float16"), -3.2, -3.2, None, )
2025-03-14 13:16:00.997657 test begin: paddle.nn.functional.hardtanh(Tensor([3, 3, 477218589],"float16"), -3.4, 0, None, )

[Pass] paddle.nn.functional.hardtanh(Tensor([3, 3, 477218589],"float16"), -3.4, 0, None, )
2025-03-14 13:34:23.302980 test begin: paddle.nn.functional.hardtanh(Tensor([3, 3, 477218589],"float16"), 0, 0, None, )

[Pass] paddle.nn.functional.hardtanh(Tensor([3, 3, 477218589],"float16"), 0, 0, None, )
2025-03-14 13:50:47.482276 test begin: paddle.nn.functional.hardtanh(Tensor([3, 3, 477218589],"float16"), 0, 1.3, None, )

[Pass] paddle.nn.functional.hardtanh(Tensor([3, 3, 477218589],"float16"), 0, 1.3, None, )
2025-03-14 14:08:42.053642 test begin: paddle.nn.functional.hardtanh(Tensor([3, 3, 477218589],"float16"), 1, 1, None, )

[Pass] paddle.nn.functional.hardtanh(Tensor([3, 3, 477218589],"float16"), 1, 1, None, )
2025-03-14 14:24:39.950447 test begin: paddle.nn.functional.hardtanh(Tensor([3, 3, 477218589],"float16"), 2.3, 3.5, None, )

[Pass] paddle.nn.functional.hardtanh(Tensor([3, 3, 477218589],"float16"), 2.3, 3.5, None, )
2025-03-14 14:41:00.562389 test begin: paddle.nn.functional.hardtanh(Tensor([3, 477218589, 3],"float16"), -3.2, -3.2, None, )

[Pass] paddle.nn.functional.hardtanh(Tensor([3, 477218589, 3],"float16"), -3.2, -3.2, None, )
2025-03-14 14:57:15.358290 test begin: paddle.nn.functional.hardtanh(Tensor([3, 477218589, 3],"float16"), -3.4, 0, None, )

[Pass] paddle.nn.functional.hardtanh(Tensor([3, 477218589, 3],"float16"), -3.4, 0, None, )
2025-03-14 15:15:29.456143 test begin: paddle.nn.functional.hardtanh(Tensor([3, 477218589, 3],"float16"), 0, 0, None, )

[Pass] paddle.nn.functional.hardtanh(Tensor([3, 477218589, 3],"float16"), 0, 0, None, )
2025-03-14 15:32:05.312964 test begin: paddle.nn.functional.hardtanh(Tensor([3, 477218589, 3],"float16"), 0, 1.3, None, )

[Pass] paddle.nn.functional.hardtanh(Tensor([3, 477218589, 3],"float16"), 0, 1.3, None, )
2025-03-14 15:50:50.908168 test begin: paddle.nn.functional.hardtanh(Tensor([3, 477218589, 3],"float16"), 1, 1, None, )

[Pass] paddle.nn.functional.hardtanh(Tensor([3, 477218589, 3],"float16"), 1, 1, None, )
2025-03-14 16:07:38.740483 test begin: paddle.nn.functional.hardtanh(Tensor([3, 477218589, 3],"float16"), 2.3, 3.5, None, )

[Pass] paddle.nn.functional.hardtanh(Tensor([3, 477218589, 3],"float16"), 2.3, 3.5, None, )
2025-03-14 16:24:40.354552 test begin: paddle.nn.functional.hardtanh(Tensor([4294967297],"float16"), -1.0, 1.0, None, )

[Pass] paddle.nn.functional.hardtanh(Tensor([4294967297],"float16"), -1.0, 1.0, None, )
2025-03-14 16:41:46.148703 test begin: paddle.nn.functional.hardtanh(Tensor([477218589, 3, 3],"float16"), -3.2, -3.2, None, )

[Pass] paddle.nn.functional.hardtanh(Tensor([477218589, 3, 3],"float16"), -3.2, -3.2, None, )
2025-03-14 16:59:14.113351 test begin: paddle.nn.functional.hardtanh(Tensor([477218589, 3, 3],"float16"), -3.4, 0, None, )

[Pass] paddle.nn.functional.hardtanh(Tensor([477218589, 3, 3],"float16"), -3.4, 0, None, )
2025-03-14 17:18:23.372684 test begin: paddle.nn.functional.hardtanh(Tensor([477218589, 3, 3],"float16"), 0, 0, None, )

[Pass] paddle.nn.functional.hardtanh(Tensor([477218589, 3, 3],"float16"), 0, 0, None, )
2025-03-14 17:35:29.177168 test begin: paddle.nn.functional.hardtanh(Tensor([477218589, 3, 3],"float16"), 0, 1.3, None, )

[Pass] paddle.nn.functional.hardtanh(Tensor([477218589, 3, 3],"float16"), 0, 1.3, None, )
2025-03-14 17:54:09.122865 test begin: paddle.nn.functional.hardtanh(Tensor([477218589, 3, 3],"float16"), 1, 1, None, )

[Pass] paddle.nn.functional.hardtanh(Tensor([477218589, 3, 3],"float16"), 1, 1, None, )
2025-03-14 18:10:55.292720 test begin: paddle.nn.functional.hardtanh(Tensor([477218589, 3, 3],"float16"), 2.3, 3.5, None, )

[Pass] paddle.nn.functional.hardtanh(Tensor([477218589, 3, 3],"float16"), 2.3, 3.5, None, )
2025-03-14 18:28:06.487343 test begin: paddle.nn.functional.hardtanh(x=Tensor([2281701379],"float32"), )

[Pass] paddle.nn.functional.hardtanh(x=Tensor([2281701379],"float32"), )
2025-03-14 18:31:57.031435 test begin: paddle.nn.functional.hardtanh(x=Tensor([3, 3, 477218589],"float16"), max=-3.2, min=-3.2, )

[Pass] paddle.nn.functional.hardtanh(x=Tensor([3, 3, 477218589],"float16"), max=-3.2, min=-3.2, )
2025-03-14 18:48:39.217801 test begin: paddle.nn.functional.hardtanh(x=Tensor([3, 3, 477218589],"float16"), max=0, min=-3.4, )

[Pass] paddle.nn.functional.hardtanh(x=Tensor([3, 3, 477218589],"float16"), max=0, min=-3.4, )
2025-03-14 19:06:56.831887 test begin: paddle.nn.functional.hardtanh(x=Tensor([3, 3, 477218589],"float16"), max=0, min=0, )

[Pass] paddle.nn.functional.hardtanh(x=Tensor([3, 3, 477218589],"float16"), max=0, min=0, )
2025-03-14 19:23:57.432108 test begin: paddle.nn.functional.hardtanh(x=Tensor([3, 3, 477218589],"float16"), max=1, min=1, )

[Pass] paddle.nn.functional.hardtanh(x=Tensor([3, 3, 477218589],"float16"), max=1, min=1, )
2025-03-14 19:40:45.485980 test begin: paddle.nn.functional.hardtanh(x=Tensor([3, 3, 477218589],"float16"), max=1.3, min=0, )

[Pass] paddle.nn.functional.hardtanh(x=Tensor([3, 3, 477218589],"float16"), max=1.3, min=0, )
2025-03-14 19:59:45.534630 test begin: paddle.nn.functional.hardtanh(x=Tensor([3, 3, 477218589],"float16"), max=3.5, min=2.3, )

[Pass] paddle.nn.functional.hardtanh(x=Tensor([3, 3, 477218589],"float16"), max=3.5, min=2.3, )
2025-03-14 20:17:15.801198 test begin: paddle.nn.functional.hardtanh(x=Tensor([3, 477218589, 3],"float16"), max=-3.2, min=-3.2, )

[Pass] paddle.nn.functional.hardtanh(x=Tensor([3, 477218589, 3],"float16"), max=-3.2, min=-3.2, )
2025-03-14 20:33:47.206525 test begin: paddle.nn.functional.hardtanh(x=Tensor([3, 477218589, 3],"float16"), max=0, min=-3.4, )

[Pass] paddle.nn.functional.hardtanh(x=Tensor([3, 477218589, 3],"float16"), max=0, min=-3.4, )
2025-03-14 20:52:19.359408 test begin: paddle.nn.functional.hardtanh(x=Tensor([3, 477218589, 3],"float16"), max=0, min=0, )

[Pass] paddle.nn.functional.hardtanh(x=Tensor([3, 477218589, 3],"float16"), max=0, min=0, )
2025-03-14 21:09:01.938547 test begin: paddle.nn.functional.hardtanh(x=Tensor([3, 477218589, 3],"float16"), max=1, min=1, )

[Pass] paddle.nn.functional.hardtanh(x=Tensor([3, 477218589, 3],"float16"), max=1, min=1, )
2025-03-14 21:25:22.285217 test begin: paddle.nn.functional.hardtanh(x=Tensor([3, 477218589, 3],"float16"), max=1.3, min=0, )

[Pass] paddle.nn.functional.hardtanh(x=Tensor([3, 477218589, 3],"float16"), max=1.3, min=0, )
2025-03-14 21:43:30.892481 test begin: paddle.nn.functional.hardtanh(x=Tensor([3, 477218589, 3],"float16"), max=3.5, min=2.3, )

[Pass] paddle.nn.functional.hardtanh(x=Tensor([3, 477218589, 3],"float16"), max=3.5, min=2.3, )
2025-03-14 22:00:01.760925 test begin: paddle.nn.functional.hardtanh(x=Tensor([4294967297],"float16"), )

[Pass] paddle.nn.functional.hardtanh(x=Tensor([4294967297],"float16"), )
2025-03-14 22:16:16.278806 test begin: paddle.nn.functional.hardtanh(x=Tensor([477218589, 3, 3],"float16"), max=-3.2, min=-3.2, )

[Pass] paddle.nn.functional.hardtanh(x=Tensor([477218589, 3, 3],"float16"), max=-3.2, min=-3.2, )
2025-03-14 22:32:29.783167 test begin: paddle.nn.functional.hardtanh(x=Tensor([477218589, 3, 3],"float16"), max=0, min=-3.4, )

[Pass] paddle.nn.functional.hardtanh(x=Tensor([477218589, 3, 3],"float16"), max=0, min=-3.4, )
2025-03-14 22:50:51.340484 test begin: paddle.nn.functional.hardtanh(x=Tensor([477218589, 3, 3],"float16"), max=0, min=0, )

[Pass] paddle.nn.functional.hardtanh(x=Tensor([477218589, 3, 3],"float16"), max=0, min=0, )
2025-03-14 23:07:13.291836 test begin: paddle.nn.functional.hardtanh(x=Tensor([477218589, 3, 3],"float16"), max=1, min=1, )

[Pass] paddle.nn.functional.hardtanh(x=Tensor([477218589, 3, 3],"float16"), max=1, min=1, )
2025-03-14 23:24:01.624552 test begin: paddle.nn.functional.hardtanh(x=Tensor([477218589, 3, 3],"float16"), max=1.3, min=0, )

[Pass] paddle.nn.functional.hardtanh(x=Tensor([477218589, 3, 3],"float16"), max=1.3, min=0, )
2025-03-14 23:42:46.487077 test begin: paddle.nn.functional.hardtanh(x=Tensor([477218589, 3, 3],"float16"), max=3.5, min=2.3, )

[Pass] paddle.nn.functional.hardtanh(x=Tensor([477218589, 3, 3],"float16"), max=3.5, min=2.3, )
2025-03-14 23:59:08.766476 test begin: paddle.nn.functional.layer_norm(Tensor([10, 20],"float16"), list[20,], Tensor([20],"float16"), Tensor([4294967297],"float16"), )

[torch error] paddle.nn.functional.layer_norm(Tensor([10, 20],"float16"), list[20,], Tensor([20],"float16"), Tensor([4294967297],"float16"), ) 
 Expected bias to be of same shape as normalized_shape, but got bias of shape [4294967297] and normalized_shape = [20]
2025-03-14 23:59:12.824916 test begin: paddle.nn.functional.layer_norm(Tensor([10, 20],"float16"), list[20,], Tensor([4294967297],"float16"), Tensor([20],"float16"), )

[torch error] paddle.nn.functional.layer_norm(Tensor([10, 20],"float16"), list[20,], Tensor([4294967297],"float16"), Tensor([20],"float16"), ) 
 Expected weight to be of same shape as normalized_shape, but got weight of shape [4294967297] and normalized_shape = [20]
2025-03-14 23:59:14.366380 test begin: paddle.nn.functional.layer_norm(Tensor([10, 429496730],"float16"), list[20,], Tensor([20],"float16"), Tensor([20],"float16"), )

[torch error] paddle.nn.functional.layer_norm(Tensor([10, 429496730],"float16"), list[20,], Tensor([20],"float16"), Tensor([20],"float16"), ) 
 Given normalized_shape=[20], expected input with shape [*, 20], but got input of size[10, 429496730]
2025-03-14 23:59:16.144190 test begin: paddle.nn.functional.layer_norm(Tensor([126762, 10, 60, 30],"float32"), list[10,60,30,], weight=None, bias=None, epsilon=1e-05, )

[Pass] paddle.nn.functional.layer_norm(Tensor([126762, 10, 60, 30],"float32"), list[10,60,30,], weight=None, bias=None, epsilon=1e-05, )
2025-03-15 00:02:57.579105 test begin: paddle.nn.functional.layer_norm(Tensor([128, 278529, 64],"float32"), list[64,], None, None, )

[Pass] paddle.nn.functional.layer_norm(Tensor([128, 278529, 64],"float32"), list[64,], None, None, )
2025-03-15 00:06:49.485861 test begin: paddle.nn.functional.layer_norm(Tensor([128, 278529, 64],"float32"), list[64,], Tensor([64],"float32"), Tensor([64],"float32"), )

[Pass] paddle.nn.functional.layer_norm(Tensor([128, 278529, 64],"float32"), list[64,], Tensor([64],"float32"), Tensor([64],"float32"), )
2025-03-15 00:10:18.481334 test begin: paddle.nn.functional.layer_norm(Tensor([128, 64, 278529],"float32"), list[64,], None, None, )

[torch error] paddle.nn.functional.layer_norm(Tensor([128, 64, 278529],"float32"), list[64,], None, None, ) 
 Given normalized_shape=[64], expected input with shape [*, 64], but got input of size[128, 64, 278529]
2025-03-15 00:10:22.273092 test begin: paddle.nn.functional.layer_norm(Tensor([128, 64, 278529],"float32"), list[64,], Tensor([64],"float32"), Tensor([64],"float32"), )

[torch error] paddle.nn.functional.layer_norm(Tensor([128, 64, 278529],"float32"), list[64,], Tensor([64],"float32"), Tensor([64],"float32"), ) 
 Given normalized_shape=[64], expected input with shape [*, 64], but got input of size[128, 64, 278529]
2025-03-15 00:10:23.287801 test begin: paddle.nn.functional.layer_norm(Tensor([128, 64, 64],"float32"), list[64,], Tensor([2281701379],"float32"), Tensor([64],"float32"), )

[torch error] paddle.nn.functional.layer_norm(Tensor([128, 64, 64],"float32"), list[64,], Tensor([2281701379],"float32"), Tensor([64],"float32"), ) 
 Expected weight to be of same shape as normalized_shape, but got weight of shape [2281701379] and normalized_shape = [64]
2025-03-15 00:10:24.273142 test begin: paddle.nn.functional.layer_norm(Tensor([128, 64, 64],"float32"), list[64,], Tensor([64],"float32"), Tensor([2281701379],"float32"), )

[torch error] paddle.nn.functional.layer_norm(Tensor([128, 64, 64],"float32"), list[64,], Tensor([64],"float32"), Tensor([2281701379],"float32"), ) 
 Expected bias to be of same shape as normalized_shape, but got bias of shape [2281701379] and normalized_shape = [64]
2025-03-15 00:10:25.557541 test begin: paddle.nn.functional.layer_norm(Tensor([14260634, 10, 4, 4],"float32"), list[10,4,4,], )

[Pass] paddle.nn.functional.layer_norm(Tensor([14260634, 10, 4, 4],"float32"), list[10,4,4,], )
2025-03-15 00:13:25.324200 test begin: paddle.nn.functional.layer_norm(Tensor([14260634, 10, 4, 4],"float32"), tuple(10,4,4,), )

[Pass] paddle.nn.functional.layer_norm(Tensor([14260634, 10, 4, 4],"float32"), tuple(10,4,4,), )
2025-03-15 00:16:43.215208 test begin: paddle.nn.functional.layer_norm(Tensor([17, 129],"float32"), list[129,], Tensor([2281701379],"float32"), None, )

[torch error] paddle.nn.functional.layer_norm(Tensor([17, 129],"float32"), list[129,], Tensor([2281701379],"float32"), None, ) 
 Expected weight to be of same shape as normalized_shape, but got weight of shape [2281701379] and normalized_shape = [129]
2025-03-15 00:16:47.031198 test begin: paddle.nn.functional.layer_norm(Tensor([17, 134217729],"float32"), list[129,], Tensor([129],"float32"), None, )

[torch error] paddle.nn.functional.layer_norm(Tensor([17, 134217729],"float32"), list[129,], Tensor([129],"float32"), None, ) 
 Given normalized_shape=[129], expected input with shape [*, 129], but got input of size[17, 134217729]
2025-03-15 00:16:48.499459 test begin: paddle.nn.functional.layer_norm(Tensor([17687608, 129],"float32"), list[129,], Tensor([129],"float32"), None, )

[Pass] paddle.nn.functional.layer_norm(Tensor([17687608, 129],"float32"), list[129,], Tensor([129],"float32"), None, )
2025-03-15 00:20:20.666004 test begin: paddle.nn.functional.layer_norm(Tensor([2, 100],"float32"), list[100,], weight=Tensor([100],"float32"), bias=Tensor([2281701379],"float32"), epsilon=1e-05, )

[torch error] paddle.nn.functional.layer_norm(Tensor([2, 100],"float32"), list[100,], weight=Tensor([100],"float32"), bias=Tensor([2281701379],"float32"), epsilon=1e-05, ) 
 Expected bias to be of same shape as normalized_shape, but got bias of shape [2281701379] and normalized_shape = [100]
2025-03-15 00:20:24.592018 test begin: paddle.nn.functional.layer_norm(Tensor([2, 100],"float32"), list[100,], weight=Tensor([2281701379],"float32"), bias=Tensor([100],"float32"), epsilon=1e-05, )

[torch error] paddle.nn.functional.layer_norm(Tensor([2, 100],"float32"), list[100,], weight=Tensor([2281701379],"float32"), bias=Tensor([100],"float32"), epsilon=1e-05, ) 
 Expected weight to be of same shape as normalized_shape, but got weight of shape [2281701379] and normalized_shape = [100]
2025-03-15 00:20:25.793599 test begin: paddle.nn.functional.layer_norm(Tensor([2, 1140850690],"float32"), list[100,], weight=Tensor([100],"float32"), bias=Tensor([100],"float32"), epsilon=1e-05, )

[torch error] paddle.nn.functional.layer_norm(Tensor([2, 1140850690],"float32"), list[100,], weight=Tensor([100],"float32"), bias=Tensor([100],"float32"), epsilon=1e-05, ) 
 Given normalized_shape=[100], expected input with shape [*, 100], but got input of size[2, 1140850690]
2025-03-15 00:20:26.804898 test begin: paddle.nn.functional.layer_norm(Tensor([2, 1140850690],"float32"), list[768,], None, None, )

[torch error] paddle.nn.functional.layer_norm(Tensor([2, 1140850690],"float32"), list[768,], None, None, ) 
 Given normalized_shape=[768], expected input with shape [*, 768], but got input of size[2, 1140850690]
2025-03-15 00:20:28.145370 test begin: paddle.nn.functional.layer_norm(Tensor([2, 119304648, 6, 3],"float16"), list[6,6,3,], weight=None, bias=None, epsilon=1e-05, )

[torch error] paddle.nn.functional.layer_norm(Tensor([2, 119304648, 6, 3],"float16"), list[6,6,3,], weight=None, bias=None, epsilon=1e-05, ) 
 Given normalized_shape=[6, 6, 3], expected input with shape [*, 6, 6, 3], but got input of size[2, 119304648, 6, 3]
2025-03-15 00:20:32.353374 test begin: paddle.nn.functional.layer_norm(Tensor([2, 32, 35651585],"float32"), list[32,128,], )

[torch error] paddle.nn.functional.layer_norm(Tensor([2, 32, 35651585],"float32"), list[32,128,], ) 
 Given normalized_shape=[32, 128], expected input with shape [*, 32, 128], but got input of size[2, 32, 35651585]
2025-03-15 00:20:34.684692 test begin: paddle.nn.functional.layer_norm(Tensor([2, 6, 119304648, 3],"float16"), list[6,6,3,], weight=None, bias=None, epsilon=1e-05, )

[torch error] paddle.nn.functional.layer_norm(Tensor([2, 6, 119304648, 3],"float16"), list[6,6,3,], weight=None, bias=None, epsilon=1e-05, ) 
 Given normalized_shape=[6, 6, 3], expected input with shape [*, 6, 6, 3], but got input of size[2, 6, 119304648, 3]
2025-03-15 00:20:36.315197 test begin: paddle.nn.functional.layer_norm(Tensor([2, 6, 6, 31690297],"float32"), list[6,6,3,], weight=None, bias=None, epsilon=1e-05, )

[torch error] paddle.nn.functional.layer_norm(Tensor([2, 6, 6, 31690297],"float32"), list[6,6,3,], weight=None, bias=None, epsilon=1e-05, ) 
 Given normalized_shape=[6, 6, 3], expected input with shape [*, 6, 6, 3], but got input of size[2, 6, 6, 31690297]
2025-03-15 00:20:37.524131 test begin: paddle.nn.functional.layer_norm(Tensor([2, 6, 6, 59652324],"float16"), list[6,6,3,], weight=None, bias=None, epsilon=1e-05, )

[torch error] paddle.nn.functional.layer_norm(Tensor([2, 6, 6, 59652324],"float16"), list[6,6,3,], weight=None, bias=None, epsilon=1e-05, ) 
 Given normalized_shape=[6, 6, 3], expected input with shape [*, 6, 6, 3], but got input of size[2, 6, 6, 59652324]
2025-03-15 00:20:38.997068 test begin: paddle.nn.functional.layer_norm(Tensor([2, 6, 63380594, 3],"float32"), list[6,6,3,], weight=None, bias=None, epsilon=1e-05, )

[torch error] paddle.nn.functional.layer_norm(Tensor([2, 6, 63380594, 3],"float32"), list[6,6,3,], weight=None, bias=None, epsilon=1e-05, ) 
 Given normalized_shape=[6, 6, 3], expected input with shape [*, 6, 6, 3], but got input of size[2, 6, 63380594, 3]
2025-03-15 00:20:40.136213 test begin: paddle.nn.functional.layer_norm(Tensor([2, 63380594, 6, 3],"float32"), list[6,6,3,], weight=None, bias=None, epsilon=1e-05, )

[torch error] paddle.nn.functional.layer_norm(Tensor([2, 63380594, 6, 3],"float32"), list[6,6,3,], weight=None, bias=None, epsilon=1e-05, ) 
 Given normalized_shape=[6, 6, 3], expected input with shape [*, 6, 6, 3], but got input of size[2, 63380594, 6, 3]
2025-03-15 00:20:41.227418 test begin: paddle.nn.functional.layer_norm(Tensor([2, 8912897, 128],"float32"), list[32,128,], )

[torch error] paddle.nn.functional.layer_norm(Tensor([2, 8912897, 128],"float32"), list[32,128,], ) 
 Given normalized_shape=[32, 128], expected input with shape [*, 32, 128], but got input of size[2, 8912897, 128]
2025-03-15 00:20:43.216471 test begin: paddle.nn.functional.layer_norm(Tensor([20, 10, 162979, 70],"float32"), list[10,60,70,], weight=None, bias=None, epsilon=1e-05, )

[torch error] paddle.nn.functional.layer_norm(Tensor([20, 10, 162979, 70],"float32"), list[10,60,70,], weight=None, bias=None, epsilon=1e-05, ) 
 Given normalized_shape=[10, 60, 70], expected input with shape [*, 10, 60, 70], but got input of size[20, 10, 162979, 70]
2025-03-15 00:20:45.074333 test begin: paddle.nn.functional.layer_norm(Tensor([20, 10, 380284, 30],"float32"), list[10,60,30,], weight=None, bias=None, epsilon=1e-05, )

[torch error] paddle.nn.functional.layer_norm(Tensor([20, 10, 380284, 30],"float32"), list[10,60,30,], weight=None, bias=None, epsilon=1e-05, ) 
 Given normalized_shape=[10, 60, 30], expected input with shape [*, 10, 60, 30], but got input of size[20, 10, 380284, 30]
2025-03-15 00:20:46.717728 test begin: paddle.nn.functional.layer_norm(Tensor([20, 10, 60, 190142],"float32"), list[10,60,30,], weight=None, bias=None, epsilon=1e-05, )

[torch error] paddle.nn.functional.layer_norm(Tensor([20, 10, 60, 190142],"float32"), list[10,60,30,], weight=None, bias=None, epsilon=1e-05, ) 
 Given normalized_shape=[10, 60, 30], expected input with shape [*, 10, 60, 30], but got input of size[20, 10, 60, 190142]
2025-03-15 00:20:48.144149 test begin: paddle.nn.functional.layer_norm(Tensor([20, 10, 60, 190142],"float32"), list[10,60,70,], weight=None, bias=None, epsilon=1e-05, )

[torch error] paddle.nn.functional.layer_norm(Tensor([20, 10, 60, 190142],"float32"), list[10,60,70,], weight=None, bias=None, epsilon=1e-05, ) 
 Given normalized_shape=[10, 60, 70], expected input with shape [*, 10, 60, 70], but got input of size[20, 10, 60, 190142]
2025-03-15 00:20:50.756976 test begin: paddle.nn.functional.layer_norm(Tensor([20, 27164, 60, 70],"float32"), list[10,60,70,], weight=None, bias=None, epsilon=1e-05, )

[torch error] paddle.nn.functional.layer_norm(Tensor([20, 27164, 60, 70],"float32"), list[10,60,70,], weight=None, bias=None, epsilon=1e-05, ) 
 Given normalized_shape=[10, 60, 70], expected input with shape [*, 10, 60, 70], but got input of size[20, 27164, 60, 70]
2025-03-15 00:20:52.750190 test begin: paddle.nn.functional.layer_norm(Tensor([20, 63381, 60, 30],"float32"), list[10,60,30,], weight=None, bias=None, epsilon=1e-05, )

[torch error] paddle.nn.functional.layer_norm(Tensor([20, 63381, 60, 30],"float32"), list[10,60,30,], weight=None, bias=None, epsilon=1e-05, ) 
 Given normalized_shape=[10, 60, 30], expected input with shape [*, 10, 60, 30], but got input of size[20, 63381, 60, 30]
2025-03-15 00:20:54.399175 test begin: paddle.nn.functional.layer_norm(Tensor([21126865, 6, 6, 3],"float32"), list[6,6,3,], weight=None, bias=None, epsilon=1e-05, )

[Pass] paddle.nn.functional.layer_norm(Tensor([21126865, 6, 6, 3],"float32"), list[6,6,3,], weight=None, bias=None, epsilon=1e-05, )
2025-03-15 00:24:30.833167 test begin: paddle.nn.functional.layer_norm(Tensor([214748365, 20],"float16"), list[20,], Tensor([20],"float16"), Tensor([20],"float16"), )

[Pass] paddle.nn.functional.layer_norm(Tensor([214748365, 20],"float16"), list[20,], Tensor([20],"float16"), Tensor([20],"float16"), )
2025-03-15 00:41:47.738004 test begin: paddle.nn.functional.layer_norm(Tensor([22817014, 100],"float32"), list[100,], weight=Tensor([100],"float32"), bias=Tensor([100],"float32"), epsilon=1e-05, )

[Pass] paddle.nn.functional.layer_norm(Tensor([22817014, 100],"float32"), list[100,], weight=Tensor([100],"float32"), bias=Tensor([100],"float32"), epsilon=1e-05, )
2025-03-15 00:44:39.322055 test begin: paddle.nn.functional.layer_norm(Tensor([278529, 64, 128],"float32"), list[64,128,], None, None, )

[Pass] paddle.nn.functional.layer_norm(Tensor([278529, 64, 128],"float32"), list[64,128,], None, None, )
2025-03-15 00:47:34.422442 test begin: paddle.nn.functional.layer_norm(Tensor([2970966, 768],"float32"), list[768,], None, None, )

[Pass] paddle.nn.functional.layer_norm(Tensor([2970966, 768],"float32"), list[768,], None, None, )
2025-03-15 00:50:38.728463 test begin: paddle.nn.functional.layer_norm(Tensor([3, 4],"float32"), list[4,], Tensor([2281701379],"float32"), Tensor([4],"float32"), )

[torch error] paddle.nn.functional.layer_norm(Tensor([3, 4],"float32"), list[4,], Tensor([2281701379],"float32"), Tensor([4],"float32"), ) 
 Expected weight to be of same shape as normalized_shape, but got weight of shape [2281701379] and normalized_shape = [4]
2025-03-15 00:50:42.795530 test begin: paddle.nn.functional.layer_norm(Tensor([3, 4],"float32"), list[4,], Tensor([4],"float32"), Tensor([2281701379],"float32"), )

[torch error] paddle.nn.functional.layer_norm(Tensor([3, 4],"float32"), list[4,], Tensor([4],"float32"), Tensor([2281701379],"float32"), ) 
 Expected bias to be of same shape as normalized_shape, but got bias of shape [2281701379] and normalized_shape = [4]
2025-03-15 00:50:44.859671 test begin: paddle.nn.functional.layer_norm(Tensor([3, 760567127],"float32"), list[4,], None, None, )

[torch error] paddle.nn.functional.layer_norm(Tensor([3, 760567127],"float32"), list[4,], None, None, ) 
 Given normalized_shape=[4], expected input with shape [*, 4], but got input of size[3, 760567127]
2025-03-15 00:50:45.923666 test begin: paddle.nn.functional.layer_norm(Tensor([3, 760567127],"float32"), list[4,], Tensor([4],"float32"), Tensor([4],"float32"), )

[torch error] paddle.nn.functional.layer_norm(Tensor([3, 760567127],"float32"), list[4,], Tensor([4],"float32"), Tensor([4],"float32"), ) 
 Given normalized_shape=[4], expected input with shape [*, 4], but got input of size[3, 760567127]
2025-03-15 00:50:47.389004 test begin: paddle.nn.functional.layer_norm(Tensor([39768216, 6, 6, 3],"float16"), list[6,6,3,], weight=None, bias=None, epsilon=1e-05, )

[Pass] paddle.nn.functional.layer_norm(Tensor([39768216, 6, 6, 3],"float16"), list[6,6,3,], weight=None, bias=None, epsilon=1e-05, )
2025-03-15 01:07:46.986837 test begin: paddle.nn.functional.layer_norm(Tensor([4, 10, 14260634, 4],"float32"), list[10,4,4,], )

[torch error] paddle.nn.functional.layer_norm(Tensor([4, 10, 14260634, 4],"float32"), list[10,4,4,], ) 
 Given normalized_shape=[10, 4, 4], expected input with shape [*, 10, 4, 4], but got input of size[4, 10, 14260634, 4]
2025-03-15 01:07:51.196880 test begin: paddle.nn.functional.layer_norm(Tensor([4, 10, 14260634, 4],"float32"), tuple(10,4,4,), )

[torch error] paddle.nn.functional.layer_norm(Tensor([4, 10, 14260634, 4],"float32"), tuple(10,4,4,), ) 
 Given normalized_shape=[10, 4, 4], expected input with shape [*, 10, 4, 4], but got input of size[4, 10, 14260634, 4]
2025-03-15 01:07:52.152654 test begin: paddle.nn.functional.layer_norm(Tensor([4, 10, 4, 14260634],"float32"), list[10,4,4,], )

[torch error] paddle.nn.functional.layer_norm(Tensor([4, 10, 4, 14260634],"float32"), list[10,4,4,], ) 
 Given normalized_shape=[10, 4, 4], expected input with shape [*, 10, 4, 4], but got input of size[4, 10, 4, 14260634]
2025-03-15 01:07:53.116468 test begin: paddle.nn.functional.layer_norm(Tensor([4, 10, 4, 14260634],"float32"), tuple(10,4,4,), )

[torch error] paddle.nn.functional.layer_norm(Tensor([4, 10, 4, 14260634],"float32"), tuple(10,4,4,), ) 
 Given normalized_shape=[10, 4, 4], expected input with shape [*, 10, 4, 4], but got input of size[4, 10, 4, 14260634]
2025-03-15 01:07:54.100471 test begin: paddle.nn.functional.layer_norm(Tensor([4, 35651585, 4, 4],"float32"), list[10,4,4,], )

[torch error] paddle.nn.functional.layer_norm(Tensor([4, 35651585, 4, 4],"float32"), list[10,4,4,], ) 
 Given normalized_shape=[10, 4, 4], expected input with shape [*, 10, 4, 4], but got input of size[4, 35651585, 4, 4]
2025-03-15 01:07:55.064678 test begin: paddle.nn.functional.layer_norm(Tensor([4, 35651585, 4, 4],"float32"), tuple(10,4,4,), )

[torch error] paddle.nn.functional.layer_norm(Tensor([4, 35651585, 4, 4],"float32"), tuple(10,4,4,), ) 
 Given normalized_shape=[10, 4, 4], expected input with shape [*, 10, 4, 4], but got input of size[4, 35651585, 4, 4]
2025-03-15 01:07:57.236071 test begin: paddle.nn.functional.layer_norm(Tensor([4456449, 512],"float32"), list[512,], None, None, )

[Pass] paddle.nn.functional.layer_norm(Tensor([4456449, 512],"float32"), list[512,], None, None, )
2025-03-15 01:10:38.521797 test begin: paddle.nn.functional.layer_norm(Tensor([54327, 10, 60, 70],"float32"), list[10,60,70,], weight=None, bias=None, epsilon=1e-05, )

[Pass] paddle.nn.functional.layer_norm(Tensor([54327, 10, 60, 70],"float32"), list[10,60,70,], weight=None, bias=None, epsilon=1e-05, )
2025-03-15 01:13:37.041189 test begin: paddle.nn.functional.layer_norm(Tensor([557057, 32, 128],"float32"), list[32,128,], )

[Pass] paddle.nn.functional.layer_norm(Tensor([557057, 32, 128],"float32"), list[32,128,], )
2025-03-15 01:16:41.323955 test begin: paddle.nn.functional.layer_norm(Tensor([557057, 64, 64],"float32"), list[64,], None, None, )

[Pass] paddle.nn.functional.layer_norm(Tensor([557057, 64, 64],"float32"), list[64,], None, None, )
2025-03-15 01:20:07.952566 test begin: paddle.nn.functional.layer_norm(Tensor([557057, 64, 64],"float32"), list[64,], Tensor([64],"float32"), Tensor([64],"float32"), )

[Pass] paddle.nn.functional.layer_norm(Tensor([557057, 64, 64],"float32"), list[64,], Tensor([64],"float32"), Tensor([64],"float32"), )
2025-03-15 01:23:04.259366 test begin: paddle.nn.functional.layer_norm(Tensor([570425345, 4],"float32"), list[4,], None, None, )

[accuracy error] backward  paddle.nn.functional.layer_norm(Tensor([570425345, 4],"float32"), list[4,], None, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 22 / 2281701380 (9.64e-07%)
Max absolute difference: 0.21717834
Max relative difference: 117.233864
 x: array([[ 1.894199, -0.547863,  2.093003, -3.439339],
       [-3.361692, -0.856432,  1.198783,  3.019341],
       [-0.135012,  0.150073,  0.29242 , -0.30748 ],...
 y: array([[ 1.894199, -0.547863,  2.093003, -3.439339],
       [-3.361692, -0.856432,  1.198783,  3.019341],
       [-0.135012,  0.150073,  0.29242 , -0.30748 ],...
2025-03-15 01:27:03.021380 test begin: paddle.nn.functional.layer_norm(Tensor([570425345, 4],"float32"), list[4,], Tensor([4],"float32"), Tensor([4],"float32"), )

[Pass] paddle.nn.functional.layer_norm(Tensor([570425345, 4],"float32"), list[4,], Tensor([4],"float32"), Tensor([4],"float32"), )
2025-03-15 01:29:55.942179 test begin: paddle.nn.functional.layer_norm(Tensor([64, 278529, 128],"float32"), list[64,128,], None, None, )

[torch error] paddle.nn.functional.layer_norm(Tensor([64, 278529, 128],"float32"), list[64,128,], None, None, ) 
 Given normalized_shape=[64, 128], expected input with shape [*, 64, 128], but got input of size[64, 278529, 128]
2025-03-15 01:29:59.763838 test begin: paddle.nn.functional.layer_norm(Tensor([64, 64, 557057],"float32"), list[64,128,], None, None, )

[torch error] paddle.nn.functional.layer_norm(Tensor([64, 64, 557057],"float32"), list[64,128,], None, None, ) 
 Given normalized_shape=[64, 128], expected input with shape [*, 64, 128], but got input of size[64, 64, 557057]
2025-03-15 01:30:02.063005 test begin: paddle.nn.functional.layer_norm(Tensor([69633, 128, 256],"float32"), list[256,], weight=Tensor([256],"float32"), bias=Tensor([256],"float32"), epsilon=1e-05, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   LayerNormGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::layer_norm_grad(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, float, int, paddle::Tensor*, paddle::Tensor*, paddle::Tensor*)
4   void phi::LayerNormGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, paddle::optional<phi::DenseTensor> const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, float, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::funcs::ln_bwd_fast_kernel_driver<float, float, float, unsigned char>(phi::GPUContext const&, int, int, float, float const*, float const*, float const*, float const*, float const*, float*, float*, float*, unsigned char const*, float, float*)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741973492 (unix time) try "date -d @1741973492" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1f9cf) received by PID 129487 (TID 0x7f096fdc2700) from PID 129487 ***]

2025-03-15 01:32:19.890877 test begin: paddle.nn.functional.layer_norm(Tensor([8, 1114113, 256],"float32"), list[256,], weight=Tensor([256],"float32"), bias=Tensor([256],"float32"), epsilon=1e-05, )

W0315 01:33:48.904594 147529 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0315 01:33:48.905836 147529 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   LayerNormGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::layer_norm_grad(paddle::Tensor const&, paddle::optional<paddle::Tensor> const&, paddle::optional<paddle::Tensor> const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, float, int, paddle::Tensor*, paddle::Tensor*, paddle::Tensor*)
4   void phi::LayerNormGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, paddle::optional<phi::DenseTensor> const&, paddle::optional<phi::DenseTensor> const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, float, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::funcs::ln_bwd_fast_kernel_driver<float, float, float, unsigned char>(phi::GPUContext const&, int, int, float, float const*, float const*, float const*, float const*, float const*, float*, float*, float*, unsigned char const*, float, float*)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1741973710 (unix time) try "date -d @1741973710" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x23fec) received by PID 147436 (TID 0x7ff02d34a700) from PID 147436 ***]

2025-03-15 01:35:56.247908 test begin: paddle.nn.functional.layer_norm(Tensor([8, 128, 2228225],"float32"), list[256,], weight=Tensor([256],"float32"), bias=Tensor([256],"float32"), epsilon=1e-05, )

W0315 01:37:00.709096 147648 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0315 01:37:00.710158 147648 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.nn.functional.layer_norm(Tensor([8, 128, 2228225],"float32"), list[256,], weight=Tensor([256],"float32"), bias=Tensor([256],"float32"), epsilon=1e-05, ) 
 Given normalized_shape=[256], expected input with shape [*, 256], but got input of size[8, 128, 2228225]
2025-03-15 01:37:04.646819 test begin: paddle.nn.functional.layer_norm(Tensor([8, 128, 256],"float32"), list[256,], weight=Tensor([2281701379],"float32"), bias=Tensor([256],"float32"), epsilon=1e-05, )

[torch error] paddle.nn.functional.layer_norm(Tensor([8, 128, 256],"float32"), list[256,], weight=Tensor([2281701379],"float32"), bias=Tensor([256],"float32"), epsilon=1e-05, ) 
 Expected weight to be of same shape as normalized_shape, but got weight of shape [2281701379] and normalized_shape = [256]
2025-03-15 01:37:06.677560 test begin: paddle.nn.functional.layer_norm(Tensor([8, 128, 256],"float32"), list[256,], weight=Tensor([256],"float32"), bias=Tensor([2281701379],"float32"), epsilon=1e-05, )

[torch error] paddle.nn.functional.layer_norm(Tensor([8, 128, 256],"float32"), list[256,], weight=Tensor([256],"float32"), bias=Tensor([2281701379],"float32"), epsilon=1e-05, ) 
 Expected bias to be of same shape as normalized_shape, but got bias of shape [2281701379] and normalized_shape = [256]
2025-03-15 01:37:08.079510 test begin: paddle.nn.functional.layer_norm(Tensor([8, 285212673],"float32"), list[512,], None, None, )

[torch error] paddle.nn.functional.layer_norm(Tensor([8, 285212673],"float32"), list[512,], None, None, ) 
 Given normalized_shape=[512], expected input with shape [*, 512], but got input of size[8, 285212673]
2025-03-15 01:37:10.459406 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([190141782, 2, 2, 3],"float32"), )

[Pass] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([190141782, 2, 2, 3],"float32"), )
2025-03-15 01:40:46.602399 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([190141782, 2, 2, 3],"float32"), epsilon=1e-05, )

[Pass] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([190141782, 2, 2, 3],"float32"), epsilon=1e-05, )
2025-03-15 01:43:56.734099 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([190141782, 2, 2, 3],"float32"), epsilon=1e-05, weight=None, bias=None, )

[Pass] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([190141782, 2, 2, 3],"float32"), epsilon=1e-05, weight=None, bias=None, )
2025-03-15 01:46:48.578591 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 190141782, 2, 3],"float32"), )

[torch error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 190141782, 2, 3],"float32"), ) 
 Given normalized_shape=[2, 2, 3], expected input with shape [*, 2, 2, 3], but got input of size[2, 190141782, 2, 3]
2025-03-15 01:46:52.928728 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 190141782, 2, 3],"float32"), epsilon=1e-05, )

[torch error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 190141782, 2, 3],"float32"), epsilon=1e-05, ) 
 Given normalized_shape=[2, 2, 3], expected input with shape [*, 2, 2, 3], but got input of size[2, 190141782, 2, 3]
2025-03-15 01:46:54.944220 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 190141782, 2, 3],"float32"), epsilon=1e-05, weight=None, bias=None, )

[torch error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 190141782, 2, 3],"float32"), epsilon=1e-05, weight=None, bias=None, ) 
 Given normalized_shape=[2, 2, 3], expected input with shape [*, 2, 2, 3], but got input of size[2, 190141782, 2, 3]
2025-03-15 01:46:57.025261 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 2, 190141782, 3],"float32"), )

[torch error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 2, 190141782, 3],"float32"), ) 
 Given normalized_shape=[2, 2, 3], expected input with shape [*, 2, 2, 3], but got input of size[2, 2, 190141782, 3]
2025-03-15 01:46:59.478036 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 2, 190141782, 3],"float32"), epsilon=1e-05, )

[torch error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 2, 190141782, 3],"float32"), epsilon=1e-05, ) 
 Given normalized_shape=[2, 2, 3], expected input with shape [*, 2, 2, 3], but got input of size[2, 2, 190141782, 3]
2025-03-15 01:47:01.360547 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 2, 190141782, 3],"float32"), epsilon=1e-05, weight=None, bias=None, )

[torch error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 2, 190141782, 3],"float32"), epsilon=1e-05, weight=None, bias=None, ) 
 Given normalized_shape=[2, 2, 3], expected input with shape [*, 2, 2, 3], but got input of size[2, 2, 190141782, 3]
2025-03-15 01:47:03.465015 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 2, 2, 285212673],"float32"), )

[torch error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 2, 2, 285212673],"float32"), ) 
 Given normalized_shape=[2, 2, 3], expected input with shape [*, 2, 2, 3], but got input of size[2, 2, 2, 285212673]
2025-03-15 01:47:05.587459 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 2, 2, 285212673],"float32"), epsilon=1e-05, )

[torch error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 2, 2, 285212673],"float32"), epsilon=1e-05, ) 
 Given normalized_shape=[2, 2, 3], expected input with shape [*, 2, 2, 3], but got input of size[2, 2, 2, 285212673]
2025-03-15 01:47:08.036831 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 2, 2, 285212673],"float32"), epsilon=1e-05, weight=None, bias=None, )

[torch error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 2, 2, 285212673],"float32"), epsilon=1e-05, weight=None, bias=None, ) 
 Given normalized_shape=[2, 2, 3], expected input with shape [*, 2, 2, 3], but got input of size[2, 2, 2, 285212673]
2025-03-15 01:47:09.632796 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 2, 2, 536870913],"float16"), )

[torch error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 2, 2, 536870913],"float16"), ) 
 Given normalized_shape=[2, 2, 3], expected input with shape [*, 2, 2, 3], but got input of size[2, 2, 2, 536870913]
2025-03-15 01:48:41.349763 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 2, 2, 536870913],"float16"), epsilon=1e-05, )

[torch error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 2, 2, 536870913],"float16"), epsilon=1e-05, ) 
 Given normalized_shape=[2, 2, 3], expected input with shape [*, 2, 2, 3], but got input of size[2, 2, 2, 536870913]
2025-03-15 01:48:43.850797 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 2, 2, 536870913],"float16"), epsilon=1e-05, weight=None, bias=None, )

[torch error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 2, 2, 536870913],"float16"), epsilon=1e-05, weight=None, bias=None, ) 
 Given normalized_shape=[2, 2, 3], expected input with shape [*, 2, 2, 3], but got input of size[2, 2, 2, 536870913]
2025-03-15 01:48:46.112859 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 2, 357913942, 3],"float16"), )

[torch error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 2, 357913942, 3],"float16"), ) 
 Given normalized_shape=[2, 2, 3], expected input with shape [*, 2, 2, 3], but got input of size[2, 2, 357913942, 3]
2025-03-15 01:48:48.391113 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 2, 357913942, 3],"float16"), epsilon=1e-05, )

[torch error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 2, 357913942, 3],"float16"), epsilon=1e-05, ) 
 Given normalized_shape=[2, 2, 3], expected input with shape [*, 2, 2, 3], but got input of size[2, 2, 357913942, 3]
2025-03-15 01:48:50.022483 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 2, 357913942, 3],"float16"), epsilon=1e-05, weight=None, bias=None, )

[torch error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 2, 357913942, 3],"float16"), epsilon=1e-05, weight=None, bias=None, ) 
 Given normalized_shape=[2, 2, 3], expected input with shape [*, 2, 2, 3], but got input of size[2, 2, 357913942, 3]
2025-03-15 01:48:52.278832 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 357913942, 2, 3],"float16"), )

[torch error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 357913942, 2, 3],"float16"), ) 
 Given normalized_shape=[2, 2, 3], expected input with shape [*, 2, 2, 3], but got input of size[2, 357913942, 2, 3]
2025-03-15 01:48:53.930628 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 357913942, 2, 3],"float16"), epsilon=1e-05, )

[torch error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 357913942, 2, 3],"float16"), epsilon=1e-05, ) 
 Given normalized_shape=[2, 2, 3], expected input with shape [*, 2, 2, 3], but got input of size[2, 357913942, 2, 3]
2025-03-15 01:48:55.714577 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 357913942, 2, 3],"float16"), epsilon=1e-05, weight=None, bias=None, )

[torch error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([2, 357913942, 2, 3],"float16"), epsilon=1e-05, weight=None, bias=None, ) 
 Given normalized_shape=[2, 2, 3], expected input with shape [*, 2, 2, 3], but got input of size[2, 357913942, 2, 3]
2025-03-15 01:48:58.133972 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([357913942, 2, 2, 3],"float16"), )

[Pass] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([357913942, 2, 2, 3],"float16"), )
2025-03-15 02:05:48.606591 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([357913942, 2, 2, 3],"float16"), epsilon=1e-05, )

[Pass] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([357913942, 2, 2, 3],"float16"), epsilon=1e-05, )
2025-03-15 02:22:10.438934 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([357913942, 2, 2, 3],"float16"), epsilon=1e-05, weight=None, bias=None, )

[Pass] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,2,3,), x=Tensor([357913942, 2, 2, 3],"float16"), epsilon=1e-05, weight=None, bias=None, )
2025-03-15 02:38:58.686133 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([2, 2, 1073741825],"float16"), )

[torch error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([2, 2, 1073741825],"float16"), ) 
 Given normalized_shape=[2, 3], expected input with shape [*, 2, 3], but got input of size[2, 2, 1073741825]
2025-03-15 02:39:02.716220 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([2, 2, 1073741825],"float16"), epsilon=1e-05, )

[torch error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([2, 2, 1073741825],"float16"), epsilon=1e-05, ) 
 Given normalized_shape=[2, 3], expected input with shape [*, 2, 3], but got input of size[2, 2, 1073741825]
2025-03-15 02:39:04.654335 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([2, 2, 570425345],"float32"), )

[torch error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([2, 2, 570425345],"float32"), ) 
 Given normalized_shape=[2, 3], expected input with shape [*, 2, 3], but got input of size[2, 2, 570425345]
2025-03-15 02:39:09.079548 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([2, 2, 570425345],"float32"), epsilon=1e-05, )

[torch error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([2, 2, 570425345],"float32"), epsilon=1e-05, ) 
 Given normalized_shape=[2, 3], expected input with shape [*, 2, 3], but got input of size[2, 2, 570425345]
2025-03-15 02:39:11.456467 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([2, 380283564, 3],"float32"), )

[torch error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([2, 380283564, 3],"float32"), ) 
 Given normalized_shape=[2, 3], expected input with shape [*, 2, 3], but got input of size[2, 380283564, 3]
2025-03-15 02:39:13.618601 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([2, 380283564, 3],"float32"), epsilon=1e-05, )

[torch error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([2, 380283564, 3],"float32"), epsilon=1e-05, ) 
 Given normalized_shape=[2, 3], expected input with shape [*, 2, 3], but got input of size[2, 380283564, 3]
2025-03-15 02:39:15.779853 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([2, 715827883, 3],"float16"), )

[torch error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([2, 715827883, 3],"float16"), ) 
 Given normalized_shape=[2, 3], expected input with shape [*, 2, 3], but got input of size[2, 715827883, 3]
2025-03-15 02:39:17.826202 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([2, 715827883, 3],"float16"), epsilon=1e-05, )

[torch error] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([2, 715827883, 3],"float16"), epsilon=1e-05, ) 
 Given normalized_shape=[2, 3], expected input with shape [*, 2, 3], but got input of size[2, 715827883, 3]
2025-03-15 02:39:20.029449 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([380283564, 2, 3],"float32"), )

[Pass] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([380283564, 2, 3],"float32"), )
2025-03-15 02:42:07.961782 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([380283564, 2, 3],"float32"), epsilon=1e-05, )

[Pass] paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([380283564, 2, 3],"float32"), epsilon=1e-05, )
2025-03-15 02:45:15.350138 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([715827883, 2, 3],"float16"), )

[accuracy error] backward  paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([715827883, 2, 3],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 12 / 4294967298 (2.79e-07%)
Max absolute difference: 0.0625
Max relative difference: 16384.
 x: array([[[ 0.3052 ,  0.1172 , -0.9375 ],
        [ 1.472  , -0.3745 , -0.583  ]],
...
 y: array([[[ 0.3052 ,  0.1173 , -0.937  ],
        [ 1.472  , -0.3745 , -0.5825 ]],
...
2025-03-15 03:06:40.412176 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([715827883, 2, 3],"float16"), epsilon=1e-05, )

[accuracy error] backward  paddle.nn.functional.layer_norm(normalized_shape=tuple(2,3,), x=Tensor([715827883, 2, 3],"float16"), epsilon=1e-05, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 12 / 4294967298 (2.79e-07%)
Max absolute difference: 0.0625
Max relative difference: 16384.
 x: array([[[ 0.3052 ,  0.1172 , -0.9375 ],
        [ 1.472  , -0.3745 , -0.583  ]],
...
 y: array([[[ 0.3052 ,  0.1173 , -0.937  ],
        [ 1.472  , -0.3745 , -0.5825 ]],
...
2025-03-15 03:28:06.675900 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(3,), x=Tensor([1431655766, 3],"float16"), )

[accuracy error] backward  paddle.nn.functional.layer_norm(normalized_shape=tuple(3,), x=Tensor([1431655766, 3],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 77045 / 4294967298 (0.00179%)
Max absolute difference: 0.25
Max relative difference: 16384.
 x: array([[-0.07715,  0.1128 , -0.03613],
       [ 0.4033 ,  0.4785 , -0.8813 ],
       [-0.02734, -0.02588,  0.05338],...
 y: array([[-0.0768 ,  0.1126 , -0.03586],
       [ 0.403  ,  0.4783 , -0.8813 ],
       [-0.02739, -0.02594,  0.05334],...
2025-03-15 03:50:03.794439 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(3,), x=Tensor([1431655766, 3],"float16"), epsilon=1e-05, )

[accuracy error] backward  paddle.nn.functional.layer_norm(normalized_shape=tuple(3,), x=Tensor([1431655766, 3],"float16"), epsilon=1e-05, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 77045 / 4294967298 (0.00179%)
Max absolute difference: 0.25
Max relative difference: 16384.
 x: array([[-0.07715,  0.1128 , -0.03613],
       [ 0.4033 ,  0.4785 , -0.8813 ],
       [-0.02734, -0.02588,  0.05338],...
 y: array([[-0.0768 ,  0.1126 , -0.03586],
       [ 0.403  ,  0.4783 , -0.8813 ],
       [-0.02739, -0.02594,  0.05334],...
2025-03-15 04:12:11.851365 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(3,), x=Tensor([2, 1140850690],"float32"), )

[torch error] paddle.nn.functional.layer_norm(normalized_shape=tuple(3,), x=Tensor([2, 1140850690],"float32"), ) 
 Given normalized_shape=[3], expected input with shape [*, 3], but got input of size[2, 1140850690]
2025-03-15 04:12:17.151457 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(3,), x=Tensor([2, 1140850690],"float32"), epsilon=1e-05, )

[torch error] paddle.nn.functional.layer_norm(normalized_shape=tuple(3,), x=Tensor([2, 1140850690],"float32"), epsilon=1e-05, ) 
 Given normalized_shape=[3], expected input with shape [*, 3], but got input of size[2, 1140850690]
2025-03-15 04:12:19.616813 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(3,), x=Tensor([2, 2147483649],"float16"), )

[torch error] paddle.nn.functional.layer_norm(normalized_shape=tuple(3,), x=Tensor([2, 2147483649],"float16"), ) 
 Given normalized_shape=[3], expected input with shape [*, 3], but got input of size[2, 2147483649]
2025-03-15 04:12:23.335948 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(3,), x=Tensor([2, 2147483649],"float16"), epsilon=1e-05, )

[torch error] paddle.nn.functional.layer_norm(normalized_shape=tuple(3,), x=Tensor([2, 2147483649],"float16"), epsilon=1e-05, ) 
 Given normalized_shape=[3], expected input with shape [*, 3], but got input of size[2, 2147483649]
2025-03-15 04:12:25.309276 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(3,), x=Tensor([760567127, 3],"float32"), )

[accuracy error] backward  paddle.nn.functional.layer_norm(normalized_shape=tuple(3,), x=Tensor([760567127, 3],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1966 / 2281701381 (8.62e-05%)
Max absolute difference: 0.3556671
Max relative difference: 574.8028
 x: array([[ 0.730589, -1.828362,  1.097773],
       [-0.15128 ,  0.129841,  0.021439],
       [-0.381511, -0.45647 ,  0.837981],...
 y: array([[ 0.730589, -1.828363,  1.097773],
       [-0.15128 ,  0.129841,  0.021439],
       [-0.381511, -0.456471,  0.837981],...
2025-03-15 04:16:26.570165 test begin: paddle.nn.functional.layer_norm(normalized_shape=tuple(3,), x=Tensor([760567127, 3],"float32"), epsilon=1e-05, )

[accuracy error] backward  paddle.nn.functional.layer_norm(normalized_shape=tuple(3,), x=Tensor([760567127, 3],"float32"), epsilon=1e-05, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 1966 / 2281701381 (8.62e-05%)
Max absolute difference: 0.3556671
Max relative difference: 574.8028
 x: array([[ 0.730589, -1.828362,  1.097773],
       [-0.15128 ,  0.129841,  0.021439],
       [-0.381511, -0.45647 ,  0.837981],...
 y: array([[ 0.730589, -1.828363,  1.097773],
       [-0.15128 ,  0.129841,  0.021439],
       [-0.381511, -0.456471,  0.837981],...
2025-03-15 04:20:06.917056 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 1, 142606337, 16],"float32"), 0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 1, 142606337, 16],"float32"), 0.2, )
2025-03-15 04:23:40.430256 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 1, 16, 142606337],"float32"), 0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 1, 16, 142606337],"float32"), 0.2, )
2025-03-15 04:27:26.642191 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 1, 32, 71303169],"float32"), 0, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 1, 32, 71303169],"float32"), 0, )
2025-03-15 04:30:57.981757 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 1, 71303169, 32],"float32"), 0, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 1, 71303169, 32],"float32"), 0, )
2025-03-15 04:34:35.311567 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 1024, 278529, 8],"float32"), 0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 1024, 278529, 8],"float32"), 0.2, )
2025-03-15 04:38:13.813471 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 1024, 8, 278529],"float32"), 0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 1024, 8, 278529],"float32"), 0.2, )
2025-03-15 04:41:33.913121 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 1273271, 1792],"float32"), )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 1273271, 1792],"float32"), )
2025-03-15 04:44:59.757216 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 1273271, 1792],"float32"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 1273271, 1792],"float32"), 0.1, )
2025-03-15 04:48:13.021311 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 128, 128, 139265],"float32"), 0, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 128, 128, 139265],"float32"), 0, )
2025-03-15 04:51:50.939413 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 128, 139265, 128],"float32"), 0, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 128, 139265, 128],"float32"), 0, )
2025-03-15 04:55:18.180295 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 128, 17825793],"float32"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 128, 17825793],"float32"), 0.1, )
2025-03-15 04:58:58.382312 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 128, 256, 69633],"float32"), negative_slope=0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 128, 256, 69633],"float32"), negative_slope=0.2, )
2025-03-15 05:02:26.452979 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 128, 278529, 64],"float32"), 0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 128, 278529, 64],"float32"), 0.2, )
2025-03-15 05:05:54.118673 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 128, 278529, 64],"float32"), 0.2, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 128, 278529, 64],"float32"), 0.2, None, )
2025-03-15 05:09:20.581816 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 128, 64, 278529],"float32"), 0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 128, 64, 278529],"float32"), 0.2, )
2025-03-15 05:12:58.233505 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 128, 64, 278529],"float32"), 0.2, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 128, 64, 278529],"float32"), 0.2, None, )
2025-03-15 05:15:50.414891 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 128, 69633, 256],"float32"), negative_slope=0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 128, 69633, 256],"float32"), negative_slope=0.2, )
2025-03-15 05:18:58.328333 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 139265, 128, 128],"float32"), 0, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 139265, 128, 128],"float32"), 0, )
2025-03-15 05:22:33.216469 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 139265, 128, 128],"float32"), negative_slope=0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 139265, 128, 128],"float32"), negative_slope=0.2, )
2025-03-15 05:25:41.965295 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 142606337, 4, 4],"float32"), 0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 142606337, 4, 4],"float32"), 0.2, )
2025-03-15 05:29:00.986562 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 142606337, 4, 4],"float32"), negative_slope=0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 142606337, 4, 4],"float32"), negative_slope=0.2, )
2025-03-15 05:32:16.108062 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 2, 16, 71303169],"float32"), 0, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 2, 16, 71303169],"float32"), 0, )
2025-03-15 05:35:43.261527 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 2, 71303169, 16],"float32"), 0, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 2, 71303169, 16],"float32"), 0, )
2025-03-15 05:39:16.440571 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 20372334, 112],"float32"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 20372334, 112],"float32"), 0.1, )
2025-03-15 05:42:11.140275 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 2048, 278529, 4],"float32"), 0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 2048, 278529, 4],"float32"), 0.2, )
2025-03-15 05:45:51.305646 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 2048, 4, 278529],"float32"), 0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 2048, 4, 278529],"float32"), 0.2, )
2025-03-15 05:48:55.325455 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 2228225, 32, 32],"float32"), 0, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 2228225, 32, 32],"float32"), 0, )
2025-03-15 05:51:43.834341 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 2228225, 32, 32],"float32"), 0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 2228225, 32, 32],"float32"), 0.2, )
2025-03-15 05:54:28.339888 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 2228225, 32, 32],"float32"), 0.2, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 2228225, 32, 32],"float32"), 0.2, None, )
2025-03-15 05:57:41.732596 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 2228225, 32, 32],"float32"), negative_slope=0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 2228225, 32, 32],"float32"), negative_slope=0.2, )
2025-03-15 06:00:57.441623 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 2281701379],"float32"), negative_slope=0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 2281701379],"float32"), negative_slope=0.2, )
2025-03-15 06:04:19.971548 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 2374300, 31, 31],"float32"), 0.2, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 2374300, 31, 31],"float32"), 0.2, None, )
2025-03-15 06:07:44.860099 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 256, 128, 69633],"float32"), negative_slope=0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 256, 128, 69633],"float32"), negative_slope=0.2, )
2025-03-15 06:11:34.068050 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 256, 139265, 64],"float32"), 0, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 256, 139265, 64],"float32"), 0, )
2025-03-15 06:14:37.959004 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 256, 278529, 32],"float32"), 0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 256, 278529, 32],"float32"), 0.2, )
2025-03-15 06:17:37.696722 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 256, 278529, 32],"float32"), 0.2, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 256, 278529, 32],"float32"), 0.2, None, )
2025-03-15 06:20:36.531508 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 256, 32, 278529],"float32"), 0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 256, 32, 278529],"float32"), 0.2, )
2025-03-15 06:23:30.261451 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 256, 32, 278529],"float32"), 0.2, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 256, 32, 278529],"float32"), 0.2, None, )
2025-03-15 06:26:15.673038 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 256, 64, 139265],"float32"), 0, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 256, 64, 139265],"float32"), 0, )
2025-03-15 06:29:39.635408 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 256, 69633, 128],"float32"), negative_slope=0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 256, 69633, 128],"float32"), negative_slope=0.2, )
2025-03-15 06:32:44.424153 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 256, 8912897],"float32"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 256, 8912897],"float32"), 0.1, )
2025-03-15 06:35:39.963392 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 32, 71303169],"float32"), )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 32, 71303169],"float32"), )
2025-03-15 06:38:54.775213 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 32, 71303169],"float32"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 32, 71303169],"float32"), 0.1, )
2025-03-15 06:41:56.441434 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 325957340, 7],"float32"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 325957340, 7],"float32"), 0.1, )
2025-03-15 06:45:03.536108 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 34817, 256, 256],"float32"), negative_slope=0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 34817, 256, 256],"float32"), negative_slope=0.2, )
2025-03-15 06:48:40.020731 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 35651585, 8, 8],"float32"), 0, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 35651585, 8, 8],"float32"), 0, )
2025-03-15 06:51:53.681016 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 35651585, 8, 8],"float32"), 0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 35651585, 8, 8],"float32"), 0.2, )
2025-03-15 06:54:54.094048 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 4, 71303169, 8],"float32"), 0, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 4, 71303169, 8],"float32"), 0, )
2025-03-15 06:57:49.461757 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 4, 8, 71303169],"float32"), 0, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 4, 8, 71303169],"float32"), 0, )
2025-03-15 07:00:40.799142 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 5093084, 448],"float32"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 5093084, 448],"float32"), 0.1, )
2025-03-15 07:03:28.527085 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 512, 1114113, 4],"float32"), negative_slope=0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 512, 1114113, 4],"float32"), negative_slope=0.2, )
2025-03-15 07:06:36.979466 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 512, 139265, 32],"float32"), negative_slope=0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 512, 139265, 32],"float32"), negative_slope=0.2, )
2025-03-15 07:09:25.605069 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 512, 143757, 31],"float32"), 0.2, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 512, 143757, 31],"float32"), 0.2, None, )
2025-03-15 07:12:39.192200 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 512, 16, 278529],"float32"), 0.2, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 512, 16, 278529],"float32"), 0.2, None, )
2025-03-15 07:15:52.130722 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 512, 16, 278529],"float32"), negative_slope=0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 512, 16, 278529],"float32"), negative_slope=0.2, )
2025-03-15 07:18:59.534391 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 512, 2, 2228225],"float32"), 0.2, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 512, 2, 2228225],"float32"), 0.2, None, )
2025-03-15 07:22:18.996555 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 512, 2228225, 2],"float32"), 0.2, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 512, 2228225, 2],"float32"), 0.2, None, )
2025-03-15 07:26:10.422634 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 512, 278529, 16],"float32"), 0.2, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 512, 278529, 16],"float32"), 0.2, None, )
2025-03-15 07:28:59.825934 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 512, 278529, 16],"float32"), negative_slope=0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 512, 278529, 16],"float32"), negative_slope=0.2, )
2025-03-15 07:32:28.275926 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 512, 31, 143757],"float32"), 0.2, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 512, 31, 143757],"float32"), 0.2, None, )
2025-03-15 07:35:37.316839 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 512, 32, 139265],"float32"), negative_slope=0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 512, 32, 139265],"float32"), negative_slope=0.2, )
2025-03-15 07:38:27.824468 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 512, 4, 1114113],"float32"), negative_slope=0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 512, 4, 1114113],"float32"), negative_slope=0.2, )
2025-03-15 07:41:53.291253 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 512, 4456449],"float32"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 512, 4456449],"float32"), 0.1, )
2025-03-15 07:45:08.313572 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 557057, 64, 64],"float32"), 0, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 557057, 64, 64],"float32"), 0, )
2025-03-15 07:47:58.225161 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 557057, 64, 64],"float32"), 0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 557057, 64, 64],"float32"), 0.2, )
2025-03-15 07:51:02.864900 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 557057, 64, 64],"float32"), 0.2, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 557057, 64, 64],"float32"), 0.2, None, )
2025-03-15 07:54:39.022220 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 570425345, 2, 2],"float32"), 0.2, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 570425345, 2, 2],"float32"), 0.2, None, )
2025-03-15 07:57:50.754190 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 64, 35651585],"float32"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 64, 35651585],"float32"), 0.1, )
2025-03-15 08:01:45.410089 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 81489335, 28],"float32"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 81489335, 28],"float32"), 0.1, )
2025-03-15 08:05:08.440566 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 8912897, 16, 16],"float32"), 0, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 8912897, 16, 16],"float32"), 0, )
2025-03-15 08:08:26.197735 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 8912897, 16, 16],"float32"), 0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 8912897, 16, 16],"float32"), 0.2, )
2025-03-15 08:11:36.642372 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 8912897, 16, 16],"float32"), 0.2, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 8912897, 16, 16],"float32"), 0.2, None, )
2025-03-15 08:14:38.083098 test begin: paddle.nn.functional.leaky_relu(Tensor([1, 8912897, 16, 16],"float32"), negative_slope=0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1, 8912897, 16, 16],"float32"), negative_slope=0.2, )
2025-03-15 08:17:30.526108 test begin: paddle.nn.functional.leaky_relu(Tensor([10, 143165577, 3],"float16"), -1, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([10, 143165577, 3],"float16"), -1, None, )
2025-03-15 08:33:40.357208 test begin: paddle.nn.functional.leaky_relu(Tensor([10, 20, 11408507],"float32"), 0.01, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([10, 20, 11408507],"float32"), 0.01, )
2025-03-15 08:36:57.765412 test begin: paddle.nn.functional.leaky_relu(Tensor([10, 228170138, 1],"float32"), 0.01, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([10, 228170138, 1],"float32"), 0.01, )
2025-03-15 08:39:56.667920 test begin: paddle.nn.functional.leaky_relu(Tensor([10, 3, 143165577],"float16"), -1, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([10, 3, 143165577],"float16"), -1, None, )
2025-03-15 08:56:05.304846 test begin: paddle.nn.functional.leaky_relu(Tensor([10700, 128, 56, 56],"float16"), )

[Pass] paddle.nn.functional.leaky_relu(Tensor([10700, 128, 56, 56],"float16"), )
2025-03-15 09:19:28.483776 test begin: paddle.nn.functional.leaky_relu(Tensor([1089, 128, 128, 128],"float32"), 0, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1089, 128, 128, 128],"float32"), 0, )
2025-03-15 09:22:42.962377 test begin: paddle.nn.functional.leaky_relu(Tensor([1114113, 512, 2, 2],"float32"), 0.2, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([1114113, 512, 2, 2],"float32"), 0.2, None, )
2025-03-15 09:25:44.312402 test begin: paddle.nn.functional.leaky_relu(Tensor([11369, 1024, 14, 14],"float32"), )

[Pass] paddle.nn.functional.leaky_relu(Tensor([11369, 1024, 14, 14],"float32"), )
2025-03-15 09:28:47.601457 test begin: paddle.nn.functional.leaky_relu(Tensor([11369, 256, 28, 28],"float32"), 0.01, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([11369, 256, 28, 28],"float32"), 0.01, None, )
2025-03-15 09:32:02.178672 test begin: paddle.nn.functional.leaky_relu(Tensor([114085069, 20, 1],"float32"), 0.01, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([114085069, 20, 1],"float32"), 0.01, )
2025-03-15 09:35:00.847604 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 1024, 10281, 34],"float16"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([12, 1024, 10281, 34],"float16"), 0.1, )
2025-03-15 09:51:58.602105 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 1024, 19, 18397],"float16"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([12, 1024, 19, 18397],"float16"), 0.1, )
2025-03-15 10:08:49.628785 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 1024, 19, 9773],"float32"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([12, 1024, 19, 9773],"float32"), 0.1, )
2025-03-15 10:11:40.578970 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 1024, 5462, 34],"float32"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([12, 1024, 5462, 34],"float32"), 0.1, )
2025-03-15 10:14:30.767464 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 128, 10281, 272],"float16"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([12, 128, 10281, 272],"float16"), 0.1, )
2025-03-15 10:32:01.181109 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 128, 152, 18397],"float16"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([12, 128, 152, 18397],"float16"), 0.1, )
2025-03-15 10:49:40.248433 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 128, 152, 9773],"float32"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([12, 128, 152, 9773],"float32"), 0.1, )
2025-03-15 10:52:48.287813 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 128, 38, 73585],"float16"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([12, 128, 38, 73585],"float16"), 0.1, )
2025-03-15 11:10:52.998892 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 128, 41121, 68],"float16"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([12, 128, 41121, 68],"float16"), 0.1, )
2025-03-15 11:28:42.629528 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 128, 5462, 272],"float32"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([12, 128, 5462, 272],"float32"), 0.1, )
2025-03-15 11:31:47.750882 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 138512, 38, 68],"float16"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([12, 138512, 38, 68],"float16"), 0.1, )
2025-03-15 11:49:17.613797 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 294338, 19, 34],"float32"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([12, 294338, 19, 34],"float32"), 0.1, )
2025-03-15 11:52:50.151929 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 4600, 152, 272],"float32"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([12, 4600, 152, 272],"float32"), 0.1, )
2025-03-15 11:56:53.824616 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 554047, 19, 34],"float16"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([12, 554047, 19, 34],"float16"), 0.1, )
2025-03-15 12:14:28.827514 test begin: paddle.nn.functional.leaky_relu(Tensor([12, 8657, 152, 272],"float16"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([12, 8657, 152, 272],"float16"), 0.1, )
2025-03-15 12:32:10.541882 test begin: paddle.nn.functional.leaky_relu(Tensor([128, 1024, 1244, 14],"float32"), )

[Pass] paddle.nn.functional.leaky_relu(Tensor([128, 1024, 1244, 14],"float32"), )
2025-03-15 12:35:23.361016 test begin: paddle.nn.functional.leaky_relu(Tensor([128, 1024, 14, 1244],"float32"), )

[Pass] paddle.nn.functional.leaky_relu(Tensor([128, 1024, 14, 1244],"float32"), )
2025-03-15 12:38:24.177559 test begin: paddle.nn.functional.leaky_relu(Tensor([128, 1024, 14, 2341],"float16"), )

[Pass] paddle.nn.functional.leaky_relu(Tensor([128, 1024, 14, 2341],"float16"), )
2025-03-15 13:02:05.674714 test begin: paddle.nn.functional.leaky_relu(Tensor([128, 1024, 2341, 14],"float16"), )

[Pass] paddle.nn.functional.leaky_relu(Tensor([128, 1024, 2341, 14],"float16"), )
2025-03-15 13:26:50.171822 test begin: paddle.nn.functional.leaky_relu(Tensor([128, 10700, 56, 56],"float16"), )

[Pass] paddle.nn.functional.leaky_relu(Tensor([128, 10700, 56, 56],"float16"), )
2025-03-15 13:50:24.540148 test begin: paddle.nn.functional.leaky_relu(Tensor([128, 128, 28, 4974],"float32"), )

[Pass] paddle.nn.functional.leaky_relu(Tensor([128, 128, 28, 4974],"float32"), )
2025-03-15 13:53:44.447139 test begin: paddle.nn.functional.leaky_relu(Tensor([128, 128, 28, 9363],"float16"), )

[Pass] paddle.nn.functional.leaky_relu(Tensor([128, 128, 28, 9363],"float16"), )
2025-03-15 14:17:20.520301 test begin: paddle.nn.functional.leaky_relu(Tensor([128, 128, 4682, 56],"float16"), )

[Pass] paddle.nn.functional.leaky_relu(Tensor([128, 128, 4682, 56],"float16"), )
2025-03-15 14:40:52.319021 test begin: paddle.nn.functional.leaky_relu(Tensor([128, 128, 4974, 28],"float32"), )

[Pass] paddle.nn.functional.leaky_relu(Tensor([128, 128, 4974, 28],"float32"), )
2025-03-15 14:44:07.467597 test begin: paddle.nn.functional.leaky_relu(Tensor([128, 128, 56, 4682],"float16"), )

[Pass] paddle.nn.functional.leaky_relu(Tensor([128, 128, 56, 4682],"float16"), )
2025-03-15 15:07:33.465555 test begin: paddle.nn.functional.leaky_relu(Tensor([128, 128, 9363, 28],"float16"), )

[Pass] paddle.nn.functional.leaky_relu(Tensor([128, 128, 9363, 28],"float16"), )
2025-03-15 15:31:01.860901 test begin: paddle.nn.functional.leaky_relu(Tensor([128, 171197, 14, 14],"float16"), )

[Pass] paddle.nn.functional.leaky_relu(Tensor([128, 171197, 14, 14],"float16"), )
2025-03-15 15:54:12.689414 test begin: paddle.nn.functional.leaky_relu(Tensor([128, 22737, 28, 28],"float32"), )

[Pass] paddle.nn.functional.leaky_relu(Tensor([128, 22737, 28, 28],"float32"), )
2025-03-15 15:57:35.397387 test begin: paddle.nn.functional.leaky_relu(Tensor([128, 42800, 28, 28],"float16"), )

[Pass] paddle.nn.functional.leaky_relu(Tensor([128, 42800, 28, 28],"float16"), )
2025-03-15 16:20:41.934434 test begin: paddle.nn.functional.leaky_relu(Tensor([128, 90948, 14, 14],"float32"), )

[Pass] paddle.nn.functional.leaky_relu(Tensor([128, 90948, 14, 14],"float32"), )
2025-03-15 16:23:54.179349 test begin: paddle.nn.functional.leaky_relu(Tensor([12986, 128, 38, 68],"float16"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([12986, 128, 38, 68],"float16"), 0.1, )
2025-03-15 16:40:53.658684 test begin: paddle.nn.functional.leaky_relu(Tensor([139265, 64, 16, 16],"float32"), 0.1, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([139265, 64, 16, 16],"float32"), 0.1, None, )
2025-03-15 16:44:20.645839 test begin: paddle.nn.functional.leaky_relu(Tensor([159159, 128, 112],"float32"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([159159, 128, 112],"float32"), 0.1, )
2025-03-15 16:47:38.453439 test begin: paddle.nn.functional.leaky_relu(Tensor([16385, 64, 64, 64],"float16"), 0.1, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([16385, 64, 64, 64],"float16"), 0.1, None, )
2025-03-15 17:04:54.052607 test begin: paddle.nn.functional.leaky_relu(Tensor([17409, 512, 16, 16],"float32"), 0.2, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([17409, 512, 16, 16],"float32"), 0.2, None, )
2025-03-15 17:08:13.933417 test begin: paddle.nn.functional.leaky_relu(Tensor([17409, 512, 16, 16],"float32"), negative_slope=0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([17409, 512, 16, 16],"float32"), negative_slope=0.2, )
2025-03-15 17:11:43.664357 test begin: paddle.nn.functional.leaky_relu(Tensor([2, 32, 35651585],"float32"), )

[Pass] paddle.nn.functional.leaky_relu(Tensor([2, 32, 35651585],"float32"), )
2025-03-15 17:14:46.821650 test begin: paddle.nn.functional.leaky_relu(Tensor([2, 636636, 1792],"float32"), )

[Pass] paddle.nn.functional.leaky_relu(Tensor([2, 636636, 1792],"float32"), )
2025-03-15 17:17:38.572256 test begin: paddle.nn.functional.leaky_relu(Tensor([21400, 1024, 14, 14],"float16"), )

[Pass] paddle.nn.functional.leaky_relu(Tensor([21400, 1024, 14, 14],"float16"), )
2025-03-15 17:40:49.630363 test begin: paddle.nn.functional.leaky_relu(Tensor([2177, 256, 64, 64],"float32"), 0, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([2177, 256, 64, 64],"float32"), 0, )
2025-03-15 17:43:37.070641 test begin: paddle.nn.functional.leaky_relu(Tensor([2228225, 1, 32, 32],"float32"), 0, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([2228225, 1, 32, 32],"float32"), 0, )
2025-03-15 17:46:51.018470 test begin: paddle.nn.functional.leaky_relu(Tensor([22737, 128, 28, 28],"float32"), )

[Pass] paddle.nn.functional.leaky_relu(Tensor([22737, 128, 28, 28],"float32"), )
2025-03-15 17:49:51.775118 test begin: paddle.nn.functional.leaky_relu(Tensor([22737, 128, 28, 28],"float32"), 0.01, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([22737, 128, 28, 28],"float32"), 0.01, None, )
2025-03-15 17:52:48.240045 test begin: paddle.nn.functional.leaky_relu(Tensor([2281701379],"float32"), 0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([2281701379],"float32"), 0.2, )
2025-03-15 17:55:56.652828 test begin: paddle.nn.functional.leaky_relu(Tensor([253522376, 3, 3],"float32"), 0.01, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([253522376, 3, 3],"float32"), 0.01, None, )
2025-03-15 17:59:16.601086 test begin: paddle.nn.functional.leaky_relu(Tensor([262145, 64, 16, 16],"float16"), 0.1, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([262145, 64, 16, 16],"float16"), 0.1, None, )
2025-03-15 18:16:48.490592 test begin: paddle.nn.functional.leaky_relu(Tensor([273, 128, 256, 256],"float32"), negative_slope=0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([273, 128, 256, 256],"float32"), negative_slope=0.2, )
2025-03-15 18:19:51.516169 test begin: paddle.nn.functional.leaky_relu(Tensor([278529, 512, 4, 4],"float32"), negative_slope=0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([278529, 512, 4, 4],"float32"), negative_slope=0.2, )
2025-03-15 18:22:38.396759 test begin: paddle.nn.functional.leaky_relu(Tensor([285213, 200, 40],"float32"), )

[Pass] paddle.nn.functional.leaky_relu(Tensor([285213, 200, 40],"float32"), )
2025-03-15 18:25:26.079031 test begin: paddle.nn.functional.leaky_relu(Tensor([3, 253522376, 3],"float32"), 0.01, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([3, 253522376, 3],"float32"), 0.01, None, )
2025-03-15 18:28:13.133819 test begin: paddle.nn.functional.leaky_relu(Tensor([3, 3, 253522376],"float32"), 0.01, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([3, 3, 253522376],"float32"), 0.01, None, )
2025-03-15 18:31:03.616238 test begin: paddle.nn.functional.leaky_relu(Tensor([3, 3, 477218589],"float16"), 0, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([3, 3, 477218589],"float16"), 0, None, )
2025-03-15 18:49:19.854379 test begin: paddle.nn.functional.leaky_relu(Tensor([3, 3, 477218589],"float16"), 0.01, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([3, 3, 477218589],"float16"), 0.01, None, )
2025-03-15 19:12:41.824048 test begin: paddle.nn.functional.leaky_relu(Tensor([3, 477218589, 3],"float16"), 0, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([3, 477218589, 3],"float16"), 0, None, )
2025-03-15 19:30:47.343335 test begin: paddle.nn.functional.leaky_relu(Tensor([3, 477218589, 3],"float16"), 0.01, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([3, 477218589, 3],"float16"), 0.01, None, )
2025-03-15 19:54:30.649414 test begin: paddle.nn.functional.leaky_relu(Tensor([30, 1901418, 40],"float32"), )

[Pass] paddle.nn.functional.leaky_relu(Tensor([30, 1901418, 40],"float32"), )
2025-03-15 19:57:36.319535 test begin: paddle.nn.functional.leaky_relu(Tensor([30, 200, 380284],"float32"), )

[Pass] paddle.nn.functional.leaky_relu(Tensor([30, 200, 380284],"float32"), )
2025-03-15 20:01:21.552042 test begin: paddle.nn.functional.leaky_relu(Tensor([318318, 256, 28],"float32"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([318318, 256, 28],"float32"), 0.1, )
2025-03-15 20:04:59.724914 test begin: paddle.nn.functional.leaky_relu(Tensor([320, 13108, 32, 32],"float16"), 0.1, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([320, 13108, 32, 32],"float16"), 0.1, None, )
2025-03-15 20:22:12.679436 test begin: paddle.nn.functional.leaky_relu(Tensor([320, 27853, 16, 16],"float32"), 0.1, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([320, 27853, 16, 16],"float32"), 0.1, None, )
2025-03-15 20:25:26.245972 test begin: paddle.nn.functional.leaky_relu(Tensor([320, 3277, 64, 64],"float16"), 0.1, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([320, 3277, 64, 64],"float16"), 0.1, None, )
2025-03-15 20:42:44.377537 test begin: paddle.nn.functional.leaky_relu(Tensor([320, 52429, 16, 16],"float16"), 0.1, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([320, 52429, 16, 16],"float16"), 0.1, None, )
2025-03-15 21:00:04.118357 test begin: paddle.nn.functional.leaky_relu(Tensor([320, 64, 13108, 16],"float16"), 0.1, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([320, 64, 13108, 16],"float16"), 0.1, None, )
2025-03-15 21:17:05.823270 test begin: paddle.nn.functional.leaky_relu(Tensor([320, 64, 16, 13108],"float16"), 0.1, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([320, 64, 16, 13108],"float16"), 0.1, None, )
2025-03-15 21:34:33.163846 test begin: paddle.nn.functional.leaky_relu(Tensor([320, 64, 16, 6964],"float32"), 0.1, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([320, 64, 16, 6964],"float32"), 0.1, None, )
2025-03-15 21:37:37.280361 test begin: paddle.nn.functional.leaky_relu(Tensor([320, 64, 32, 3482],"float32"), 0.1, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([320, 64, 32, 3482],"float32"), 0.1, None, )
2025-03-15 21:40:53.345318 test begin: paddle.nn.functional.leaky_relu(Tensor([320, 64, 32, 6554],"float16"), 0.1, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([320, 64, 32, 6554],"float16"), 0.1, None, )
2025-03-15 21:58:14.014402 test begin: paddle.nn.functional.leaky_relu(Tensor([320, 64, 3277, 64],"float16"), 0.1, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([320, 64, 3277, 64],"float16"), 0.1, None, )
2025-03-15 22:16:10.702437 test begin: paddle.nn.functional.leaky_relu(Tensor([320, 64, 3482, 32],"float32"), 0.1, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([320, 64, 3482, 32],"float32"), 0.1, None, )
2025-03-15 22:19:13.962669 test begin: paddle.nn.functional.leaky_relu(Tensor([320, 64, 64, 3277],"float16"), 0.1, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([320, 64, 64, 3277],"float16"), 0.1, None, )
2025-03-15 22:36:48.111377 test begin: paddle.nn.functional.leaky_relu(Tensor([320, 64, 6554, 32],"float16"), 0.1, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([320, 64, 6554, 32],"float16"), 0.1, None, )
2025-03-15 22:54:44.137579 test begin: paddle.nn.functional.leaky_relu(Tensor([320, 64, 6964, 16],"float32"), 0.1, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([320, 64, 6964, 16],"float32"), 0.1, None, )
2025-03-15 22:58:17.804266 test begin: paddle.nn.functional.leaky_relu(Tensor([320, 6964, 32, 32],"float32"), 0.1, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([320, 6964, 32, 32],"float32"), 0.1, None, )
2025-03-15 23:01:12.389592 test begin: paddle.nn.functional.leaky_relu(Tensor([3450, 1024, 19, 34],"float32"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([3450, 1024, 19, 34],"float32"), 0.1, )
2025-03-15 23:04:09.444752 test begin: paddle.nn.functional.leaky_relu(Tensor([34817, 1024, 8, 8],"float32"), 0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([34817, 1024, 8, 8],"float32"), 0.2, )
2025-03-15 23:07:34.765253 test begin: paddle.nn.functional.leaky_relu(Tensor([34817, 64, 32, 32],"float32"), 0.1, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([34817, 64, 32, 32],"float32"), 0.1, None, )
2025-03-15 23:10:58.260349 test begin: paddle.nn.functional.leaky_relu(Tensor([39790, 32, 1792],"float32"), )

[Pass] paddle.nn.functional.leaky_relu(Tensor([39790, 32, 1792],"float32"), )
2025-03-15 23:14:03.980960 test begin: paddle.nn.functional.leaky_relu(Tensor([39790, 32, 1792],"float32"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([39790, 32, 1792],"float32"), 0.1, )
2025-03-15 23:17:02.656216 test begin: paddle.nn.functional.leaky_relu(Tensor([4, 570425345],"float32"), negative_slope=0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([4, 570425345],"float32"), negative_slope=0.2, )
2025-03-15 23:20:38.604220 test begin: paddle.nn.functional.leaky_relu(Tensor([42800, 128, 28, 28],"float16"), )

[Pass] paddle.nn.functional.leaky_relu(Tensor([42800, 128, 28, 28],"float16"), )
2025-03-15 23:45:01.466197 test begin: paddle.nn.functional.leaky_relu(Tensor([432, 128, 152, 272],"float32"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([432, 128, 152, 272],"float32"), 0.1, )
2025-03-15 23:48:21.650216 test begin: paddle.nn.functional.leaky_relu(Tensor([4353, 128, 64, 64],"float32"), 0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([4353, 128, 64, 64],"float32"), 0.2, )
2025-03-15 23:52:22.125359 test begin: paddle.nn.functional.leaky_relu(Tensor([4353, 128, 64, 64],"float32"), 0.2, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([4353, 128, 64, 64],"float32"), 0.2, None, )
2025-03-15 23:55:45.763508 test begin: paddle.nn.functional.leaky_relu(Tensor([4353, 512, 32, 32],"float32"), negative_slope=0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([4353, 512, 32, 32],"float32"), negative_slope=0.2, )
2025-03-15 23:59:05.405925 test begin: paddle.nn.functional.leaky_relu(Tensor([4456449, 16, 32],"float32"), 0.1, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([4456449, 16, 32],"float32"), 0.1, None, )
2025-03-16 00:02:57.063951 test begin: paddle.nn.functional.leaky_relu(Tensor([4456449, 2, 16, 16],"float32"), 0, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([4456449, 2, 16, 16],"float32"), 0, )
2025-03-16 00:06:30.484997 test begin: paddle.nn.functional.leaky_relu(Tensor([4456449, 512],"float32"), negative_slope=0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([4456449, 512],"float32"), negative_slope=0.2, )
2025-03-16 00:10:19.983736 test begin: paddle.nn.functional.leaky_relu(Tensor([45474, 1024, 7, 7],"float32"), 0.01, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([45474, 1024, 7, 7],"float32"), 0.01, None, )
2025-03-16 00:13:28.318756 test begin: paddle.nn.functional.leaky_relu(Tensor([45474, 256, 14, 14],"float32"), 0.01, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([45474, 256, 14, 14],"float32"), 0.01, None, )
2025-03-16 00:16:43.307446 test begin: paddle.nn.functional.leaky_relu(Tensor([4638, 512, 31, 31],"float32"), 0.2, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([4638, 512, 31, 31],"float32"), 0.2, None, )
2025-03-16 00:20:25.968923 test begin: paddle.nn.functional.leaky_relu(Tensor([477218589, 3, 3],"float16"), -1, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([477218589, 3, 3],"float16"), -1, None, )
2025-03-16 00:37:39.997056 test begin: paddle.nn.functional.leaky_relu(Tensor([477218589, 3, 3],"float16"), 0, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([477218589, 3, 3],"float16"), 0, None, )
2025-03-16 00:55:48.911207 test begin: paddle.nn.functional.leaky_relu(Tensor([477218589, 3, 3],"float16"), 0.01, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([477218589, 3, 3],"float16"), 0.01, None, )
2025-03-16 01:18:59.835370 test begin: paddle.nn.functional.leaky_relu(Tensor([545, 256, 128, 128],"float32"), negative_slope=0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([545, 256, 128, 128],"float32"), negative_slope=0.2, )
2025-03-16 01:22:05.017123 test begin: paddle.nn.functional.leaky_relu(Tensor([5685, 128, 56, 56],"float32"), 0.01, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([5685, 128, 56, 56],"float32"), 0.01, None, )
2025-03-16 01:24:50.346516 test begin: paddle.nn.functional.leaky_relu(Tensor([636636, 512, 7],"float32"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([636636, 512, 7],"float32"), 0.1, )
2025-03-16 01:27:41.388058 test begin: paddle.nn.functional.leaky_relu(Tensor([6493, 1024, 19, 34],"float16"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([6493, 1024, 19, 34],"float16"), 0.1, )
2025-03-16 01:44:40.129145 test begin: paddle.nn.functional.leaky_relu(Tensor([65537, 64, 32, 32],"float16"), 0.1, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([65537, 64, 32, 32],"float16"), 0.1, None, )
2025-03-16 02:01:46.651375 test begin: paddle.nn.functional.leaky_relu(Tensor([69633, 2048, 4, 4],"float32"), 0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([69633, 2048, 4, 4],"float32"), 0.2, )
2025-03-16 02:04:35.436465 test begin: paddle.nn.functional.leaky_relu(Tensor([79580, 64, 448],"float32"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([79580, 64, 448],"float32"), 0.1, )
2025-03-16 02:07:33.916070 test begin: paddle.nn.functional.leaky_relu(Tensor([8, 1024, 39790, 7],"float32"), 0.01, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([8, 1024, 39790, 7],"float32"), 0.01, None, )
2025-03-16 02:10:41.425822 test begin: paddle.nn.functional.leaky_relu(Tensor([8, 1024, 7, 39790],"float32"), 0.01, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([8, 1024, 7, 39790],"float32"), 0.01, None, )
2025-03-16 02:13:51.707105 test begin: paddle.nn.functional.leaky_relu(Tensor([8, 128, 28, 79580],"float32"), 0.01, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([8, 128, 28, 79580],"float32"), 0.01, None, )
2025-03-16 02:17:14.770522 test begin: paddle.nn.functional.leaky_relu(Tensor([8, 128, 39790, 56],"float32"), 0.01, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([8, 128, 39790, 56],"float32"), 0.01, None, )
2025-03-16 02:20:07.946579 test begin: paddle.nn.functional.leaky_relu(Tensor([8, 128, 56, 39790],"float32"), 0.01, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([8, 128, 56, 39790],"float32"), 0.01, None, )
2025-03-16 02:23:12.837167 test begin: paddle.nn.functional.leaky_relu(Tensor([8, 128, 79580, 28],"float32"), 0.01, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([8, 128, 79580, 28],"float32"), 0.01, None, )
2025-03-16 02:26:16.645737 test begin: paddle.nn.functional.leaky_relu(Tensor([8, 1455167, 14, 14],"float32"), 0.01, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([8, 1455167, 14, 14],"float32"), 0.01, None, )
2025-03-16 02:29:21.434586 test begin: paddle.nn.functional.leaky_relu(Tensor([8, 16, 17825793],"float32"), 0.1, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([8, 16, 17825793],"float32"), 0.1, None, )
2025-03-16 02:32:14.116950 test begin: paddle.nn.functional.leaky_relu(Tensor([8, 256, 14, 79580],"float32"), 0.01, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([8, 256, 14, 79580],"float32"), 0.01, None, )
2025-03-16 02:35:18.850980 test begin: paddle.nn.functional.leaky_relu(Tensor([8, 256, 28, 39790],"float32"), 0.01, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([8, 256, 28, 39790],"float32"), 0.01, None, )
2025-03-16 02:38:12.376666 test begin: paddle.nn.functional.leaky_relu(Tensor([8, 256, 39790, 28],"float32"), 0.01, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([8, 256, 39790, 28],"float32"), 0.01, None, )
2025-03-16 02:41:39.814774 test begin: paddle.nn.functional.leaky_relu(Tensor([8, 256, 79580, 14],"float32"), 0.01, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([8, 256, 79580, 14],"float32"), 0.01, None, )
2025-03-16 02:45:27.297241 test begin: paddle.nn.functional.leaky_relu(Tensor([8, 285212673],"float32"), negative_slope=0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([8, 285212673],"float32"), negative_slope=0.2, )
2025-03-16 02:48:22.882826 test begin: paddle.nn.functional.leaky_relu(Tensor([8, 363792, 28, 28],"float32"), 0.01, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([8, 363792, 28, 28],"float32"), 0.01, None, )
2025-03-16 02:51:51.722467 test begin: paddle.nn.functional.leaky_relu(Tensor([8, 5820667, 7, 7],"float32"), 0.01, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([8, 5820667, 7, 7],"float32"), 0.01, None, )
2025-03-16 02:54:49.270432 test begin: paddle.nn.functional.leaky_relu(Tensor([8, 8912897, 32],"float32"), 0.1, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([8, 8912897, 32],"float32"), 0.1, None, )
2025-03-16 02:57:48.643561 test begin: paddle.nn.functional.leaky_relu(Tensor([8, 90948, 56, 56],"float32"), 0.01, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([8, 90948, 56, 56],"float32"), 0.01, None, )
2025-03-16 03:00:42.056270 test begin: paddle.nn.functional.leaky_relu(Tensor([812, 128, 152, 272],"float16"), 0.1, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([812, 128, 152, 272],"float16"), 0.1, )
2025-03-16 03:19:03.743378 test begin: paddle.nn.functional.leaky_relu(Tensor([8705, 256, 32, 32],"float32"), 0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([8705, 256, 32, 32],"float32"), 0.2, )
2025-03-16 03:22:06.729135 test begin: paddle.nn.functional.leaky_relu(Tensor([8705, 256, 32, 32],"float32"), 0.2, None, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([8705, 256, 32, 32],"float32"), 0.2, None, )
2025-03-16 03:25:21.095140 test begin: paddle.nn.functional.leaky_relu(Tensor([8912897, 1, 16, 16],"float32"), 0.2, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([8912897, 1, 16, 16],"float32"), 0.2, )
2025-03-16 03:28:27.075826 test begin: paddle.nn.functional.leaky_relu(Tensor([8912897, 4, 8, 8],"float32"), 0, )

[Pass] paddle.nn.functional.leaky_relu(Tensor([8912897, 4, 8, 8],"float32"), 0, )
2025-03-16 03:31:30.752142 test begin: paddle.nn.functional.leaky_relu(x=Tensor([2281701379],"float32"), )

[Pass] paddle.nn.functional.leaky_relu(x=Tensor([2281701379],"float32"), )
2025-03-16 03:34:37.349573 test begin: paddle.nn.functional.leaky_relu(x=Tensor([4294967297],"float16"), )

[Pass] paddle.nn.functional.leaky_relu(x=Tensor([4294967297],"float16"), )
2025-03-16 03:58:09.538234 test begin: paddle.nn.functional.leaky_relu(x=Tensor([4294967297],"float16"), negative_slope=-100, )

[Pass] paddle.nn.functional.leaky_relu(x=Tensor([4294967297],"float16"), negative_slope=-100, )
2025-03-16 04:14:58.701361 test begin: paddle.nn.functional.leaky_relu(x=Tensor([4294967297],"float16"), negative_slope=0, )

[Pass] paddle.nn.functional.leaky_relu(x=Tensor([4294967297],"float16"), negative_slope=0, )
2025-03-16 04:33:05.513018 test begin: paddle.nn.functional.log_sigmoid(Tensor([10, 10, 22817014],"float32"), None, )

[Pass] paddle.nn.functional.log_sigmoid(Tensor([10, 10, 22817014],"float32"), None, )
2025-03-16 04:36:32.361562 test begin: paddle.nn.functional.log_sigmoid(Tensor([10, 10, 42949673],"float16"), None, )

[Pass] paddle.nn.functional.log_sigmoid(Tensor([10, 10, 42949673],"float16"), None, )
2025-03-16 04:53:33.473404 test begin: paddle.nn.functional.log_sigmoid(Tensor([10, 22817014, 10],"float32"), None, )

[Pass] paddle.nn.functional.log_sigmoid(Tensor([10, 22817014, 10],"float32"), None, )
2025-03-16 04:57:01.354740 test begin: paddle.nn.functional.log_sigmoid(Tensor([10, 42949673, 10],"float16"), None, )

[Pass] paddle.nn.functional.log_sigmoid(Tensor([10, 42949673, 10],"float16"), None, )
2025-03-16 05:13:38.622079 test begin: paddle.nn.functional.log_sigmoid(Tensor([2, 2147483649],"float16"), )

[Pass] paddle.nn.functional.log_sigmoid(Tensor([2, 2147483649],"float16"), )
2025-03-16 05:29:34.921000 test begin: paddle.nn.functional.log_sigmoid(Tensor([2281701379],"float32"), )

[Pass] paddle.nn.functional.log_sigmoid(Tensor([2281701379],"float32"), )
2025-03-16 05:32:33.764249 test begin: paddle.nn.functional.log_sigmoid(Tensor([22817014, 10, 10],"float32"), None, )

[Pass] paddle.nn.functional.log_sigmoid(Tensor([22817014, 10, 10],"float32"), None, )
2025-03-16 05:35:19.303000 test begin: paddle.nn.functional.log_sigmoid(Tensor([253522376, 3, 3],"float32"), None, )

[Pass] paddle.nn.functional.log_sigmoid(Tensor([253522376, 3, 3],"float32"), None, )
2025-03-16 05:38:08.829946 test begin: paddle.nn.functional.log_sigmoid(Tensor([3, 253522376, 3],"float32"), None, )

[Pass] paddle.nn.functional.log_sigmoid(Tensor([3, 253522376, 3],"float32"), None, )
2025-03-16 05:41:00.633033 test begin: paddle.nn.functional.log_sigmoid(Tensor([3, 3, 253522376],"float32"), None, )

[Pass] paddle.nn.functional.log_sigmoid(Tensor([3, 3, 253522376],"float32"), None, )
2025-03-16 05:43:51.787860 test begin: paddle.nn.functional.log_sigmoid(Tensor([3, 3, 477218589],"float16"), None, )

[Pass] paddle.nn.functional.log_sigmoid(Tensor([3, 3, 477218589],"float16"), None, )
2025-03-16 05:59:52.920408 test begin: paddle.nn.functional.log_sigmoid(Tensor([3, 477218589, 3],"float16"), None, )

[Pass] paddle.nn.functional.log_sigmoid(Tensor([3, 477218589, 3],"float16"), None, )
2025-03-16 06:15:53.993433 test begin: paddle.nn.functional.log_sigmoid(Tensor([4294967297, 1],"float16"), )

[Pass] paddle.nn.functional.log_sigmoid(Tensor([4294967297, 1],"float16"), )
2025-03-16 06:31:46.902292 test begin: paddle.nn.functional.log_sigmoid(Tensor([4294967297],"float16"), )

[Pass] paddle.nn.functional.log_sigmoid(Tensor([4294967297],"float16"), )
2025-03-16 06:47:49.046888 test begin: paddle.nn.functional.log_sigmoid(Tensor([42949673, 10, 10],"float16"), None, )

[Pass] paddle.nn.functional.log_sigmoid(Tensor([42949673, 10, 10],"float16"), None, )
2025-03-16 07:04:15.229246 test begin: paddle.nn.functional.log_sigmoid(Tensor([477218589, 3, 3],"float16"), None, )

[Pass] paddle.nn.functional.log_sigmoid(Tensor([477218589, 3, 3],"float16"), None, )
2025-03-16 07:20:35.670436 test begin: paddle.nn.functional.log_sigmoid(Tensor([5, 858993460],"float16"), )

[Pass] paddle.nn.functional.log_sigmoid(Tensor([5, 858993460],"float16"), )
2025-03-16 07:36:32.464198 test begin: paddle.nn.functional.log_sigmoid(Tensor([858993460, 5],"float16"), )

[Pass] paddle.nn.functional.log_sigmoid(Tensor([858993460, 5],"float16"), )
2025-03-16 07:52:52.563499 test begin: paddle.nn.functional.log_sigmoid(x=Tensor([10, 10, 22817014],"float32"), )

[Pass] paddle.nn.functional.log_sigmoid(x=Tensor([10, 10, 22817014],"float32"), )
2025-03-16 07:56:13.953462 test begin: paddle.nn.functional.log_sigmoid(x=Tensor([10, 10, 42949673],"float16"), )

[Pass] paddle.nn.functional.log_sigmoid(x=Tensor([10, 10, 42949673],"float16"), )
2025-03-16 08:13:04.353943 test begin: paddle.nn.functional.log_sigmoid(x=Tensor([10, 22817014, 10],"float32"), )

[Pass] paddle.nn.functional.log_sigmoid(x=Tensor([10, 22817014, 10],"float32"), )
2025-03-16 08:16:02.778273 test begin: paddle.nn.functional.log_sigmoid(x=Tensor([10, 42949673, 10],"float16"), )

[Pass] paddle.nn.functional.log_sigmoid(x=Tensor([10, 42949673, 10],"float16"), )
2025-03-16 08:32:10.020400 test begin: paddle.nn.functional.log_sigmoid(x=Tensor([2281701379],"float32"), )

[Pass] paddle.nn.functional.log_sigmoid(x=Tensor([2281701379],"float32"), )
2025-03-16 08:35:15.362976 test begin: paddle.nn.functional.log_sigmoid(x=Tensor([22817014, 10, 10],"float32"), )

[Pass] paddle.nn.functional.log_sigmoid(x=Tensor([22817014, 10, 10],"float32"), )
2025-03-16 08:38:32.378554 test begin: paddle.nn.functional.log_sigmoid(x=Tensor([4294967297],"float16"), )

[Pass] paddle.nn.functional.log_sigmoid(x=Tensor([4294967297],"float16"), )
2025-03-16 08:54:26.768548 test begin: paddle.nn.functional.log_sigmoid(x=Tensor([42949673, 10, 10],"float16"), )

[Pass] paddle.nn.functional.log_sigmoid(x=Tensor([42949673, 10, 10],"float16"), )
2025-03-16 09:10:35.180086 test begin: paddle.nn.functional.mish(Tensor([11142, 128, 40, 40],"float32"), )

[Pass] paddle.nn.functional.mish(Tensor([11142, 128, 40, 40],"float32"), )
2025-03-16 09:13:57.177925 test begin: paddle.nn.functional.mish(Tensor([12, 1024, 10, 18569],"float32"), )

[Pass] paddle.nn.functional.mish(Tensor([12, 1024, 10, 18569],"float32"), )
2025-03-16 09:17:38.832763 test begin: paddle.nn.functional.mish(Tensor([12, 1024, 10, 34953],"float16"), )

[Pass] paddle.nn.functional.mish(Tensor([12, 1024, 10, 34953],"float16"), )
2025-03-16 09:33:54.016417 test begin: paddle.nn.functional.mish(Tensor([12, 1024, 18569, 10],"float32"), )

[Pass] paddle.nn.functional.mish(Tensor([12, 1024, 18569, 10],"float32"), )
2025-03-16 09:36:45.012486 test begin: paddle.nn.functional.mish(Tensor([12, 1024, 34953, 10],"float16"), )

[Pass] paddle.nn.functional.mish(Tensor([12, 1024, 34953, 10],"float16"), )
2025-03-16 09:52:58.908845 test begin: paddle.nn.functional.mish(Tensor([12, 118839, 40, 40],"float32"), )

[Pass] paddle.nn.functional.mish(Tensor([12, 118839, 40, 40],"float32"), )
2025-03-16 09:55:43.796828 test begin: paddle.nn.functional.mish(Tensor([12, 128, 37138, 40],"float32"), )

[Pass] paddle.nn.functional.mish(Tensor([12, 128, 37138, 40],"float32"), )
2025-03-16 09:58:28.694939 test begin: paddle.nn.functional.mish(Tensor([12, 128, 40, 37138],"float32"), )

[Pass] paddle.nn.functional.mish(Tensor([12, 128, 40, 37138],"float32"), )
2025-03-16 10:01:15.301661 test begin: paddle.nn.functional.mish(Tensor([12, 128, 40, 69906],"float16"), )

[Pass] paddle.nn.functional.mish(Tensor([12, 128, 40, 69906],"float16"), )
2025-03-16 10:17:42.020706 test begin: paddle.nn.functional.mish(Tensor([12, 128, 69906, 40],"float16"), )

[Pass] paddle.nn.functional.mish(Tensor([12, 128, 69906, 40],"float16"), )
2025-03-16 10:34:00.031473 test begin: paddle.nn.functional.mish(Tensor([12, 1901418, 10, 10],"float32"), )

[Pass] paddle.nn.functional.mish(Tensor([12, 1901418, 10, 10],"float32"), )
2025-03-16 10:37:43.295537 test begin: paddle.nn.functional.mish(Tensor([12, 223697, 40, 40],"float16"), )

[Pass] paddle.nn.functional.mish(Tensor([12, 223697, 40, 40],"float16"), )
2025-03-16 10:54:47.969520 test begin: paddle.nn.functional.mish(Tensor([12, 256, 20, 69906],"float16"), )

[Pass] paddle.nn.functional.mish(Tensor([12, 256, 20, 69906],"float16"), )
2025-03-16 11:11:44.277086 test begin: paddle.nn.functional.mish(Tensor([12, 256, 69906, 20],"float16"), )

[Pass] paddle.nn.functional.mish(Tensor([12, 256, 69906, 20],"float16"), )
2025-03-16 11:28:17.237298 test begin: paddle.nn.functional.mish(Tensor([12, 3579140, 10, 10],"float16"), )

[Pass] paddle.nn.functional.mish(Tensor([12, 3579140, 10, 10],"float16"), )
2025-03-16 11:44:57.460669 test begin: paddle.nn.functional.mish(Tensor([12, 894785, 20, 20],"float16"), )

[Pass] paddle.nn.functional.mish(Tensor([12, 894785, 20, 20],"float16"), )
2025-03-16 12:01:05.515001 test begin: paddle.nn.functional.mish(Tensor([142606337, 16],"float32"), name="mish", )

[Pass] paddle.nn.functional.mish(Tensor([142606337, 16],"float32"), name="mish", )
2025-03-16 12:04:11.132002 test begin: paddle.nn.functional.mish(Tensor([2, 1140850690],"float32"), name="mish", )

[Pass] paddle.nn.functional.mish(Tensor([2, 1140850690],"float32"), name="mish", )
2025-03-16 12:07:11.592597 test begin: paddle.nn.functional.mish(Tensor([20972, 128, 40, 40],"float16"), )

[Pass] paddle.nn.functional.mish(Tensor([20972, 128, 40, 40],"float16"), )
2025-03-16 12:23:31.597749 test begin: paddle.nn.functional.mish(Tensor([22283, 1024, 10, 10],"float32"), )

[Pass] paddle.nn.functional.mish(Tensor([22283, 1024, 10, 10],"float32"), )
2025-03-16 12:26:28.245304 test begin: paddle.nn.functional.mish(Tensor([41944, 1024, 10, 10],"float16"), )

[Pass] paddle.nn.functional.mish(Tensor([41944, 1024, 10, 10],"float16"), )
2025-03-16 12:43:17.260714 test begin: paddle.nn.functional.mish(Tensor([41944, 256, 20, 20],"float16"), )

[Pass] paddle.nn.functional.mish(Tensor([41944, 256, 20, 20],"float16"), )
2025-03-16 12:59:51.474468 test begin: paddle.nn.functional.normalize(Tensor([1, 128, 32, 557057],"float32"), axis=1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742101290 (unix time) try "date -d @1742101290" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x240c0) received by PID 147648 (TID 0x7f0def5fe700) from PID 147648 ***]

2025-03-16 13:02:12.109493 test begin: paddle.nn.functional.normalize(Tensor([1, 128, 557057, 32],"float32"), axis=1, )

W0316 13:03:37.415066 159976 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 13:03:37.416855 159976 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742101495 (unix time) try "date -d @1742101495" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x27091) received by PID 159889 (TID 0x7f848a949700) from PID 159889 ***]

2025-03-16 13:05:40.715787 test begin: paddle.nn.functional.normalize(Tensor([1, 2228225, 32, 32],"float32"), axis=1, )

W0316 13:07:06.949978 160071 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 13:07:06.951438 160071 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742101707 (unix time) try "date -d @1742101707" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x270f0) received by PID 159984 (TID 0x7f31bb4f4700) from PID 159984 ***]

2025-03-16 13:09:12.177859 test begin: paddle.nn.functional.normalize(Tensor([1, 2281701379],"float32"), axis=1, )

W0316 13:10:51.316584 160162 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 13:10:51.317767 160162 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.nn.functional.normalize(Tensor([1, 2281701379],"float32"), axis=1, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:117)

2025-03-16 13:10:52.368350 test begin: paddle.nn.functional.normalize(Tensor([1, 256, 16, 557057],"float32"), axis=1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742101951 (unix time) try "date -d @1742101951" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2714b) received by PID 160075 (TID 0x7f657a7c3700) from PID 160075 ***]

2025-03-16 13:13:17.617543 test begin: paddle.nn.functional.normalize(Tensor([1, 256, 557057, 16],"float32"), axis=1, )

W0316 13:14:41.911846 160257 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 13:14:41.913129 160257 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742102161 (unix time) try "date -d @1742102161" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x271aa) received by PID 160170 (TID 0x7f0505dc2700) from PID 160170 ***]

2025-03-16 13:16:44.370328 test begin: paddle.nn.functional.normalize(Tensor([1, 557057, 64, 64],"float32"), axis=1, )

W0316 13:18:11.754478 160354 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 13:18:11.755622 160354 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742102362 (unix time) try "date -d @1742102362" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2720b) received by PID 160267 (TID 0x7f5e29f48700) from PID 160267 ***]

2025-03-16 13:20:04.896012 test begin: paddle.nn.functional.normalize(Tensor([1, 64, 557057, 64],"float32"), axis=1, )

W0316 13:21:30.774034 160456 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 13:21:30.775198 160456 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742102575 (unix time) try "date -d @1742102575" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2726e) received by PID 160366 (TID 0x7fd2a3abb700) from PID 160366 ***]

2025-03-16 13:23:43.300305 test begin: paddle.nn.functional.normalize(Tensor([1, 64, 64, 557057],"float32"), axis=1, )

W0316 13:25:08.132077 160550 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 13:25:08.133325 160550 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742102777 (unix time) try "date -d @1742102777" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x272cf) received by PID 160463 (TID 0x7fe588949700) from PID 160463 ***]

2025-03-16 13:27:00.263610 test begin: paddle.nn.functional.normalize(Tensor([1, 8912897, 16, 16],"float32"), axis=1, )

W0316 13:28:26.259919 160646 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 13:28:26.261142 160646 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742102997 (unix time) try "date -d @1742102997" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2732c) received by PID 160556 (TID 0x7fe234949700) from PID 160556 ***]

2025-03-16 13:30:38.714962 test begin: paddle.nn.functional.normalize(Tensor([10, 228170138],"float32"), )

W0316 13:32:03.131657 160738 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 13:32:03.132886 160738 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742103313 (unix time) try "date -d @1742103313" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2738b) received by PID 160651 (TID 0x7fbc36949700) from PID 160651 ***]

2025-03-16 13:35:56.170397 test begin: paddle.nn.functional.normalize(Tensor([10, 228170138],"float32"), axis=0, )

W0316 13:37:33.119755 160850 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 13:37:33.120971 160850 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742103540 (unix time) try "date -d @1742103540" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x273fb) received by PID 160763 (TID 0x7f79ef87e700) from PID 160763 ***]

2025-03-16 13:39:47.557209 test begin: paddle.nn.functional.normalize(Tensor([10, 228170138],"float32"), p=1.5, )

W0316 13:41:15.728665 160955 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 13:41:15.729912 160955 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742103864 (unix time) try "date -d @1742103864" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x27461) received by PID 160865 (TID 0x7f4762949700) from PID 160865 ***]

2025-03-16 13:45:09.784627 test begin: paddle.nn.functional.normalize(Tensor([10, 429496730],"float16"), )

W0316 13:46:48.761991 161066 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 13:46:48.763131 161066 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.nn.functional.normalize(Tensor([10, 429496730],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2343049977 / 4294967300 (54.6%)
Max absolute difference: 0.02202
Max relative difference: 395.
 x: array([[-0.00843 ,  0.007565,  0.0177  , ..., -0.016   , -0.000625,
         0.01819 ],
       [-0.002844, -0.003262, -0.01523 , ..., -0.01482 ,  0.000542,...
 y: array([[-3.189e-05,  2.861e-05,  6.694e-05, ..., -6.050e-05, -2.384e-06,
         6.878e-05],
       [-1.073e-05, -1.234e-05, -5.758e-05, ..., -5.603e-05,  2.027e-06,...
2025-03-16 14:11:03.454108 test begin: paddle.nn.functional.normalize(Tensor([1006, 2268093],"float32"), )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742105632 (unix time) try "date -d @1742105632" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x274d3) received by PID 160979 (TID 0x7f0697f48700) from PID 160979 ***]

2025-03-16 14:14:37.714908 test begin: paddle.nn.functional.normalize(Tensor([1006, 4269352],"float16"), )

W0316 14:16:17.043977 161201 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 14:16:17.045709 161201 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.nn.functional.normalize(Tensor([1006, 4269352],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2271748578 / 4294968112 (52.9%)
Max absolute difference: 0.02126
Max relative difference: 39.
 x: array([[ 0.005257,  0.01055 , -0.001193, ...,  0.01333 , -0.02167 ,
        -0.01765 ],
       [-0.02051 , -0.004444, -0.01087 , ..., -0.01012 , -0.01346 ,...
 y: array([[ 1.994e-04,  4.001e-04, -4.524e-05, ...,  5.054e-04, -8.221e-04,
        -6.695e-04],
       [-7.777e-04, -1.686e-04, -4.125e-04, ..., -3.836e-04, -5.102e-04,...
2025-03-16 14:34:28.357958 test begin: paddle.nn.functional.normalize(Tensor([11883862, 192],"float32"), axis=1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742107082 (unix time) try "date -d @1742107082" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2755a) received by PID 161114 (TID 0x7ff54ddc2700) from PID 161114 ***]

2025-03-16 14:38:49.194833 test begin: paddle.nn.functional.normalize(Tensor([12, 190141782],"float32"), axis=-1, )

W0316 14:40:15.240255 161347 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 14:40:15.241436 161347 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742107386 (unix time) try "date -d @1742107386" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x275ec) received by PID 161260 (TID 0x7f6b9c949700) from PID 161260 ***]

2025-03-16 14:43:54.986276 test begin: paddle.nn.functional.normalize(Tensor([17409, 128, 32, 32],"float32"), axis=1, )

W0316 14:45:23.230690 161451 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 14:45:23.231940 161451 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742107610 (unix time) try "date -d @1742107610" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x27654) received by PID 161364 (TID 0x7fa26c7c3700) from PID 161364 ***]

2025-03-16 14:47:35.965534 test begin: paddle.nn.functional.normalize(Tensor([2, 16297867, 7, 10],"float32"), axis=1, )

W0316 14:49:00.003000 161542 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 14:49:00.004243 161542 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742107836 (unix time) try "date -d @1742107836" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x276af) received by PID 161455 (TID 0x7f6383137700) from PID 161455 ***]

2025-03-16 14:51:21.083283 test begin: paddle.nn.functional.normalize(Tensor([2, 2147483649],"float16"), p=2, axis=-1, )

W0316 14:53:09.389818 161635 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 14:53:09.391039 161635 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.nn.functional.normalize(Tensor([2, 2147483649],"float16"), p=2, axis=-1, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:117)

2025-03-16 14:53:10.247563 test begin: paddle.nn.functional.normalize(Tensor([2, 8, 14260634, 10],"float32"), axis=1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742108177 (unix time) try "date -d @1742108177" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2770c) received by PID 161548 (TID 0x7f372bdc2700) from PID 161548 ***]

2025-03-16 14:57:03.610106 test begin: paddle.nn.functional.normalize(Tensor([2, 8, 7, 20372334],"float32"), axis=1, )

W0316 14:58:30.593077 161734 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 14:58:30.594504 161734 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742108391 (unix time) try "date -d @1742108391" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2776e) received by PID 161646 (TID 0x7fb8ffd0b700) from PID 161646 ***]

2025-03-16 15:00:36.160964 test begin: paddle.nn.functional.normalize(Tensor([207427399, 11],"float32"), axis=1, )

W0316 15:02:01.795697 161834 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 15:02:01.796928 161834 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742108600 (unix time) try "date -d @1742108600" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x277d3) received by PID 161747 (TID 0x7f1e49f48700) from PID 161747 ***]

2025-03-16 15:04:05.049731 test begin: paddle.nn.functional.normalize(Tensor([2281701379],"float32"), axis=0, )

W0316 15:05:31.274627 161925 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 15:05:31.275844 161925 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.nn.functional.normalize(Tensor([2281701379],"float32"), axis=0, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:117)

2025-03-16 15:05:32.437120 test begin: paddle.nn.functional.normalize(Tensor([2281701379],"float32"), axis=0, epsilon=1e-10, )

[paddle error] paddle.nn.functional.normalize(Tensor([2281701379],"float32"), axis=0, epsilon=1e-10, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:117)

2025-03-16 15:05:53.180441 test begin: paddle.nn.functional.normalize(Tensor([2281701379],"float32"), axis=0, epsilon=1e-12, )

[paddle error] paddle.nn.functional.normalize(Tensor([2281701379],"float32"), axis=0, epsilon=1e-12, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:117)

2025-03-16 15:06:24.091959 test begin: paddle.nn.functional.normalize(Tensor([228170138, 10],"float32"), )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742108901 (unix time) try "date -d @1742108901" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2782e) received by PID 161838 (TID 0x7faa88949700) from PID 161838 ***]

2025-03-16 15:09:06.059237 test begin: paddle.nn.functional.normalize(Tensor([228170138, 10],"float32"), axis=0, )

W0316 15:10:44.447283 162020 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 15:10:44.448547 162020 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742109261 (unix time) try "date -d @1742109261" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2788d) received by PID 161933 (TID 0x7f18b6949700) from PID 161933 ***]

2025-03-16 15:15:04.078496 test begin: paddle.nn.functional.normalize(Tensor([228170138, 10],"float32"), p=1.5, )

W0316 15:16:40.491727 162118 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 15:16:40.493034 162118 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742109495 (unix time) try "date -d @1742109495" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x278ef) received by PID 162031 (TID 0x7f43d6949700) from PID 162031 ***]

2025-03-16 15:18:58.926304 test begin: paddle.nn.functional.normalize(Tensor([253522376, 9],"float32"), axis=1, )

W0316 15:20:24.475100 162211 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 15:20:24.476250 162211 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742109701 (unix time) try "date -d @1742109701" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2794c) received by PID 162124 (TID 0x7f7a0f2b7700) from PID 162124 ***]

2025-03-16 15:22:24.617467 test begin: paddle.nn.functional.normalize(Tensor([2970966, 768],"float32"), axis=-1, )

W0316 15:23:51.087054 162303 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 15:23:51.088327 162303 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742109918 (unix time) try "date -d @1742109918" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x279a8) received by PID 162216 (TID 0x7fa0f7dc2700) from PID 162216 ***]

2025-03-16 15:26:05.882642 test begin: paddle.nn.functional.normalize(Tensor([325957340, 7],"float32"), axis=0, )

W0316 15:27:29.295279 162394 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 15:27:29.297096 162394 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742110299 (unix time) try "date -d @1742110299" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x27a03) received by PID 162307 (TID 0x7fd571277700) from PID 162307 ***]

2025-03-16 15:32:24.973698 test begin: paddle.nn.functional.normalize(Tensor([325957340, 7],"float32"), axis=1, )

W0316 15:33:51.823141 162495 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 15:33:51.824395 162495 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742110508 (unix time) try "date -d @1742110508" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x27a68) received by PID 162408 (TID 0x7f43a2949700) from PID 162408 ***]

2025-03-16 15:35:53.830108 test begin: paddle.nn.functional.normalize(Tensor([34817, 256, 16, 16],"float32"), axis=1, )

W0316 15:37:20.881235 162586 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 15:37:20.883265 162586 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742110723 (unix time) try "date -d @1742110723" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x27ac3) received by PID 162499 (TID 0x7f021a7c3700) from PID 162499 ***]

2025-03-16 15:39:25.297428 test begin: paddle.nn.functional.normalize(Tensor([35651585, 64],"float32"), axis=-1, )

W0316 15:41:12.696678 162677 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 15:41:12.698041 162677 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742110957 (unix time) try "date -d @1742110957" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x27b1e) received by PID 162590 (TID 0x7f354bb85700) from PID 162590 ***]

2025-03-16 15:43:25.594801 test begin: paddle.nn.functional.normalize(Tensor([35651585, 64],"float32"), axis=1, )

W0316 15:44:59.193768 162767 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 15:44:59.196560 162767 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742111197 (unix time) try "date -d @1742111197" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x27b78) received by PID 162680 (TID 0x7fc555dc2700) from PID 162680 ***]

2025-03-16 15:47:20.141802 test begin: paddle.nn.functional.normalize(Tensor([4, 570425345],"float32"), axis=0, )

W0316 15:48:45.387516 162860 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 15:48:45.388995 162860 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742111432 (unix time) try "date -d @1742111432" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x27bd5) received by PID 162773 (TID 0x7fcc847c3700) from PID 162773 ***]

2025-03-16 15:51:16.208203 test begin: paddle.nn.functional.normalize(Tensor([4074467, 8, 7, 10],"float32"), axis=1, )

W0316 15:52:48.285199 162955 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 15:52:48.286409 162955 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742111666 (unix time) try "date -d @1742111666" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x27c34) received by PID 162868 (TID 0x7ff4b7d0b700) from PID 162868 ***]

2025-03-16 15:55:11.514184 test begin: paddle.nn.functional.normalize(Tensor([4194305, 1024],"float16"), p=2, axis=-1, )

W0316 15:56:59.677018 163053 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 15:56:59.678335 163053 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<phi::dtype::float16, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742112366 (unix time) try "date -d @1742112366" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x27c96) received by PID 162966 (TID 0x7f74fff48700) from PID 162966 ***]

2025-03-16 16:06:48.270806 test begin: paddle.nn.functional.normalize(Tensor([4456449, 512],"float32"), )

W0316 16:08:13.799343 163152 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 16:08:13.800472 163152 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742112586 (unix time) try "date -d @1742112586" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x27cf9) received by PID 163065 (TID 0x7f3219f48700) from PID 163065 ***]

2025-03-16 16:10:33.139676 test begin: paddle.nn.functional.normalize(Tensor([45, 50704476],"float32"), axis=0, )

W0316 16:11:57.897145 163246 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 16:11:57.898391 163246 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742112793 (unix time) try "date -d @1742112793" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x27d57) received by PID 163159 (TID 0x7fc19f1f7700) from PID 163159 ***]

2025-03-16 16:14:01.221344 test begin: paddle.nn.functional.normalize(Tensor([456340276, 5],"float32"), axis=0, )

W0316 16:15:25.611322 163340 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 16:15:25.612444 163340 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742113230 (unix time) try "date -d @1742113230" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x27db5) received by PID 163253 (TID 0x7f8b367c3700) from PID 163253 ***]

2025-03-16 16:21:15.666853 test begin: paddle.nn.functional.normalize(Tensor([570425345, 4],"float32"), axis=0, )

W0316 16:22:50.795109 163451 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 16:22:50.796896 163451 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742113739 (unix time) try "date -d @1742113739" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x27e24) received by PID 163364 (TID 0x7f87c7f48700) from PID 163364 ***]

2025-03-16 16:29:42.968452 test begin: paddle.nn.functional.normalize(Tensor([60, 38028357],"float32"), axis=0, )

W0316 16:31:07.350068 163557 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 16:31:07.351276 163557 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742113935 (unix time) try "date -d @1742113935" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x27e8e) received by PID 163470 (TID 0x7f32c12b7700) from PID 163470 ***]

2025-03-16 16:32:58.766904 test begin: paddle.nn.functional.normalize(Tensor([760567127, 3],"float32"), axis=0, )

W0316 16:34:26.638661 163657 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 16:34:26.639827 163657 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742114452 (unix time) try "date -d @1742114452" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x27ef2) received by PID 163570 (TID 0x7f3c37277700) from PID 163570 ***]

2025-03-16 16:41:39.076451 test begin: paddle.nn.functional.normalize(Tensor([80, 28521268],"float32"), axis=-1, )

W0316 16:43:22.202879 163782 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 16:43:22.204161 163782 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742114702 (unix time) try "date -d @1742114702" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x27f6e) received by PID 163694 (TID 0x7f106a949700) from PID 163694 ***]

2025-03-16 16:45:45.708676 test begin: paddle.nn.functional.normalize(Tensor([8388609, 512],"float16"), )

W0316 16:47:28.335803   344 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 16:47:28.337095   344 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<phi::dtype::float16, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742115381 (unix time) try "date -d @1742115381" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x27fd4) received by PID 163796 (TID 0x7f8f836f8700) from PID 163796 ***]

2025-03-16 16:57:06.697690 test begin: paddle.nn.functional.normalize(Tensor([8705, 64, 64, 64],"float32"), axis=1, )

W0316 16:58:38.926659   475 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 16:58:38.927881   475 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742115632 (unix time) try "date -d @1742115632" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x177) received by PID 375 (TID 0x7f6cb3935700) from PID 375 ***]

2025-03-16 17:01:17.781188 test begin: paddle.nn.functional.normalize(x=Tensor([1, 2281701379],"float32"), axis=-1, )

W0316 17:02:50.193709   573 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 17:02:50.194880   573 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.nn.functional.normalize(x=Tensor([1, 2281701379],"float32"), axis=-1, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:117)

2025-03-16 17:02:51.355838 test begin: paddle.nn.functional.normalize(x=Tensor([1073741825, 4],"float16"), )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<phi::dtype::float16, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742116352 (unix time) try "date -d @1742116352" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1e6) received by PID 486 (TID 0x7f1971abb700) from PID 486 ***]

2025-03-16 17:13:19.496677 test begin: paddle.nn.functional.normalize(x=Tensor([143165577, 5, 6],"float16"), )

W0316 17:14:59.452838   713 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 17:14:59.454095   713 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<phi::dtype::float16, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742116967 (unix time) try "date -d @1742116967" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x265) received by PID 613 (TID 0x7f945787e700) from PID 613 ***]

2025-03-16 17:23:34.163474 test begin: paddle.nn.functional.normalize(x=Tensor([2, 1140850690],"float32"), )

W0316 17:25:00.174459   912 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 17:25:00.175642   912 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742117623 (unix time) try "date -d @1742117623" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x339) received by PID 825 (TID 0x7fe2f07c3700) from PID 825 ***]

2025-03-16 17:34:31.412487 test begin: paddle.nn.functional.normalize(x=Tensor([2, 2147483649],"float16"), )

W0316 17:36:10.664855  1035 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 17:36:10.666067  1035 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.nn.functional.normalize(x=Tensor([2, 2147483649],"float16"), ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:117)

2025-03-16 17:36:11.556414 test begin: paddle.nn.functional.normalize(x=Tensor([20452226, 5, 6, 7],"float16"), )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<phi::dtype::float16, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742118268 (unix time) try "date -d @1742118268" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x3b4) received by PID 948 (TID 0x7fdd56949700) from PID 948 ***]

2025-03-16 17:45:10.629033 test begin: paddle.nn.functional.normalize(x=Tensor([20452226, 5, 6, 7],"float16"), p=1, )

W0316 17:46:52.176689  1153 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 17:46:52.177955  1153 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<phi::dtype::float16, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742118885 (unix time) try "date -d @1742118885" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x42a) received by PID 1066 (TID 0x7feae5f48700) from PID 1066 ***]

2025-03-16 17:55:29.170284 test begin: paddle.nn.functional.normalize(x=Tensor([20452226, 5, 6, 7],"float16"), p=4, )

W0316 17:57:32.907292  2865 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 17:57:32.908524  2865 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.nn.functional.normalize(x=Tensor([20452226, 5, 6, 7],"float16"), p=4, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[[[-6.3965e-01, -5.1074e-01, -8.6328e-01, ..., -4.9414e-01,
           7.9590e-01,  9.7168e-01],
         [ 5.2197e-01, -2.0178e-01, -6.6846e-01, ..., -8.6230e-01,...
 y: array([[[[-6.3965e-01, -5.1074e-01, -8.6328e-01, ..., -4.9414e-01,
           7.9590e-01,  9.7168e-01],
         [ 5.2197e-01, -2.0178e-01, -6.6846e-01, ..., -8.6230e-01,...
2025-03-16 17:59:28.478907 test begin: paddle.nn.functional.normalize(x=Tensor([20452226, 5, 6, 7],"float16"), p=4, axis=3, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<phi::dtype::float16, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742119667 (unix time) try "date -d @1742119667" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xad9) received by PID 2777 (TID 0x7f0e0fdc2700) from PID 2777 ***]

2025-03-16 18:08:32.040428 test begin: paddle.nn.functional.normalize(x=Tensor([2147483649, 2],"float16"), p=1.2, )

W0316 18:10:17.405480 13164 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 18:10:17.406916 13164 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.nn.functional.normalize(x=Tensor([2147483649, 2],"float16"), p=1.2, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:117)

2025-03-16 18:10:18.751401 test begin: paddle.nn.functional.normalize(x=Tensor([2970966, 768],"float32"), axis=-1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742119990 (unix time) try "date -d @1742119990" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x32cc) received by PID 13004 (TID 0x7fbf692b7700) from PID 13004 ***]

2025-03-16 18:13:53.276752 test begin: paddle.nn.functional.normalize(x=Tensor([3, 1431655766],"float16"), )

W0316 18:15:33.340917 19160 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 18:15:33.342116 19160 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 101, in func_timeout
    raise FunctionTimedOut('', timeout, func, args, kwargs)
func_timeout.exceptions.FunctionTimedOut: Function test (args=(<tester.accuracy.APITestAccuracy object at 0x7f38eda6c250>,)) (kwargs={}) timed out after 1800.000000 seconds.

2025-03-16 18:44:03.616361 test begin: paddle.nn.functional.normalize(x=Tensor([4, 1073741825],"float16"), p=1.2, )

W0316 18:45:44.391780 53564 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 18:45:44.392993 53564 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<phi::dtype::float16, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742122851 (unix time) try "date -d @1742122851" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xd0a5) received by PID 53413 (TID 0x7f01167c3700) from PID 53413 ***]

2025-03-16 19:01:38.461522 test begin: paddle.nn.functional.normalize(x=Tensor([4, 178956971, 6],"float16"), )

W0316 19:03:22.826279 74024 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 19:03:22.827418 74024 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.nn.functional.normalize(x=Tensor([4, 178956971, 6],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2338864060 / 4294967304 (54.5%)
Max absolute difference: 0.02197
Max relative difference: 255.
 x: array([[[-0.02103 ,  0.01657 ,  0.01322 ,  0.015045,  0.01156 ,
         -0.013626],
        [ 0.01175 , -0.00243 ,  0.002094,  0.01962 , -0.02121 ,...
 y: array([[[-1.2326e-04,  9.7036e-05,  7.7426e-05,  8.8096e-05,
          6.7711e-05, -7.9870e-05],
        [ 6.8843e-05, -1.4246e-05,  1.2279e-05,  1.1498e-04,...
2025-03-16 19:25:46.341171 test begin: paddle.nn.functional.normalize(x=Tensor([4, 25565282, 6, 7],"float16"), )

[accuracy error] paddle.nn.functional.normalize(x=Tensor([4, 25565282, 6, 7],"float16"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2318939113 / 4294967376 (54%)
Max absolute difference: 0.02176
Max relative difference: 96.
 x: array([[[[-2.1027e-02,  1.6571e-02,  1.3222e-02, ...,  1.1559e-02,
          -1.3626e-02,  1.1749e-02],
         [-2.4300e-03,  2.0943e-03,  1.9623e-02, ...,  1.8585e-02,...
 y: array([[[[-3.2592e-04,  2.5678e-04,  2.0504e-04, ...,  1.7905e-04,
          -2.1124e-04,  1.8215e-04],
         [-3.7670e-05,  3.2425e-05,  3.0446e-04, ...,  2.8825e-04,...
2025-03-16 19:45:40.242835 test begin: paddle.nn.functional.normalize(x=Tensor([4, 25565282, 6, 7],"float16"), p=1, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<phi::dtype::float16, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742126082 (unix time) try "date -d @1742126082" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x12086) received by PID 73862 (TID 0x7fe27dd0b700) from PID 73862 ***]

2025-03-16 19:55:29.948959 test begin: paddle.nn.functional.normalize(x=Tensor([4, 25565282, 6, 7],"float16"), p=4, )

W0316 19:57:10.188402 134976 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 19:57:10.189898 134976 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.nn.functional.normalize(x=Tensor([4, 25565282, 6, 7],"float16"), p=4, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 3957720401 / 4294967376 (92.1%)
Max absolute difference: 0.1277
Max relative difference: 9.
 x: array([[[[ 1.4856e-01,  6.4148e-02, -8.2031e-02, ..., -9.4421e-02,
          -4.2389e-02,  1.2720e-01],
         [ 1.0577e-01,  9.8389e-02,  6.9824e-02, ..., -9.3445e-02,...
 y: array([[[[ 2.1011e-02,  9.0790e-03, -1.1604e-02, ..., -1.3367e-02,
          -5.9967e-03,  1.7990e-02],
         [ 1.4961e-02,  1.3908e-02,  9.8801e-03, ..., -1.3214e-02,...
2025-03-16 20:11:54.479606 test begin: paddle.nn.functional.normalize(x=Tensor([4, 25565282, 6, 7],"float16"), p=4, axis=3, )

[accuracy error] paddle.nn.functional.normalize(x=Tensor([4, 25565282, 6, 7],"float16"), p=4, axis=3, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 7 / 4294967376 (1.63e-07%)
Max absolute difference: 0.1045
Max relative difference: 0.12054
 x: array([[[[ 8.1250e-01,  3.5107e-01, -4.4873e-01, ..., -5.1660e-01,
          -2.3193e-01,  6.9629e-01],
         [ 7.2217e-01,  6.7139e-01,  4.7681e-01, ..., -6.3770e-01,...
 y: array([[[[ 8.1250e-01,  3.5107e-01, -4.4873e-01, ..., -5.1660e-01,
          -2.3193e-01,  6.9629e-01],
         [ 7.2217e-01,  6.7139e-01,  4.7681e-01, ..., -6.3770e-01,...
2025-03-16 20:24:34.259054 test begin: paddle.nn.functional.normalize(x=Tensor([4, 5, 214748365],"float16"), )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<phi::dtype::float16, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742128368 (unix time) try "date -d @1742128368" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x20e9e) received by PID 134814 (TID 0x7f486a7c3700) from PID 134814 ***]

2025-03-16 20:33:32.267528 test begin: paddle.nn.functional.normalize(x=Tensor([4, 5, 30678338, 7],"float16"), )

W0316 20:35:10.244431 14761 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 20:35:10.245724 14761 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<phi::dtype::float16, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742128979 (unix time) try "date -d @1742128979" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x3907) received by PID 14599 (TID 0x7fd355dc2700) from PID 14599 ***]

2025-03-16 20:43:43.162967 test begin: paddle.nn.functional.normalize(x=Tensor([4, 5, 30678338, 7],"float16"), p=1, )

W0316 20:45:24.481719 26428 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 20:45:24.482985 26428 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<phi::dtype::float16, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742129602 (unix time) try "date -d @1742129602" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x669a) received by PID 26266 (TID 0x7f6373f48700) from PID 26266 ***]

2025-03-16 20:54:03.881604 test begin: paddle.nn.functional.normalize(x=Tensor([4, 5, 30678338, 7],"float16"), p=4, )

W0316 20:55:43.966698 38157 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 20:55:43.968097 38157 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.nn.functional.normalize(x=Tensor([4, 5, 30678338, 7],"float16"), p=4, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[[[ 1.1407e-01, -7.0264e-01,  1.3989e-01, ..., -1.0834e-02,
           6.4795e-01,  1.9604e-01],
         [ 7.8906e-01,  4.3384e-01,  1.3269e-01, ...,  6.3965e-01,...
 y: array([[[[ 1.1407e-01, -7.0264e-01,  1.3989e-01, ..., -1.0834e-02,
           6.4795e-01,  1.9604e-01],
         [ 7.8906e-01,  4.3384e-01,  1.3269e-01, ...,  6.3965e-01,...
2025-03-16 20:57:37.624257 test begin: paddle.nn.functional.normalize(x=Tensor([4, 5, 30678338, 7],"float16"), p=4, axis=3, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<phi::dtype::float16, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742130373 (unix time) try "date -d @1742130373" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x946f) received by PID 37999 (TID 0x7f70e334a700) from PID 37999 ***]

2025-03-16 21:06:59.739505 test begin: paddle.nn.functional.normalize(x=Tensor([4, 5, 6, 35791395],"float16"), )

W0316 21:08:42.617507 52964 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 21:08:42.618677 52964 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<phi::dtype::float16, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742131008 (unix time) try "date -d @1742131008" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xce42) received by PID 52802 (TID 0x7f6626949700) from PID 52802 ***]

2025-03-16 21:17:32.669488 test begin: paddle.nn.functional.normalize(x=Tensor([4, 5, 6, 35791395],"float16"), p=1, )

W0316 21:19:24.449410 65138 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 21:19:24.450619 65138 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<phi::dtype::float16, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<phi::dtype::float16, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<phi::dtype::float16, phi::dtype::float16>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742131646 (unix time) try "date -d @1742131646" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xfdd0) received by PID 64976 (TID 0x7f0a3bdc2700) from PID 64976 ***]

2025-03-16 21:28:10.537907 test begin: paddle.nn.functional.normalize(x=Tensor([4, 5, 6, 35791395],"float16"), p=4, )

W0316 21:29:51.886087 77478 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 21:29:51.887408 77478 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[accuracy error] paddle.nn.functional.normalize(x=Tensor([4, 5, 6, 35791395],"float16"), p=4, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[[[-0.07153 , -0.5767  , -0.2218  , ...,  0.08203 ,  0.338   ,
           0.3562  ],
         [ 0.6396  , -0.2151  ,  0.9204  , ...,  0.797   ,  0.7393  ,...
 y: array([[[[-0.07153 , -0.5767  , -0.2218  , ...,  0.08203 ,  0.338   ,
           0.3562  ],
         [ 0.6396  , -0.2151  ,  0.9204  , ...,  0.797   ,  0.7393  ,...
2025-03-16 21:31:40.592835 test begin: paddle.nn.functional.normalize(x=Tensor([4, 5, 6, 35791395],"float16"), p=4, axis=3, )

[accuracy error] paddle.nn.functional.normalize(x=Tensor([4, 5, 6, 35791395],"float16"), p=4, axis=3, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 3962186910 / 4294967400 (92.3%)
Max absolute difference: 0.1294
Max relative difference: 10.
 x: array([[[[-0.00813 , -0.09125 , -0.0331  , ...,  0.00969 ,  0.05634 ,
           0.0617  ],
         [ 0.09973 , -0.02379 ,  0.11786 , ...,  0.1362  ,  0.0795  ,...
 y: array([[[[-1.0576e-03, -1.1864e-02, -4.3068e-03, ...,  1.2608e-03,
           7.3280e-03,  8.0261e-03],
         [ 1.2970e-02, -3.0937e-03,  1.5335e-02, ...,  1.7715e-02,...
2025-03-16 21:46:50.149002 test begin: paddle.nn.functional.normalize(x=Tensor([4294967297],"float16"), axis=0, )

[paddle error] paddle.nn.functional.normalize(x=Tensor([4294967297],"float16"), axis=0, ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at ../paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-03-16 21:47:12.632214 test begin: paddle.nn.functional.normalize(x=Tensor([570425345, 4],"float32"), )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   DivideGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::divide_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::DivideGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::GetGradXAndYOut<float, phi::funcs::DivGradXYFunctor<float, float> >(phi::GPUContext const&, phi::Place const&, int, std::vector<phi::DenseTensor const*, std::allocator<phi::DenseTensor const*> >, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, phi::funcs::DivGradXYFunctor<float, float>)
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742133019 (unix time) try "date -d @1742133019" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x12e04) received by PID 77316 (TID 0x7f97efdc2700) from PID 77316 ***]

2025-03-16 21:51:04.826294 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([100, 42949673],"float16"), -1, 1e-06, False, None, )

W0316 21:52:27.490579 103673 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 21:52:27.491641 103673 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([100, 42949673],"float16"), -1, 1e-06, False, None, ) 
 The size of tensor a (100) must match the size of tensor b (42949673) at non-singleton dimension 1
2025-03-16 21:52:29.390666 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([100, 42949673],"float16"), -1, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([100, 42949673],"float16"), -1, 1e-06, True, None, ) 
 The size of tensor a (100) must match the size of tensor b (42949673) at non-singleton dimension 1
2025-03-16 21:52:31.632710 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([100, 42949673],"float16"), -math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([100, 42949673],"float16"), -math.inf, 1e-06, False, None, ) 
 The size of tensor a (100) must match the size of tensor b (42949673) at non-singleton dimension 1
2025-03-16 21:52:33.237393 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([100, 42949673],"float16"), -math.inf, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([100, 42949673],"float16"), -math.inf, 1e-06, True, None, ) 
 The size of tensor a (100) must match the size of tensor b (42949673) at non-singleton dimension 1
2025-03-16 21:52:35.586865 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([100, 42949673],"float16"), 0, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([100, 42949673],"float16"), 0, 1e-06, False, None, ) 
 The size of tensor a (100) must match the size of tensor b (42949673) at non-singleton dimension 1
2025-03-16 21:52:37.566940 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([100, 42949673],"float16"), 0, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([100, 42949673],"float16"), 0, 1e-06, True, None, ) 
 The size of tensor a (100) must match the size of tensor b (42949673) at non-singleton dimension 1
2025-03-16 21:52:39.508370 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([100, 42949673],"float16"), 1, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([100, 42949673],"float16"), 1, 1e-06, False, None, ) 
 The size of tensor a (100) must match the size of tensor b (42949673) at non-singleton dimension 1
2025-03-16 21:52:40.845122 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([100, 42949673],"float16"), 1, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([100, 42949673],"float16"), 1, 1e-06, True, None, ) 
 The size of tensor a (100) must match the size of tensor b (42949673) at non-singleton dimension 1
2025-03-16 21:52:43.054236 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([100, 42949673],"float16"), 2, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([100, 42949673],"float16"), 2, 1e-06, False, None, ) 
 The size of tensor a (100) must match the size of tensor b (42949673) at non-singleton dimension 1
2025-03-16 21:52:44.448333 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([100, 42949673],"float16"), 2, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([100, 42949673],"float16"), 2, 1e-06, True, None, ) 
 The size of tensor a (100) must match the size of tensor b (42949673) at non-singleton dimension 1
2025-03-16 21:52:46.693514 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([100, 42949673],"float16"), math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([100, 42949673],"float16"), math.inf, 1e-06, False, None, ) 
 The size of tensor a (100) must match the size of tensor b (42949673) at non-singleton dimension 1
2025-03-16 21:52:48.116285 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([100, 42949673],"float16"), math.inf, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([100, 42949673],"float16"), math.inf, 1e-06, True, None, ) 
 The size of tensor a (100) must match the size of tensor b (42949673) at non-singleton dimension 1
2025-03-16 21:52:50.058772 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([42949673, 100],"float16"), -1, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([42949673, 100],"float16"), -1, 1e-06, False, None, ) 
 The size of tensor a (100) must match the size of tensor b (42949673) at non-singleton dimension 0
2025-03-16 21:52:51.890397 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([42949673, 100],"float16"), -1, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([42949673, 100],"float16"), -1, 1e-06, True, None, ) 
 The size of tensor a (100) must match the size of tensor b (42949673) at non-singleton dimension 0
2025-03-16 21:52:54.108170 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([42949673, 100],"float16"), -math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([42949673, 100],"float16"), -math.inf, 1e-06, False, None, ) 
 The size of tensor a (100) must match the size of tensor b (42949673) at non-singleton dimension 0
2025-03-16 21:52:56.029365 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([42949673, 100],"float16"), -math.inf, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([42949673, 100],"float16"), -math.inf, 1e-06, True, None, ) 
 The size of tensor a (100) must match the size of tensor b (42949673) at non-singleton dimension 0
2025-03-16 21:52:57.368147 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([42949673, 100],"float16"), 0, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([42949673, 100],"float16"), 0, 1e-06, False, None, ) 
 The size of tensor a (100) must match the size of tensor b (42949673) at non-singleton dimension 0
2025-03-16 21:52:59.354005 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([42949673, 100],"float16"), 0, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([42949673, 100],"float16"), 0, 1e-06, True, None, ) 
 The size of tensor a (100) must match the size of tensor b (42949673) at non-singleton dimension 0
2025-03-16 21:53:01.652132 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([42949673, 100],"float16"), 1, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([42949673, 100],"float16"), 1, 1e-06, False, None, ) 
 The size of tensor a (100) must match the size of tensor b (42949673) at non-singleton dimension 0
2025-03-16 21:53:03.187170 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([42949673, 100],"float16"), 1, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([42949673, 100],"float16"), 1, 1e-06, True, None, ) 
 The size of tensor a (100) must match the size of tensor b (42949673) at non-singleton dimension 0
2025-03-16 21:53:05.169476 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([42949673, 100],"float16"), 2, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([42949673, 100],"float16"), 2, 1e-06, False, None, ) 
 The size of tensor a (100) must match the size of tensor b (42949673) at non-singleton dimension 0
2025-03-16 21:53:07.087998 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([42949673, 100],"float16"), 2, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([42949673, 100],"float16"), 2, 1e-06, True, None, ) 
 The size of tensor a (100) must match the size of tensor b (42949673) at non-singleton dimension 0
2025-03-16 21:53:08.998801 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([42949673, 100],"float16"), math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([42949673, 100],"float16"), math.inf, 1e-06, False, None, ) 
 The size of tensor a (100) must match the size of tensor b (42949673) at non-singleton dimension 0
2025-03-16 21:53:10.897406 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([42949673, 100],"float16"), math.inf, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float16"), Tensor([42949673, 100],"float16"), math.inf, 1e-06, True, None, ) 
 The size of tensor a (100) must match the size of tensor b (42949673) at non-singleton dimension 0
2025-03-16 21:53:13.169660 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([100, 22817014],"float32"), -1, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([100, 22817014],"float32"), -1, 1e-06, False, None, ) 
 The size of tensor a (100) must match the size of tensor b (22817014) at non-singleton dimension 1
2025-03-16 21:54:19.586486 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([100, 22817014],"float32"), -1, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([100, 22817014],"float32"), -1, 1e-06, True, None, ) 
 The size of tensor a (100) must match the size of tensor b (22817014) at non-singleton dimension 1
2025-03-16 21:54:21.893876 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([100, 22817014],"float32"), -math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([100, 22817014],"float32"), -math.inf, 1e-06, False, None, ) 
 The size of tensor a (100) must match the size of tensor b (22817014) at non-singleton dimension 1
2025-03-16 21:54:23.794015 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([100, 22817014],"float32"), -math.inf, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([100, 22817014],"float32"), -math.inf, 1e-06, True, None, ) 
 The size of tensor a (100) must match the size of tensor b (22817014) at non-singleton dimension 1
2025-03-16 21:54:26.273654 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([100, 22817014],"float32"), 0, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([100, 22817014],"float32"), 0, 1e-06, False, None, ) 
 The size of tensor a (100) must match the size of tensor b (22817014) at non-singleton dimension 1
2025-03-16 21:54:28.160928 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([100, 22817014],"float32"), 0, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([100, 22817014],"float32"), 0, 1e-06, True, None, ) 
 The size of tensor a (100) must match the size of tensor b (22817014) at non-singleton dimension 1
2025-03-16 21:54:30.637355 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([100, 22817014],"float32"), 1, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([100, 22817014],"float32"), 1, 1e-06, False, None, ) 
 The size of tensor a (100) must match the size of tensor b (22817014) at non-singleton dimension 1
2025-03-16 21:54:33.135807 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([100, 22817014],"float32"), 1, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([100, 22817014],"float32"), 1, 1e-06, True, None, ) 
 The size of tensor a (100) must match the size of tensor b (22817014) at non-singleton dimension 1
2025-03-16 21:54:35.682493 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([100, 22817014],"float32"), 2, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([100, 22817014],"float32"), 2, 1e-06, False, None, ) 
 The size of tensor a (100) must match the size of tensor b (22817014) at non-singleton dimension 1
2025-03-16 21:54:38.672356 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([100, 22817014],"float32"), 2, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([100, 22817014],"float32"), 2, 1e-06, True, None, ) 
 The size of tensor a (100) must match the size of tensor b (22817014) at non-singleton dimension 1
2025-03-16 21:54:40.832871 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([100, 22817014],"float32"), 2.0, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([100, 22817014],"float32"), 2.0, 1e-06, False, None, ) 
 The size of tensor a (100) must match the size of tensor b (22817014) at non-singleton dimension 1
2025-03-16 21:54:42.959540 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([100, 22817014],"float32"), math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([100, 22817014],"float32"), math.inf, 1e-06, False, None, ) 
 The size of tensor a (100) must match the size of tensor b (22817014) at non-singleton dimension 1
2025-03-16 21:54:46.512237 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([100, 22817014],"float32"), math.inf, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([100, 22817014],"float32"), math.inf, 1e-06, True, None, ) 
 The size of tensor a (100) must match the size of tensor b (22817014) at non-singleton dimension 1
2025-03-16 21:54:52.989590 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([2281701379, 1],"float32"), 2.0, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([2281701379, 1],"float32"), 2.0, 1e-06, False, None, ) 
 The size of tensor a (100) must match the size of tensor b (2281701379) at non-singleton dimension 0
2025-03-16 21:54:54.261475 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([2281701379],"float32"), 2.0, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([2281701379],"float32"), 2.0, 1e-06, False, None, ) 
 The size of tensor a (100) must match the size of tensor b (2281701379) at non-singleton dimension 1
2025-03-16 21:54:55.710571 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([22817014, 100],"float32"), -1, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([22817014, 100],"float32"), -1, 1e-06, False, None, ) 
 The size of tensor a (100) must match the size of tensor b (22817014) at non-singleton dimension 0
2025-03-16 21:54:57.166318 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([22817014, 100],"float32"), -1, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([22817014, 100],"float32"), -1, 1e-06, True, None, ) 
 The size of tensor a (100) must match the size of tensor b (22817014) at non-singleton dimension 0
2025-03-16 21:54:58.597825 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([22817014, 100],"float32"), -math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([22817014, 100],"float32"), -math.inf, 1e-06, False, None, ) 
 The size of tensor a (100) must match the size of tensor b (22817014) at non-singleton dimension 0
2025-03-16 21:55:00.884432 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([22817014, 100],"float32"), -math.inf, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([22817014, 100],"float32"), -math.inf, 1e-06, True, None, ) 
 The size of tensor a (100) must match the size of tensor b (22817014) at non-singleton dimension 0
2025-03-16 21:55:02.440068 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([22817014, 100],"float32"), 0, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([22817014, 100],"float32"), 0, 1e-06, False, None, ) 
 The size of tensor a (100) must match the size of tensor b (22817014) at non-singleton dimension 0
2025-03-16 21:55:04.714982 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([22817014, 100],"float32"), 0, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([22817014, 100],"float32"), 0, 1e-06, True, None, ) 
 The size of tensor a (100) must match the size of tensor b (22817014) at non-singleton dimension 0
2025-03-16 21:55:06.294509 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([22817014, 100],"float32"), 1, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([22817014, 100],"float32"), 1, 1e-06, False, None, ) 
 The size of tensor a (100) must match the size of tensor b (22817014) at non-singleton dimension 0
2025-03-16 21:55:07.812050 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([22817014, 100],"float32"), 1, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([22817014, 100],"float32"), 1, 1e-06, True, None, ) 
 The size of tensor a (100) must match the size of tensor b (22817014) at non-singleton dimension 0
2025-03-16 21:55:09.261550 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([22817014, 100],"float32"), 2, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([22817014, 100],"float32"), 2, 1e-06, False, None, ) 
 The size of tensor a (100) must match the size of tensor b (22817014) at non-singleton dimension 0
2025-03-16 21:55:11.533850 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([22817014, 100],"float32"), 2, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([22817014, 100],"float32"), 2, 1e-06, True, None, ) 
 The size of tensor a (100) must match the size of tensor b (22817014) at non-singleton dimension 0
2025-03-16 21:55:13.115302 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([22817014, 100],"float32"), math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([22817014, 100],"float32"), math.inf, 1e-06, False, None, ) 
 The size of tensor a (100) must match the size of tensor b (22817014) at non-singleton dimension 0
2025-03-16 21:55:14.567367 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([22817014, 100],"float32"), math.inf, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 100],"float32"), Tensor([22817014, 100],"float32"), math.inf, 1e-06, True, None, ) 
 The size of tensor a (100) must match the size of tensor b (22817014) at non-singleton dimension 0
2025-03-16 21:55:16.021962 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 100],"float32"), -1, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 100],"float32"), -1, 1e-06, False, None, ) 
 The size of tensor a (22817014) must match the size of tensor b (100) at non-singleton dimension 1
2025-03-16 21:55:17.486898 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 100],"float32"), -1, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 100],"float32"), -1, 1e-06, True, None, ) 
 The size of tensor a (22817014) must match the size of tensor b (100) at non-singleton dimension 1
2025-03-16 21:55:18.926240 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 100],"float32"), -math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 100],"float32"), -math.inf, 1e-06, False, None, ) 
 The size of tensor a (22817014) must match the size of tensor b (100) at non-singleton dimension 1
2025-03-16 21:55:20.558876 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 100],"float32"), -math.inf, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 100],"float32"), -math.inf, 1e-06, True, None, ) 
 The size of tensor a (22817014) must match the size of tensor b (100) at non-singleton dimension 1
2025-03-16 21:55:22.967311 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 100],"float32"), 0, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 100],"float32"), 0, 1e-06, False, None, ) 
 The size of tensor a (22817014) must match the size of tensor b (100) at non-singleton dimension 1
2025-03-16 21:55:24.527004 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 100],"float32"), 0, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 100],"float32"), 0, 1e-06, True, None, ) 
 The size of tensor a (22817014) must match the size of tensor b (100) at non-singleton dimension 1
2025-03-16 21:55:25.947446 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 100],"float32"), 1, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 100],"float32"), 1, 1e-06, False, None, ) 
 The size of tensor a (22817014) must match the size of tensor b (100) at non-singleton dimension 1
2025-03-16 21:55:28.191303 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 100],"float32"), 1, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 100],"float32"), 1, 1e-06, True, None, ) 
 The size of tensor a (22817014) must match the size of tensor b (100) at non-singleton dimension 1
2025-03-16 21:55:29.732849 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 100],"float32"), 2, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 100],"float32"), 2, 1e-06, False, None, ) 
 The size of tensor a (22817014) must match the size of tensor b (100) at non-singleton dimension 1
2025-03-16 21:55:31.348652 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 100],"float32"), 2, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 100],"float32"), 2, 1e-06, True, None, ) 
 The size of tensor a (22817014) must match the size of tensor b (100) at non-singleton dimension 1
2025-03-16 21:55:33.386938 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 100],"float32"), math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 100],"float32"), math.inf, 1e-06, False, None, ) 
 The size of tensor a (22817014) must match the size of tensor b (100) at non-singleton dimension 1
2025-03-16 21:55:35.692860 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 100],"float32"), math.inf, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 100],"float32"), math.inf, 1e-06, True, None, ) 
 The size of tensor a (22817014) must match the size of tensor b (100) at non-singleton dimension 1
2025-03-16 21:55:37.241806 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 1],"float32"), 2.0, 1e-06, False, None, )

[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 1],"float32"), 2.0, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 48.48047
Max relative difference: 0.02557256
 x: array([2424.619 , 1372.2343, 1523.7072, 2648.1948, 2235.8643, 2504.5466,
       2299.3323, 1512.4667, 1607.3282, 2645.5042, 1649.9613, 2217.071 ,
       1361.8657, 1458.1766, 1726.0618, 1622.6511, 1416.5043, 1359.6395,...
 y: array([2473.0994, 1403.9735, 1562.091 , 2687.9426, 2282.095 , 2549.6226,
       2347.0032, 1549.1464, 1648.1295, 2685.2983, 1690.755 , 2262.7615,
       1387.458 , 1482.3981, 1769.8553, 1663.572 , 1443.1317, 1380.919 ,...
2025-03-16 21:56:01.633794 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), -1, 1e-06, False, None, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742133396 (unix time) try "date -d @1742133396" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x194f9) received by PID 103673 (TID 0x7f3bb3237700) from PID 103673 ***]

2025-03-16 21:57:22.257062 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), -1, 1e-06, True, None, )

W0316 21:58:55.632700 111067 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 21:58:55.633881 111067 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742133548 (unix time) try "date -d @1742133548" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1b13d) received by PID 110909 (TID 0x7f4d3a949700) from PID 110909 ***]

2025-03-16 21:59:49.068378 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), -math.inf, 1e-06, False, None, )

W0316 22:01:15.352874 113934 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 22:01:15.354017 113934 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742133677 (unix time) try "date -d @1742133677" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1bc6c) received by PID 113772 (TID 0x7fbd4ddc2700) from PID 113772 ***]

2025-03-16 22:02:00.050082 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), -math.inf, 1e-06, True, None, )

W0316 22:03:31.554020 116577 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 22:03:31.555300 116577 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742133813 (unix time) try "date -d @1742133813" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1c6bf) received by PID 116415 (TID 0x7f6c407c3700) from PID 116415 ***]

2025-03-16 22:04:17.104416 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), 0, 1e-06, False, None, )

W0316 22:05:38.399891 119292 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 22:05:38.400988 119292 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), 0, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 6039798.
Max relative difference: 0.2647059
 x: array([16777216., 16777216., 16777216., 16777216., 16777216., 16777216.,
       16777216., 16777216., 16777216., 16777216., 16777216., 16777216.,
       16777216., 16777216., 16777216., 16777216., 16777216., 16777216.,...
 y: array([22817014., 22817014., 22817014., 22817014., 22817014., 22817014.,
       22817014., 22817014., 22817014., 22817014., 22817014., 22817014.,
       22817014., 22817014., 22817014., 22817014., 22817014., 22817014.,...
2025-03-16 22:05:40.238120 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), 0, 1e-06, True, None, )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), 0, 1e-06, True, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 6039798.
Max relative difference: 0.2647059
 x: array([[16777216.],
       [16777216.],
       [16777216.],...
 y: array([[22817014.],
       [22817014.],
       [22817014.],...
2025-03-16 22:05:53.957936 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), 1, 1e-06, False, None, )

[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), 1, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 4.8137093
Max relative difference: 0.2109701
 x: array([27.63073, 27.63073, 27.63073, 27.63073, 27.63073, 27.63073,
       27.63073, 27.63073, 27.63073, 27.63073, 27.63073, 27.63073,
       27.63073, 27.63073, 27.63073, 27.63073, 27.63073, 27.63073,...
 y: array([22.81702, 22.81702, 22.81702, 22.81702, 22.81702, 22.81702,
       22.81702, 22.81702, 22.81702, 22.81702, 22.81702, 22.81702,
       22.81702, 22.81702, 22.81702, 22.81702, 22.81702, 22.81702,...
2025-03-16 22:06:33.810513 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), 1, 1e-06, True, None, )

[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), 1, 1e-06, True, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 4.8137093
Max relative difference: 0.2109701
 x: array([[27.63073],
       [27.63073],
       [27.63073],...
 y: array([[22.81702],
       [22.81702],
       [22.81702],...
2025-03-16 22:07:10.370164 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), 2, 1e-06, False, None, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742134069 (unix time) try "date -d @1742134069" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1d15a) received by PID 119130 (TID 0x7f1d094f4700) from PID 119130 ***]

2025-03-16 22:08:35.151698 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), 2, 1e-06, True, None, )

W0316 22:10:01.397289 124194 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 22:10:01.398456 124194 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742134214 (unix time) try "date -d @1742134214" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1e4c6) received by PID 124102 (TID 0x7f512e949700) from PID 124102 ***]

2025-03-16 22:10:58.963588 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), 2.0, 1e-06, False, None, )

W0316 22:12:25.864354 127057 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 22:12:25.865514 127057 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742134358 (unix time) try "date -d @1742134358" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1efae) received by PID 126894 (TID 0x7f3e767c3700) from PID 126894 ***]

2025-03-16 22:13:20.840697 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), math.inf, 1e-06, False, None, )

W0316 22:14:51.737486 129888 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 22:14:51.739521 129888 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742134493 (unix time) try "date -d @1742134493" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1fabd) received by PID 129725 (TID 0x7fd4f5abb700) from PID 129725 ***]

2025-03-16 22:15:38.693818 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100, 22817014],"float32"), math.inf, 1e-06, True, None, )

W0316 22:17:05.470633 132606 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 22:17:05.471941 132606 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742134627 (unix time) try "date -d @1742134627" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2055d) received by PID 132445 (TID 0x7f6d2c949700) from PID 132445 ***]

2025-03-16 22:17:55.068569 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100],"float32"), 2.0, 1e-06, False, None, )

W0316 22:18:58.632535 135159 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 22:18:58.633602 135159 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 22817014],"float32"), Tensor([100],"float32"), 2.0, 1e-06, False, None, ) 
 The size of tensor a (22817014) must match the size of tensor b (100) at non-singleton dimension 1
2025-03-16 22:19:00.552301 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 100],"float16"), -1, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 100],"float16"), -1, 1e-06, False, None, ) 
 The size of tensor a (42949673) must match the size of tensor b (100) at non-singleton dimension 1
2025-03-16 22:20:16.266501 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 100],"float16"), -1, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 100],"float16"), -1, 1e-06, True, None, ) 
 The size of tensor a (42949673) must match the size of tensor b (100) at non-singleton dimension 1
2025-03-16 22:20:18.029712 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 100],"float16"), -math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 100],"float16"), -math.inf, 1e-06, False, None, ) 
 The size of tensor a (42949673) must match the size of tensor b (100) at non-singleton dimension 1
2025-03-16 22:20:19.510726 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 100],"float16"), -math.inf, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 100],"float16"), -math.inf, 1e-06, True, None, ) 
 The size of tensor a (42949673) must match the size of tensor b (100) at non-singleton dimension 1
2025-03-16 22:20:21.663339 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 100],"float16"), 0, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 100],"float16"), 0, 1e-06, False, None, ) 
 The size of tensor a (42949673) must match the size of tensor b (100) at non-singleton dimension 1
2025-03-16 22:20:23.849910 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 100],"float16"), 0, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 100],"float16"), 0, 1e-06, True, None, ) 
 The size of tensor a (42949673) must match the size of tensor b (100) at non-singleton dimension 1
2025-03-16 22:20:25.987692 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 100],"float16"), 1, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 100],"float16"), 1, 1e-06, False, None, ) 
 The size of tensor a (42949673) must match the size of tensor b (100) at non-singleton dimension 1
2025-03-16 22:20:28.131726 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 100],"float16"), 1, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 100],"float16"), 1, 1e-06, True, None, ) 
 The size of tensor a (42949673) must match the size of tensor b (100) at non-singleton dimension 1
2025-03-16 22:20:30.215258 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 100],"float16"), 2, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 100],"float16"), 2, 1e-06, False, None, ) 
 The size of tensor a (42949673) must match the size of tensor b (100) at non-singleton dimension 1
2025-03-16 22:20:32.049361 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 100],"float16"), 2, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 100],"float16"), 2, 1e-06, True, None, ) 
 The size of tensor a (42949673) must match the size of tensor b (100) at non-singleton dimension 1
2025-03-16 22:20:33.836957 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 100],"float16"), math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 100],"float16"), math.inf, 1e-06, False, None, ) 
 The size of tensor a (42949673) must match the size of tensor b (100) at non-singleton dimension 1
2025-03-16 22:20:35.624054 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 100],"float16"), math.inf, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 100],"float16"), math.inf, 1e-06, True, None, ) 
 The size of tensor a (42949673) must match the size of tensor b (100) at non-singleton dimension 1
2025-03-16 22:20:37.499345 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), -1, 1e-06, False, None, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742134892 (unix time) try "date -d @1742134892" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x20ff7) received by PID 135159 (TID 0x7f7307dc2700) from PID 135159 ***]

2025-03-16 22:22:19.058523 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), -1, 1e-06, True, None, )

W0316 22:24:01.649928 140460 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 22:24:01.651185 140460 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742135076 (unix time) try "date -d @1742135076" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x22409) received by PID 140297 (TID 0x7f3984949700) from PID 140297 ***]

2025-03-16 22:25:21.062942 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), -math.inf, 1e-06, False, None, )

W0316 22:27:06.358839 144002 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 22:27:06.359994 144002 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742135233 (unix time) try "date -d @1742135233" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x231e3) received by PID 143843 (TID 0x7f456c949700) from PID 143843 ***]

2025-03-16 22:27:54.117143 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), -math.inf, 1e-06, True, None, )

W0316 22:29:59.371234 147023 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 22:29:59.372510 147023 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742135406 (unix time) try "date -d @1742135406" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x23dad) received by PID 146861 (TID 0x7f1731b85700) from PID 146861 ***]

2025-03-16 22:30:47.521358 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), 0, 1e-06, False, None, )

W0316 22:32:21.064827 150569 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 22:32:21.066108 150569 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), 0, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([2048., 2048., 2048., 2048., 2048., 2048., 2048., 2048., 2048.,
       2048., 2048., 2048., 2048., 2048., 2048., 2048., 2048., 2048.,
       2048., 2048., 2048., 2048., 2048., 2048., 2048., 2048., 2048.,...
 y: array([inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,
       inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,
       inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf,...
2025-03-16 22:32:25.077739 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), 0, 1e-06, True, None, )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), 0, 1e-06, True, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[2048.],
       [2048.],
       [2048.],...
 y: array([[inf],
       [inf],
       [inf],...
2025-03-16 22:32:38.303111 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), 1, 1e-06, False, None, )

[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), 1, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 43.53
Max relative difference: 1.
 x: array([0.003906, 0.003906, 0.003906, 0.003906, 0.003906, 0.003906,
       0.003906, 0.003906, 0.003906, 0.003906, 0.003906, 0.003906,
       0.003906, 0.003906, 0.003906, 0.003906, 0.003906, 0.003906,...
 y: array([43.53, 43.53, 43.53, 43.53, 43.53, 43.53, 43.53, 43.53, 43.53,
       43.53, 43.53, 43.53, 43.53, 43.53, 43.53, 43.53, 43.53, 43.53,
       43.53, 43.53, 43.53, 43.53, 43.53, 43.53, 43.53, 43.53, 43.53,...
2025-03-16 22:33:32.600350 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), 1, 1e-06, True, None, )

[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), 1, 1e-06, True, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 43.53
Max relative difference: 1.
 x: array([[0.003906],
       [0.003906],
       [0.003906],...
 y: array([[43.53],
       [43.53],
       [43.53],...
2025-03-16 22:34:29.874880 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), 2, 1e-06, False, None, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742135727 (unix time) try "date -d @1742135727" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x24b9c) received by PID 150428 (TID 0x7fbe2b34a700) from PID 150428 ***]

2025-03-16 22:36:09.613108 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), 2, 1e-06, True, None, )

W0316 22:38:01.719780 157041 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 22:38:01.720917 157041 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742135916 (unix time) try "date -d @1742135916" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x264d3) received by PID 156883 (TID 0x7f4a891f7700) from PID 156883 ***]

2025-03-16 22:39:21.834968 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), math.inf, 1e-06, False, None, )

W0316 22:41:03.359588 160737 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 22:41:03.360857 160737 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742136070 (unix time) try "date -d @1742136070" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x2733f) received by PID 160575 (TID 0x7f745d6f8700) from PID 160575 ***]

2025-03-16 22:41:55.461517 test begin: paddle.nn.functional.pairwise_distance(Tensor([100, 42949673],"float16"), Tensor([100, 42949673],"float16"), math.inf, 1e-06, True, None, )

W0316 22:43:39.745568 163753 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 22:43:39.746728 163753 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742136226 (unix time) try "date -d @1742136226" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x27f07) received by PID 163591 (TID 0x7fee57dc2700) from PID 163591 ***]

2025-03-16 22:44:27.977766 test begin: paddle.nn.functional.pairwise_distance(Tensor([1431655766, 3],"float16"), Tensor([1431655766, 3],"float16"), -math.inf, 1e-06, False, None, )

CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 22.92 GiB is free. Process 111341 has 56.27 GiB memory in use. Of the allocated memory 52.00 GiB is allocated by PyTorch, and 2.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 22:46:02.758893 test begin: paddle.nn.functional.pairwise_distance(Tensor([1431655766, 3],"float16"), Tensor([1431655766, 3],"float16"), 0, 1e-06, False, None, )

W0316 22:47:38.088536  5070 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 22:47:38.089741  5070 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[Pass] paddle.nn.functional.pairwise_distance(Tensor([1431655766, 3],"float16"), Tensor([1431655766, 3],"float16"), 0, 1e-06, False, None, )
2025-03-16 22:50:07.033416 test begin: paddle.nn.functional.pairwise_distance(Tensor([1431655766, 3],"float16"), Tensor([1431655766, 3],"float16"), 1, 1e-06, False, None, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742136908 (unix time) try "date -d @1742136908" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x133b) received by PID 4923 (TID 0x7febf5b85700) from PID 4923 ***]

2025-03-16 22:55:51.616411 test begin: paddle.nn.functional.pairwise_distance(Tensor([1431655766, 3],"float16"), Tensor([1431655766, 3],"float16"), 2, 1e-06, False, None, )

W0316 22:57:34.643591 16434 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 22:57:34.644852 16434 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742137319 (unix time) try "date -d @1742137319" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x3f94) received by PID 16276 (TID 0x7f5a9dabb700) from PID 16276 ***]

2025-03-16 23:02:44.802529 test begin: paddle.nn.functional.pairwise_distance(Tensor([1431655766, 3],"float16"), Tensor([1431655766, 3],"float16"), math.inf, 1e-06, False, None, )

CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 22.92 GiB is free. Process 156587 has 56.27 GiB memory in use. Of the allocated memory 52.00 GiB is allocated by PyTorch, and 2.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-16 23:04:18.620040 test begin: paddle.nn.functional.pairwise_distance(Tensor([1431655766, 3],"float16"), Tensor([2, 3],"float16"), -math.inf, 1e-06, False, None, )

W0316 23:05:41.634969 26154 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 23:05:41.636417 26154 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.nn.functional.pairwise_distance(Tensor([1431655766, 3],"float16"), Tensor([2, 3],"float16"), -math.inf, 1e-06, False, None, ) 
 The size of tensor a (1431655766) must match the size of tensor b (2) at non-singleton dimension 0
2025-03-16 23:05:43.569612 test begin: paddle.nn.functional.pairwise_distance(Tensor([1431655766, 3],"float16"), Tensor([2, 3],"float16"), 0, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([1431655766, 3],"float16"), Tensor([2, 3],"float16"), 0, 1e-06, False, None, ) 
 The size of tensor a (1431655766) must match the size of tensor b (2) at non-singleton dimension 0
2025-03-16 23:05:45.961295 test begin: paddle.nn.functional.pairwise_distance(Tensor([1431655766, 3],"float16"), Tensor([2, 3],"float16"), 1, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([1431655766, 3],"float16"), Tensor([2, 3],"float16"), 1, 1e-06, False, None, ) 
 The size of tensor a (1431655766) must match the size of tensor b (2) at non-singleton dimension 0
2025-03-16 23:05:48.738326 test begin: paddle.nn.functional.pairwise_distance(Tensor([1431655766, 3],"float16"), Tensor([2, 3],"float16"), 2, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([1431655766, 3],"float16"), Tensor([2, 3],"float16"), 2, 1e-06, False, None, ) 
 The size of tensor a (1431655766) must match the size of tensor b (2) at non-singleton dimension 0
2025-03-16 23:05:50.389399 test begin: paddle.nn.functional.pairwise_distance(Tensor([1431655766, 3],"float16"), Tensor([2, 3],"float16"), math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([1431655766, 3],"float16"), Tensor([2, 3],"float16"), math.inf, 1e-06, False, None, ) 
 The size of tensor a (1431655766) must match the size of tensor b (2) at non-singleton dimension 0
2025-03-16 23:05:51.860001 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 1140850690],"float32"), Tensor([2, 1140850690],"float32"), -math.inf, 1e-06, False, None, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742137704 (unix time) try "date -d @1742137704" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x662a) received by PID 26154 (TID 0x7f2ce87c3700) from PID 26154 ***]

2025-03-16 23:09:07.905669 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 1140850690],"float32"), Tensor([2, 1140850690],"float32"), 0, 1e-06, False, None, )

W0316 23:10:31.163581 31811 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 23:10:31.164781 31811 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([2, 1140850690],"float32"), Tensor([2, 1140850690],"float32"), 0, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2 / 2 (100%)
Max absolute difference: 1.1240735e+09
Max relative difference: 0.9852941
 x: array([16777216., 16777216.], dtype=float32)
 y: array([1.140851e+09, 1.140851e+09], dtype=float32)
2025-03-16 23:11:31.059156 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 1140850690],"float32"), Tensor([2, 1140850690],"float32"), 1, 1e-06, False, None, )

[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([2, 1140850690],"float32"), Tensor([2, 1140850690],"float32"), 1, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2 / 2 (100%)
Max absolute difference: 1108.851
Max relative difference: 0.97195077
 x: array([32., 32.], dtype=float32)
 y: array([1140.851, 1140.851], dtype=float32)
2025-03-16 23:19:08.943760 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 1140850690],"float32"), Tensor([2, 1140850690],"float32"), 2, 1e-06, False, None, )

[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([2, 1140850690],"float32"), Tensor([2, 1140850690],"float32"), 2, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2 / 2 (100%)
Max absolute difference: 0.0282522
Max relative difference: 0.83644617
 x: array([0.005524, 0.005524], dtype=float32)
 y: array([0.033776, 0.033776], dtype=float32)
2025-03-16 23:26:48.928351 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 1140850690],"float32"), Tensor([2, 1140850690],"float32"), math.inf, 1e-06, False, None, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742138894 (unix time) try "date -d @1742138894" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x7ba1) received by PID 31649 (TID 0x7fe04e7c3700) from PID 31649 ***]

2025-03-16 23:29:01.243879 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 1140850690],"float32"), Tensor([2, 3],"float32"), -math.inf, 1e-06, False, None, )

W0316 23:30:03.491372 54777 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 23:30:03.492458 54777 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.nn.functional.pairwise_distance(Tensor([2, 1140850690],"float32"), Tensor([2, 3],"float32"), -math.inf, 1e-06, False, None, ) 
 The size of tensor a (1140850690) must match the size of tensor b (3) at non-singleton dimension 1
2025-03-16 23:30:05.329920 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 1140850690],"float32"), Tensor([2, 3],"float32"), 0, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2, 1140850690],"float32"), Tensor([2, 3],"float32"), 0, 1e-06, False, None, ) 
 The size of tensor a (1140850690) must match the size of tensor b (3) at non-singleton dimension 1
2025-03-16 23:30:06.638250 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 1140850690],"float32"), Tensor([2, 3],"float32"), 1, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2, 1140850690],"float32"), Tensor([2, 3],"float32"), 1, 1e-06, False, None, ) 
 The size of tensor a (1140850690) must match the size of tensor b (3) at non-singleton dimension 1
2025-03-16 23:30:08.519652 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 1140850690],"float32"), Tensor([2, 3],"float32"), 2, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2, 1140850690],"float32"), Tensor([2, 3],"float32"), 2, 1e-06, False, None, ) 
 The size of tensor a (1140850690) must match the size of tensor b (3) at non-singleton dimension 1
2025-03-16 23:30:10.394212 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 1140850690],"float32"), Tensor([2, 3],"float32"), math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2, 1140850690],"float32"), Tensor([2, 3],"float32"), math.inf, 1e-06, False, None, ) 
 The size of tensor a (1140850690) must match the size of tensor b (3) at non-singleton dimension 1
2025-03-16 23:30:12.267959 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 2147483649],"float16"), -math.inf, 1e-06, False, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 2147483649],"float16"), -math.inf, 1e-06, False, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:117)

2025-03-16 23:32:16.896415 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 2147483649],"float16"), 0, 1e-06, False, None, )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 2147483649],"float16"), 0, 1e-06, False, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:117)

2025-03-16 23:32:24.607821 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 2147483649],"float16"), 1, 1e-06, False, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 2147483649],"float16"), 1, 1e-06, False, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:117)

2025-03-16 23:32:47.725150 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 2147483649],"float16"), 2, 1e-06, False, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 2147483649],"float16"), 2, 1e-06, False, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:117)

2025-03-16 23:33:09.804761 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 2147483649],"float16"), math.inf, 1e-06, False, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 2147483649],"float16"), math.inf, 1e-06, False, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:117)

2025-03-16 23:33:33.445511 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 3],"float16"), -math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 3],"float16"), -math.inf, 1e-06, False, None, ) 
 The size of tensor a (2147483649) must match the size of tensor b (3) at non-singleton dimension 1
2025-03-16 23:33:36.101013 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 3],"float16"), 0, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 3],"float16"), 0, 1e-06, False, None, ) 
 The size of tensor a (2147483649) must match the size of tensor b (3) at non-singleton dimension 1
2025-03-16 23:33:37.916861 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 3],"float16"), 1, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 3],"float16"), 1, 1e-06, False, None, ) 
 The size of tensor a (2147483649) must match the size of tensor b (3) at non-singleton dimension 1
2025-03-16 23:33:39.759593 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 3],"float16"), 2, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 3],"float16"), 2, 1e-06, False, None, ) 
 The size of tensor a (2147483649) must match the size of tensor b (3) at non-singleton dimension 1
2025-03-16 23:33:41.607906 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 3],"float16"), math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2, 2147483649],"float16"), Tensor([2, 3],"float16"), math.inf, 1e-06, False, None, ) 
 The size of tensor a (2147483649) must match the size of tensor b (3) at non-singleton dimension 1
2025-03-16 23:33:43.422652 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float16"), Tensor([1431655766, 3],"float16"), -math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float16"), Tensor([1431655766, 3],"float16"), -math.inf, 1e-06, False, None, ) 
 The size of tensor a (2) must match the size of tensor b (1431655766) at non-singleton dimension 0
2025-03-16 23:33:45.258055 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float16"), Tensor([1431655766, 3],"float16"), 0, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float16"), Tensor([1431655766, 3],"float16"), 0, 1e-06, False, None, ) 
 The size of tensor a (2) must match the size of tensor b (1431655766) at non-singleton dimension 0
2025-03-16 23:33:47.090761 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float16"), Tensor([1431655766, 3],"float16"), 1, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float16"), Tensor([1431655766, 3],"float16"), 1, 1e-06, False, None, ) 
 The size of tensor a (2) must match the size of tensor b (1431655766) at non-singleton dimension 0
2025-03-16 23:33:48.934709 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float16"), Tensor([1431655766, 3],"float16"), 2, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float16"), Tensor([1431655766, 3],"float16"), 2, 1e-06, False, None, ) 
 The size of tensor a (2) must match the size of tensor b (1431655766) at non-singleton dimension 0
2025-03-16 23:33:50.769840 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float16"), Tensor([1431655766, 3],"float16"), math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float16"), Tensor([1431655766, 3],"float16"), math.inf, 1e-06, False, None, ) 
 The size of tensor a (2) must match the size of tensor b (1431655766) at non-singleton dimension 0
2025-03-16 23:33:52.586296 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float16"), Tensor([2, 2147483649],"float16"), -math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float16"), Tensor([2, 2147483649],"float16"), -math.inf, 1e-06, False, None, ) 
 The size of tensor a (3) must match the size of tensor b (2147483649) at non-singleton dimension 1
2025-03-16 23:33:54.418100 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float16"), Tensor([2, 2147483649],"float16"), 0, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float16"), Tensor([2, 2147483649],"float16"), 0, 1e-06, False, None, ) 
 The size of tensor a (3) must match the size of tensor b (2147483649) at non-singleton dimension 1
2025-03-16 23:33:56.235448 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float16"), Tensor([2, 2147483649],"float16"), 1, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float16"), Tensor([2, 2147483649],"float16"), 1, 1e-06, False, None, ) 
 The size of tensor a (3) must match the size of tensor b (2147483649) at non-singleton dimension 1
2025-03-16 23:33:58.059360 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float16"), Tensor([2, 2147483649],"float16"), 2, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float16"), Tensor([2, 2147483649],"float16"), 2, 1e-06, False, None, ) 
 The size of tensor a (3) must match the size of tensor b (2147483649) at non-singleton dimension 1
2025-03-16 23:33:59.907342 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float16"), Tensor([2, 2147483649],"float16"), math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float16"), Tensor([2, 2147483649],"float16"), math.inf, 1e-06, False, None, ) 
 The size of tensor a (3) must match the size of tensor b (2147483649) at non-singleton dimension 1
2025-03-16 23:34:01.780294 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float32"), Tensor([2, 1140850690],"float32"), -math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float32"), Tensor([2, 1140850690],"float32"), -math.inf, 1e-06, False, None, ) 
 The size of tensor a (3) must match the size of tensor b (1140850690) at non-singleton dimension 1
2025-03-16 23:34:05.945391 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float32"), Tensor([2, 1140850690],"float32"), 0, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float32"), Tensor([2, 1140850690],"float32"), 0, 1e-06, False, None, ) 
 The size of tensor a (3) must match the size of tensor b (1140850690) at non-singleton dimension 1
2025-03-16 23:34:07.274016 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float32"), Tensor([2, 1140850690],"float32"), 1, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float32"), Tensor([2, 1140850690],"float32"), 1, 1e-06, False, None, ) 
 The size of tensor a (3) must match the size of tensor b (1140850690) at non-singleton dimension 1
2025-03-16 23:34:08.617804 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float32"), Tensor([2, 1140850690],"float32"), 2, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float32"), Tensor([2, 1140850690],"float32"), 2, 1e-06, False, None, ) 
 The size of tensor a (3) must match the size of tensor b (1140850690) at non-singleton dimension 1
2025-03-16 23:34:10.207364 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float32"), Tensor([2, 1140850690],"float32"), math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float32"), Tensor([2, 1140850690],"float32"), math.inf, 1e-06, False, None, ) 
 The size of tensor a (3) must match the size of tensor b (1140850690) at non-singleton dimension 1
2025-03-16 23:34:11.556144 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float32"), Tensor([760567127, 3],"float32"), -math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float32"), Tensor([760567127, 3],"float32"), -math.inf, 1e-06, False, None, ) 
 The size of tensor a (2) must match the size of tensor b (760567127) at non-singleton dimension 0
2025-03-16 23:34:13.448070 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float32"), Tensor([760567127, 3],"float32"), 0, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float32"), Tensor([760567127, 3],"float32"), 0, 1e-06, False, None, ) 
 The size of tensor a (2) must match the size of tensor b (760567127) at non-singleton dimension 0
2025-03-16 23:34:15.336124 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float32"), Tensor([760567127, 3],"float32"), 1, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float32"), Tensor([760567127, 3],"float32"), 1, 1e-06, False, None, ) 
 The size of tensor a (2) must match the size of tensor b (760567127) at non-singleton dimension 0
2025-03-16 23:34:17.216383 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float32"), Tensor([760567127, 3],"float32"), 2, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float32"), Tensor([760567127, 3],"float32"), 2, 1e-06, False, None, ) 
 The size of tensor a (2) must match the size of tensor b (760567127) at non-singleton dimension 0
2025-03-16 23:34:19.071658 test begin: paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float32"), Tensor([760567127, 3],"float32"), math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2, 3],"float32"), Tensor([760567127, 3],"float32"), math.inf, 1e-06, False, None, ) 
 The size of tensor a (2) must match the size of tensor b (760567127) at non-singleton dimension 0
2025-03-16 23:34:20.642926 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:117)

2025-03-16 23:34:41.161212 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), -1, 1e-06, False, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), -1, 1e-06, False, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:117)

2025-03-16 23:35:04.913616 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), -1, 1e-06, True, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), -1, 1e-06, True, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:117)

2025-03-16 23:35:29.422423 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), -math.inf, 1e-06, False, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), -math.inf, 1e-06, False, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:117)

2025-03-16 23:35:53.352999 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), -math.inf, 1e-06, True, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), -math.inf, 1e-06, True, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:117)

2025-03-16 23:36:16.076439 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 0, 1e-06, False, None, )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 0, 1e-06, False, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:117)

2025-03-16 23:36:24.669016 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 0, 1e-06, True, None, )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 0, 1e-06, True, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:117)

2025-03-16 23:36:36.293838 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 1, 1e-06, False, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 1, 1e-06, False, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:117)

2025-03-16 23:37:00.731508 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 1, 1e-06, True, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 1, 1e-06, True, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:117)

2025-03-16 23:37:22.138807 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 2, 1e-06, False, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 2, 1e-06, False, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:117)

2025-03-16 23:37:46.482594 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 2, 1e-06, True, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), 2, 1e-06, True, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:117)

2025-03-16 23:38:08.647589 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), math.inf, 1e-06, False, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), math.inf, 1e-06, False, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:117)

2025-03-16 23:38:32.159787 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), math.inf, 1e-06, True, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), math.inf, 1e-06, True, None, ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:117)

2025-03-16 23:38:56.549861 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([5],"float32"), )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([5],"float32"), ) 
 The size of tensor a (2281701379) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-16 23:38:59.195215 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([5],"float32"), -1, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([5],"float32"), -1, 1e-06, False, None, ) 
 The size of tensor a (2281701379) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-16 23:39:00.775792 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([5],"float32"), -1, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([5],"float32"), -1, 1e-06, True, None, ) 
 The size of tensor a (2281701379) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-16 23:39:02.118476 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([5],"float32"), -math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([5],"float32"), -math.inf, 1e-06, False, None, ) 
 The size of tensor a (2281701379) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-16 23:39:04.011389 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([5],"float32"), -math.inf, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([5],"float32"), -math.inf, 1e-06, True, None, ) 
 The size of tensor a (2281701379) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-16 23:39:05.946148 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([5],"float32"), 0, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([5],"float32"), 0, 1e-06, False, None, ) 
 The size of tensor a (2281701379) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-16 23:39:07.876795 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([5],"float32"), 0, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([5],"float32"), 0, 1e-06, True, None, ) 
 The size of tensor a (2281701379) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-16 23:39:09.808686 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([5],"float32"), 1, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([5],"float32"), 1, 1e-06, False, None, ) 
 The size of tensor a (2281701379) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-16 23:39:11.768186 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([5],"float32"), 1, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([5],"float32"), 1, 1e-06, True, None, ) 
 The size of tensor a (2281701379) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-16 23:39:13.758159 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([5],"float32"), 2, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([5],"float32"), 2, 1e-06, False, None, ) 
 The size of tensor a (2281701379) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-16 23:39:15.749612 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([5],"float32"), 2, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([5],"float32"), 2, 1e-06, True, None, ) 
 The size of tensor a (2281701379) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-16 23:39:17.694077 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([5],"float32"), math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([5],"float32"), math.inf, 1e-06, False, None, ) 
 The size of tensor a (2281701379) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-16 23:39:19.718245 test begin: paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([5],"float32"), math.inf, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([2281701379],"float32"), Tensor([5],"float32"), math.inf, 1e-06, True, None, ) 
 The size of tensor a (2281701379) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-16 23:39:21.676785 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([100, 100],"float32"), -1, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([100, 100],"float32"), -1, 1e-06, False, None, ) 
 The size of tensor a (22817014) must match the size of tensor b (100) at non-singleton dimension 0
2025-03-16 23:39:23.609994 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([100, 100],"float32"), -1, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([100, 100],"float32"), -1, 1e-06, True, None, ) 
 The size of tensor a (22817014) must match the size of tensor b (100) at non-singleton dimension 0
2025-03-16 23:39:26.160680 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([100, 100],"float32"), -math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([100, 100],"float32"), -math.inf, 1e-06, False, None, ) 
 The size of tensor a (22817014) must match the size of tensor b (100) at non-singleton dimension 0
2025-03-16 23:39:28.181532 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([100, 100],"float32"), -math.inf, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([100, 100],"float32"), -math.inf, 1e-06, True, None, ) 
 The size of tensor a (22817014) must match the size of tensor b (100) at non-singleton dimension 0
2025-03-16 23:39:30.781903 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([100, 100],"float32"), 0, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([100, 100],"float32"), 0, 1e-06, False, None, ) 
 The size of tensor a (22817014) must match the size of tensor b (100) at non-singleton dimension 0
2025-03-16 23:39:32.777060 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([100, 100],"float32"), 0, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([100, 100],"float32"), 0, 1e-06, True, None, ) 
 The size of tensor a (22817014) must match the size of tensor b (100) at non-singleton dimension 0
2025-03-16 23:39:34.762229 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([100, 100],"float32"), 1, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([100, 100],"float32"), 1, 1e-06, False, None, ) 
 The size of tensor a (22817014) must match the size of tensor b (100) at non-singleton dimension 0
2025-03-16 23:39:36.723020 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([100, 100],"float32"), 1, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([100, 100],"float32"), 1, 1e-06, True, None, ) 
 The size of tensor a (22817014) must match the size of tensor b (100) at non-singleton dimension 0
2025-03-16 23:39:38.672300 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([100, 100],"float32"), 2, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([100, 100],"float32"), 2, 1e-06, False, None, ) 
 The size of tensor a (22817014) must match the size of tensor b (100) at non-singleton dimension 0
2025-03-16 23:39:40.513683 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([100, 100],"float32"), 2, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([100, 100],"float32"), 2, 1e-06, True, None, ) 
 The size of tensor a (22817014) must match the size of tensor b (100) at non-singleton dimension 0
2025-03-16 23:39:42.273790 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([100, 100],"float32"), math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([100, 100],"float32"), math.inf, 1e-06, False, None, ) 
 The size of tensor a (22817014) must match the size of tensor b (100) at non-singleton dimension 0
2025-03-16 23:39:44.218042 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([100, 100],"float32"), math.inf, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([100, 100],"float32"), math.inf, 1e-06, True, None, ) 
 The size of tensor a (22817014) must match the size of tensor b (100) at non-singleton dimension 0
2025-03-16 23:39:45.853757 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([100, 1],"float32"), 2.0, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([100, 1],"float32"), 2.0, 1e-06, False, None, ) 
 The size of tensor a (22817014) must match the size of tensor b (100) at non-singleton dimension 0
2025-03-16 23:39:46.822606 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([100],"float32"), 2.0, 1e-06, False, None, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SubtractGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::subtract_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::SubtractGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::default_elementwise_sub_grad<float>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, int)
6   void phi::funcs::ReduceKernel<float, float, phi::kps::AddFunctor, phi::kps::InverseFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::InverseFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
7   phi::DenseTensor::~DenseTensor()
8   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
9   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742139599 (unix time) try "date -d @1742139599" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xd5f9) received by PID 54777 (TID 0x7feabfdc2700) from PID 54777 ***]

2025-03-16 23:40:42.711761 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), -1, 1e-06, False, None, )

W0316 23:42:09.251204 68458 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 23:42:09.252452 68458 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742139730 (unix time) try "date -d @1742139730" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x10ac8) received by PID 68296 (TID 0x7f0698949700) from PID 68296 ***]

2025-03-16 23:42:53.493677 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), -1, 1e-06, True, None, )

W0316 23:44:54.038210 71021 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 23:44:54.042004 71021 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742139896 (unix time) try "date -d @1742139896" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x114ce) received by PID 70862 (TID 0x7f5b5ddc2700) from PID 70862 ***]

2025-03-16 23:45:39.311966 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), -math.inf, 1e-06, False, None, )

W0316 23:47:18.033987 73669 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 23:47:18.035578 73669 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742140040 (unix time) try "date -d @1742140040" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x11f69) received by PID 73577 (TID 0x7fcc852b7700) from PID 73577 ***]

2025-03-16 23:48:07.646657 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), -math.inf, 1e-06, True, None, )

W0316 23:49:48.817258 76156 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 23:49:48.818492 76156 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742140191 (unix time) try "date -d @1742140191" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x128de) received by PID 75998 (TID 0x7ff4f9f48700) from PID 75998 ***]

2025-03-16 23:50:36.801062 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), 0, 1e-06, False, None, )

W0316 23:52:05.627138 78649 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 23:52:05.628880 78649 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[Pass] paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), 0, 1e-06, False, None, )
2025-03-16 23:52:07.929884 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), 0, 1e-06, True, None, )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[Pass] paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), 0, 1e-06, True, None, )
2025-03-16 23:52:20.847668 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), 1, 1e-06, False, None, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742140368 (unix time) try "date -d @1742140368" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13297) received by PID 78487 (TID 0x7fa38987e700) from PID 78487 ***]

2025-03-16 23:53:32.361355 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), 1, 1e-06, True, None, )

W0316 23:55:15.261191 81518 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 23:55:15.262370 81518 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742140517 (unix time) try "date -d @1742140517" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x13dcc) received by PID 81356 (TID 0x7fcbddd0b700) from PID 81356 ***]

2025-03-16 23:56:02.567850 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), 2, 1e-06, False, None, )

W0316 23:57:45.234485 84010 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0316 23:57:45.235843 84010 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742140668 (unix time) try "date -d @1742140668" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x14788) received by PID 83848 (TID 0x7fe1bdf48700) from PID 83848 ***]

2025-03-16 23:58:30.820940 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), 2, 1e-06, True, None, )

W0317 00:00:11.616106 86406 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0317 00:00:11.618518 86406 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742140814 (unix time) try "date -d @1742140814" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x15128) received by PID 86312 (TID 0x7f1c227c3700) from PID 86312 ***]

2025-03-17 00:00:59.804300 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), math.inf, 1e-06, False, None, )

W0317 00:02:43.686380 88898 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0317 00:02:43.687573 88898 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742140965 (unix time) try "date -d @1742140965" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x15aa0) received by PID 88736 (TID 0x7fbc9934a700) from PID 88736 ***]

2025-03-17 00:03:27.383049 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 100],"float32"), math.inf, 1e-06, True, None, )

W0317 00:05:12.404752 91313 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0317 00:05:12.406108 91313 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742141114 (unix time) try "date -d @1742141114" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x16413) received by PID 91155 (TID 0x7f970a7c3700) from PID 91155 ***]

2025-03-17 00:06:01.835410 test begin: paddle.nn.functional.pairwise_distance(Tensor([22817014, 100],"float32"), Tensor([22817014, 1],"float32"), 2.0, 1e-06, False, None, )

W0317 00:07:34.835503 93880 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0317 00:07:34.836836 93880 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742141256 (unix time) try "date -d @1742141256" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x16e16) received by PID 93718 (TID 0x7f1aeb4f4700) from PID 93718 ***]

2025-03-17 00:08:24.324955 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), -1, 1e-06, False, None, )

W0317 00:10:18.956300 96221 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0317 00:10:18.957494 96221 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), -1, 1e-06, False, None, ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at ../paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-03-17 00:10:20.348324 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), -1, 1e-06, True, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), -1, 1e-06, True, None, ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at ../paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-03-17 00:10:45.446811 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), -math.inf, 1e-06, False, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), -math.inf, 1e-06, False, None, ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at ../paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-03-17 00:11:12.064859 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), -math.inf, 1e-06, True, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), -math.inf, 1e-06, True, None, ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at ../paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-03-17 00:11:37.048431 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 0, 1e-06, False, None, )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[paddle error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 0, 1e-06, False, None, ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at ../paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-03-17 00:11:47.083023 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 0, 1e-06, True, None, )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[paddle error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 0, 1e-06, True, None, ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at ../paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-03-17 00:11:58.228589 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 1, 1e-06, False, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 1, 1e-06, False, None, ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at ../paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-03-17 00:12:21.545901 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 1, 1e-06, True, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 1, 1e-06, True, None, ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at ../paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-03-17 00:12:44.652795 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 2, 1e-06, False, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 2, 1e-06, False, None, ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at ../paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-03-17 00:13:08.466495 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 2, 1e-06, True, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), 2, 1e-06, True, None, ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at ../paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-03-17 00:13:35.631308 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), math.inf, 1e-06, False, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), math.inf, 1e-06, False, None, ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at ../paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-03-17 00:14:02.475245 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), math.inf, 1e-06, True, None, )

[paddle error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([4294967297],"float16"), math.inf, 1e-06, True, None, ) 
 (InvalidArgument) The 0-th dimension of input tensor is expected to be equal with the 0-th dimension of output tensor 1 or 1, but received 4294967297.
  [Hint: Expected in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1 == true, but received in_dim[in_idx] == out_dims[in_idx] || in_dim[in_idx] == 1:0 != true:1.] (at ../paddle/phi/kernels/funcs/dims_simplifier.h:144)

2025-03-17 00:14:27.484783 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([5],"float16"), -1, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([5],"float16"), -1, 1e-06, False, None, ) 
 The size of tensor a (4294967297) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-17 00:14:29.722137 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([5],"float16"), -1, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([5],"float16"), -1, 1e-06, True, None, ) 
 The size of tensor a (4294967297) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-17 00:14:31.614913 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([5],"float16"), -math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([5],"float16"), -math.inf, 1e-06, False, None, ) 
 The size of tensor a (4294967297) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-17 00:14:33.498442 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([5],"float16"), -math.inf, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([5],"float16"), -math.inf, 1e-06, True, None, ) 
 The size of tensor a (4294967297) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-17 00:14:35.369658 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([5],"float16"), 0, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([5],"float16"), 0, 1e-06, False, None, ) 
 The size of tensor a (4294967297) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-17 00:14:37.221148 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([5],"float16"), 0, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([5],"float16"), 0, 1e-06, True, None, ) 
 The size of tensor a (4294967297) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-17 00:14:39.097265 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([5],"float16"), 1, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([5],"float16"), 1, 1e-06, False, None, ) 
 The size of tensor a (4294967297) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-17 00:14:40.953578 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([5],"float16"), 1, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([5],"float16"), 1, 1e-06, True, None, ) 
 The size of tensor a (4294967297) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-17 00:14:42.524566 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([5],"float16"), 2, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([5],"float16"), 2, 1e-06, False, None, ) 
 The size of tensor a (4294967297) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-17 00:14:44.641096 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([5],"float16"), 2, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([5],"float16"), 2, 1e-06, True, None, ) 
 The size of tensor a (4294967297) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-17 00:14:47.082567 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([5],"float16"), math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([5],"float16"), math.inf, 1e-06, False, None, ) 
 The size of tensor a (4294967297) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-17 00:14:49.035458 test begin: paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([5],"float16"), math.inf, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([4294967297],"float16"), Tensor([5],"float16"), math.inf, 1e-06, True, None, ) 
 The size of tensor a (4294967297) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-17 00:14:50.914216 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([100, 100],"float16"), -1, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([100, 100],"float16"), -1, 1e-06, False, None, ) 
 The size of tensor a (42949673) must match the size of tensor b (100) at non-singleton dimension 0
2025-03-17 00:14:53.397265 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([100, 100],"float16"), -1, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([100, 100],"float16"), -1, 1e-06, True, None, ) 
 The size of tensor a (42949673) must match the size of tensor b (100) at non-singleton dimension 0
2025-03-17 00:14:55.297469 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([100, 100],"float16"), -math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([100, 100],"float16"), -math.inf, 1e-06, False, None, ) 
 The size of tensor a (42949673) must match the size of tensor b (100) at non-singleton dimension 0
2025-03-17 00:14:56.844699 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([100, 100],"float16"), -math.inf, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([100, 100],"float16"), -math.inf, 1e-06, True, None, ) 
 The size of tensor a (42949673) must match the size of tensor b (100) at non-singleton dimension 0
2025-03-17 00:14:58.178666 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([100, 100],"float16"), 0, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([100, 100],"float16"), 0, 1e-06, False, None, ) 
 The size of tensor a (42949673) must match the size of tensor b (100) at non-singleton dimension 0
2025-03-17 00:15:00.111257 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([100, 100],"float16"), 0, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([100, 100],"float16"), 0, 1e-06, True, None, ) 
 The size of tensor a (42949673) must match the size of tensor b (100) at non-singleton dimension 0
2025-03-17 00:15:01.609714 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([100, 100],"float16"), 1, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([100, 100],"float16"), 1, 1e-06, False, None, ) 
 The size of tensor a (42949673) must match the size of tensor b (100) at non-singleton dimension 0
2025-03-17 00:15:02.647285 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([100, 100],"float16"), 1, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([100, 100],"float16"), 1, 1e-06, True, None, ) 
 The size of tensor a (42949673) must match the size of tensor b (100) at non-singleton dimension 0
2025-03-17 00:15:03.973976 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([100, 100],"float16"), 2, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([100, 100],"float16"), 2, 1e-06, False, None, ) 
 The size of tensor a (42949673) must match the size of tensor b (100) at non-singleton dimension 0
2025-03-17 00:15:05.897585 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([100, 100],"float16"), 2, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([100, 100],"float16"), 2, 1e-06, True, None, ) 
 The size of tensor a (42949673) must match the size of tensor b (100) at non-singleton dimension 0
2025-03-17 00:15:07.739389 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([100, 100],"float16"), math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([100, 100],"float16"), math.inf, 1e-06, False, None, ) 
 The size of tensor a (42949673) must match the size of tensor b (100) at non-singleton dimension 0
2025-03-17 00:15:10.175611 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([100, 100],"float16"), math.inf, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([100, 100],"float16"), math.inf, 1e-06, True, None, ) 
 The size of tensor a (42949673) must match the size of tensor b (100) at non-singleton dimension 0
2025-03-17 00:15:11.853195 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), -1, 1e-06, False, None, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742141741 (unix time) try "date -d @1742141741" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1773b) received by PID 96059 (TID 0x7ff36a949700) from PID 96059 ***]

2025-03-17 00:16:25.552999 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), -1, 1e-06, True, None, )

W0317 00:18:17.961653 104001 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0317 00:18:17.962886 104001 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742141907 (unix time) try "date -d @1742141907" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1959f) received by PID 103839 (TID 0x7f0a2b1b7700) from PID 103839 ***]

2025-03-17 00:19:10.411115 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), -math.inf, 1e-06, False, None, )

W0317 00:21:06.833875 106713 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0317 00:21:06.836110 106713 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742142079 (unix time) try "date -d @1742142079" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1a03a) received by PID 106554 (TID 0x7fd591dc2700) from PID 106554 ***]

2025-03-17 00:22:01.277159 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), -math.inf, 1e-06, True, None, )

W0317 00:23:53.890686 109436 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0317 00:23:53.892098 109436 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742142245 (unix time) try "date -d @1742142245" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1ab21) received by PID 109345 (TID 0x7f7767dc2700) from PID 109345 ***]

2025-03-17 00:24:48.400131 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), 0, 1e-06, False, None, )

W0317 00:26:21.674997 112452 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0317 00:26:21.676729 112452 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[Pass] paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), 0, 1e-06, False, None, )
2025-03-17 00:26:26.165095 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), 0, 1e-06, True, None, )

One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[Pass] paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), 0, 1e-06, True, None, )
2025-03-17 00:26:40.383701 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), 1, 1e-06, False, None, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742142433 (unix time) try "date -d @1742142433" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1b6a2) received by PID 112290 (TID 0x7f159a7c3700) from PID 112290 ***]

2025-03-17 00:27:56.137521 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), 1, 1e-06, True, None, )

W0317 00:29:33.899823 116184 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0317 00:29:33.900998 116184 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742142585 (unix time) try "date -d @1742142585" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1c57a) received by PID 116090 (TID 0x7fda8334a700) from PID 116090 ***]

2025-03-17 00:30:27.049992 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), 2, 1e-06, False, None, )

W0317 00:32:09.746220 119196 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0317 00:32:09.747387 119196 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742142741 (unix time) try "date -d @1742142741" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1d0fa) received by PID 119034 (TID 0x7f602df48700) from PID 119034 ***]

2025-03-17 00:33:03.741179 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), 2, 1e-06, True, None, )

W0317 00:34:46.647848 122215 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0317 00:34:46.649003 122215 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742142898 (unix time) try "date -d @1742142898" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1dcc9) received by PID 122057 (TID 0x7fd349d0b700) from PID 122057 ***]

2025-03-17 00:35:40.743526 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), math.inf, 1e-06, False, None, )

W0317 00:37:26.387517 125307 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0317 00:37:26.389312 125307 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742143058 (unix time) try "date -d @1742143058" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1e8dd) received by PID 125149 (TID 0x7fc247dc2700) from PID 125149 ***]

2025-03-17 00:38:21.068819 test begin: paddle.nn.functional.pairwise_distance(Tensor([42949673, 100],"float16"), Tensor([42949673, 100],"float16"), math.inf, 1e-06, True, None, )

W0317 00:40:00.962718 128476 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0317 00:40:00.963969 128476 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742143212 (unix time) try "date -d @1742143212" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x1f53d) received by PID 128317 (TID 0x7f4b19137700) from PID 128317 ***]

2025-03-17 00:40:59.056491 test begin: paddle.nn.functional.pairwise_distance(Tensor([5, 5],"float16"), Tensor([5, 858993460],"float16"), 2, 1e-06, False, None, )

W0317 00:42:18.276764 131419 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0317 00:42:18.277876 131419 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.nn.functional.pairwise_distance(Tensor([5, 5],"float16"), Tensor([5, 858993460],"float16"), 2, 1e-06, False, None, ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 1
2025-03-17 00:42:20.230261 test begin: paddle.nn.functional.pairwise_distance(Tensor([5, 5],"float16"), Tensor([858993460, 5],"float16"), 2, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([5, 5],"float16"), Tensor([858993460, 5],"float16"), 2, 1e-06, False, None, ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 0
2025-03-17 00:42:22.417671 test begin: paddle.nn.functional.pairwise_distance(Tensor([5, 858993460],"float16"), Tensor([5, 5],"float16"), 2, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([5, 858993460],"float16"), Tensor([5, 5],"float16"), 2, 1e-06, False, None, ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 1
2025-03-17 00:42:24.842632 test begin: paddle.nn.functional.pairwise_distance(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), 2, 1e-06, False, None, )

[accuracy error] paddle.nn.functional.pairwise_distance(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), 2, 1e-06, False, None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 5 / 5 (100%)
Max absolute difference: 0.0297
Max relative difference: 1.
 x: array([0., 0., 0., 0., 0.], dtype=float16)
 y: array([0.0297, 0.0297, 0.0297, 0.0297, 0.0297], dtype=float16)
2025-03-17 00:48:37.812583 test begin: paddle.nn.functional.pairwise_distance(Tensor([5],"float16"), Tensor([4294967297],"float16"), -1, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([5],"float16"), Tensor([4294967297],"float16"), -1, 1e-06, False, None, ) 
 The size of tensor a (5) must match the size of tensor b (4294967297) at non-singleton dimension 0
2025-03-17 00:48:41.767849 test begin: paddle.nn.functional.pairwise_distance(Tensor([5],"float16"), Tensor([4294967297],"float16"), -1, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([5],"float16"), Tensor([4294967297],"float16"), -1, 1e-06, True, None, ) 
 The size of tensor a (5) must match the size of tensor b (4294967297) at non-singleton dimension 0
2025-03-17 00:48:44.142582 test begin: paddle.nn.functional.pairwise_distance(Tensor([5],"float16"), Tensor([4294967297],"float16"), -math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([5],"float16"), Tensor([4294967297],"float16"), -math.inf, 1e-06, False, None, ) 
 The size of tensor a (5) must match the size of tensor b (4294967297) at non-singleton dimension 0
2025-03-17 00:48:46.119351 test begin: paddle.nn.functional.pairwise_distance(Tensor([5],"float16"), Tensor([4294967297],"float16"), -math.inf, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([5],"float16"), Tensor([4294967297],"float16"), -math.inf, 1e-06, True, None, ) 
 The size of tensor a (5) must match the size of tensor b (4294967297) at non-singleton dimension 0
2025-03-17 00:48:48.094896 test begin: paddle.nn.functional.pairwise_distance(Tensor([5],"float16"), Tensor([4294967297],"float16"), 0, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([5],"float16"), Tensor([4294967297],"float16"), 0, 1e-06, False, None, ) 
 The size of tensor a (5) must match the size of tensor b (4294967297) at non-singleton dimension 0
2025-03-17 00:48:49.955158 test begin: paddle.nn.functional.pairwise_distance(Tensor([5],"float16"), Tensor([4294967297],"float16"), 0, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([5],"float16"), Tensor([4294967297],"float16"), 0, 1e-06, True, None, ) 
 The size of tensor a (5) must match the size of tensor b (4294967297) at non-singleton dimension 0
2025-03-17 00:48:51.954746 test begin: paddle.nn.functional.pairwise_distance(Tensor([5],"float16"), Tensor([4294967297],"float16"), 1, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([5],"float16"), Tensor([4294967297],"float16"), 1, 1e-06, False, None, ) 
 The size of tensor a (5) must match the size of tensor b (4294967297) at non-singleton dimension 0
2025-03-17 00:48:53.928684 test begin: paddle.nn.functional.pairwise_distance(Tensor([5],"float16"), Tensor([4294967297],"float16"), 1, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([5],"float16"), Tensor([4294967297],"float16"), 1, 1e-06, True, None, ) 
 The size of tensor a (5) must match the size of tensor b (4294967297) at non-singleton dimension 0
2025-03-17 00:48:55.799366 test begin: paddle.nn.functional.pairwise_distance(Tensor([5],"float16"), Tensor([4294967297],"float16"), 2, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([5],"float16"), Tensor([4294967297],"float16"), 2, 1e-06, False, None, ) 
 The size of tensor a (5) must match the size of tensor b (4294967297) at non-singleton dimension 0
2025-03-17 00:48:58.135158 test begin: paddle.nn.functional.pairwise_distance(Tensor([5],"float16"), Tensor([4294967297],"float16"), 2, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([5],"float16"), Tensor([4294967297],"float16"), 2, 1e-06, True, None, ) 
 The size of tensor a (5) must match the size of tensor b (4294967297) at non-singleton dimension 0
2025-03-17 00:48:59.819817 test begin: paddle.nn.functional.pairwise_distance(Tensor([5],"float16"), Tensor([4294967297],"float16"), math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([5],"float16"), Tensor([4294967297],"float16"), math.inf, 1e-06, False, None, ) 
 The size of tensor a (5) must match the size of tensor b (4294967297) at non-singleton dimension 0
2025-03-17 00:49:01.734534 test begin: paddle.nn.functional.pairwise_distance(Tensor([5],"float16"), Tensor([4294967297],"float16"), math.inf, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([5],"float16"), Tensor([4294967297],"float16"), math.inf, 1e-06, True, None, ) 
 The size of tensor a (5) must match the size of tensor b (4294967297) at non-singleton dimension 0
2025-03-17 00:49:04.004389 test begin: paddle.nn.functional.pairwise_distance(Tensor([5],"float32"), Tensor([2281701379],"float32"), )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([5],"float32"), Tensor([2281701379],"float32"), ) 
 The size of tensor a (5) must match the size of tensor b (2281701379) at non-singleton dimension 0
2025-03-17 00:50:07.871070 test begin: paddle.nn.functional.pairwise_distance(Tensor([5],"float32"), Tensor([2281701379],"float32"), -1, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([5],"float32"), Tensor([2281701379],"float32"), -1, 1e-06, False, None, ) 
 The size of tensor a (5) must match the size of tensor b (2281701379) at non-singleton dimension 0
2025-03-17 00:50:10.226967 test begin: paddle.nn.functional.pairwise_distance(Tensor([5],"float32"), Tensor([2281701379],"float32"), -1, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([5],"float32"), Tensor([2281701379],"float32"), -1, 1e-06, True, None, ) 
 The size of tensor a (5) must match the size of tensor b (2281701379) at non-singleton dimension 0
2025-03-17 00:50:11.819462 test begin: paddle.nn.functional.pairwise_distance(Tensor([5],"float32"), Tensor([2281701379],"float32"), -math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([5],"float32"), Tensor([2281701379],"float32"), -math.inf, 1e-06, False, None, ) 
 The size of tensor a (5) must match the size of tensor b (2281701379) at non-singleton dimension 0
2025-03-17 00:50:12.854040 test begin: paddle.nn.functional.pairwise_distance(Tensor([5],"float32"), Tensor([2281701379],"float32"), -math.inf, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([5],"float32"), Tensor([2281701379],"float32"), -math.inf, 1e-06, True, None, ) 
 The size of tensor a (5) must match the size of tensor b (2281701379) at non-singleton dimension 0
2025-03-17 00:50:15.042431 test begin: paddle.nn.functional.pairwise_distance(Tensor([5],"float32"), Tensor([2281701379],"float32"), 0, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([5],"float32"), Tensor([2281701379],"float32"), 0, 1e-06, False, None, ) 
 The size of tensor a (5) must match the size of tensor b (2281701379) at non-singleton dimension 0
2025-03-17 00:50:16.627343 test begin: paddle.nn.functional.pairwise_distance(Tensor([5],"float32"), Tensor([2281701379],"float32"), 0, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([5],"float32"), Tensor([2281701379],"float32"), 0, 1e-06, True, None, ) 
 The size of tensor a (5) must match the size of tensor b (2281701379) at non-singleton dimension 0
2025-03-17 00:50:18.818106 test begin: paddle.nn.functional.pairwise_distance(Tensor([5],"float32"), Tensor([2281701379],"float32"), 1, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([5],"float32"), Tensor([2281701379],"float32"), 1, 1e-06, False, None, ) 
 The size of tensor a (5) must match the size of tensor b (2281701379) at non-singleton dimension 0
2025-03-17 00:50:20.453776 test begin: paddle.nn.functional.pairwise_distance(Tensor([5],"float32"), Tensor([2281701379],"float32"), 1, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([5],"float32"), Tensor([2281701379],"float32"), 1, 1e-06, True, None, ) 
 The size of tensor a (5) must match the size of tensor b (2281701379) at non-singleton dimension 0
2025-03-17 00:50:22.639139 test begin: paddle.nn.functional.pairwise_distance(Tensor([5],"float32"), Tensor([2281701379],"float32"), 2, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([5],"float32"), Tensor([2281701379],"float32"), 2, 1e-06, False, None, ) 
 The size of tensor a (5) must match the size of tensor b (2281701379) at non-singleton dimension 0
2025-03-17 00:50:24.271413 test begin: paddle.nn.functional.pairwise_distance(Tensor([5],"float32"), Tensor([2281701379],"float32"), 2, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([5],"float32"), Tensor([2281701379],"float32"), 2, 1e-06, True, None, ) 
 The size of tensor a (5) must match the size of tensor b (2281701379) at non-singleton dimension 0
2025-03-17 00:50:25.264401 test begin: paddle.nn.functional.pairwise_distance(Tensor([5],"float32"), Tensor([2281701379],"float32"), math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([5],"float32"), Tensor([2281701379],"float32"), math.inf, 1e-06, False, None, ) 
 The size of tensor a (5) must match the size of tensor b (2281701379) at non-singleton dimension 0
2025-03-17 00:50:26.309283 test begin: paddle.nn.functional.pairwise_distance(Tensor([5],"float32"), Tensor([2281701379],"float32"), math.inf, 1e-06, True, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([5],"float32"), Tensor([2281701379],"float32"), math.inf, 1e-06, True, None, ) 
 The size of tensor a (5) must match the size of tensor b (2281701379) at non-singleton dimension 0
2025-03-17 00:50:27.330728 test begin: paddle.nn.functional.pairwise_distance(Tensor([760567127, 3],"float32"), Tensor([2, 3],"float32"), -math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([760567127, 3],"float32"), Tensor([2, 3],"float32"), -math.inf, 1e-06, False, None, ) 
 The size of tensor a (760567127) must match the size of tensor b (2) at non-singleton dimension 0
2025-03-17 00:50:29.512982 test begin: paddle.nn.functional.pairwise_distance(Tensor([760567127, 3],"float32"), Tensor([2, 3],"float32"), 0, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([760567127, 3],"float32"), Tensor([2, 3],"float32"), 0, 1e-06, False, None, ) 
 The size of tensor a (760567127) must match the size of tensor b (2) at non-singleton dimension 0
2025-03-17 00:50:32.019740 test begin: paddle.nn.functional.pairwise_distance(Tensor([760567127, 3],"float32"), Tensor([2, 3],"float32"), 1, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([760567127, 3],"float32"), Tensor([2, 3],"float32"), 1, 1e-06, False, None, ) 
 The size of tensor a (760567127) must match the size of tensor b (2) at non-singleton dimension 0
2025-03-17 00:50:34.525650 test begin: paddle.nn.functional.pairwise_distance(Tensor([760567127, 3],"float32"), Tensor([2, 3],"float32"), 2, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([760567127, 3],"float32"), Tensor([2, 3],"float32"), 2, 1e-06, False, None, ) 
 The size of tensor a (760567127) must match the size of tensor b (2) at non-singleton dimension 0
2025-03-17 00:50:36.127106 test begin: paddle.nn.functional.pairwise_distance(Tensor([760567127, 3],"float32"), Tensor([2, 3],"float32"), math.inf, 1e-06, False, None, )

[torch error] paddle.nn.functional.pairwise_distance(Tensor([760567127, 3],"float32"), Tensor([2, 3],"float32"), math.inf, 1e-06, False, None, ) 
 The size of tensor a (760567127) must match the size of tensor b (2) at non-singleton dimension 0
2025-03-17 00:50:37.253731 test begin: paddle.nn.functional.pairwise_distance(Tensor([760567127, 3],"float32"), Tensor([760567127, 3],"float32"), -math.inf, 1e-06, False, None, )

CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 4.62 GiB is free. Process 71289 has 74.56 GiB memory in use. Of the allocated memory 67.29 GiB is allocated by PyTorch, and 5.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-17 00:50:47.238234 test begin: paddle.nn.functional.pairwise_distance(Tensor([760567127, 3],"float32"), Tensor([760567127, 3],"float32"), 0, 1e-06, False, None, )

W0317 00:52:00.097007 142964 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0317 00:52:00.098151 142964 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
[Pass] paddle.nn.functional.pairwise_distance(Tensor([760567127, 3],"float32"), Tensor([760567127, 3],"float32"), 0, 1e-06, False, None, )
2025-03-17 00:52:25.214076 test begin: paddle.nn.functional.pairwise_distance(Tensor([760567127, 3],"float32"), Tensor([760567127, 3],"float32"), 1, 1e-06, False, None, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742143997 (unix time) try "date -d @1742143997" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x22dde) received by PID 142814 (TID 0x7f343e7c3700) from PID 142814 ***]

2025-03-17 00:54:01.137522 test begin: paddle.nn.functional.pairwise_distance(Tensor([760567127, 3],"float32"), Tensor([760567127, 3],"float32"), 2, 1e-06, False, None, )

W0317 00:55:28.235107 146741 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0317 00:55:28.237049 146741 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742144155 (unix time) try "date -d @1742144155" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x23c93) received by PID 146579 (TID 0x7f9ba9744700) from PID 146579 ***]

2025-03-17 00:56:41.205623 test begin: paddle.nn.functional.pairwise_distance(Tensor([760567127, 3],"float32"), Tensor([760567127, 3],"float32"), math.inf, 1e-06, False, None, )

CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 4.62 GiB is free. Process 74619 has 74.56 GiB memory in use. Of the allocated memory 67.29 GiB is allocated by PyTorch, and 5.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-17 00:57:55.031464 test begin: paddle.nn.functional.pairwise_distance(Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), 2, 1e-06, False, None, )

W0317 00:59:15.619849 151191 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0317 00:59:15.620895 151191 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.nn.functional.pairwise_distance(Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), 2, 1e-06, False, None, ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-17 00:59:17.557782 test begin: paddle.nn.functional.pairwise_distance(Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), 2, 1e-06, False, None, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742144543 (unix time) try "date -d @1742144543" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x24e97) received by PID 151191 (TID 0x7fadd6949700) from PID 151191 ***]

2025-03-17 01:03:07.729664 test begin: paddle.nn.functional.pairwise_distance(x=Tensor([100, 100],"float32"), y=Tensor([100, 22817014],"float32"), p=2.0, epsilon=1e-06, keepdim=False, )

W0317 01:04:12.702828 157284 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0317 01:04:12.703868 157284 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.nn.functional.pairwise_distance(x=Tensor([100, 100],"float32"), y=Tensor([100, 22817014],"float32"), p=2.0, epsilon=1e-06, keepdim=False, ) 
 The size of tensor a (100) must match the size of tensor b (22817014) at non-singleton dimension 1
2025-03-17 01:04:14.712497 test begin: paddle.nn.functional.pairwise_distance(x=Tensor([100, 100],"float32"), y=Tensor([2281701379, 1],"float32"), p=2.0, epsilon=1e-06, keepdim=False, )

[torch error] paddle.nn.functional.pairwise_distance(x=Tensor([100, 100],"float32"), y=Tensor([2281701379, 1],"float32"), p=2.0, epsilon=1e-06, keepdim=False, ) 
 The size of tensor a (100) must match the size of tensor b (2281701379) at non-singleton dimension 0
2025-03-17 01:04:16.710938 test begin: paddle.nn.functional.pairwise_distance(x=Tensor([100, 100],"float32"), y=Tensor([2281701379],"float32"), p=2.0, epsilon=1e-06, keepdim=False, )

[torch error] paddle.nn.functional.pairwise_distance(x=Tensor([100, 100],"float32"), y=Tensor([2281701379],"float32"), p=2.0, epsilon=1e-06, keepdim=False, ) 
 The size of tensor a (100) must match the size of tensor b (2281701379) at non-singleton dimension 1
2025-03-17 01:04:19.093408 test begin: paddle.nn.functional.pairwise_distance(x=Tensor([100, 22817014],"float32"), y=Tensor([100, 1],"float32"), p=2.0, epsilon=1e-06, keepdim=False, )

[accuracy error] paddle.nn.functional.pairwise_distance(x=Tensor([100, 22817014],"float32"), y=Tensor([100, 1],"float32"), p=2.0, epsilon=1e-06, keepdim=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 48.72998
Max relative difference: 0.02611764
 x: array([1624.8907, 1456.8463, 1635.0465, 1854.6152, 1366.802 , 2690.6802,
       1372.3794, 1450.8221, 1414.147 , 1370.0483, 2538.2524, 1519.2133,
       2501.5957, 1413.6201, 1368.8726, 2054.798 , 2609.573 , 1390.5199,...
 y: array([1665.6793, 1480.7848, 1675.6985, 1900.0422, 1394.711 , 2729.9836,
       1404.1162, 1473.9222, 1441.3047, 1399.8834, 2581.719 , 1557.0298,
       2546.8914, 1440.8644, 1397.9742, 2089.6165, 2650.2993, 1421.8079,...
2025-03-17 01:04:43.586421 test begin: paddle.nn.functional.pairwise_distance(x=Tensor([100, 22817014],"float32"), y=Tensor([100, 22817014],"float32"), p=2.0, epsilon=1e-06, keepdim=False, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742144718 (unix time) try "date -d @1742144718" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x26664) received by PID 157284 (TID 0x7f2359f48700) from PID 157284 ***]

2025-03-17 01:06:00.236232 test begin: paddle.nn.functional.pairwise_distance(x=Tensor([100, 22817014],"float32"), y=Tensor([100],"float32"), p=2.0, epsilon=1e-06, keepdim=False, )

W0317 01:07:06.674119 160687 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0317 01:07:06.675218 160687 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.nn.functional.pairwise_distance(x=Tensor([100, 22817014],"float32"), y=Tensor([100],"float32"), p=2.0, epsilon=1e-06, keepdim=False, ) 
 The size of tensor a (22817014) must match the size of tensor b (100) at non-singleton dimension 1
2025-03-17 01:07:08.561386 test begin: paddle.nn.functional.pairwise_distance(x=Tensor([22817014, 100],"float32"), y=Tensor([100, 1],"float32"), p=2.0, epsilon=1e-06, keepdim=False, )

[torch error] paddle.nn.functional.pairwise_distance(x=Tensor([22817014, 100],"float32"), y=Tensor([100, 1],"float32"), p=2.0, epsilon=1e-06, keepdim=False, ) 
 The size of tensor a (22817014) must match the size of tensor b (100) at non-singleton dimension 0
2025-03-17 01:07:11.092319 test begin: paddle.nn.functional.pairwise_distance(x=Tensor([22817014, 100],"float32"), y=Tensor([100],"float32"), p=2.0, epsilon=1e-06, keepdim=False, )

terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   SubtractGradNode::operator()(paddle::small_vector<std::vector<paddle::Tensor, std::allocator<paddle::Tensor> >, 15u>&, bool, bool)
3   paddle::experimental::subtract_grad(paddle::Tensor const&, paddle::Tensor const&, paddle::Tensor const&, int, paddle::Tensor*, paddle::Tensor*)
4   void phi::SubtractGradKernel<float, phi::GPUContext>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, int, phi::DenseTensor*, phi::DenseTensor*)
5   void phi::default_elementwise_sub_grad<float>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor const&, phi::DenseTensor*, phi::DenseTensor*, int)
6   void phi::funcs::ReduceKernel<float, float, phi::kps::AddFunctor, phi::kps::InverseFunctor<float, float>, false>(phi::GPUContext const&, phi::DenseTensor const&, phi::DenseTensor*, phi::kps::InverseFunctor<float, float> const&, std::vector<int, std::allocator<int> > const&)
7   phi::DenseTensor::~DenseTensor()
8   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
9   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742144844 (unix time) try "date -d @1742144844" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x273af) received by PID 160687 (TID 0x7f76e9f48700) from PID 160687 ***]

2025-03-17 01:08:10.457569 test begin: paddle.nn.functional.pairwise_distance(x=Tensor([22817014, 100],"float32"), y=Tensor([22817014, 1],"float32"), p=2.0, epsilon=1e-06, keepdim=False, )

W0317 01:09:28.273705 163416 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0317 01:09:28.274869 163416 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at ../paddle/phi/core/platform/device/gpu/gpu_info.cc:348)



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release()
1   phi::DenseTensor::~DenseTensor()
2   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
3   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1742144969 (unix time) try "date -d @1742144969" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x27db6) received by PID 163254 (TID 0x7f60467c3700) from PID 163254 ***]

2025-03-17 01:10:15.145666 test begin: paddle.nn.functional.relu(Tensor([1, 1, 2281701379],"float32"), )

W0317 01:11:48.282830  2322 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0317 01:11:48.284179  2322 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.nn.functional.relu(Tensor([1, 1, 2281701379],"float32"), )
2025-03-17 01:14:47.056229 test begin: paddle.nn.functional.relu(Tensor([1, 10, 228170138],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([1, 10, 228170138],"float32"), )
2025-03-17 01:17:57.995975 test begin: paddle.nn.functional.relu(Tensor([1, 100, 22817014],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([1, 100, 22817014],"float32"), )
2025-03-17 01:20:58.508863 test begin: paddle.nn.functional.relu(Tensor([1, 10164, 224489],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([1, 10164, 224489],"float32"), )
2025-03-17 01:25:19.397692 test begin: paddle.nn.functional.relu(Tensor([1, 1024, 10, 222823],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([1, 1024, 10, 222823],"float32"), None, )
2025-03-17 01:29:34.620839 test begin: paddle.nn.functional.relu(Tensor([1, 1024, 100, 22283],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([1, 1024, 100, 22283],"float32"), )
2025-03-17 01:33:53.761900 test begin: paddle.nn.functional.relu(Tensor([1, 1024, 12, 185686],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([1, 1024, 12, 185686],"float32"), None, )
2025-03-17 01:37:10.408539 test begin: paddle.nn.functional.relu(Tensor([1, 1024, 123791, 18],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([1, 1024, 123791, 18],"float32"), None, )
2025-03-17 01:39:56.321594 test begin: paddle.nn.functional.relu(Tensor([1, 1024, 136, 16385],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([1, 1024, 136, 16385],"float32"), )
2025-03-17 01:42:59.287064 test begin: paddle.nn.functional.relu(Tensor([1, 1024, 139265, 16],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([1, 1024, 139265, 16],"float32"), None, )
2025-03-17 01:45:58.842618 test begin: paddle.nn.functional.relu(Tensor([1, 1024, 14, 159159],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([1, 1024, 14, 159159],"float32"), )
2025-03-17 01:48:57.718421 test begin: paddle.nn.functional.relu(Tensor([1, 1024, 14, 159159],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([1, 1024, 14, 159159],"float32"), None, )
2025-03-17 01:52:12.425045 test begin: paddle.nn.functional.relu(Tensor([1, 1024, 14660, 152],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([1, 1024, 14660, 152],"float32"), )
2025-03-17 01:55:29.987894 test begin: paddle.nn.functional.relu(Tensor([1, 1024, 148, 15056],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([1, 1024, 148, 15056],"float32"), )
2025-03-17 01:58:40.663013 test begin: paddle.nn.functional.relu(Tensor([1, 1024, 152, 14660],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([1, 1024, 152, 14660],"float32"), )
2025-03-17 02:01:54.944245 test begin: paddle.nn.functional.relu(Tensor([1, 1024, 159159, 14],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([1, 1024, 159159, 14],"float32"), )
2025-03-17 02:05:26.387247 test begin: paddle.nn.functional.relu(Tensor([1, 1024, 159159, 14],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([1, 1024, 159159, 14],"float32"), None, )
2025-03-17 02:08:36.187090 test begin: paddle.nn.functional.relu(Tensor([1, 1024, 16, 139265],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([1, 1024, 16, 139265],"float32"), None, )
2025-03-17 02:11:44.174261 test begin: paddle.nn.functional.relu(Tensor([1, 1024, 18, 123791],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([1, 1024, 18, 123791],"float32"), None, )
2025-03-17 02:15:16.255824 test begin: paddle.nn.functional.relu(Tensor([1, 1024, 22283, 100],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([1, 1024, 22283, 100],"float32"), )
2025-03-17 02:18:28.758194 test begin: paddle.nn.functional.relu(Tensor([1, 1024, 23211, 96],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([1, 1024, 23211, 96],"float32"), )
2025-03-17 02:21:57.784262 test begin: paddle.nn.functional.relu(Tensor([1, 1024, 25321, 88],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([1, 1024, 25321, 88],"float32"), )
2025-03-17 02:25:14.113439 test begin: paddle.nn.functional.relu(Tensor([1, 1024, 69633, 32],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([1, 1024, 69633, 32],"float32"), None, )
2025-03-17 02:28:27.728810 test begin: paddle.nn.functional.relu(Tensor([1, 1024, 85701, 26],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([1, 1024, 85701, 26],"float32"), None, )
2025-03-17 02:31:15.941297 test begin: paddle.nn.functional.relu(Tensor([1, 11, 207427399],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([1, 11, 207427399],"float32"), None, )
2025-03-17 02:34:44.939962 test begin: paddle.nn.functional.relu(Tensor([1, 1100, 2074274],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([1, 1100, 2074274],"float32"), None, )
2025-03-17 02:38:33.219882 test begin: paddle.nn.functional.relu(Tensor([1, 11641334, 14, 14],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([1, 11641334, 14, 14],"float32"), )
2025-03-17 02:41:23.091445 test begin: paddle.nn.functional.relu(Tensor([1, 11641334, 14, 14],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([1, 11641334, 14, 14],"float32"), None, )
2025-03-17 02:45:00.376121 test begin: paddle.nn.functional.relu(Tensor([1, 150112, 100, 152],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([1, 150112, 100, 152],"float32"), )
2025-03-17 02:48:04.020771 test begin: paddle.nn.functional.relu(Tensor([1, 150112, 152, 100],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([1, 150112, 152, 100],"float32"), )
2025-03-17 02:52:02.045753 test begin: paddle.nn.functional.relu(Tensor([1, 160593, 148, 96],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([1, 160593, 148, 96],"float32"), )
2025-03-17 02:56:08.022430 test begin: paddle.nn.functional.relu(Tensor([1, 18, 126761188],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([1, 18, 126761188],"float32"), None, )
2025-03-17 02:58:57.390199 test begin: paddle.nn.functional.relu(Tensor([1, 190651, 136, 88],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([1, 190651, 136, 88],"float32"), )
2025-03-17 03:01:58.475465 test begin: paddle.nn.functional.relu(Tensor([1, 21, 108652447],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([1, 21, 108652447],"float32"), None, )
2025-03-17 03:05:12.867782 test begin: paddle.nn.functional.relu(Tensor([1, 22, 103713700],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([1, 22, 103713700],"float32"), None, )
2025-03-17 03:08:00.083581 test begin: paddle.nn.functional.relu(Tensor([1, 2281701379],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([1, 2281701379],"float32"), )
2025-03-17 03:11:00.664368 test begin: paddle.nn.functional.relu(Tensor([1, 2281701379],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([1, 2281701379],"float32"), None, )
2025-03-17 03:14:09.495036 test begin: paddle.nn.functional.relu(Tensor([1, 4294967297],"float16"), None, )

[Pass] paddle.nn.functional.relu(Tensor([1, 4294967297],"float16"), None, )
2025-03-17 03:33:44.026562 test begin: paddle.nn.functional.relu(Tensor([1, 570425345, 4],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([1, 570425345, 4],"float32"), )
2025-03-17 03:36:55.381035 test begin: paddle.nn.functional.relu(Tensor([1, 5941931, 12, 32],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([1, 5941931, 12, 32],"float32"), None, )
2025-03-17 03:39:46.107181 test begin: paddle.nn.functional.relu(Tensor([1, 61667605, 37],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([1, 61667605, 37],"float32"), )
2025-03-17 03:42:31.976129 test begin: paddle.nn.functional.relu(Tensor([1, 7042289, 18, 18],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([1, 7042289, 18, 18],"float32"), None, )
2025-03-17 03:45:13.949136 test begin: paddle.nn.functional.relu(Tensor([1, 71303169, 32],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([1, 71303169, 32],"float32"), )
2025-03-17 03:48:01.526094 test begin: paddle.nn.functional.relu(Tensor([1, 8775775, 10, 26],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([1, 8775775, 10, 26],"float32"), None, )
2025-03-17 03:51:02.204739 test begin: paddle.nn.functional.relu(Tensor([1, 8912897, 16, 16],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([1, 8912897, 16, 16],"float32"), None, )
2025-03-17 03:53:58.303196 test begin: paddle.nn.functional.relu(Tensor([1, 8912897, 256],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([1, 8912897, 256],"float32"), )
2025-03-17 03:56:57.080376 test begin: paddle.nn.functional.relu(Tensor([1, 8912897, 256],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([1, 8912897, 256],"float32"), None, )
2025-03-17 03:59:55.250909 test begin: paddle.nn.functional.relu(Tensor([10, 429496730],"float16"), None, )

[Pass] paddle.nn.functional.relu(Tensor([10, 429496730],"float16"), None, )
2025-03-17 04:18:01.714279 test begin: paddle.nn.functional.relu(Tensor([11369, 1024, 14, 14],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([11369, 1024, 14, 14],"float32"), )
2025-03-17 04:21:36.127760 test begin: paddle.nn.functional.relu(Tensor([11369, 1024, 14, 14],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([11369, 1024, 14, 14],"float32"), None, )
2025-03-17 04:24:47.475014 test begin: paddle.nn.functional.relu(Tensor([1140850690, 2],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([1140850690, 2],"float32"), )
2025-03-17 04:28:22.445428 test begin: paddle.nn.functional.relu(Tensor([1422, 128, 4, 56, 56],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([1422, 128, 4, 56, 56],"float32"), )
2025-03-17 04:31:19.535635 test begin: paddle.nn.functional.relu(Tensor([142606337, 16],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([142606337, 16],"float32"), )
2025-03-17 04:34:07.226805 test begin: paddle.nn.functional.relu(Tensor([147, 1024, 100, 152],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([147, 1024, 100, 152],"float32"), )
2025-03-17 04:37:04.230431 test begin: paddle.nn.functional.relu(Tensor([147, 1024, 152, 100],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([147, 1024, 152, 100],"float32"), )
2025-03-17 04:39:58.764985 test begin: paddle.nn.functional.relu(Tensor([157, 1024, 148, 96],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([157, 1024, 148, 96],"float32"), )
2025-03-17 04:42:42.362037 test begin: paddle.nn.functional.relu(Tensor([17825793, 128],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([17825793, 128],"float32"), )
2025-03-17 04:45:24.911591 test begin: paddle.nn.functional.relu(Tensor([187, 1024, 136, 88],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([187, 1024, 136, 88],"float32"), )
2025-03-17 04:48:10.562729 test begin: paddle.nn.functional.relu(Tensor([22737, 2048, 1, 7, 7],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([22737, 2048, 1, 7, 7],"float32"), None, )
2025-03-17 04:50:53.412792 test begin: paddle.nn.functional.relu(Tensor([2281701379, 1],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([2281701379, 1],"float32"), )
2025-03-17 04:53:46.706155 test begin: paddle.nn.functional.relu(Tensor([2281701379],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([2281701379],"float32"), )
2025-03-17 04:56:45.145929 test begin: paddle.nn.functional.relu(Tensor([228170138, 10],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([228170138, 10],"float32"), )
2025-03-17 04:59:27.910377 test begin: paddle.nn.functional.relu(Tensor([2843, 1024, 4, 14, 14],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([2843, 1024, 4, 14, 14],"float32"), )
2025-03-17 05:02:08.565077 test begin: paddle.nn.functional.relu(Tensor([2843, 128, 32, 14, 14],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([2843, 128, 32, 14, 14],"float32"), )
2025-03-17 05:04:52.530818 test begin: paddle.nn.functional.relu(Tensor([2843, 256, 4, 28, 28],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([2843, 256, 4, 28, 28],"float32"), None, )
2025-03-17 05:07:37.546355 test begin: paddle.nn.functional.relu(Tensor([405132, 22, 256],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([405132, 22, 256],"float32"), None, )
2025-03-17 05:10:39.133490 test begin: paddle.nn.functional.relu(Tensor([424424, 21, 256],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([424424, 21, 256],"float32"), None, )
2025-03-17 05:13:19.614956 test begin: paddle.nn.functional.relu(Tensor([429496730, 10],"float16"), None, )

[Pass] paddle.nn.functional.relu(Tensor([429496730, 10],"float16"), None, )
2025-03-17 05:31:29.145360 test begin: paddle.nn.functional.relu(Tensor([45474, 1024, 1, 7, 7],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([45474, 1024, 1, 7, 7],"float32"), None, )
2025-03-17 05:34:26.712619 test begin: paddle.nn.functional.relu(Tensor([495161, 18, 256],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([495161, 18, 256],"float32"), None, )
2025-03-17 05:37:13.422756 test begin: paddle.nn.functional.relu(Tensor([557057, 4096],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([557057, 4096],"float32"), None, )
2025-03-17 05:40:16.225182 test begin: paddle.nn.functional.relu(Tensor([5685, 1024, 2, 14, 14],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([5685, 1024, 2, 14, 14],"float32"), None, )
2025-03-17 05:43:12.421522 test begin: paddle.nn.functional.relu(Tensor([5685, 128, 4, 28, 28],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([5685, 128, 4, 28, 28],"float32"), )
2025-03-17 05:46:11.778807 test begin: paddle.nn.functional.relu(Tensor([5685, 16, 32, 28, 28],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([5685, 16, 32, 28, 28],"float32"), )
2025-03-17 05:49:08.922459 test begin: paddle.nn.functional.relu(Tensor([570425345, 1, 4],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([570425345, 1, 4],"float32"), )
2025-03-17 05:51:57.572345 test begin: paddle.nn.functional.relu(Tensor([57042535, 40],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([57042535, 40],"float32"), None, )
2025-03-17 05:55:10.936306 test begin: paddle.nn.functional.relu(Tensor([5803, 1024, 12, 32],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([5803, 1024, 12, 32],"float32"), None, )
2025-03-17 05:58:34.084282 test begin: paddle.nn.functional.relu(Tensor([61667605, 1, 37],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([61667605, 1, 37],"float32"), )
2025-03-17 06:01:20.805453 test begin: paddle.nn.functional.relu(Tensor([6166761, 10, 37],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([6166761, 10, 37],"float32"), )
2025-03-17 06:04:06.490191 test begin: paddle.nn.functional.relu(Tensor([6878, 1024, 18, 18],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([6878, 1024, 18, 18],"float32"), None, )
2025-03-17 06:07:00.365496 test begin: paddle.nn.functional.relu(Tensor([711, 128, 8, 56, 56],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([711, 128, 8, 56, 56],"float32"), None, )
2025-03-17 06:09:48.814513 test begin: paddle.nn.functional.relu(Tensor([71303169, 32],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([71303169, 32],"float32"), None, )
2025-03-17 06:12:50.359780 test begin: paddle.nn.functional.relu(Tensor([713032, 100, 32],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([713032, 100, 32],"float32"), )
2025-03-17 06:15:52.299555 test begin: paddle.nn.functional.relu(Tensor([8, 1024, 1, 39790, 7],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([8, 1024, 1, 39790, 7],"float32"), None, )
2025-03-17 06:18:49.923532 test begin: paddle.nn.functional.relu(Tensor([8, 1024, 1, 7, 39790],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([8, 1024, 1, 7, 39790],"float32"), None, )
2025-03-17 06:21:49.482522 test begin: paddle.nn.functional.relu(Tensor([8, 1024, 1422, 14, 14],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([8, 1024, 1422, 14, 14],"float32"), )
2025-03-17 06:25:13.474199 test begin: paddle.nn.functional.relu(Tensor([8, 1024, 1422, 14, 14],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([8, 1024, 1422, 14, 14],"float32"), None, )
2025-03-17 06:28:13.222003 test begin: paddle.nn.functional.relu(Tensor([8, 1024, 2, 14, 9948],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([8, 1024, 2, 14, 9948],"float32"), None, )
2025-03-17 06:31:22.880883 test begin: paddle.nn.functional.relu(Tensor([8, 1024, 2, 9948, 14],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([8, 1024, 2, 9948, 14],"float32"), None, )
2025-03-17 06:34:14.365229 test begin: paddle.nn.functional.relu(Tensor([8, 1024, 4, 14, 4974],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([8, 1024, 4, 14, 4974],"float32"), )
2025-03-17 06:37:19.377124 test begin: paddle.nn.functional.relu(Tensor([8, 1024, 4, 4974, 14],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([8, 1024, 4, 4974, 14],"float32"), )
2025-03-17 06:40:27.079030 test begin: paddle.nn.functional.relu(Tensor([8, 1024, 5685, 7, 7],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([8, 1024, 5685, 7, 7],"float32"), None, )
2025-03-17 06:43:26.760251 test begin: paddle.nn.functional.relu(Tensor([8, 11369, 32, 28, 28],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([8, 11369, 32, 28, 28],"float32"), )
2025-03-17 06:46:34.033141 test begin: paddle.nn.functional.relu(Tensor([8, 11369, 8, 56, 56],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([8, 11369, 8, 56, 56],"float32"), None, )
2025-03-17 06:49:21.531557 test begin: paddle.nn.functional.relu(Tensor([8, 128, 11369, 14, 14],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([8, 128, 11369, 14, 14],"float32"), )
2025-03-17 06:52:23.637143 test begin: paddle.nn.functional.relu(Tensor([8, 128, 2843, 28, 28],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([8, 128, 2843, 28, 28],"float32"), )
2025-03-17 06:55:38.903856 test begin: paddle.nn.functional.relu(Tensor([8, 128, 32, 14, 4974],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([8, 128, 32, 14, 4974],"float32"), )
2025-03-17 06:59:00.725722 test begin: paddle.nn.functional.relu(Tensor([8, 128, 32, 4974, 14],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([8, 128, 32, 4974, 14],"float32"), )
2025-03-17 07:02:01.919106 test begin: paddle.nn.functional.relu(Tensor([8, 128, 4, 19895, 28],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([8, 128, 4, 19895, 28],"float32"), )
2025-03-17 07:05:06.599838 test begin: paddle.nn.functional.relu(Tensor([8, 128, 4, 28, 19895],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([8, 128, 4, 28, 19895],"float32"), )
2025-03-17 07:07:57.458265 test begin: paddle.nn.functional.relu(Tensor([8, 128, 4, 56, 9948],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([8, 128, 4, 56, 9948],"float32"), )
2025-03-17 07:10:57.407418 test begin: paddle.nn.functional.relu(Tensor([8, 128, 4, 9948, 56],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([8, 128, 4, 9948, 56],"float32"), )
2025-03-17 07:13:51.158202 test begin: paddle.nn.functional.relu(Tensor([8, 128, 711, 56, 56],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([8, 128, 711, 56, 56],"float32"), )
2025-03-17 07:17:01.276901 test begin: paddle.nn.functional.relu(Tensor([8, 128, 711, 56, 56],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([8, 128, 711, 56, 56],"float32"), None, )
2025-03-17 07:20:00.090569 test begin: paddle.nn.functional.relu(Tensor([8, 128, 8, 4974, 56],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([8, 128, 8, 4974, 56],"float32"), None, )
2025-03-17 07:23:00.339564 test begin: paddle.nn.functional.relu(Tensor([8, 128, 8, 56, 4974],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([8, 128, 8, 56, 4974],"float32"), None, )
2025-03-17 07:26:03.714964 test begin: paddle.nn.functional.relu(Tensor([8, 16, 22737, 28, 28],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([8, 16, 22737, 28, 28],"float32"), )
2025-03-17 07:28:52.491705 test begin: paddle.nn.functional.relu(Tensor([8, 16, 32, 19895, 28],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([8, 16, 32, 19895, 28],"float32"), )
2025-03-17 07:31:39.643515 test begin: paddle.nn.functional.relu(Tensor([8, 16, 32, 28, 19895],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([8, 16, 32, 28, 19895],"float32"), )
2025-03-17 07:34:56.547196 test begin: paddle.nn.functional.relu(Tensor([8, 2048, 1, 19895, 7],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([8, 2048, 1, 19895, 7],"float32"), None, )
2025-03-17 07:38:33.174130 test begin: paddle.nn.functional.relu(Tensor([8, 2048, 1, 7, 19895],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([8, 2048, 1, 7, 19895],"float32"), None, )
2025-03-17 07:41:32.235039 test begin: paddle.nn.functional.relu(Tensor([8, 2048, 2843, 7, 7],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([8, 2048, 2843, 7, 7],"float32"), None, )
2025-03-17 07:44:56.570342 test begin: paddle.nn.functional.relu(Tensor([8, 22737, 4, 56, 56],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([8, 22737, 4, 56, 56],"float32"), )
2025-03-17 07:48:13.177859 test begin: paddle.nn.functional.relu(Tensor([8, 256, 1422, 28, 28],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([8, 256, 1422, 28, 28],"float32"), None, )
2025-03-17 07:51:16.645300 test begin: paddle.nn.functional.relu(Tensor([8, 256, 4, 28, 9948],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([8, 256, 4, 28, 9948],"float32"), None, )
2025-03-17 07:54:01.935598 test begin: paddle.nn.functional.relu(Tensor([8, 256, 4, 9948, 28],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([8, 256, 4, 9948, 28],"float32"), None, )
2025-03-17 07:57:05.914633 test begin: paddle.nn.functional.relu(Tensor([8, 363792, 4, 14, 14],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([8, 363792, 4, 14, 14],"float32"), )
2025-03-17 07:59:50.185501 test begin: paddle.nn.functional.relu(Tensor([8, 45474, 32, 14, 14],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([8, 45474, 32, 14, 14],"float32"), )
2025-03-17 08:02:35.183084 test begin: paddle.nn.functional.relu(Tensor([8, 5820667, 1, 7, 7],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([8, 5820667, 1, 7, 7],"float32"), None, )
2025-03-17 08:05:30.347291 test begin: paddle.nn.functional.relu(Tensor([8, 727584, 2, 14, 14],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([8, 727584, 2, 14, 14],"float32"), None, )
2025-03-17 08:08:15.171916 test begin: paddle.nn.functional.relu(Tensor([8, 90948, 4, 28, 28],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([8, 90948, 4, 28, 28],"float32"), )
2025-03-17 08:11:18.039468 test begin: paddle.nn.functional.relu(Tensor([8, 90948, 4, 28, 28],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([8, 90948, 4, 28, 28],"float32"), None, )
2025-03-17 08:14:13.456885 test begin: paddle.nn.functional.relu(Tensor([810264, 11, 256],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([810264, 11, 256],"float32"), None, )
2025-03-17 08:16:57.767815 test begin: paddle.nn.functional.relu(Tensor([8103, 1100, 256],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([8103, 1100, 256],"float32"), None, )
2025-03-17 08:19:40.554858 test begin: paddle.nn.functional.relu(Tensor([8571, 1024, 10, 26],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([8571, 1024, 10, 26],"float32"), None, )
2025-03-17 08:22:50.203096 test begin: paddle.nn.functional.relu(Tensor([8705, 1024, 16, 16],"float32"), None, )

[Pass] paddle.nn.functional.relu(Tensor([8705, 1024, 16, 16],"float32"), None, )
2025-03-17 08:26:10.682693 test begin: paddle.nn.functional.relu(Tensor([877, 10164, 256],"float32"), )

[Pass] paddle.nn.functional.relu(Tensor([877, 10164, 256],"float32"), )
2025-03-17 08:29:09.470237 test begin: paddle.nn.functional.relu(x=Tensor([10, 10, 42949673],"float16"), )

[Pass] paddle.nn.functional.relu(x=Tensor([10, 10, 42949673],"float16"), )
2025-03-17 08:47:12.000300 test begin: paddle.nn.functional.relu(x=Tensor([10, 42949673, 10],"float16"), )

[Pass] paddle.nn.functional.relu(x=Tensor([10, 42949673, 10],"float16"), )
2025-03-17 09:05:04.306450 test begin: paddle.nn.functional.relu(x=Tensor([253522376, 3, 3],"float32"), )

[Pass] paddle.nn.functional.relu(x=Tensor([253522376, 3, 3],"float32"), )
2025-03-17 09:08:27.479928 test begin: paddle.nn.functional.relu(x=Tensor([3, 253522376, 3],"float32"), )

[Pass] paddle.nn.functional.relu(x=Tensor([3, 253522376, 3],"float32"), )
2025-03-17 09:11:34.902103 test begin: paddle.nn.functional.relu(x=Tensor([3, 3, 253522376],"float32"), )

[Pass] paddle.nn.functional.relu(x=Tensor([3, 3, 253522376],"float32"), )
2025-03-17 09:14:16.312474 test begin: paddle.nn.functional.relu(x=Tensor([3, 3, 477218589],"float16"), )

[Pass] paddle.nn.functional.relu(x=Tensor([3, 3, 477218589],"float16"), )
2025-03-17 09:32:00.220221 test begin: paddle.nn.functional.relu(x=Tensor([3, 477218589, 3],"float16"), )

[Pass] paddle.nn.functional.relu(x=Tensor([3, 477218589, 3],"float16"), )
2025-03-17 09:50:13.229914 test begin: paddle.nn.functional.relu(x=Tensor([42949673, 10, 10],"float16"), )

[Pass] paddle.nn.functional.relu(x=Tensor([42949673, 10, 10],"float16"), )
2025-03-17 10:08:01.007831 test begin: paddle.nn.functional.relu(x=Tensor([477218589, 3, 3],"float16"), )

[Pass] paddle.nn.functional.relu(x=Tensor([477218589, 3, 3],"float16"), )
2025-03-17 10:26:15.148173 test begin: paddle.nn.functional.relu6(Tensor([1, 11641334, 14, 14],"float32"), )

[Pass] paddle.nn.functional.relu6(Tensor([1, 11641334, 14, 14],"float32"), )
2025-03-17 10:29:55.608566 test begin: paddle.nn.functional.relu6(Tensor([1, 11641334, 14, 14],"float32"), None, )

[Pass] paddle.nn.functional.relu6(Tensor([1, 11641334, 14, 14],"float32"), None, )
2025-03-17 10:32:49.829094 test begin: paddle.nn.functional.relu6(Tensor([1, 1280, 254655, 7],"float32"), )

[Pass] paddle.nn.functional.relu6(Tensor([1, 1280, 254655, 7],"float32"), )
2025-03-17 10:36:19.860832 test begin: paddle.nn.functional.relu6(Tensor([1, 1280, 254655, 7],"float32"), None, )

[Pass] paddle.nn.functional.relu6(Tensor([1, 1280, 254655, 7],"float32"), None, )
2025-03-17 10:39:27.453820 test begin: paddle.nn.functional.relu6(Tensor([1, 1280, 7, 254655],"float32"), )

[Pass] paddle.nn.functional.relu6(Tensor([1, 1280, 7, 254655],"float32"), )
2025-03-17 10:42:40.212120 test begin: paddle.nn.functional.relu6(Tensor([1, 1280, 7, 254655],"float32"), None, )

[Pass] paddle.nn.functional.relu6(Tensor([1, 1280, 7, 254655],"float32"), None, )
2025-03-17 10:45:46.628979 test begin: paddle.nn.functional.relu6(Tensor([1, 144, 28, 565899],"float32"), )

[Pass] paddle.nn.functional.relu6(Tensor([1, 144, 28, 565899],"float32"), )
2025-03-17 10:48:57.436542 test begin: paddle.nn.functional.relu6(Tensor([1, 144, 28, 565899],"float32"), None, )

[Pass] paddle.nn.functional.relu6(Tensor([1, 144, 28, 565899],"float32"), None, )
2025-03-17 10:52:03.950183 test begin: paddle.nn.functional.relu6(Tensor([1, 144, 282950, 56],"float32"), )

[Pass] paddle.nn.functional.relu6(Tensor([1, 144, 282950, 56],"float32"), )
2025-03-17 10:55:06.425078 test begin: paddle.nn.functional.relu6(Tensor([1, 144, 282950, 56],"float32"), None, )

[Pass] paddle.nn.functional.relu6(Tensor([1, 144, 282950, 56],"float32"), None, )
2025-03-17 10:58:38.995159 test begin: paddle.nn.functional.relu6(Tensor([1, 144, 56, 282950],"float32"), )

[Pass] paddle.nn.functional.relu6(Tensor([1, 144, 56, 282950],"float32"), )
2025-03-17 11:02:13.420344 test begin: paddle.nn.functional.relu6(Tensor([1, 144, 56, 282950],"float32"), None, )

[Pass] paddle.nn.functional.relu6(Tensor([1, 144, 56, 282950],"float32"), None, )
2025-03-17 11:05:52.696752 test begin: paddle.nn.functional.relu6(Tensor([1, 144, 565899, 28],"float32"), )

[Pass] paddle.nn.functional.relu6(Tensor([1, 144, 565899, 28],"float32"), )
2025-03-17 11:09:58.882289 test begin: paddle.nn.functional.relu6(Tensor([1, 144, 565899, 28],"float32"), None, )

[Pass] paddle.nn.functional.relu6(Tensor([1, 144, 565899, 28],"float32"), None, )
2025-03-17 11:12:58.708762 test begin: paddle.nn.functional.relu6(Tensor([1, 192, 14, 848848],"float32"), )

[Pass] paddle.nn.functional.relu6(Tensor([1, 192, 14, 848848],"float32"), )
2025-03-17 11:16:44.679322 test begin: paddle.nn.functional.relu6(Tensor([1, 192, 14, 848848],"float32"), None, )

[Pass] paddle.nn.functional.relu6(Tensor([1, 192, 14, 848848],"float32"), None, )
2025-03-17 11:20:01.460843 test begin: paddle.nn.functional.relu6(Tensor([1, 192, 28, 424424],"float32"), )

[Pass] paddle.nn.functional.relu6(Tensor([1, 192, 28, 424424],"float32"), )
2025-03-17 11:23:50.679426 test begin: paddle.nn.functional.relu6(Tensor([1, 192, 28, 424424],"float32"), None, )

CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 5.66 GiB is free. Process 32555 has 27.09 GiB memory in use. Process 152129 has 46.42 GiB memory in use. Of the allocated memory 25.50 GiB is allocated by PyTorch, and 5.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-17 11:24:10.076658 test begin: paddle.nn.functional.relu6(Tensor([1, 192, 424424, 28],"float32"), )

CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 5.66 GiB is free. Process 152129 has 46.42 GiB memory in use. Process 79224 has 27.09 GiB memory in use. Of the allocated memory 25.50 GiB is allocated by PyTorch, and 5.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-17 11:26:05.004949 test begin: paddle.nn.functional.relu6(Tensor([1, 192, 424424, 28],"float32"), None, )

CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 5.66 GiB is free. Process 152129 has 46.42 GiB memory in use. Process 121379 has 27.09 GiB memory in use. Of the allocated memory 25.50 GiB is allocated by PyTorch, and 5.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-17 11:27:56.474747 test begin: paddle.nn.functional.relu6(Tensor([1, 192, 848848, 14],"float32"), )

CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 5.66 GiB is free. Process 152129 has 46.42 GiB memory in use. Process 151809 has 27.09 GiB memory in use. Of the allocated memory 25.50 GiB is allocated by PyTorch, and 5.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-17 11:29:49.794790 test begin: paddle.nn.functional.relu6(Tensor([1, 192, 848848, 14],"float32"), None, )

W0317 11:31:30.331228 111545 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0317 11:31:30.332549 111545 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.nn.functional.relu6(Tensor([1, 192, 848848, 14],"float32"), None, )
2025-03-17 11:34:02.602235 test begin: paddle.nn.functional.relu6(Tensor([1, 2910334, 28, 28],"float32"), )

[Pass] paddle.nn.functional.relu6(Tensor([1, 2910334, 28, 28],"float32"), )
2025-03-17 11:37:23.278353 test begin: paddle.nn.functional.relu6(Tensor([1, 2910334, 28, 28],"float32"), None, )

[paddle error] paddle.nn.functional.relu6(Tensor([1, 2910334, 28, 28],"float32"), None, ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   egr::GradTensorHolder::CopyValueFromTensor(unsigned long, unsigned long, paddle::Tensor const&, bool)
3   paddle::Tensor::copy_(paddle::Tensor const&, phi::Place const&, bool)
4   void phi::Copy<phi::DeviceContext>(phi::DeviceContext const&, phi::DenseTensor const&, phi::Place, bool, phi::DenseTensor*)
5   phi::DeviceContext::Alloc(phi::TensorBase*, phi::DataType, unsigned long, bool, bool) const
6   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 8.500002GB memory on GPU 0, 73.623962GB memory has been allocated and available memory is only 5.560913GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-17 11:39:25.264089 test begin: paddle.nn.functional.relu6(Tensor([1, 46565335, 7, 7],"float32"), )

[Pass] paddle.nn.functional.relu6(Tensor([1, 46565335, 7, 7],"float32"), )
2025-03-17 11:42:44.792920 test begin: paddle.nn.functional.relu6(Tensor([1, 46565335, 7, 7],"float32"), None, )

[Pass] paddle.nn.functional.relu6(Tensor([1, 46565335, 7, 7],"float32"), None, )
2025-03-17 11:45:59.788621 test begin: paddle.nn.functional.relu6(Tensor([1, 727584, 56, 56],"float32"), )

[Pass] paddle.nn.functional.relu6(Tensor([1, 727584, 56, 56],"float32"), )
2025-03-17 11:49:30.069187 test begin: paddle.nn.functional.relu6(Tensor([1, 727584, 56, 56],"float32"), None, )

[Pass] paddle.nn.functional.relu6(Tensor([1, 727584, 56, 56],"float32"), None, )
2025-03-17 11:52:46.997406 test begin: paddle.nn.functional.relu6(Tensor([15158, 192, 28, 28],"float32"), )

[Pass] paddle.nn.functional.relu6(Tensor([15158, 192, 28, 28],"float32"), )
2025-03-17 11:56:06.740392 test begin: paddle.nn.functional.relu6(Tensor([15158, 192, 28, 28],"float32"), None, )

[Pass] paddle.nn.functional.relu6(Tensor([15158, 192, 28, 28],"float32"), None, )
2025-03-17 11:59:15.992021 test begin: paddle.nn.functional.relu6(Tensor([2, 300, 3802836],"float32"), )

[Pass] paddle.nn.functional.relu6(Tensor([2, 300, 3802836],"float32"), )
2025-03-17 12:02:18.197894 test begin: paddle.nn.functional.relu6(Tensor([2, 557057, 2048],"float32"), )

[Pass] paddle.nn.functional.relu6(Tensor([2, 557057, 2048],"float32"), )
2025-03-17 12:05:20.307842 test begin: paddle.nn.functional.relu6(Tensor([20211, 144, 28, 28],"float32"), )

[Pass] paddle.nn.functional.relu6(Tensor([20211, 144, 28, 28],"float32"), )
2025-03-17 12:08:27.323801 test begin: paddle.nn.functional.relu6(Tensor([20211, 144, 28, 28],"float32"), None, )

[Pass] paddle.nn.functional.relu6(Tensor([20211, 144, 28, 28],"float32"), None, )
2025-03-17 12:11:29.460076 test begin: paddle.nn.functional.relu6(Tensor([2281701379],"float32"), None, )

[Pass] paddle.nn.functional.relu6(Tensor([2281701379],"float32"), None, )
2025-03-17 12:14:49.260444 test begin: paddle.nn.functional.relu6(Tensor([3, 3, 477218589],"float16"), None, )

[Pass] paddle.nn.functional.relu6(Tensor([3, 3, 477218589],"float16"), None, )
2025-03-17 12:34:07.909207 test begin: paddle.nn.functional.relu6(Tensor([3, 477218589, 3],"float16"), None, )

[Pass] paddle.nn.functional.relu6(Tensor([3, 477218589, 3],"float16"), None, )
2025-03-17 12:52:14.577977 test begin: paddle.nn.functional.relu6(Tensor([36380, 1280, 7, 7],"float32"), )

[Pass] paddle.nn.functional.relu6(Tensor([36380, 1280, 7, 7],"float32"), )
2025-03-17 12:55:06.412328 test begin: paddle.nn.functional.relu6(Tensor([36380, 1280, 7, 7],"float32"), None, )

[Pass] paddle.nn.functional.relu6(Tensor([36380, 1280, 7, 7],"float32"), None, )
2025-03-17 12:58:09.563241 test begin: paddle.nn.functional.relu6(Tensor([3714, 300, 2048],"float32"), )

[Pass] paddle.nn.functional.relu6(Tensor([3714, 300, 2048],"float32"), )
2025-03-17 13:01:12.163274 test begin: paddle.nn.functional.relu6(Tensor([4294967297],"float16"), None, )

[Pass] paddle.nn.functional.relu6(Tensor([4294967297],"float16"), None, )
2025-03-17 13:19:35.455543 test begin: paddle.nn.functional.relu6(Tensor([4456449, 16, 32],"float32"), None, )

[Pass] paddle.nn.functional.relu6(Tensor([4456449, 16, 32],"float32"), None, )
2025-03-17 13:23:37.409154 test begin: paddle.nn.functional.relu6(Tensor([477218589, 3, 3],"float16"), None, )

[Pass] paddle.nn.functional.relu6(Tensor([477218589, 3, 3],"float16"), None, )
2025-03-17 13:43:24.341642 test begin: paddle.nn.functional.relu6(Tensor([5053, 144, 56, 56],"float32"), )

[Pass] paddle.nn.functional.relu6(Tensor([5053, 144, 56, 56],"float32"), )
2025-03-17 13:47:41.849329 test begin: paddle.nn.functional.relu6(Tensor([5053, 144, 56, 56],"float32"), None, )

[Pass] paddle.nn.functional.relu6(Tensor([5053, 144, 56, 56],"float32"), None, )
2025-03-17 13:52:17.899790 test begin: paddle.nn.functional.relu6(Tensor([60632, 192, 14, 14],"float32"), )

[Pass] paddle.nn.functional.relu6(Tensor([60632, 192, 14, 14],"float32"), )
2025-03-17 13:55:17.750824 test begin: paddle.nn.functional.relu6(Tensor([60632, 192, 14, 14],"float32"), None, )

[Pass] paddle.nn.functional.relu6(Tensor([60632, 192, 14, 14],"float32"), None, )
2025-03-17 13:58:43.219670 test begin: paddle.nn.functional.relu6(Tensor([8, 16, 17825793],"float32"), None, )

[Pass] paddle.nn.functional.relu6(Tensor([8, 16, 17825793],"float32"), None, )
2025-03-17 14:01:42.079220 test begin: paddle.nn.functional.relu6(Tensor([8, 8912897, 32],"float32"), None, )

[Pass] paddle.nn.functional.relu6(Tensor([8, 8912897, 32],"float32"), None, )
2025-03-17 14:05:15.704481 test begin: paddle.nn.functional.relu6(x=Tensor([2281701379],"float32"), )

[Pass] paddle.nn.functional.relu6(x=Tensor([2281701379],"float32"), )
2025-03-17 14:08:14.013260 test begin: paddle.nn.functional.relu6(x=Tensor([3, 3, 477218589],"float16"), )

[paddle error] paddle.nn.functional.relu6(x=Tensor([3, 3, 477218589],"float16"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   egr::GradTensorHolder::CopyValueFromTensor(unsigned long, unsigned long, paddle::Tensor const&, bool)
3   paddle::Tensor::copy_(paddle::Tensor const&, phi::Place const&, bool)
4   void phi::Copy<phi::DeviceContext>(phi::DeviceContext const&, phi::DenseTensor const&, phi::Place, bool, phi::DenseTensor*)
5   phi::DeviceContext::Alloc(phi::TensorBase*, phi::DataType, unsigned long, bool, bool) const
6   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 8.000000GB memory on GPU 0, 71.936462GB memory has been allocated and available memory is only 7.248413GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-17 14:17:46.277553 test begin: paddle.nn.functional.relu6(x=Tensor([3, 477218589, 3],"float16"), )

2025-03-17 14:18:01.063362 test begin: paddle.nn.functional.relu6(x=Tensor([4294967297],"float16"), )

CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 7.25 GiB is free. Process 26880 has 46.34 GiB memory in use. Process 19489 has 25.59 GiB memory in use. Of the allocated memory 24.00 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-17 14:20:29.239779 test begin: paddle.nn.functional.relu6(x=Tensor([477218589, 3, 3],"float16"), )

W0317 14:22:38.668511  5418 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0317 14:22:38.670082  5418 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.nn.functional.relu6(x=Tensor([477218589, 3, 3],"float16"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   egr::GradTensorHolder::CopyValueFromTensor(unsigned long, unsigned long, paddle::Tensor const&, bool)
3   paddle::Tensor::copy_(paddle::Tensor const&, phi::Place const&, bool)
4   void phi::Copy<phi::DeviceContext>(phi::DeviceContext const&, phi::DenseTensor const&, phi::Place, bool, phi::DenseTensor*)
5   phi::DeviceContext::Alloc(phi::TensorBase*, phi::DataType, unsigned long, bool, bool) const
6   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 8.000000GB memory on GPU 0, 71.998962GB memory has been allocated and available memory is only 7.185913GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-17 14:32:40.934353 test begin: paddle.nn.functional.rrelu(Tensor([1, 2, 3, 715827883],"float16"), 0.05, 0.25, training=False, )

[torch error] paddle.nn.functional.rrelu(Tensor([1, 2, 3, 715827883],"float16"), 0.05, 0.25, training=False, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 7.19 GiB is free. Process 94500 has 25.59 GiB memory in use. Process 11117 has 46.40 GiB memory in use. Of the allocated memory 16.00 GiB is allocated by PyTorch, and 4.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-17 14:32:44.588389 test begin: paddle.nn.functional.rrelu(Tensor([1, 2, 3, 715827883],"float16"), 0.1, 0.33, training=False, )

[torch error] paddle.nn.functional.rrelu(Tensor([1, 2, 3, 715827883],"float16"), 0.1, 0.33, training=False, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 7.19 GiB is free. Process 94500 has 25.59 GiB memory in use. Process 11117 has 46.40 GiB memory in use. Of the allocated memory 16.00 GiB is allocated by PyTorch, and 4.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-17 14:32:46.722670 test begin: paddle.nn.functional.rrelu(Tensor([1, 2, 536870913, 4],"float16"), 0.05, 0.25, training=False, )

[torch error] paddle.nn.functional.rrelu(Tensor([1, 2, 536870913, 4],"float16"), 0.05, 0.25, training=False, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 7.19 GiB is free. Process 94500 has 25.59 GiB memory in use. Process 11117 has 46.40 GiB memory in use. Of the allocated memory 16.00 GiB is allocated by PyTorch, and 4.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-17 14:32:48.910338 test begin: paddle.nn.functional.rrelu(Tensor([1, 2, 536870913, 4],"float16"), 0.1, 0.33, training=False, )

[torch error] paddle.nn.functional.rrelu(Tensor([1, 2, 536870913, 4],"float16"), 0.1, 0.33, training=False, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 7.19 GiB is free. Process 94500 has 25.59 GiB memory in use. Process 11117 has 46.40 GiB memory in use. Of the allocated memory 16.00 GiB is allocated by PyTorch, and 4.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-17 14:32:51.086674 test begin: paddle.nn.functional.rrelu(Tensor([1, 357913942, 3, 4],"float16"), 0.05, 0.25, training=False, )

[torch error] paddle.nn.functional.rrelu(Tensor([1, 357913942, 3, 4],"float16"), 0.05, 0.25, training=False, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 3.28 GiB is free. Process 94500 has 25.59 GiB memory in use. Process 11117 has 50.30 GiB memory in use. Of the allocated memory 16.00 GiB is allocated by PyTorch, and 4.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-17 14:32:53.097645 test begin: paddle.nn.functional.rrelu(Tensor([1, 357913942, 3, 4],"float16"), 0.1, 0.33, training=False, )

[torch error] paddle.nn.functional.rrelu(Tensor([1, 357913942, 3, 4],"float16"), 0.1, 0.33, training=False, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 3.28 GiB is free. Process 94500 has 25.59 GiB memory in use. Process 11117 has 50.30 GiB memory in use. Of the allocated memory 16.00 GiB is allocated by PyTorch, and 4.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-17 14:32:55.491456 test begin: paddle.nn.functional.rrelu(Tensor([178956971, 2, 3, 4],"float16"), 0.05, 0.25, training=False, )

[torch error] paddle.nn.functional.rrelu(Tensor([178956971, 2, 3, 4],"float16"), 0.05, 0.25, training=False, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 3.28 GiB is free. Process 94500 has 25.59 GiB memory in use. Process 11117 has 50.30 GiB memory in use. Of the allocated memory 16.00 GiB is allocated by PyTorch, and 4.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-17 14:32:57.292443 test begin: paddle.nn.functional.rrelu(Tensor([178956971, 2, 3, 4],"float16"), 0.1, 0.33, training=False, )

[torch error] paddle.nn.functional.rrelu(Tensor([178956971, 2, 3, 4],"float16"), 0.1, 0.33, training=False, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 3.28 GiB is free. Process 94500 has 25.59 GiB memory in use. Process 11117 has 50.30 GiB memory in use. Of the allocated memory 16.00 GiB is allocated by PyTorch, and 4.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-17 14:33:00.808697 test begin: paddle.nn.functional.rrelu(Tensor([2, 107374183, 4, 5],"float16"), 0.1, 0.3, training=False, )

[torch error] paddle.nn.functional.rrelu(Tensor([2, 107374183, 4, 5],"float16"), 0.1, 0.3, training=False, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 3.28 GiB is free. Process 94500 has 25.59 GiB memory in use. Process 11117 has 50.30 GiB memory in use. Of the allocated memory 16.00 GiB is allocated by PyTorch, and 4.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-17 14:33:03.080919 test begin: paddle.nn.functional.rrelu(Tensor([2, 107374183, 4, 5],"float16"), 0.3, 0.300000009, training=True, )

[torch error] paddle.nn.functional.rrelu(Tensor([2, 107374183, 4, 5],"float16"), 0.3, 0.300000009, training=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 3.28 GiB is free. Process 94500 has 25.59 GiB memory in use. Process 11117 has 50.30 GiB memory in use. Of the allocated memory 16.00 GiB is allocated by PyTorch, and 4.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-17 14:33:05.621349 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 143165577, 5],"float16"), 0.1, 0.3, training=False, )

[torch error] paddle.nn.functional.rrelu(Tensor([2, 3, 143165577, 5],"float16"), 0.1, 0.3, training=False, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 3.28 GiB is free. Process 94500 has 25.59 GiB memory in use. Process 11117 has 50.30 GiB memory in use. Of the allocated memory 16.00 GiB is allocated by PyTorch, and 4.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-17 14:33:07.220743 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 143165577, 5],"float16"), 0.3, 0.300000009, training=True, )

[torch error] paddle.nn.functional.rrelu(Tensor([2, 3, 143165577, 5],"float16"), 0.3, 0.300000009, training=True, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 3.28 GiB is free. Process 94500 has 25.59 GiB memory in use. Process 11117 has 50.30 GiB memory in use. Of the allocated memory 16.00 GiB is allocated by PyTorch, and 4.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-17 14:33:11.405690 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 4, 178956971],"float16"), 0.1, 0.3, training=False, )

[accuracy error] paddle.nn.functional.rrelu(Tensor([2, 3, 4, 178956971],"float16"), 0.1, 0.3, training=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 7 / 4294967304 (1.63e-07%)
Max absolute difference: 0.361
Max relative difference: 1.
 x: array([[[[-0.0531  , -0.05835 ,  0.308   , ...,  0.2576  ,  0.4446  ,
           0.2756  ],
         [ 0.469   , -0.07996 , -0.0296  , ...,  0.3003  ,  0.1124  ,...
 y: array([[[[-0.05313 , -0.05835 ,  0.308   , ...,  0.2576  ,  0.4446  ,
           0.2756  ],
         [ 0.469   , -0.07996 , -0.02962 , ...,  0.3003  ,  0.1124  ,...
2025-03-17 14:47:32.919745 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 4, 178956971],"float16"), 0.3, 0.300000009, training=True, )

[accuracy error] paddle.nn.functional.rrelu(Tensor([2, 3, 4, 178956971],"float16"), 0.3, 0.300000009, training=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4108795124 / 4294967304 (95.7%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[-0.0797  , -0.0875  ,  0.308   , ...,  0.2576  ,  0.4446  ,
           0.2756  ],
         [ 0.469   , -0.12    , -0.04443 , ...,  0.3003  ,  0.1124  ,...
 y: array([[[[-0.0797, -0.0875,  0.308 , ...,  0.    ,  0.    ,  0.    ],
         [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],
         [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],...
2025-03-17 14:58:56.508839 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 4, 95070891],"float32"), 0.1, 0.3, training=False, )

[cuda error] paddle.nn.functional.rrelu(Tensor([2, 3, 4, 95070891],"float32"), 0.1, 0.3, training=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-17 15:02:34.731035 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 4, 95070891],"float32"), 0.3, 0.300000009, training=True, )

[accuracy error] paddle.nn.functional.rrelu(Tensor([2, 3, 4, 95070891],"float32"), 0.3, 0.300000009, training=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2182828714 / 2281701384 (95.7%)
Max absolute difference: 0.5
Max relative difference: inf
 x: array([[[[ 3.395617e-01,  4.701345e-01,  8.662650e-02, ...,
          -1.015324e-02,  3.818778e-01,  1.313297e-01],
         [ 1.739767e-01, -6.382489e-02, -1.866520e-02, ...,...
 y: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
2025-03-17 15:04:38.239221 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 76056713, 5],"float32"), 0.1, 0.3, training=False, )

[cuda error] paddle.nn.functional.rrelu(Tensor([2, 3, 76056713, 5],"float32"), 0.1, 0.3, training=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-17 15:06:30.149525 test begin: paddle.nn.functional.rrelu(Tensor([2, 3, 76056713, 5],"float32"), 0.3, 0.300000009, training=True, )

[accuracy error] paddle.nn.functional.rrelu(Tensor([2, 3, 76056713, 5],"float32"), 0.3, 0.300000009, training=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2182828719 / 2281701390 (95.7%)
Max absolute difference: 0.5
Max relative difference: inf
 x: array([[[[ 3.395617e-01,  4.701345e-01,  8.662650e-02,  2.809898e-01,
          -1.329210e-01],
         [ 1.067662e-01,  1.556489e-01,  2.144405e-01,  9.555900e-02,...
 y: array([[[[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],...
2025-03-17 15:08:39.007424 test begin: paddle.nn.functional.rrelu(Tensor([2, 57042535, 4, 5],"float32"), 0.1, 0.3, training=False, )

[cuda error] paddle.nn.functional.rrelu(Tensor([2, 57042535, 4, 5],"float32"), 0.1, 0.3, training=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-17 15:10:27.896419 test begin: paddle.nn.functional.rrelu(Tensor([2, 57042535, 4, 5],"float32"), 0.3, 0.300000009, training=True, )

[accuracy error] paddle.nn.functional.rrelu(Tensor([2, 57042535, 4, 5],"float32"), 0.3, 0.300000009, training=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2182828729 / 2281701400 (95.7%)
Max absolute difference: 0.5
Max relative difference: inf
 x: array([[[[ 3.395617e-01,  4.701345e-01,  8.662650e-02,  2.809898e-01,
          -1.329210e-01],
         [ 1.067662e-01,  1.556489e-01,  2.144405e-01,  9.555900e-02,...
 y: array([[[[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],...
2025-03-17 15:12:33.205699 test begin: paddle.nn.functional.rrelu(Tensor([38028357, 3, 4, 5],"float32"), 0.1, 0.3, training=False, )

[cuda error] paddle.nn.functional.rrelu(Tensor([38028357, 3, 4, 5],"float32"), 0.1, 0.3, training=False, ) 
 (External) CUDA error(9), invalid configuration argument. 
  [Hint: 'cudaErrorInvalidConfiguration'. This indicates that a kernel launch is requesting resources that can never be satisfied by the current device. Requestingmore shared memory per block than the device supports will trigger this error, as will requesting too many threads or blocks.See cudaDeviceProp for more device limitations.] (at ../paddle/fluid/pybind/eager_functions.cc:1388)

2025-03-17 15:14:27.325424 test begin: paddle.nn.functional.rrelu(Tensor([38028357, 3, 4, 5],"float32"), 0.3, 0.300000009, training=True, )

[accuracy error] paddle.nn.functional.rrelu(Tensor([38028357, 3, 4, 5],"float32"), 0.3, 0.300000009, training=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2182828749 / 2281701420 (95.7%)
Max absolute difference: 0.5
Max relative difference: inf
 x: array([[[[ 0.339562,  0.470134,  0.086626,  0.28099 , -0.132921],
         [ 0.106766,  0.155649,  0.21444 ,  0.095559,  0.267009],
         [-0.004045, -0.139   ,  0.457139, -0.137002, -0.013196],...
 y: array([[[[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],...
2025-03-17 15:16:22.777111 test begin: paddle.nn.functional.rrelu(Tensor([71582789, 3, 4, 5],"float16"), 0.1, 0.3, training=False, )

[accuracy error] paddle.nn.functional.rrelu(Tensor([71582789, 3, 4, 5],"float16"), 0.1, 0.3, training=False, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 42 / 4294967340 (9.78e-07%)
Max absolute difference: 0.4988
Max relative difference: 1.
 x: array([[[[-0.0531  , -0.05835 ,  0.308   , -0.02565 ,  0.1033  ],
         [ 0.3086  ,  0.1168  , -0.0875  , -0.01581 , -0.0646  ],
         [ 0.324   , -0.09283 ,  0.3591  ,  0.325   , -0.07635 ],...
 y: array([[[[-0.05313 , -0.05835 ,  0.308   , -0.02567 ,  0.1033  ],
         [ 0.3086  ,  0.1168  , -0.0875  , -0.01581 , -0.0646  ],
         [ 0.324   , -0.0929  ,  0.3591  ,  0.325   , -0.07635 ],...
2025-03-17 15:29:53.786425 test begin: paddle.nn.functional.rrelu(Tensor([71582789, 3, 4, 5],"float16"), 0.3, 0.300000009, training=True, )

[accuracy error] paddle.nn.functional.rrelu(Tensor([71582789, 3, 4, 5],"float16"), 0.3, 0.300000009, training=True, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 4108795089 / 4294967340 (95.7%)
Max absolute difference: 0.5
Max relative difference: 0.
 x: array([[[[-0.0797  , -0.0875  ,  0.308   , -0.03848 ,  0.1033  ],
         [ 0.3086  ,  0.1168  , -0.1313  , -0.02371 , -0.0969  ],
         [ 0.324   , -0.1393  ,  0.3591  ,  0.325   , -0.11456 ],...
 y: array([[[[-0.0797 , -0.0875 ,  0.308  , -0.03848,  0.1033 ],
         [ 0.3086 ,  0.1168 , -0.1313 , -0.02371, -0.0969 ],
         [ 0.324  , -0.1393 ,  0.3591 ,  0.325  , -0.11456],...
2025-03-17 15:41:19.290525 test begin: paddle.nn.functional.sigmoid(Tensor([1, 1, 10, 228170138],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([1, 1, 10, 228170138],"float32"), )
2025-03-17 15:45:02.608599 test begin: paddle.nn.functional.sigmoid(Tensor([1, 1, 11, 207427399],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([1, 1, 11, 207427399],"float32"), )
2025-03-17 15:48:18.353910 test begin: paddle.nn.functional.sigmoid(Tensor([1, 1, 13, 175515491],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([1, 1, 13, 175515491],"float32"), )
2025-03-17 15:51:46.879609 test begin: paddle.nn.functional.sigmoid(Tensor([1, 1, 14, 162978670],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([1, 1, 14, 162978670],"float32"), )
2025-03-17 15:55:07.523967 test begin: paddle.nn.functional.sigmoid(Tensor([1, 1, 15, 152113426],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([1, 1, 15, 152113426],"float32"), )
2025-03-17 15:58:41.179492 test begin: paddle.nn.functional.sigmoid(Tensor([1, 1, 152113426, 15],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([1, 1, 152113426, 15],"float32"), )
2025-03-17 16:02:26.837437 test begin: paddle.nn.functional.sigmoid(Tensor([1, 1, 162978670, 14],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([1, 1, 162978670, 14],"float32"), )
2025-03-17 16:05:56.623933 test begin: paddle.nn.functional.sigmoid(Tensor([1, 1, 175515491, 13],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([1, 1, 175515491, 13],"float32"), )
2025-03-17 16:09:44.711299 test begin: paddle.nn.functional.sigmoid(Tensor([1, 1, 207427399, 11],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([1, 1, 207427399, 11],"float32"), )
2025-03-17 16:12:46.562391 test begin: paddle.nn.functional.sigmoid(Tensor([1, 1, 228170138, 10],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([1, 1, 228170138, 10],"float32"), )
2025-03-17 16:16:20.550505 test begin: paddle.nn.functional.sigmoid(Tensor([1, 100, 22817014],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([1, 100, 22817014],"float32"), )
2025-03-17 16:19:43.452205 test begin: paddle.nn.functional.sigmoid(Tensor([1, 10140896, 15, 15],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([1, 10140896, 15, 15],"float32"), )
2025-03-17 16:23:01.068213 test begin: paddle.nn.functional.sigmoid(Tensor([1, 107374183, 5, 2, 4],"float16"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([1, 107374183, 5, 2, 4],"float16"), )
2025-03-17 16:39:41.709093 test begin: paddle.nn.functional.sigmoid(Tensor([1, 11, 207427399],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([1, 11, 207427399],"float32"), )
2025-03-17 16:43:24.168616 test begin: paddle.nn.functional.sigmoid(Tensor([1, 1100, 2074274],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([1, 1100, 2074274],"float32"), )
2025-03-17 16:46:50.323589 test begin: paddle.nn.functional.sigmoid(Tensor([1, 1140850690, 2],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([1, 1140850690, 2],"float32"), )
2025-03-17 16:50:28.893214 test begin: paddle.nn.functional.sigmoid(Tensor([1, 11641334, 14, 14],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([1, 11641334, 14, 14],"float32"), )
2025-03-17 16:54:11.242079 test begin: paddle.nn.functional.sigmoid(Tensor([1, 13501192, 13, 13],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([1, 13501192, 13, 13],"float32"), )
2025-03-17 16:58:00.050635 test begin: paddle.nn.functional.sigmoid(Tensor([1, 18, 126761188],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([1, 18, 126761188],"float32"), )
2025-03-17 17:01:58.814224 test begin: paddle.nn.functional.sigmoid(Tensor([1, 18857037, 11, 11],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([1, 18857037, 11, 11],"float32"), )
2025-03-17 17:05:35.170592 test begin: paddle.nn.functional.sigmoid(Tensor([1, 192, 11883862],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([1, 192, 11883862],"float32"), )
2025-03-17 17:09:19.670228 test begin: paddle.nn.functional.sigmoid(Tensor([1, 207427399, 11],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([1, 207427399, 11],"float32"), )
2025-03-17 17:12:58.579859 test begin: paddle.nn.functional.sigmoid(Tensor([1, 2281701379],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([1, 2281701379],"float32"), )
2025-03-17 17:16:28.964396 test begin: paddle.nn.functional.sigmoid(Tensor([1, 22817014, 10, 10],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([1, 22817014, 10, 10],"float32"), )
2025-03-17 17:19:53.313303 test begin: paddle.nn.functional.sigmoid(Tensor([1, 3, 178956971, 2, 4],"float16"), )

CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 6.06 GiB is free. Process 94500 has 25.59 GiB memory in use. Process 110797 has 46.40 GiB memory in use. Of the allocated memory 24.00 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-17 17:20:14.400196 test begin: paddle.nn.functional.sigmoid(Tensor([1, 3, 5, 2, 143165577],"float16"), )

CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.15 GiB is free. Process 110797 has 50.30 GiB memory in use. Process 31725 has 25.59 GiB memory in use. Of the allocated memory 24.00 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-17 17:22:25.961969 test begin: paddle.nn.functional.sigmoid(Tensor([1, 3, 5, 2, 76056713],"float32"), )

CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 670.38 MiB is free. Process 110797 has 50.30 GiB memory in use. Process 93730 has 27.09 GiB memory in use. Of the allocated memory 25.50 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-17 17:24:44.584605 test begin: paddle.nn.functional.sigmoid(Tensor([1, 3, 5, 38028357, 4],"float32"), )

CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 670.38 MiB is free. Process 110797 has 50.30 GiB memory in use. Process 150293 has 27.09 GiB memory in use. Of the allocated memory 25.50 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-17 17:26:50.668467 test begin: paddle.nn.functional.sigmoid(Tensor([1, 3, 5, 71582789, 4],"float16"), )

CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.15 GiB is free. Process 110797 has 50.30 GiB memory in use. Process 37637 has 25.59 GiB memory in use. Of the allocated memory 24.00 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-17 17:29:22.038485 test begin: paddle.nn.functional.sigmoid(Tensor([1, 3, 95070891, 2, 4],"float32"), )

W0317 17:31:09.173831 16374 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0317 17:31:09.175348 16374 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.nn.functional.sigmoid(Tensor([1, 3, 95070891, 2, 4],"float32"), )
2025-03-17 17:34:19.035991 test begin: paddle.nn.functional.sigmoid(Tensor([1, 570425345, 4],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([1, 570425345, 4],"float32"), )
2025-03-17 17:37:45.944712 test begin: paddle.nn.functional.sigmoid(Tensor([1, 57042535, 5, 2, 4],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([1, 57042535, 5, 2, 4],"float32"), )
2025-03-17 17:41:28.480515 test begin: paddle.nn.functional.sigmoid(Tensor([10, 107374183, 4],"float16"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([10, 107374183, 4],"float16"), None, )
2025-03-17 18:00:31.116726 test begin: paddle.nn.functional.sigmoid(Tensor([10, 228170138],"float32"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([10, 228170138],"float32"), None, )
2025-03-17 18:04:17.282965 test begin: paddle.nn.functional.sigmoid(Tensor([10, 499, 457255],"float32"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([10, 499, 457255],"float32"), None, )
2025-03-17 18:08:14.034155 test begin: paddle.nn.functional.sigmoid(Tensor([10, 499, 860715],"float16"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([10, 499, 860715],"float16"), None, )
2025-03-17 18:24:49.829408 test begin: paddle.nn.functional.sigmoid(Tensor([10, 57042535, 4],"float32"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([10, 57042535, 4],"float32"), None, )
2025-03-17 18:28:00.640246 test begin: paddle.nn.functional.sigmoid(Tensor([10140896, 1, 15, 15],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([10140896, 1, 15, 15],"float32"), )
2025-03-17 18:31:27.628429 test begin: paddle.nn.functional.sigmoid(Tensor([10186167, 224, 1, 1],"float32"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([10186167, 224, 1, 1],"float32"), None, )
2025-03-17 18:35:03.348748 test begin: paddle.nn.functional.sigmoid(Tensor([1080352, 192, 11],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([1080352, 192, 11],"float32"), )
2025-03-17 18:38:38.211348 test begin: paddle.nn.functional.sigmoid(Tensor([11408507, 100, 2],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([11408507, 100, 2],"float32"), )
2025-03-17 18:41:52.713539 test begin: paddle.nn.functional.sigmoid(Tensor([1143137, 499, 4],"float32"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([1143137, 499, 4],"float32"), None, )
2025-03-17 18:46:05.174205 test begin: paddle.nn.functional.sigmoid(Tensor([11641334, 1, 14, 14],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([11641334, 1, 14, 14],"float32"), )
2025-03-17 18:50:00.688667 test begin: paddle.nn.functional.sigmoid(Tensor([12, 118839, 40, 40, 1],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([12, 118839, 40, 40, 1],"float32"), )
2025-03-17 18:54:08.691826 test begin: paddle.nn.functional.sigmoid(Tensor([12, 1901418, 10, 10, 1],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([12, 1901418, 10, 10, 1],"float32"), )
2025-03-17 18:57:47.388993 test begin: paddle.nn.functional.sigmoid(Tensor([12, 3, 10, 10, 633806],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([12, 3, 10, 10, 633806],"float32"), )
2025-03-17 19:00:56.511612 test begin: paddle.nn.functional.sigmoid(Tensor([12, 3, 10, 6338060, 1],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([12, 3, 10, 6338060, 1],"float32"), )
2025-03-17 19:04:09.272613 test begin: paddle.nn.functional.sigmoid(Tensor([12, 3, 1584515, 40, 1],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([12, 3, 1584515, 40, 1],"float32"), )
2025-03-17 19:07:37.947816 test begin: paddle.nn.functional.sigmoid(Tensor([12, 3, 20, 20, 158452],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([12, 3, 20, 20, 158452],"float32"), )
2025-03-17 19:11:43.560415 test begin: paddle.nn.functional.sigmoid(Tensor([12, 3, 20, 3169030, 1],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([12, 3, 20, 3169030, 1],"float32"), )
2025-03-17 19:14:58.240126 test begin: paddle.nn.functional.sigmoid(Tensor([12, 3, 3169030, 20, 1],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([12, 3, 3169030, 20, 1],"float32"), )
2025-03-17 19:18:15.105977 test begin: paddle.nn.functional.sigmoid(Tensor([12, 3, 40, 1584515, 1],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([12, 3, 40, 1584515, 1],"float32"), )
2025-03-17 19:21:48.507255 test begin: paddle.nn.functional.sigmoid(Tensor([12, 3, 40, 40, 39613],"float32"), )

[paddle error] paddle.nn.functional.sigmoid(Tensor([12, 3, 40, 40, 39613],"float32"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   egr::GradTensorHolder::CopyValueFromTensor(unsigned long, unsigned long, paddle::Tensor const&, bool)
3   paddle::Tensor::copy_(paddle::Tensor const&, phi::Place const&, bool)
4   void phi::Copy<phi::DeviceContext>(phi::DeviceContext const&, phi::DenseTensor const&, phi::Place, bool, phi::DenseTensor*)
5   phi::DeviceContext::Alloc(phi::TensorBase*, phi::DataType, unsigned long, bool, bool) const
6   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 8.500028GB memory on GPU 0, 78.530212GB memory has been allocated and available memory is only 670.375000MB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-17 19:24:14.566575 test begin: paddle.nn.functional.sigmoid(Tensor([12, 3, 6338060, 10, 1],"float32"), )

2025-03-17 19:24:33.113965 test begin: paddle.nn.functional.sigmoid(Tensor([12, 475355, 20, 20, 1],"float32"), )

CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 670.38 MiB is free. Process 70533 has 50.30 GiB memory in use. Process 42990 has 27.09 GiB memory in use. Of the allocated memory 25.50 GiB is allocated by PyTorch, and 5.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-17 19:26:15.638268 test begin: paddle.nn.functional.sigmoid(Tensor([128, 1024, 1, 17409],"float32"), None, )

CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 670.38 MiB is free. Process 70533 has 50.30 GiB memory in use. Process 76423 has 27.09 GiB memory in use. Of the allocated memory 25.50 GiB is allocated by PyTorch, and 4.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-17 19:28:09.225804 test begin: paddle.nn.functional.sigmoid(Tensor([128, 1024, 1, 32769],"float16"), None, )

W0317 19:30:23.426851 105683 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0317 19:30:23.428064 105683 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.nn.functional.sigmoid(Tensor([128, 1024, 1, 32769],"float16"), None, )
2025-03-17 19:46:33.169595 test begin: paddle.nn.functional.sigmoid(Tensor([128, 1024, 17409, 1],"float32"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([128, 1024, 17409, 1],"float32"), None, )
2025-03-17 19:51:15.863725 test begin: paddle.nn.functional.sigmoid(Tensor([128, 1024, 32769, 1],"float16"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([128, 1024, 32769, 1],"float16"), None, )
2025-03-17 20:08:33.758883 test begin: paddle.nn.functional.sigmoid(Tensor([128, 17825793, 1, 1],"float32"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([128, 17825793, 1, 1],"float32"), None, )
2025-03-17 20:11:58.695615 test begin: paddle.nn.functional.sigmoid(Tensor([128, 224, 1, 149797],"float16"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([128, 224, 1, 149797],"float16"), None, )
2025-03-17 20:28:55.099803 test begin: paddle.nn.functional.sigmoid(Tensor([128, 224, 1, 79580],"float32"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([128, 224, 1, 79580],"float32"), None, )
2025-03-17 20:32:13.690611 test begin: paddle.nn.functional.sigmoid(Tensor([128, 224, 149797, 1],"float16"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([128, 224, 149797, 1],"float16"), None, )
2025-03-17 20:49:30.004796 test begin: paddle.nn.functional.sigmoid(Tensor([128, 224, 79580, 1],"float32"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([128, 224, 79580, 1],"float32"), None, )
2025-03-17 20:53:09.940388 test begin: paddle.nn.functional.sigmoid(Tensor([128, 256, 1, 131073],"float16"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([128, 256, 1, 131073],"float16"), None, )
2025-03-17 21:11:12.306279 test begin: paddle.nn.functional.sigmoid(Tensor([128, 256, 131073, 1],"float16"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([128, 256, 131073, 1],"float16"), None, )
2025-03-17 21:28:11.197834 test begin: paddle.nn.functional.sigmoid(Tensor([128, 33554433, 1, 1],"float16"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([128, 33554433, 1, 1],"float16"), None, )
2025-03-17 21:45:17.627258 test begin: paddle.nn.functional.sigmoid(Tensor([13, 175515491],"float32"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([13, 175515491],"float32"), None, )
2025-03-17 21:49:14.178446 test begin: paddle.nn.functional.sigmoid(Tensor([13501192, 1, 13, 13],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([13501192, 1, 13, 13],"float32"), )
2025-03-17 21:52:59.351430 test begin: paddle.nn.functional.sigmoid(Tensor([152113426, 15],"float32"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([152113426, 15],"float32"), None, )
2025-03-17 21:56:39.427437 test begin: paddle.nn.functional.sigmoid(Tensor([15474, 147456],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([15474, 147456],"float32"), )
2025-03-17 22:00:36.846237 test begin: paddle.nn.functional.sigmoid(Tensor([16, 142606337],"float32"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([16, 142606337],"float32"), None, )
2025-03-17 22:04:27.806991 test begin: paddle.nn.functional.sigmoid(Tensor([16777217, 256, 1, 1],"float16"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([16777217, 256, 1, 1],"float16"), None, )
2025-03-17 22:22:20.714927 test begin: paddle.nn.functional.sigmoid(Tensor([16849, 135424],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([16849, 135424],"float32"), )
2025-03-17 22:25:40.859196 test begin: paddle.nn.functional.sigmoid(Tensor([181896, 12544],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([181896, 12544],"float32"), )
2025-03-17 22:29:42.931118 test begin: paddle.nn.functional.sigmoid(Tensor([18416, 123904],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([18416, 123904],"float32"), )
2025-03-17 22:33:14.014533 test begin: paddle.nn.functional.sigmoid(Tensor([18857037, 1, 11, 11],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([18857037, 1, 11, 11],"float32"), )
2025-03-17 22:36:39.019216 test begin: paddle.nn.functional.sigmoid(Tensor([19014179, 3, 5, 2, 4],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([19014179, 3, 5, 2, 4],"float32"), )
2025-03-17 22:40:03.821014 test begin: paddle.nn.functional.sigmoid(Tensor([1901418, 3, 20, 20, 1],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([1901418, 3, 20, 20, 1],"float32"), )
2025-03-17 22:43:14.413928 test begin: paddle.nn.functional.sigmoid(Tensor([19173962, 224, 1, 1],"float16"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([19173962, 224, 1, 1],"float16"), None, )
2025-03-17 22:59:50.194830 test begin: paddle.nn.functional.sigmoid(Tensor([2151788, 499, 4],"float16"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([2151788, 499, 4],"float16"), None, )
2025-03-17 23:16:19.526085 test begin: paddle.nn.functional.sigmoid(Tensor([2228225, 1024, 1, 1],"float32"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([2228225, 1024, 1, 1],"float32"), None, )
2025-03-17 23:20:29.696041 test begin: paddle.nn.functional.sigmoid(Tensor([2228225, 1024],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([2228225, 1024],"float32"), )
2025-03-17 23:23:31.934045 test begin: paddle.nn.functional.sigmoid(Tensor([2281701379],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([2281701379],"float32"), )
2025-03-17 23:26:55.410871 test begin: paddle.nn.functional.sigmoid(Tensor([2281701379],"float32"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([2281701379],"float32"), None, )
2025-03-17 23:30:39.221863 test begin: paddle.nn.functional.sigmoid(Tensor([228170138, 10],"float32"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([228170138, 10],"float32"), None, )
2025-03-17 23:34:40.374213 test begin: paddle.nn.functional.sigmoid(Tensor([22817014, 1, 10, 10],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([22817014, 1, 10, 10],"float32"), )
2025-03-17 23:38:22.532125 test begin: paddle.nn.functional.sigmoid(Tensor([22817014, 1, 100],"float32"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([22817014, 1, 100],"float32"), None, )
2025-03-17 23:41:41.213375 test begin: paddle.nn.functional.sigmoid(Tensor([24, 95070891],"float32"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([24, 95070891],"float32"), None, )
2025-03-17 23:44:49.987226 test begin: paddle.nn.functional.sigmoid(Tensor([285212673, 8],"float32"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([285212673, 8],"float32"), None, )
2025-03-17 23:47:49.855867 test begin: paddle.nn.functional.sigmoid(Tensor([31690297, 18, 4],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([31690297, 18, 4],"float32"), )
2025-03-17 23:51:19.581121 test begin: paddle.nn.functional.sigmoid(Tensor([325957340, 7],"float32"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([325957340, 7],"float32"), None, )
2025-03-17 23:55:15.381638 test begin: paddle.nn.functional.sigmoid(Tensor([35791395, 3, 5, 2, 4],"float16"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([35791395, 3, 5, 2, 4],"float16"), )
2025-03-18 00:11:40.324368 test begin: paddle.nn.functional.sigmoid(Tensor([4, 570425345],"float32"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([4, 570425345],"float32"), None, )
2025-03-18 00:15:03.710772 test begin: paddle.nn.functional.sigmoid(Tensor([4194305, 1024, 1, 1],"float16"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([4194305, 1024, 1, 1],"float16"), None, )
2025-03-18 00:31:53.682558 test begin: paddle.nn.functional.sigmoid(Tensor([4294967297],"float16"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([4294967297],"float16"), )
2025-03-18 00:48:34.589155 test begin: paddle.nn.functional.sigmoid(Tensor([4294967297],"float16"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([4294967297],"float16"), None, )
2025-03-18 01:05:27.241195 test begin: paddle.nn.functional.sigmoid(Tensor([42949673, 1, 100],"float16"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([42949673, 1, 100],"float16"), None, )
2025-03-18 01:22:35.897866 test begin: paddle.nn.functional.sigmoid(Tensor([475355, 3, 40, 40, 1],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([475355, 3, 40, 40, 1],"float32"), )
2025-03-18 01:25:56.892333 test begin: paddle.nn.functional.sigmoid(Tensor([51856850, 11, 4],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([51856850, 11, 4],"float32"), )
2025-03-18 01:29:31.411537 test begin: paddle.nn.functional.sigmoid(Tensor([518569, 1100, 4],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([518569, 1100, 4],"float32"), )
2025-03-18 01:33:01.989317 test begin: paddle.nn.functional.sigmoid(Tensor([7605672, 3, 10, 10, 1],"float32"), )

[Pass] paddle.nn.functional.sigmoid(Tensor([7605672, 3, 10, 10, 1],"float32"), )
2025-03-18 01:36:37.058159 test begin: paddle.nn.functional.sigmoid(Tensor([8, 1, 285212673],"float32"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([8, 1, 285212673],"float32"), None, )
2025-03-18 01:39:38.601152 test begin: paddle.nn.functional.sigmoid(Tensor([8, 1, 536870913],"float16"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([8, 1, 536870913],"float16"), None, )
2025-03-18 01:56:41.752895 test begin: paddle.nn.functional.sigmoid(Tensor([8, 2852127, 100],"float32"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([8, 2852127, 100],"float32"), None, )
2025-03-18 01:59:42.638758 test begin: paddle.nn.functional.sigmoid(Tensor([8, 5368710, 100],"float16"), None, )

[Pass] paddle.nn.functional.sigmoid(Tensor([8, 5368710, 100],"float16"), None, )
2025-03-18 02:16:26.029987 test begin: paddle.nn.functional.sigmoid(x=Tensor([2281701379],"float32"), )

[Pass] paddle.nn.functional.sigmoid(x=Tensor([2281701379],"float32"), )
2025-03-18 02:19:37.545527 test begin: paddle.nn.functional.sigmoid(x=Tensor([4294967297],"float16"), )

[Pass] paddle.nn.functional.sigmoid(x=Tensor([4294967297],"float16"), )
2025-03-18 02:36:13.483984 test begin: paddle.nn.functional.silu(Tensor([1, 1, 2281701379],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([1, 1, 2281701379],"float32"), )
2025-03-18 02:39:17.605992 test begin: paddle.nn.functional.silu(Tensor([1, 1078309, 46, 46],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([1, 1078309, 46, 46],"float32"), )
2025-03-18 02:42:35.895367 test begin: paddle.nn.functional.silu(Tensor([1, 11, 207427399],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([1, 11, 207427399],"float32"), )
2025-03-18 02:45:41.120700 test begin: paddle.nn.functional.silu(Tensor([1, 128, 193759, 92],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([1, 128, 193759, 92],"float32"), )
2025-03-18 02:49:03.270873 test begin: paddle.nn.functional.silu(Tensor([1, 128, 23, 775035],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([1, 128, 23, 775035],"float32"), )
2025-03-18 02:52:03.904197 test begin: paddle.nn.functional.silu(Tensor([1, 128, 24, 742742],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([1, 128, 24, 742742],"float32"), )
2025-03-18 02:55:15.091365 test begin: paddle.nn.functional.silu(Tensor([1, 128, 371371, 48],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([1, 128, 371371, 48],"float32"), )
2025-03-18 02:57:57.375199 test begin: paddle.nn.functional.silu(Tensor([1, 128, 387518, 46],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([1, 128, 387518, 46],"float32"), )
2025-03-18 03:01:02.590669 test begin: paddle.nn.functional.silu(Tensor([1, 128, 46, 387518],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([1, 128, 46, 387518],"float32"), )
2025-03-18 03:04:38.770522 test begin: paddle.nn.functional.silu(Tensor([1, 128, 48, 371371],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([1, 128, 48, 371371],"float32"), )
2025-03-18 03:07:50.491551 test begin: paddle.nn.functional.silu(Tensor([1, 128, 742742, 24],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([1, 128, 742742, 24],"float32"), )
2025-03-18 03:10:51.340946 test begin: paddle.nn.functional.silu(Tensor([1, 128, 775035, 23],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([1, 128, 775035, 23],"float32"), )
2025-03-18 03:13:43.876891 test begin: paddle.nn.functional.silu(Tensor([1, 128, 92, 193759],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([1, 128, 92, 193759],"float32"), )
2025-03-18 03:16:42.222362 test begin: paddle.nn.functional.silu(Tensor([1, 13421773, 320],"float16"), )

[Pass] paddle.nn.functional.silu(Tensor([1, 13421773, 320],"float16"), )
2025-03-18 03:32:53.466882 test begin: paddle.nn.functional.silu(Tensor([1, 159159, 14336],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([1, 159159, 14336],"float32"), )
2025-03-18 03:35:48.048949 test begin: paddle.nn.functional.silu(Tensor([1, 166597, 13696],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([1, 166597, 13696],"float32"), )
2025-03-18 03:38:54.770211 test begin: paddle.nn.functional.silu(Tensor([1, 2, 1140850690],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([1, 2, 1140850690],"float32"), )
2025-03-18 03:42:09.268626 test begin: paddle.nn.functional.silu(Tensor([1, 2, 2147483649],"float16"), )

[Pass] paddle.nn.functional.silu(Tensor([1, 2, 2147483649],"float16"), )
2025-03-18 03:58:13.542657 test begin: paddle.nn.functional.silu(Tensor([1, 257, 8878216],"float32"), None, )

[Pass] paddle.nn.functional.silu(Tensor([1, 257, 8878216],"float32"), None, )
2025-03-18 04:01:11.912613 test begin: paddle.nn.functional.silu(Tensor([1, 269578, 92, 92],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([1, 269578, 92, 92],"float32"), )
2025-03-18 04:03:58.422942 test begin: paddle.nn.functional.silu(Tensor([1, 285212673, 8],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([1, 285212673, 8],"float32"), )
2025-03-18 04:07:13.002827 test begin: paddle.nn.functional.silu(Tensor([1, 3961288, 24, 24],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([1, 3961288, 24, 24],"float32"), )
2025-03-18 04:10:08.657916 test begin: paddle.nn.functional.silu(Tensor([1, 4313236, 23, 23],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([1, 4313236, 23, 23],"float32"), )
2025-03-18 04:13:12.465641 test begin: paddle.nn.functional.silu(Tensor([1, 4456449, 512],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([1, 4456449, 512],"float32"), )
2025-03-18 04:16:24.768710 test begin: paddle.nn.functional.silu(Tensor([1, 835789, 2730],"float32"), None, )

[Pass] paddle.nn.functional.silu(Tensor([1, 835789, 2730],"float32"), None, )
2025-03-18 04:19:27.917446 test begin: paddle.nn.functional.silu(Tensor([1, 990322, 48, 48],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([1, 990322, 48, 48],"float32"), )
2025-03-18 04:22:24.652931 test begin: paddle.nn.functional.silu(Tensor([10, 228170138],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([10, 228170138],"float32"), )
2025-03-18 04:25:06.767321 test begin: paddle.nn.functional.silu(Tensor([10, 228170138],"float32"), None, )

[Pass] paddle.nn.functional.silu(Tensor([10, 228170138],"float32"), None, )
2025-03-18 04:27:50.406887 test begin: paddle.nn.functional.silu(Tensor([100, 22817014],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([100, 22817014],"float32"), )
2025-03-18 04:30:52.823869 test begin: paddle.nn.functional.silu(Tensor([1089, 128, 128, 128],"float32"), None, )

[Pass] paddle.nn.functional.silu(Tensor([1089, 128, 128, 128],"float32"), None, )
2025-03-18 04:33:32.939066 test begin: paddle.nn.functional.silu(Tensor([11, 207427399],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([11, 207427399],"float32"), )
2025-03-18 04:36:32.005865 test begin: paddle.nn.functional.silu(Tensor([12, 190141782],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([12, 190141782],"float32"), )
2025-03-18 04:39:36.191575 test begin: paddle.nn.functional.silu(Tensor([128, 1089, 128, 128],"float32"), None, )

[Pass] paddle.nn.functional.silu(Tensor([128, 1089, 128, 128],"float32"), None, )
2025-03-18 04:42:38.075896 test begin: paddle.nn.functional.silu(Tensor([128, 128, 1089, 128],"float32"), None, )

[Pass] paddle.nn.functional.silu(Tensor([128, 128, 1089, 128],"float32"), None, )
2025-03-18 04:46:13.120709 test begin: paddle.nn.functional.silu(Tensor([128, 128, 128, 1089],"float32"), None, )

[Pass] paddle.nn.functional.silu(Tensor([128, 128, 128, 1089],"float32"), None, )
2025-03-18 04:49:09.209395 test begin: paddle.nn.functional.silu(Tensor([128, 128, 128, 2049],"float16"), None, )

[Pass] paddle.nn.functional.silu(Tensor([128, 128, 128, 2049],"float16"), None, )
2025-03-18 05:05:36.390243 test begin: paddle.nn.functional.silu(Tensor([128, 128, 16, 16385],"float16"), None, )

[Pass] paddle.nn.functional.silu(Tensor([128, 128, 16, 16385],"float16"), None, )
2025-03-18 05:21:44.213542 test begin: paddle.nn.functional.silu(Tensor([128, 128, 16, 8705],"float32"), None, )

[Pass] paddle.nn.functional.silu(Tensor([128, 128, 16, 8705],"float32"), None, )
2025-03-18 05:24:38.069937 test begin: paddle.nn.functional.silu(Tensor([128, 128, 16385, 16],"float16"), None, )

[Pass] paddle.nn.functional.silu(Tensor([128, 128, 16385, 16],"float16"), None, )
2025-03-18 05:41:23.868061 test begin: paddle.nn.functional.silu(Tensor([128, 128, 2049, 128],"float16"), None, )

[Pass] paddle.nn.functional.silu(Tensor([128, 128, 2049, 128],"float16"), None, )
2025-03-18 05:57:41.052332 test begin: paddle.nn.functional.silu(Tensor([128, 128, 4097, 64],"float16"), None, )

[Pass] paddle.nn.functional.silu(Tensor([128, 128, 4097, 64],"float16"), None, )
2025-03-18 06:14:27.678144 test begin: paddle.nn.functional.silu(Tensor([128, 128, 64, 4097],"float16"), None, )

[Pass] paddle.nn.functional.silu(Tensor([128, 128, 64, 4097],"float16"), None, )
2025-03-18 06:31:41.573617 test begin: paddle.nn.functional.silu(Tensor([128, 128, 8705, 16],"float32"), None, )

[Pass] paddle.nn.functional.silu(Tensor([128, 128, 8705, 16],"float32"), None, )
2025-03-18 06:35:01.839072 test begin: paddle.nn.functional.silu(Tensor([128, 131073, 16, 16],"float16"), None, )

[Pass] paddle.nn.functional.silu(Tensor([128, 131073, 16, 16],"float16"), None, )
2025-03-18 06:51:31.806208 test begin: paddle.nn.functional.silu(Tensor([128, 2049, 128, 128],"float16"), None, )

[Pass] paddle.nn.functional.silu(Tensor([128, 2049, 128, 128],"float16"), None, )
2025-03-18 07:07:44.850574 test begin: paddle.nn.functional.silu(Tensor([128, 34817, 512],"float32"), None, )

[Pass] paddle.nn.functional.silu(Tensor([128, 34817, 512],"float32"), None, )
2025-03-18 07:10:52.040879 test begin: paddle.nn.functional.silu(Tensor([128, 577, 30894],"float32"), None, )

[Pass] paddle.nn.functional.silu(Tensor([128, 577, 30894],"float32"), None, )
2025-03-18 07:13:53.216022 test begin: paddle.nn.functional.silu(Tensor([128, 69633, 16, 16],"float32"), None, )

[Pass] paddle.nn.functional.silu(Tensor([128, 69633, 16, 16],"float32"), None, )
2025-03-18 07:16:55.780257 test begin: paddle.nn.functional.silu(Tensor([128, 8193, 64, 64],"float16"), None, )

[Pass] paddle.nn.functional.silu(Tensor([128, 8193, 64, 64],"float16"), None, )
2025-03-18 07:33:29.207192 test begin: paddle.nn.functional.silu(Tensor([131073, 128, 16, 16],"float16"), None, )

[Pass] paddle.nn.functional.silu(Tensor([131073, 128, 16, 16],"float16"), None, )
2025-03-18 07:49:42.236829 test begin: paddle.nn.functional.silu(Tensor([142606337, 2, 8],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([142606337, 2, 8],"float32"), )
2025-03-18 07:52:45.341954 test begin: paddle.nn.functional.silu(Tensor([159159, 1, 14336],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([159159, 1, 14336],"float32"), )
2025-03-18 07:55:59.890336 test begin: paddle.nn.functional.silu(Tensor([159159, 14336],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([159159, 14336],"float32"), )
2025-03-18 07:59:00.826823 test begin: paddle.nn.functional.silu(Tensor([166597, 1, 13696],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([166597, 1, 13696],"float32"), )
2025-03-18 08:02:17.293288 test begin: paddle.nn.functional.silu(Tensor([17341, 257, 512],"float32"), None, )

[Pass] paddle.nn.functional.silu(Tensor([17341, 257, 512],"float32"), None, )
2025-03-18 08:05:21.636852 test begin: paddle.nn.functional.silu(Tensor([2, 1140850690],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([2, 1140850690],"float32"), )
2025-03-18 08:08:03.819180 test begin: paddle.nn.functional.silu(Tensor([2, 2228225, 512],"float32"), None, )

[Pass] paddle.nn.functional.silu(Tensor([2, 2228225, 512],"float32"), None, )
2025-03-18 08:11:06.915732 test begin: paddle.nn.functional.silu(Tensor([2, 257, 4439108],"float32"), None, )

[Pass] paddle.nn.functional.silu(Tensor([2, 257, 4439108],"float32"), None, )
2025-03-18 08:14:14.338551 test begin: paddle.nn.functional.silu(Tensor([2049, 128, 128, 128],"float16"), None, )

[Pass] paddle.nn.functional.silu(Tensor([2049, 128, 128, 128],"float16"), None, )
2025-03-18 08:30:32.507458 test begin: paddle.nn.functional.silu(Tensor([2107, 128, 92, 92],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([2107, 128, 92, 92],"float32"), )
2025-03-18 08:33:33.933241 test begin: paddle.nn.functional.silu(Tensor([2281701379],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([2281701379],"float32"), )
2025-03-18 08:36:54.704892 test begin: paddle.nn.functional.silu(Tensor([228170138, 10],"float32"), None, )

[Pass] paddle.nn.functional.silu(Tensor([228170138, 10],"float32"), None, )
2025-03-18 08:40:10.895166 test begin: paddle.nn.functional.silu(Tensor([22817014, 100],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([22817014, 100],"float32"), )
2025-03-18 08:43:17.666848 test begin: paddle.nn.functional.silu(Tensor([297097, 16, 480],"float32"), None, )

[Pass] paddle.nn.functional.silu(Tensor([297097, 16, 480],"float32"), None, )
2025-03-18 08:46:10.048819 test begin: paddle.nn.functional.silu(Tensor([30948, 128, 24, 24],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([30948, 128, 24, 24],"float32"), )
2025-03-18 08:49:08.441003 test begin: paddle.nn.functional.silu(Tensor([3253, 257, 2730],"float32"), None, )

[Pass] paddle.nn.functional.silu(Tensor([3253, 257, 2730],"float32"), None, )
2025-03-18 08:52:12.710993 test begin: paddle.nn.functional.silu(Tensor([33698, 128, 23, 23],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([33698, 128, 23, 23],"float32"), )
2025-03-18 08:55:14.119371 test begin: paddle.nn.functional.silu(Tensor([405132, 11, 512],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([405132, 11, 512],"float32"), )
2025-03-18 08:58:09.645346 test begin: paddle.nn.functional.silu(Tensor([512, 16, 278529],"float32"), None, )

[Pass] paddle.nn.functional.silu(Tensor([512, 16, 278529],"float32"), None, )
2025-03-18 09:01:22.460701 test begin: paddle.nn.functional.silu(Tensor([512, 16, 524289],"float16"), None, )

[Pass] paddle.nn.functional.silu(Tensor([512, 16, 524289],"float16"), None, )
2025-03-18 09:17:21.153356 test begin: paddle.nn.functional.silu(Tensor([512, 17477, 480],"float16"), None, )

[Pass] paddle.nn.functional.silu(Tensor([512, 17477, 480],"float16"), None, )
2025-03-18 09:34:05.965329 test begin: paddle.nn.functional.silu(Tensor([512, 9285, 480],"float32"), None, )

[Pass] paddle.nn.functional.silu(Tensor([512, 9285, 480],"float32"), None, )
2025-03-18 09:36:56.464523 test begin: paddle.nn.functional.silu(Tensor([559241, 16, 480],"float16"), None, )

[Pass] paddle.nn.functional.silu(Tensor([559241, 16, 480],"float16"), None, )
2025-03-18 09:53:52.392799 test begin: paddle.nn.functional.silu(Tensor([6710887, 2, 320],"float16"), )

[accuracy error] paddle.nn.functional.silu(Tensor([6710887, 2, 320],"float16"), ) 
 Unable to allocate 8.00 GiB for an array with shape (4294967680,) and data type float16
2025-03-18 09:57:52.199341 test begin: paddle.nn.functional.silu(Tensor([69633, 128, 16, 16],"float32"), None, )

[accuracy error] paddle.nn.functional.silu(Tensor([69633, 128, 16, 16],"float32"), None, ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[[[-1.834830e-01,  2.637255e-01, -1.110430e-01, ...,
          -1.860021e-01,  2.122699e-02,  2.812294e-01],
         [-5.360174e-02,  8.508399e-02, -1.806500e-01, ...,...
 y: array([[[[-1.834830e-01,  2.637255e-01, -1.110430e-01, ...,
          -1.860021e-01,  2.122699e-02,  2.812294e-01],
         [-5.360174e-02,  8.508399e-02, -1.806500e-01, ...,...
2025-03-18 09:58:52.208668 test begin: paddle.nn.functional.silu(Tensor([7724, 577, 512],"float32"), None, )

[accuracy error] backward  paddle.nn.functional.silu(Tensor([7724, 577, 512],"float32"), None, ) 
 Unable to allocate 8.50 GiB for an array with shape (2281854976,) and data type float32
2025-03-18 10:01:44.611051 test begin: paddle.nn.functional.silu(Tensor([7737, 128, 48, 48],"float32"), )

[accuracy error] paddle.nn.functional.silu(Tensor([7737, 128, 48, 48],"float32"), ) 
 Unable to allocate 8.50 GiB for an array with shape (2281734144,) and data type float32
2025-03-18 10:03:39.450345 test begin: paddle.nn.functional.silu(Tensor([8193, 128, 64, 64],"float16"), None, )

[Pass] paddle.nn.functional.silu(Tensor([8193, 128, 64, 64],"float16"), None, )
2025-03-18 10:20:26.330341 test begin: paddle.nn.functional.silu(Tensor([8425, 128, 46, 46],"float32"), )

[Pass] paddle.nn.functional.silu(Tensor([8425, 128, 46, 46],"float32"), )
2025-03-18 10:24:06.372147 test begin: paddle.nn.functional.silu(x=Tensor([25352238, 10, 3, 3],"float32"), )

[Pass] paddle.nn.functional.silu(x=Tensor([25352238, 10, 3, 3],"float32"), )
2025-03-18 10:27:03.589201 test begin: paddle.nn.functional.silu(x=Tensor([3, 10, 25352238, 3],"float32"), )

[Pass] paddle.nn.functional.silu(x=Tensor([3, 10, 25352238, 3],"float32"), )
2025-03-18 10:30:00.468546 test begin: paddle.nn.functional.silu(x=Tensor([3, 10, 3, 25352238],"float32"), )

[Pass] paddle.nn.functional.silu(x=Tensor([3, 10, 3, 25352238],"float32"), )
2025-03-18 10:33:17.273747 test begin: paddle.nn.functional.silu(x=Tensor([3, 10, 3, 47721859],"float16"), )

[Pass] paddle.nn.functional.silu(x=Tensor([3, 10, 3, 47721859],"float16"), )
2025-03-18 10:49:56.888361 test begin: paddle.nn.functional.silu(x=Tensor([3, 10, 47721859, 3],"float16"), )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 86, in func_timeout
    thread.join(timeout)
  File "/usr/lib/python3.9/threading.py", line 1064, in join
    self._wait_for_tstate_lock(timeout=max(timeout, 0))
  File "/usr/lib/python3.9/threading.py", line 1080, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt
2025-03-18 11:13:04.558749 test begin: paddle.nn.functional.silu(x=Tensor([3, 159072863, 3, 3],"float16"), )

W0318 11:15:00.296928 161091 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0318 11:15:00.298218 161091 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.nn.functional.silu(x=Tensor([3, 159072863, 3, 3],"float16"), )
2025-03-18 11:30:35.951377 test begin: paddle.nn.functional.silu(x=Tensor([3, 84507459, 3, 3],"float32"), )

[Pass] paddle.nn.functional.silu(x=Tensor([3, 84507459, 3, 3],"float32"), )
2025-03-18 11:34:24.270434 test begin: paddle.nn.functional.silu(x=Tensor([4, 1, 3, 357913942],"float16"), )

[Pass] paddle.nn.functional.silu(x=Tensor([4, 1, 3, 357913942],"float16"), )
2025-03-18 11:50:20.182824 test begin: paddle.nn.functional.silu(x=Tensor([4, 1, 357913942, 3],"float16"), )

[paddle error] paddle.nn.functional.silu(x=Tensor([4, 1, 357913942, 3],"float16"), ) 
 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   egr::Grad(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
1   egr::RunBackward(std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&, bool, std::vector<paddle::Tensor, std::allocator<paddle::Tensor> > const&)
2   egr::GradTensorHolder::CopyValueFromTensor(unsigned long, unsigned long, paddle::Tensor const&, bool)
3   paddle::Tensor::copy_(paddle::Tensor const&, phi::Place const&, bool)
4   void phi::Copy<phi::DeviceContext>(phi::DeviceContext const&, phi::DenseTensor const&, phi::Place, bool, phi::DenseTensor*)
5   phi::DeviceContext::Alloc(phi::TensorBase*, phi::DataType, unsigned long, bool, bool) const
6   phi::DeviceContext::Impl::Alloc(phi::TensorBase*, phi::Place const&, phi::DataType, unsigned long, bool, bool) const
7   phi::DenseTensor::AllocateFrom(phi::Allocator*, phi::DataType, unsigned long, bool)
8   paddle::memory::allocation::Allocator::Allocate(unsigned long)
9   paddle::memory::allocation::CUDAAllocator::AllocateImpl(unsigned long)
10  std::string phi::enforce::GetCompleteTraceBackString<std::string >(std::string&&, char const*, int)
11  common::enforce::GetCurrentTraceBackString[abi:cxx11](bool)

----------------------
Error Message Summary:
----------------------
ResourceExhaustedError: 

Out of memory error on GPU 0. Cannot allocate 8.000000GB memory on GPU 0, 77.030212GB memory has been allocated and available memory is only 2.154663GB.

Please check whether there is any other process using GPU 0.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 
 (at ../paddle/phi/core/memory/allocation/cuda_allocator.cc:84)

2025-03-18 11:58:35.228595 test begin: paddle.nn.functional.silu(x=Tensor([4, 119304648, 3, 3],"float16"), )

[torch error] paddle.nn.functional.silu(x=Tensor([4, 119304648, 3, 3],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.15 GiB is free. Process 13306 has 25.59 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:58:37.001805 test begin: paddle.nn.functional.silu(x=Tensor([477218589, 1, 3, 3],"float16"), )

[torch error] paddle.nn.functional.silu(x=Tensor([477218589, 1, 3, 3],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.15 GiB is free. Process 13306 has 25.59 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:58:38.918661 test begin: paddle.nn.functional.silu(x=Tensor([47721859, 10, 3, 3],"float16"), )

[torch error] paddle.nn.functional.silu(x=Tensor([47721859, 10, 3, 3],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.15 GiB is free. Process 13306 has 25.59 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:58:40.987637 test begin: paddle.nn.functional.softplus(Tensor([1, 1431655766, 3],"float16"), )

[torch error] paddle.nn.functional.softplus(Tensor([1, 1431655766, 3],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.15 GiB is free. Process 13306 has 25.59 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:58:43.279282 test begin: paddle.nn.functional.softplus(Tensor([1, 2, 1140850690],"float32"), )

[torch error] paddle.nn.functional.softplus(Tensor([1, 2, 1140850690],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:58:47.368741 test begin: paddle.nn.functional.softplus(Tensor([1, 2, 2147483649],"float16"), )

[torch error] paddle.nn.functional.softplus(Tensor([1, 2, 2147483649],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:58:49.619101 test begin: paddle.nn.functional.softplus(Tensor([1, 760567127, 3],"float32"), )

[torch error] paddle.nn.functional.softplus(Tensor([1, 760567127, 3],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:58:51.361804 test begin: paddle.nn.functional.softplus(Tensor([11883862, 3, 64],"float32"), )

[torch error] paddle.nn.functional.softplus(Tensor([11883862, 3, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:58:53.744350 test begin: paddle.nn.functional.softplus(Tensor([13, 10, 17551550],"float32"), )

[torch error] paddle.nn.functional.softplus(Tensor([13, 10, 17551550],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:58:55.190125 test begin: paddle.nn.functional.softplus(Tensor([13, 1007, 174296],"float32"), )

[torch error] paddle.nn.functional.softplus(Tensor([13, 1007, 174296],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:58:57.321060 test begin: paddle.nn.functional.softplus(Tensor([13, 2742430, 64],"float32"), )

[torch error] paddle.nn.functional.softplus(Tensor([13, 2742430, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:58:59.258944 test begin: paddle.nn.functional.softplus(Tensor([13, 3, 58505164],"float32"), )

[torch error] paddle.nn.functional.softplus(Tensor([13, 3, 58505164],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:59:01.274238 test begin: paddle.nn.functional.softplus(Tensor([1431655766, 3],"float16"), )

[torch error] paddle.nn.functional.softplus(Tensor([1431655766, 3],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:59:03.635008 test begin: paddle.nn.functional.softplus(Tensor([2, 1140850690],"float32"), )

[torch error] paddle.nn.functional.softplus(Tensor([2, 1140850690],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:59:05.658996 test begin: paddle.nn.functional.softplus(Tensor([2, 2147483649],"float16"), )

[torch error] paddle.nn.functional.softplus(Tensor([2, 2147483649],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:59:07.788620 test begin: paddle.nn.functional.softplus(Tensor([2281701379, 1],"float32"), )

[torch error] paddle.nn.functional.softplus(Tensor([2281701379, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:59:09.893683 test begin: paddle.nn.functional.softplus(Tensor([2281701379],"float32"), )

[torch error] paddle.nn.functional.softplus(Tensor([2281701379],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:59:12.059707 test begin: paddle.nn.functional.softplus(Tensor([2281701379],"float32"), 1, 15, None, )

[torch error] paddle.nn.functional.softplus(Tensor([2281701379],"float32"), 1, 15, None, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:59:14.427285 test begin: paddle.nn.functional.softplus(Tensor([3, 3, 477218589],"float16"), -1e-06, 20, None, )

[torch error] paddle.nn.functional.softplus(Tensor([3, 3, 477218589],"float16"), -1e-06, 20, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:59:16.839445 test begin: paddle.nn.functional.softplus(Tensor([3, 3, 477218589],"float16"), -3, 20, None, )

[torch error] paddle.nn.functional.softplus(Tensor([3, 3, 477218589],"float16"), -3, 20, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:59:18.638194 test begin: paddle.nn.functional.softplus(Tensor([3, 3, 477218589],"float16"), 1e-06, 20, None, )

[torch error] paddle.nn.functional.softplus(Tensor([3, 3, 477218589],"float16"), 1e-06, 20, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:59:20.405579 test begin: paddle.nn.functional.softplus(Tensor([3, 3, 477218589],"float16"), 2, 20, None, )

[torch error] paddle.nn.functional.softplus(Tensor([3, 3, 477218589],"float16"), 2, 20, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:59:22.384345 test begin: paddle.nn.functional.softplus(Tensor([3, 3, 477218589],"float16"), 3, -5, None, )

[torch error] paddle.nn.functional.softplus(Tensor([3, 3, 477218589],"float16"), 3, -5, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:59:24.629532 test begin: paddle.nn.functional.softplus(Tensor([3, 3, 477218589],"float16"), 3, 5, None, )

[torch error] paddle.nn.functional.softplus(Tensor([3, 3, 477218589],"float16"), 3, 5, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:59:26.573371 test begin: paddle.nn.functional.softplus(Tensor([3, 477218589, 3],"float16"), -1e-06, 20, None, )

[torch error] paddle.nn.functional.softplus(Tensor([3, 477218589, 3],"float16"), -1e-06, 20, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:59:27.944186 test begin: paddle.nn.functional.softplus(Tensor([3, 477218589, 3],"float16"), -3, 20, None, )

[torch error] paddle.nn.functional.softplus(Tensor([3, 477218589, 3],"float16"), -3, 20, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:59:29.930933 test begin: paddle.nn.functional.softplus(Tensor([3, 477218589, 3],"float16"), 1e-06, 20, None, )

[torch error] paddle.nn.functional.softplus(Tensor([3, 477218589, 3],"float16"), 1e-06, 20, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:59:31.944859 test begin: paddle.nn.functional.softplus(Tensor([3, 477218589, 3],"float16"), 2, 20, None, )

[torch error] paddle.nn.functional.softplus(Tensor([3, 477218589, 3],"float16"), 2, 20, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:59:34.180539 test begin: paddle.nn.functional.softplus(Tensor([3, 477218589, 3],"float16"), 3, -5, None, )

[torch error] paddle.nn.functional.softplus(Tensor([3, 477218589, 3],"float16"), 3, -5, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:59:36.133380 test begin: paddle.nn.functional.softplus(Tensor([3, 477218589, 3],"float16"), 3, 5, None, )

[torch error] paddle.nn.functional.softplus(Tensor([3, 477218589, 3],"float16"), 3, 5, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:59:37.593747 test begin: paddle.nn.functional.softplus(Tensor([35404, 1007, 64],"float32"), )

[torch error] paddle.nn.functional.softplus(Tensor([35404, 1007, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:59:40.050815 test begin: paddle.nn.functional.softplus(Tensor([3565159, 10, 64],"float32"), )

[torch error] paddle.nn.functional.softplus(Tensor([3565159, 10, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:59:41.787715 test begin: paddle.nn.functional.softplus(Tensor([380283564, 2, 3],"float32"), )

[torch error] paddle.nn.functional.softplus(Tensor([380283564, 2, 3],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:59:43.997959 test begin: paddle.nn.functional.softplus(Tensor([4294967297],"float16"), )

[torch error] paddle.nn.functional.softplus(Tensor([4294967297],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:59:46.169539 test begin: paddle.nn.functional.softplus(Tensor([4294967297],"float16"), 1, 15, None, )

[torch error] paddle.nn.functional.softplus(Tensor([4294967297],"float16"), 1, 15, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:59:48.302257 test begin: paddle.nn.functional.softplus(Tensor([477218589, 3, 3],"float16"), -1e-06, 20, None, )

[torch error] paddle.nn.functional.softplus(Tensor([477218589, 3, 3],"float16"), -1e-06, 20, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:59:50.107365 test begin: paddle.nn.functional.softplus(Tensor([477218589, 3, 3],"float16"), -3, 20, None, )

[torch error] paddle.nn.functional.softplus(Tensor([477218589, 3, 3],"float16"), -3, 20, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:59:52.406633 test begin: paddle.nn.functional.softplus(Tensor([477218589, 3, 3],"float16"), 1e-06, 20, None, )

[torch error] paddle.nn.functional.softplus(Tensor([477218589, 3, 3],"float16"), 1e-06, 20, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:59:54.403224 test begin: paddle.nn.functional.softplus(Tensor([477218589, 3, 3],"float16"), 2, 20, None, )

[torch error] paddle.nn.functional.softplus(Tensor([477218589, 3, 3],"float16"), 2, 20, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:59:56.357161 test begin: paddle.nn.functional.softplus(Tensor([477218589, 3, 3],"float16"), 3, -5, None, )

[torch error] paddle.nn.functional.softplus(Tensor([477218589, 3, 3],"float16"), 3, -5, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 11:59:58.363613 test begin: paddle.nn.functional.softplus(Tensor([477218589, 3, 3],"float16"), 3, 5, None, )

[torch error] paddle.nn.functional.softplus(Tensor([477218589, 3, 3],"float16"), 3, 5, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:00:00.263429 test begin: paddle.nn.functional.softplus(Tensor([715827883, 2, 3],"float16"), )

[torch error] paddle.nn.functional.softplus(Tensor([715827883, 2, 3],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:00:02.258791 test begin: paddle.nn.functional.softplus(Tensor([760567127, 3],"float32"), )

[torch error] paddle.nn.functional.softplus(Tensor([760567127, 3],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:00:05.822014 test begin: paddle.nn.functional.softplus(Tensor([8, 285212673],"float32"), )

[torch error] paddle.nn.functional.softplus(Tensor([8, 285212673],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:00:08.014552 test begin: paddle.nn.functional.softplus(x=Tensor([2281701379],"float32"), beta=1, threshold=15, )

[torch error] paddle.nn.functional.softplus(x=Tensor([2281701379],"float32"), beta=1, threshold=15, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:00:09.900896 test begin: paddle.nn.functional.softplus(x=Tensor([3, 3, 477218589],"float16"), beta=-1e-06, threshold=20, )

[torch error] paddle.nn.functional.softplus(x=Tensor([3, 3, 477218589],"float16"), beta=-1e-06, threshold=20, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:00:13.247547 test begin: paddle.nn.functional.softplus(x=Tensor([3, 3, 477218589],"float16"), beta=-3, threshold=20, )

[torch error] paddle.nn.functional.softplus(x=Tensor([3, 3, 477218589],"float16"), beta=-3, threshold=20, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:00:15.163908 test begin: paddle.nn.functional.softplus(x=Tensor([3, 3, 477218589],"float16"), beta=1e-06, threshold=20, )

[torch error] paddle.nn.functional.softplus(x=Tensor([3, 3, 477218589],"float16"), beta=1e-06, threshold=20, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:00:17.263364 test begin: paddle.nn.functional.softplus(x=Tensor([3, 3, 477218589],"float16"), beta=2, threshold=20, )

[torch error] paddle.nn.functional.softplus(x=Tensor([3, 3, 477218589],"float16"), beta=2, threshold=20, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:00:19.261513 test begin: paddle.nn.functional.softplus(x=Tensor([3, 3, 477218589],"float16"), beta=3, threshold=-5, )

[torch error] paddle.nn.functional.softplus(x=Tensor([3, 3, 477218589],"float16"), beta=3, threshold=-5, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:00:21.149297 test begin: paddle.nn.functional.softplus(x=Tensor([3, 3, 477218589],"float16"), beta=3, threshold=5, )

[torch error] paddle.nn.functional.softplus(x=Tensor([3, 3, 477218589],"float16"), beta=3, threshold=5, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:00:23.232282 test begin: paddle.nn.functional.softplus(x=Tensor([3, 477218589, 3],"float16"), beta=-1e-06, threshold=20, )

[torch error] paddle.nn.functional.softplus(x=Tensor([3, 477218589, 3],"float16"), beta=-1e-06, threshold=20, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:00:25.354927 test begin: paddle.nn.functional.softplus(x=Tensor([3, 477218589, 3],"float16"), beta=-3, threshold=20, )

[torch error] paddle.nn.functional.softplus(x=Tensor([3, 477218589, 3],"float16"), beta=-3, threshold=20, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:00:27.249790 test begin: paddle.nn.functional.softplus(x=Tensor([3, 477218589, 3],"float16"), beta=1e-06, threshold=20, )

[torch error] paddle.nn.functional.softplus(x=Tensor([3, 477218589, 3],"float16"), beta=1e-06, threshold=20, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:00:29.115857 test begin: paddle.nn.functional.softplus(x=Tensor([3, 477218589, 3],"float16"), beta=2, threshold=20, )

[torch error] paddle.nn.functional.softplus(x=Tensor([3, 477218589, 3],"float16"), beta=2, threshold=20, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:00:30.963863 test begin: paddle.nn.functional.softplus(x=Tensor([3, 477218589, 3],"float16"), beta=3, threshold=-5, )

[torch error] paddle.nn.functional.softplus(x=Tensor([3, 477218589, 3],"float16"), beta=3, threshold=-5, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:00:32.954931 test begin: paddle.nn.functional.softplus(x=Tensor([3, 477218589, 3],"float16"), beta=3, threshold=5, )

[torch error] paddle.nn.functional.softplus(x=Tensor([3, 477218589, 3],"float16"), beta=3, threshold=5, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:00:35.201009 test begin: paddle.nn.functional.softplus(x=Tensor([4294967297],"float16"), beta=1, threshold=15, )

[torch error] paddle.nn.functional.softplus(x=Tensor([4294967297],"float16"), beta=1, threshold=15, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:00:36.783360 test begin: paddle.nn.functional.softplus(x=Tensor([477218589, 3, 3],"float16"), beta=-1e-06, threshold=20, )

[torch error] paddle.nn.functional.softplus(x=Tensor([477218589, 3, 3],"float16"), beta=-1e-06, threshold=20, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:00:39.007397 test begin: paddle.nn.functional.softplus(x=Tensor([477218589, 3, 3],"float16"), beta=-3, threshold=20, )

[torch error] paddle.nn.functional.softplus(x=Tensor([477218589, 3, 3],"float16"), beta=-3, threshold=20, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:00:40.359490 test begin: paddle.nn.functional.softplus(x=Tensor([477218589, 3, 3],"float16"), beta=1e-06, threshold=20, )

[torch error] paddle.nn.functional.softplus(x=Tensor([477218589, 3, 3],"float16"), beta=1e-06, threshold=20, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:00:42.214792 test begin: paddle.nn.functional.softplus(x=Tensor([477218589, 3, 3],"float16"), beta=2, threshold=20, )

[torch error] paddle.nn.functional.softplus(x=Tensor([477218589, 3, 3],"float16"), beta=2, threshold=20, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:00:44.430646 test begin: paddle.nn.functional.softplus(x=Tensor([477218589, 3, 3],"float16"), beta=3, threshold=-5, )

[torch error] paddle.nn.functional.softplus(x=Tensor([477218589, 3, 3],"float16"), beta=3, threshold=-5, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:00:46.066820 test begin: paddle.nn.functional.softplus(x=Tensor([477218589, 3, 3],"float16"), beta=3, threshold=5, )

[torch error] paddle.nn.functional.softplus(x=Tensor([477218589, 3, 3],"float16"), beta=3, threshold=5, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:00:47.736152 test begin: paddle.nn.functional.softshrink(Tensor([2281701379],"float32"), 0.5, None, )

[torch error] paddle.nn.functional.softshrink(Tensor([2281701379],"float32"), 0.5, None, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:00:49.824145 test begin: paddle.nn.functional.softshrink(Tensor([3, 3, 477218589],"float16"), 0, None, )

[torch error] paddle.nn.functional.softshrink(Tensor([3, 3, 477218589],"float16"), 0, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:00:51.681473 test begin: paddle.nn.functional.softshrink(Tensor([3, 3, 477218589],"float16"), 5, None, )

[torch error] paddle.nn.functional.softshrink(Tensor([3, 3, 477218589],"float16"), 5, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:00:53.561148 test begin: paddle.nn.functional.softshrink(Tensor([3, 477218589, 3],"float16"), 0, None, )

[torch error] paddle.nn.functional.softshrink(Tensor([3, 477218589, 3],"float16"), 0, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:00:55.428134 test begin: paddle.nn.functional.softshrink(Tensor([3, 477218589, 3],"float16"), 5, None, )

[torch error] paddle.nn.functional.softshrink(Tensor([3, 477218589, 3],"float16"), 5, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:00:57.322305 test begin: paddle.nn.functional.softshrink(Tensor([4294967297],"float16"), 0.5, None, )

[torch error] paddle.nn.functional.softshrink(Tensor([4294967297],"float16"), 0.5, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:00:59.182909 test begin: paddle.nn.functional.softshrink(Tensor([477218589, 3, 3],"float16"), 0, None, )

[torch error] paddle.nn.functional.softshrink(Tensor([477218589, 3, 3],"float16"), 0, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:01:01.091750 test begin: paddle.nn.functional.softshrink(Tensor([477218589, 3, 3],"float16"), 5, None, )

[torch error] paddle.nn.functional.softshrink(Tensor([477218589, 3, 3],"float16"), 5, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:01:02.948700 test begin: paddle.nn.functional.softshrink(x=Tensor([2281701379],"float32"), threshold=0.5, )

[torch error] paddle.nn.functional.softshrink(x=Tensor([2281701379],"float32"), threshold=0.5, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:01:06.620675 test begin: paddle.nn.functional.softshrink(x=Tensor([3, 3, 477218589],"float16"), threshold=0, )

[torch error] paddle.nn.functional.softshrink(x=Tensor([3, 3, 477218589],"float16"), threshold=0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:01:08.631218 test begin: paddle.nn.functional.softshrink(x=Tensor([3, 3, 477218589],"float16"), threshold=5, )

[torch error] paddle.nn.functional.softshrink(x=Tensor([3, 3, 477218589],"float16"), threshold=5, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:01:10.643603 test begin: paddle.nn.functional.softshrink(x=Tensor([3, 477218589, 3],"float16"), threshold=0, )

[torch error] paddle.nn.functional.softshrink(x=Tensor([3, 477218589, 3],"float16"), threshold=0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:01:12.492614 test begin: paddle.nn.functional.softshrink(x=Tensor([3, 477218589, 3],"float16"), threshold=5, )

[torch error] paddle.nn.functional.softshrink(x=Tensor([3, 477218589, 3],"float16"), threshold=5, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:01:14.550700 test begin: paddle.nn.functional.softshrink(x=Tensor([4294967297],"float16"), threshold=0.5, )

[torch error] paddle.nn.functional.softshrink(x=Tensor([4294967297],"float16"), threshold=0.5, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:01:16.414343 test begin: paddle.nn.functional.softshrink(x=Tensor([477218589, 3, 3],"float16"), threshold=0, )

[torch error] paddle.nn.functional.softshrink(x=Tensor([477218589, 3, 3],"float16"), threshold=0, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:01:18.285630 test begin: paddle.nn.functional.softshrink(x=Tensor([477218589, 3, 3],"float16"), threshold=5, )

[torch error] paddle.nn.functional.softshrink(x=Tensor([477218589, 3, 3],"float16"), threshold=5, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:01:20.298252 test begin: paddle.nn.functional.softsign(Tensor([17825793, 128],"float32"), )

[torch error] paddle.nn.functional.softsign(Tensor([17825793, 128],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:01:22.763198 test begin: paddle.nn.functional.softsign(Tensor([2281701379],"float32"), None, )

[torch error] paddle.nn.functional.softsign(Tensor([2281701379],"float32"), None, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:01:24.903899 test begin: paddle.nn.functional.softsign(Tensor([3, 3, 477218589],"float16"), None, )

[torch error] paddle.nn.functional.softsign(Tensor([3, 3, 477218589],"float16"), None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:01:27.211316 test begin: paddle.nn.functional.softsign(Tensor([3, 477218589, 3],"float16"), None, )

[torch error] paddle.nn.functional.softsign(Tensor([3, 477218589, 3],"float16"), None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:01:29.374284 test begin: paddle.nn.functional.softsign(Tensor([300, 7605672],"float32"), )

[torch error] paddle.nn.functional.softsign(Tensor([300, 7605672],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:01:31.486122 test begin: paddle.nn.functional.softsign(Tensor([32, 71303169],"float32"), )

[torch error] paddle.nn.functional.softsign(Tensor([32, 71303169],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:01:33.602754 test begin: paddle.nn.functional.softsign(Tensor([4294967297],"float16"), None, )

[torch error] paddle.nn.functional.softsign(Tensor([4294967297],"float16"), None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:01:36.170564 test begin: paddle.nn.functional.softsign(Tensor([477218589, 3, 3],"float16"), None, )

[torch error] paddle.nn.functional.softsign(Tensor([477218589, 3, 3],"float16"), None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:01:38.501331 test begin: paddle.nn.functional.softsign(Tensor([557057, 4096],"float32"), )

[torch error] paddle.nn.functional.softsign(Tensor([557057, 4096],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:01:40.597533 test begin: paddle.nn.functional.softsign(x=Tensor([2281701379],"float32"), )

[torch error] paddle.nn.functional.softsign(x=Tensor([2281701379],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:01:42.966847 test begin: paddle.nn.functional.softsign(x=Tensor([3, 3, 477218589],"float16"), )

[torch error] paddle.nn.functional.softsign(x=Tensor([3, 3, 477218589],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:01:44.928421 test begin: paddle.nn.functional.softsign(x=Tensor([3, 477218589, 3],"float16"), )

[torch error] paddle.nn.functional.softsign(x=Tensor([3, 477218589, 3],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:01:46.990602 test begin: paddle.nn.functional.softsign(x=Tensor([4294967297],"float16"), )

[torch error] paddle.nn.functional.softsign(x=Tensor([4294967297],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:01:48.920409 test begin: paddle.nn.functional.softsign(x=Tensor([477218589, 3, 3],"float16"), )

[torch error] paddle.nn.functional.softsign(x=Tensor([477218589, 3, 3],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:01:50.981006 test begin: paddle.nn.functional.tanh(Tensor([1, 2281701379],"float32"), )

[torch error] paddle.nn.functional.tanh(Tensor([1, 2281701379],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:01:52.427172 test begin: paddle.nn.functional.tanh(Tensor([1, 2281701379],"float32"), None, )

[torch error] paddle.nn.functional.tanh(Tensor([1, 2281701379],"float32"), None, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:01:54.543336 test begin: paddle.nn.functional.tanh(Tensor([1, 3, 256, 2970966],"float32"), None, )

[torch error] paddle.nn.functional.tanh(Tensor([1, 3, 256, 2970966],"float32"), None, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:01:56.505077 test begin: paddle.nn.functional.tanh(Tensor([1, 3, 2970966, 256],"float32"), None, )

[torch error] paddle.nn.functional.tanh(Tensor([1, 3, 2970966, 256],"float32"), None, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:01:58.463077 test begin: paddle.nn.functional.tanh(Tensor([1, 34817, 256, 256],"float32"), None, )

[torch error] paddle.nn.functional.tanh(Tensor([1, 34817, 256, 256],"float32"), None, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:02:00.585983 test begin: paddle.nn.functional.tanh(Tensor([10, 228170138],"float32"), None, )

[torch error] paddle.nn.functional.tanh(Tensor([10, 228170138],"float32"), None, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:02:02.568212 test begin: paddle.nn.functional.tanh(Tensor([11606, 3, 256, 256],"float32"), None, )

[torch error] paddle.nn.functional.tanh(Tensor([11606, 3, 256, 256],"float32"), None, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:02:04.534417 test begin: paddle.nn.functional.tanh(Tensor([13, 175515491],"float32"), )

[torch error] paddle.nn.functional.tanh(Tensor([13, 175515491],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:02:06.496726 test begin: paddle.nn.functional.tanh(Tensor([13, 256, 685608],"float32"), None, )

[torch error] paddle.nn.functional.tanh(Tensor([13, 256, 685608],"float32"), None, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:02:08.454200 test begin: paddle.nn.functional.tanh(Tensor([13, 85701, 2048],"float32"), None, )

[torch error] paddle.nn.functional.tanh(Tensor([13, 85701, 2048],"float32"), None, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:02:10.568803 test begin: paddle.nn.functional.tanh(Tensor([142606337, 16],"float32"), )

[torch error] paddle.nn.functional.tanh(Tensor([142606337, 16],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:02:12.537749 test begin: paddle.nn.functional.tanh(Tensor([285212673, 8],"float32"), None, )

[torch error] paddle.nn.functional.tanh(Tensor([285212673, 8],"float32"), None, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:02:14.594056 test begin: paddle.nn.functional.tanh(Tensor([35651585, 64],"float32"), )

[torch error] paddle.nn.functional.tanh(Tensor([35651585, 64],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:02:16.846867 test begin: paddle.nn.functional.tanh(Tensor([4353, 256, 2048],"float32"), None, )

[torch error] paddle.nn.functional.tanh(Tensor([4353, 256, 2048],"float32"), None, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:02:18.522519 test begin: paddle.nn.functional.tanh(Tensor([570425345, 4],"float32"), None, )

[torch error] paddle.nn.functional.tanh(Tensor([570425345, 4],"float32"), None, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:02:20.494147 test begin: paddle.nn.functional.tanh(Tensor([63380594, 36],"float32"), None, )

[torch error] paddle.nn.functional.tanh(Tensor([63380594, 36],"float32"), None, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:02:22.861677 test begin: paddle.nn.functional.tanh(Tensor([71303169, 32],"float32"), )

[torch error] paddle.nn.functional.tanh(Tensor([71303169, 32],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:02:24.424285 test begin: paddle.nn.functional.tanh(Tensor([71303169, 32],"float32"), None, )

[torch error] paddle.nn.functional.tanh(Tensor([71303169, 32],"float32"), None, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:02:26.465563 test begin: paddle.nn.functional.tanh(Tensor([95070891, 24],"float32"), )

[torch error] paddle.nn.functional.tanh(Tensor([95070891, 24],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:02:28.491633 test begin: paddle.nn.functional.tanhshrink(Tensor([2281701379],"float32"), None, )

[torch error] paddle.nn.functional.tanhshrink(Tensor([2281701379],"float32"), None, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:02:30.547344 test begin: paddle.nn.functional.tanhshrink(Tensor([3, 3, 477218589],"float16"), None, )

[torch error] paddle.nn.functional.tanhshrink(Tensor([3, 3, 477218589],"float16"), None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:02:33.159417 test begin: paddle.nn.functional.tanhshrink(Tensor([3, 477218589, 3],"float16"), None, )

[torch error] paddle.nn.functional.tanhshrink(Tensor([3, 477218589, 3],"float16"), None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:02:35.027633 test begin: paddle.nn.functional.tanhshrink(Tensor([4294967297],"float16"), None, )

[torch error] paddle.nn.functional.tanhshrink(Tensor([4294967297],"float16"), None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:02:36.895542 test begin: paddle.nn.functional.tanhshrink(Tensor([477218589, 3, 3],"float16"), None, )

[torch error] paddle.nn.functional.tanhshrink(Tensor([477218589, 3, 3],"float16"), None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:02:38.935923 test begin: paddle.nn.functional.tanhshrink(x=Tensor([2281701379],"float32"), )

[torch error] paddle.nn.functional.tanhshrink(x=Tensor([2281701379],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:02:41.805987 test begin: paddle.nn.functional.tanhshrink(x=Tensor([3, 3, 477218589],"float16"), )

[torch error] paddle.nn.functional.tanhshrink(x=Tensor([3, 3, 477218589],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:02:43.619784 test begin: paddle.nn.functional.tanhshrink(x=Tensor([3, 477218589, 3],"float16"), )

[torch error] paddle.nn.functional.tanhshrink(x=Tensor([3, 477218589, 3],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:02:45.483286 test begin: paddle.nn.functional.tanhshrink(x=Tensor([4294967297],"float16"), )

[torch error] paddle.nn.functional.tanhshrink(x=Tensor([4294967297],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:02:47.375712 test begin: paddle.nn.functional.tanhshrink(x=Tensor([477218589, 3, 3],"float16"), )

[torch error] paddle.nn.functional.tanhshrink(x=Tensor([477218589, 3, 3],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:02:49.305243 test begin: paddle.nn.functional.thresholded_relu(Tensor([10, 1, 143165577, 3],"float16"), -1, 0.0, None, )

[torch error] paddle.nn.functional.thresholded_relu(Tensor([10, 1, 143165577, 3],"float16"), -1, 0.0, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:02:51.098813 test begin: paddle.nn.functional.thresholded_relu(Tensor([10, 1, 143165577, 3],"float16"), 0, 0.0, None, )

[torch error] paddle.nn.functional.thresholded_relu(Tensor([10, 1, 143165577, 3],"float16"), 0, 0.0, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:02:52.963364 test begin: paddle.nn.functional.thresholded_relu(Tensor([10, 1, 4, 107374183],"float16"), -1, 0.0, None, )

[torch error] paddle.nn.functional.thresholded_relu(Tensor([10, 1, 4, 107374183],"float16"), -1, 0.0, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:02:54.828395 test begin: paddle.nn.functional.thresholded_relu(Tensor([10, 1, 4, 107374183],"float16"), 0, 0.0, None, )

[torch error] paddle.nn.functional.thresholded_relu(Tensor([10, 1, 4, 107374183],"float16"), 0, 0.0, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:02:56.835220 test begin: paddle.nn.functional.thresholded_relu(Tensor([10, 35791395, 4, 3],"float16"), -1, 0.0, None, )

[torch error] paddle.nn.functional.thresholded_relu(Tensor([10, 35791395, 4, 3],"float16"), -1, 0.0, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:02:58.724786 test begin: paddle.nn.functional.thresholded_relu(Tensor([10, 35791395, 4, 3],"float16"), 0, 0.0, None, )

[torch error] paddle.nn.functional.thresholded_relu(Tensor([10, 35791395, 4, 3],"float16"), 0, 0.0, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:03:00.593819 test begin: paddle.nn.functional.thresholded_relu(Tensor([100, 2535224, 3, 3],"float32"), 1.0, 0.0, None, )

[torch error] paddle.nn.functional.thresholded_relu(Tensor([100, 2535224, 3, 3],"float32"), 1.0, 0.0, None, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:03:03.610713 test begin: paddle.nn.functional.thresholded_relu(Tensor([100, 4, 1901418, 3],"float32"), 1.0, 0.0, None, )

[torch error] paddle.nn.functional.thresholded_relu(Tensor([100, 4, 1901418, 3],"float32"), 1.0, 0.0, None, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:03:05.561715 test begin: paddle.nn.functional.thresholded_relu(Tensor([100, 4, 3, 1901418],"float32"), 1.0, 0.0, None, )

[torch error] paddle.nn.functional.thresholded_relu(Tensor([100, 4, 3, 1901418],"float32"), 1.0, 0.0, None, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:03:07.763113 test begin: paddle.nn.functional.thresholded_relu(Tensor([100, 4, 3, 3579140],"float16"), 1.0, 0.0, None, )

[torch error] paddle.nn.functional.thresholded_relu(Tensor([100, 4, 3, 3579140],"float16"), 1.0, 0.0, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:03:09.612338 test begin: paddle.nn.functional.thresholded_relu(Tensor([100, 4, 3579140, 3],"float16"), 1.0, 0.0, None, )

[torch error] paddle.nn.functional.thresholded_relu(Tensor([100, 4, 3579140, 3],"float16"), 1.0, 0.0, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:03:11.500070 test begin: paddle.nn.functional.thresholded_relu(Tensor([100, 4772186, 3, 3],"float16"), 1.0, 0.0, None, )

[torch error] paddle.nn.functional.thresholded_relu(Tensor([100, 4772186, 3, 3],"float16"), 1.0, 0.0, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:03:13.370663 test begin: paddle.nn.functional.thresholded_relu(Tensor([119304648, 4, 3, 3],"float16"), 1.0, 0.0, None, )

[torch error] paddle.nn.functional.thresholded_relu(Tensor([119304648, 4, 3, 3],"float16"), 1.0, 0.0, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:03:15.263449 test begin: paddle.nn.functional.thresholded_relu(Tensor([357913942, 1, 4, 3],"float16"), -1, 0.0, None, )

[torch error] paddle.nn.functional.thresholded_relu(Tensor([357913942, 1, 4, 3],"float16"), -1, 0.0, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:03:17.157235 test begin: paddle.nn.functional.thresholded_relu(Tensor([357913942, 1, 4, 3],"float16"), 0, 0.0, None, )

[torch error] paddle.nn.functional.thresholded_relu(Tensor([357913942, 1, 4, 3],"float16"), 0, 0.0, None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:03:19.125011 test begin: paddle.nn.functional.thresholded_relu(Tensor([63380594, 4, 3, 3],"float32"), 1.0, 0.0, None, )

[torch error] paddle.nn.functional.thresholded_relu(Tensor([63380594, 4, 3, 3],"float32"), 1.0, 0.0, None, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:03:21.485520 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=False, reduction="mean", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=False, reduction="mean", name=None, ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 1
2025-03-18 12:03:23.504989 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=False, reduction="none", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=False, reduction="none", name=None, ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 1
2025-03-18 12:03:25.573206 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=False, reduction="sum", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=False, reduction="sum", name=None, ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 1
2025-03-18 12:03:27.574558 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=True, reduction="mean", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=True, reduction="mean", name=None, ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 1
2025-03-18 12:03:29.846792 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), Tensor([858993460, 5],"float16"), margin=0.3, swap=False, reduction="mean", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), Tensor([858993460, 5],"float16"), margin=0.3, swap=False, reduction="mean", name=None, ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 0
2025-03-18 12:03:31.328180 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), Tensor([858993460, 5],"float16"), margin=0.3, swap=False, reduction="none", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), Tensor([858993460, 5],"float16"), margin=0.3, swap=False, reduction="none", name=None, ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 0
2025-03-18 12:03:33.256989 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), Tensor([858993460, 5],"float16"), margin=0.3, swap=False, reduction="sum", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), Tensor([858993460, 5],"float16"), margin=0.3, swap=False, reduction="sum", name=None, ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 0
2025-03-18 12:03:35.292084 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), Tensor([858993460, 5],"float16"), margin=0.3, swap=True, reduction="mean", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), Tensor([858993460, 5],"float16"), margin=0.3, swap=True, reduction="mean", name=None, ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 0
2025-03-18 12:03:37.163972 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="mean", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="mean", name=None, ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 1
2025-03-18 12:03:40.341959 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="none", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="none", name=None, ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 1
2025-03-18 12:03:41.878610 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="sum", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="sum", name=None, ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 1
2025-03-18 12:03:44.223700 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=True, reduction="mean", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=True, reduction="mean", name=None, ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 1
2025-03-18 12:03:45.774518 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="mean", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="mean", name=None, ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 0
2025-03-18 12:03:47.541843 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="none", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="none", name=None, ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 0
2025-03-18 12:03:49.527524 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="sum", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="sum", name=None, ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 0
2025-03-18 12:03:51.378929 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=True, reduction="mean", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=True, reduction="mean", name=None, ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 0
2025-03-18 12:03:53.345687 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="mean", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="mean", name=None, ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 1
2025-03-18 12:03:55.350140 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="none", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="none", name=None, ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 1
2025-03-18 12:03:57.344280 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="sum", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="sum", name=None, ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 1
2025-03-18 12:03:59.331876 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=True, reduction="mean", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=True, reduction="mean", name=None, ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 1
2025-03-18 12:04:01.456232 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=False, reduction="mean", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=False, reduction="mean", name=None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:04:02.839964 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=False, reduction="none", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=False, reduction="none", name=None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:04:04.747899 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=False, reduction="sum", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=False, reduction="sum", name=None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:04:06.615661 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=True, reduction="mean", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=True, reduction="mean", name=None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:04:08.471552 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="mean", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="mean", name=None, ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-18 12:04:10.687033 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="none", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="none", name=None, ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-18 12:04:12.318217 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="sum", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="sum", name=None, ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-18 12:04:14.331127 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=True, reduction="mean", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=True, reduction="mean", name=None, ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-18 12:04:16.387627 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), margin=0.3, swap=False, reduction="mean", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), margin=0.3, swap=False, reduction="mean", name=None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:04:18.642317 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), margin=0.3, swap=False, reduction="none", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), margin=0.3, swap=False, reduction="none", name=None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:04:20.346989 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), margin=0.3, swap=False, reduction="sum", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), margin=0.3, swap=False, reduction="sum", name=None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:04:22.607879 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), margin=0.3, swap=True, reduction="mean", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), margin=0.3, swap=True, reduction="mean", name=None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:04:23.984075 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 1
2025-03-18 12:04:26.222218 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 1
2025-03-18 12:04:27.979925 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 1
2025-03-18 12:04:30.027972 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 1
2025-03-18 12:04:31.401902 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 0
2025-03-18 12:04:33.268324 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 0
2025-03-18 12:04:35.147661 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 0
2025-03-18 12:04:37.419069 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 0
2025-03-18 12:04:39.063809 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 1
2025-03-18 12:04:40.900347 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 1
2025-03-18 12:04:43.135683 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 1
2025-03-18 12:04:44.516981 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 1
2025-03-18 12:04:46.454456 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 0
2025-03-18 12:04:48.406502 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 0
2025-03-18 12:04:50.354405 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 0
2025-03-18 12:04:52.470181 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 0
2025-03-18 12:04:54.319459 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 1
2025-03-18 12:04:56.330272 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 1
2025-03-18 12:04:58.494220 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 1
2025-03-18 12:05:00.538619 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 1
2025-03-18 12:05:02.948197 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:05:05.549494 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:05:07.709508 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:05:09.783526 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:05:12.003317 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-18 12:05:13.545468 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-18 12:05:15.480552 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-18 12:05:17.436822 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-18 12:05:19.344831 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:05:21.580472 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:05:22.958322 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:05:24.828183 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:05:26.820658 test begin: paddle.nn.functional.unfold(Tensor([10, 3, 1188387, 64],"float32"), 3, 1, 1, tuple(1,1,), )

[torch error] paddle.nn.functional.unfold(Tensor([10, 3, 1188387, 64],"float32"), 3, 1, 1, tuple(1,1,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:05:31.218392 test begin: paddle.nn.functional.unfold(Tensor([10, 3, 1188387, 64],"float32"), 3, 1, tuple(1,1,), 1, )

[torch error] paddle.nn.functional.unfold(Tensor([10, 3, 1188387, 64],"float32"), 3, 1, tuple(1,1,), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:05:33.747379 test begin: paddle.nn.functional.unfold(Tensor([10, 3, 1188387, 64],"float32"), 3, tuple(1,1,), 1, 1, )

[torch error] paddle.nn.functional.unfold(Tensor([10, 3, 1188387, 64],"float32"), 3, tuple(1,1,), 1, 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:05:35.369014 test begin: paddle.nn.functional.unfold(Tensor([10, 3, 1188387, 64],"float32"), 3, tuple(1,1,), tuple(1,1,), tuple(1,1,), )

[torch error] paddle.nn.functional.unfold(Tensor([10, 3, 1188387, 64],"float32"), 3, tuple(1,1,), tuple(1,1,), tuple(1,1,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:05:37.585169 test begin: paddle.nn.functional.unfold(Tensor([10, 3, 1188387, 64],"float32"), tuple(3,3,), tuple(1,1,), tuple(1,1,), tuple(1,1,), )

[torch error] paddle.nn.functional.unfold(Tensor([10, 3, 1188387, 64],"float32"), tuple(3,3,), tuple(1,1,), tuple(1,1,), tuple(1,1,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:05:39.666674 test begin: paddle.nn.functional.unfold(Tensor([10, 3, 64, 1188387],"float32"), 3, 1, 1, tuple(1,1,), )

[torch error] paddle.nn.functional.unfold(Tensor([10, 3, 64, 1188387],"float32"), 3, 1, 1, tuple(1,1,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:05:41.937101 test begin: paddle.nn.functional.unfold(Tensor([10, 3, 64, 1188387],"float32"), 3, 1, tuple(1,1,), 1, )

[torch error] paddle.nn.functional.unfold(Tensor([10, 3, 64, 1188387],"float32"), 3, 1, tuple(1,1,), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:05:44.494405 test begin: paddle.nn.functional.unfold(Tensor([10, 3, 64, 1188387],"float32"), 3, tuple(1,1,), 1, 1, )

[torch error] paddle.nn.functional.unfold(Tensor([10, 3, 64, 1188387],"float32"), 3, tuple(1,1,), 1, 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:05:46.666813 test begin: paddle.nn.functional.unfold(Tensor([10, 3, 64, 1188387],"float32"), 3, tuple(1,1,), tuple(1,1,), tuple(1,1,), )

[torch error] paddle.nn.functional.unfold(Tensor([10, 3, 64, 1188387],"float32"), 3, tuple(1,1,), tuple(1,1,), tuple(1,1,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:05:48.532379 test begin: paddle.nn.functional.unfold(Tensor([10, 3, 64, 1188387],"float32"), tuple(3,3,), tuple(1,1,), tuple(1,1,), tuple(1,1,), )

[torch error] paddle.nn.functional.unfold(Tensor([10, 3, 64, 1188387],"float32"), tuple(3,3,), tuple(1,1,), tuple(1,1,), tuple(1,1,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:05:50.732153 test begin: paddle.nn.functional.unfold(Tensor([10, 55706, 64, 64],"float32"), 3, 1, 1, tuple(1,1,), )

[torch error] paddle.nn.functional.unfold(Tensor([10, 55706, 64, 64],"float32"), 3, 1, 1, tuple(1,1,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:05:52.846133 test begin: paddle.nn.functional.unfold(Tensor([10, 55706, 64, 64],"float32"), 3, 1, tuple(1,1,), 1, )

[torch error] paddle.nn.functional.unfold(Tensor([10, 55706, 64, 64],"float32"), 3, 1, tuple(1,1,), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:05:54.950886 test begin: paddle.nn.functional.unfold(Tensor([10, 55706, 64, 64],"float32"), 3, tuple(1,1,), 1, 1, )

[torch error] paddle.nn.functional.unfold(Tensor([10, 55706, 64, 64],"float32"), 3, tuple(1,1,), 1, 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:05:56.970150 test begin: paddle.nn.functional.unfold(Tensor([10, 55706, 64, 64],"float32"), 3, tuple(1,1,), tuple(1,1,), tuple(1,1,), )

[torch error] paddle.nn.functional.unfold(Tensor([10, 55706, 64, 64],"float32"), 3, tuple(1,1,), tuple(1,1,), tuple(1,1,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:05:59.032483 test begin: paddle.nn.functional.unfold(Tensor([10, 55706, 64, 64],"float32"), tuple(3,3,), tuple(1,1,), tuple(1,1,), tuple(1,1,), )

[torch error] paddle.nn.functional.unfold(Tensor([10, 55706, 64, 64],"float32"), tuple(3,3,), tuple(1,1,), tuple(1,1,), tuple(1,1,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:06:01.140923 test begin: paddle.nn.functional.unfold(Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=3, strides=1, paddings=0, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=3, strides=1, paddings=0, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:06:03.017693 test begin: paddle.nn.functional.unfold(Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=list[2,4,], strides=1, paddings=1, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=list[2,4,], strides=1, paddings=1, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:06:04.898200 test begin: paddle.nn.functional.unfold(Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=list[2,4,], strides=2, paddings=1, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=list[2,4,], strides=2, paddings=1, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:06:06.922060 test begin: paddle.nn.functional.unfold(Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=list[3,3,], strides=1, paddings=0, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=list[3,3,], strides=1, paddings=0, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:06:08.931532 test begin: paddle.nn.functional.unfold(Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=list[3,3,], strides=1, paddings=1, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=list[3,3,], strides=1, paddings=1, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:06:10.961290 test begin: paddle.nn.functional.unfold(Tensor([15158, 3, 224, 224],"float32"), 16, 16, )

[torch error] paddle.nn.functional.unfold(Tensor([15158, 3, 224, 224],"float32"), 16, 16, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:06:13.447576 test begin: paddle.nn.functional.unfold(Tensor([185686, 3, 64, 64],"float32"), 3, 1, 1, tuple(1,1,), )

[torch error] paddle.nn.functional.unfold(Tensor([185686, 3, 64, 64],"float32"), 3, 1, 1, tuple(1,1,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:06:15.744588 test begin: paddle.nn.functional.unfold(Tensor([185686, 3, 64, 64],"float32"), 3, 1, tuple(1,1,), 1, )

[torch error] paddle.nn.functional.unfold(Tensor([185686, 3, 64, 64],"float32"), 3, 1, tuple(1,1,), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:06:17.224953 test begin: paddle.nn.functional.unfold(Tensor([185686, 3, 64, 64],"float32"), 3, tuple(1,1,), 1, 1, )

[torch error] paddle.nn.functional.unfold(Tensor([185686, 3, 64, 64],"float32"), 3, tuple(1,1,), 1, 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:06:19.194798 test begin: paddle.nn.functional.unfold(Tensor([185686, 3, 64, 64],"float32"), 3, tuple(1,1,), tuple(1,1,), tuple(1,1,), )

[torch error] paddle.nn.functional.unfold(Tensor([185686, 3, 64, 64],"float32"), 3, tuple(1,1,), tuple(1,1,), tuple(1,1,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:06:21.131338 test begin: paddle.nn.functional.unfold(Tensor([185686, 3, 64, 64],"float32"), tuple(3,3,), tuple(1,1,), tuple(1,1,), tuple(1,1,), )

[torch error] paddle.nn.functional.unfold(Tensor([185686, 3, 64, 64],"float32"), tuple(3,3,), tuple(1,1,), tuple(1,1,), tuple(1,1,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:06:23.088257 test begin: paddle.nn.functional.unfold(Tensor([2, 11408507, 10, 10],"float32"), kernel_sizes=3, strides=1, paddings=0, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 11408507, 10, 10],"float32"), kernel_sizes=3, strides=1, paddings=0, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:06:25.063795 test begin: paddle.nn.functional.unfold(Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=3, strides=1, paddings=0, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=3, strides=1, paddings=0, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:06:27.652972 test begin: paddle.nn.functional.unfold(Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=list[2,4,], strides=1, paddings=1, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=list[2,4,], strides=1, paddings=1, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:06:29.407989 test begin: paddle.nn.functional.unfold(Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=list[2,4,], strides=2, paddings=1, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=list[2,4,], strides=2, paddings=1, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:06:31.372867 test begin: paddle.nn.functional.unfold(Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=list[3,3,], strides=1, paddings=0, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=list[3,3,], strides=1, paddings=0, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:06:33.717561 test begin: paddle.nn.functional.unfold(Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=list[3,3,], strides=1, paddings=1, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=list[3,3,], strides=1, paddings=1, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:06:35.938462 test begin: paddle.nn.functional.unfold(Tensor([2, 3, 10, 38028357],"float32"), kernel_sizes=3, strides=1, paddings=0, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 3, 10, 38028357],"float32"), kernel_sizes=3, strides=1, paddings=0, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:06:37.921703 test begin: paddle.nn.functional.unfold(Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=3, strides=1, paddings=0, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=3, strides=1, paddings=0, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:06:40.342673 test begin: paddle.nn.functional.unfold(Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=list[2,4,], strides=1, paddings=1, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=list[2,4,], strides=1, paddings=1, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:06:42.070412 test begin: paddle.nn.functional.unfold(Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=list[2,4,], strides=2, paddings=1, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=list[2,4,], strides=2, paddings=1, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:06:44.147593 test begin: paddle.nn.functional.unfold(Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=list[3,3,], strides=1, paddings=0, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=list[3,3,], strides=1, paddings=0, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:06:46.469335 test begin: paddle.nn.functional.unfold(Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=list[3,3,], strides=1, paddings=1, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=list[3,3,], strides=1, paddings=1, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:06:47.967075 test begin: paddle.nn.functional.unfold(Tensor([2, 3, 38028357, 10],"float32"), kernel_sizes=3, strides=1, paddings=0, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 3, 38028357, 10],"float32"), kernel_sizes=3, strides=1, paddings=0, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:06:50.351133 test begin: paddle.nn.functional.unfold(Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=3, strides=1, paddings=0, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=3, strides=1, paddings=0, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:06:52.287040 test begin: paddle.nn.functional.unfold(Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=list[2,4,], strides=1, paddings=1, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=list[2,4,], strides=1, paddings=1, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:06:54.261696 test begin: paddle.nn.functional.unfold(Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=list[2,4,], strides=2, paddings=1, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=list[2,4,], strides=2, paddings=1, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:06:56.155079 test begin: paddle.nn.functional.unfold(Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=list[3,3,], strides=1, paddings=0, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=list[3,3,], strides=1, paddings=0, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:06:58.004817 test begin: paddle.nn.functional.unfold(Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=list[3,3,], strides=1, paddings=1, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=list[3,3,], strides=1, paddings=1, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:06:59.893856 test begin: paddle.nn.functional.unfold(Tensor([64, 3, 224, 53053],"float32"), 16, 16, )

[torch error] paddle.nn.functional.unfold(Tensor([64, 3, 224, 53053],"float32"), 16, 16, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:07:01.653302 test begin: paddle.nn.functional.unfold(Tensor([64, 3, 53053, 224],"float32"), 16, 16, )

[torch error] paddle.nn.functional.unfold(Tensor([64, 3, 53053, 224],"float32"), 16, 16, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:07:03.848166 test begin: paddle.nn.functional.unfold(Tensor([64, 711, 224, 224],"float32"), 16, 16, )

[torch error] paddle.nn.functional.unfold(Tensor([64, 711, 224, 224],"float32"), 16, 16, ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.51 GiB is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:07:06.129866 test begin: paddle.nn.functional.unfold(Tensor([7605672, 3, 10, 10],"float32"), kernel_sizes=3, strides=1, paddings=0, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([7605672, 3, 10, 10],"float32"), kernel_sizes=3, strides=1, paddings=0, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:07:08.278431 test begin: paddle.nn.functional.unfold(x=Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=3, )

[torch error] paddle.nn.functional.unfold(x=Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=3, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:07:10.135094 test begin: paddle.nn.functional.unfold(x=Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=list[2,4,], paddings=1, )

[torch error] paddle.nn.functional.unfold(x=Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=list[2,4,], paddings=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:07:12.374393 test begin: paddle.nn.functional.unfold(x=Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=list[2,4,], paddings=1, strides=2, )

[torch error] paddle.nn.functional.unfold(x=Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=list[2,4,], paddings=1, strides=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:07:14.034733 test begin: paddle.nn.functional.unfold(x=Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=list[3,3,], )

[torch error] paddle.nn.functional.unfold(x=Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=list[3,3,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:07:15.919801 test begin: paddle.nn.functional.unfold(x=Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=list[3,3,], paddings=1, )

[torch error] paddle.nn.functional.unfold(x=Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=list[3,3,], paddings=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:07:18.170191 test begin: paddle.nn.functional.unfold(x=Tensor([2, 11408507, 10, 10],"float32"), kernel_sizes=3, )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 11408507, 10, 10],"float32"), kernel_sizes=3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:07:21.474910 test begin: paddle.nn.functional.unfold(x=Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=3, )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=3, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:07:23.118198 test begin: paddle.nn.functional.unfold(x=Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=list[2,4,], paddings=1, )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=list[2,4,], paddings=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:07:24.981489 test begin: paddle.nn.functional.unfold(x=Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=list[2,4,], paddings=1, strides=2, )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=list[2,4,], paddings=1, strides=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:07:27.014233 test begin: paddle.nn.functional.unfold(x=Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=list[3,3,], )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=list[3,3,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:07:28.880608 test begin: paddle.nn.functional.unfold(x=Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=list[3,3,], paddings=1, )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=list[3,3,], paddings=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:07:31.105512 test begin: paddle.nn.functional.unfold(x=Tensor([2, 3, 10, 38028357],"float32"), kernel_sizes=3, )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 3, 10, 38028357],"float32"), kernel_sizes=3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:07:33.633465 test begin: paddle.nn.functional.unfold(x=Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=3, )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=3, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:07:35.476657 test begin: paddle.nn.functional.unfold(x=Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=list[2,4,], paddings=1, )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=list[2,4,], paddings=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:07:37.042522 test begin: paddle.nn.functional.unfold(x=Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=list[2,4,], paddings=1, strides=2, )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=list[2,4,], paddings=1, strides=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:07:38.890823 test begin: paddle.nn.functional.unfold(x=Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=list[3,3,], )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=list[3,3,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:07:40.768492 test begin: paddle.nn.functional.unfold(x=Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=list[3,3,], paddings=1, )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=list[3,3,], paddings=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:07:42.587497 test begin: paddle.nn.functional.unfold(x=Tensor([2, 3, 38028357, 10],"float32"), kernel_sizes=3, )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 3, 38028357, 10],"float32"), kernel_sizes=3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:07:44.580337 test begin: paddle.nn.functional.unfold(x=Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=3, )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=3, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:07:46.752158 test begin: paddle.nn.functional.unfold(x=Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=list[2,4,], paddings=1, )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=list[2,4,], paddings=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:07:48.933992 test begin: paddle.nn.functional.unfold(x=Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=list[2,4,], paddings=1, strides=2, )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=list[2,4,], paddings=1, strides=2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:07:51.247479 test begin: paddle.nn.functional.unfold(x=Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=list[3,3,], )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=list[3,3,], ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:07:52.753762 test begin: paddle.nn.functional.unfold(x=Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=list[3,3,], paddings=1, )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=list[3,3,], paddings=1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:07:55.002931 test begin: paddle.nn.functional.unfold(x=Tensor([7605672, 3, 10, 10],"float32"), kernel_sizes=3, )

[torch error] paddle.nn.functional.unfold(x=Tensor([7605672, 3, 10, 10],"float32"), kernel_sizes=3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:07:56.978097 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([10, 15],"float32"),Tensor([2281701379],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([10, 15],"float32"),Tensor([2281701379],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:07:58.923014 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([10, 20],"float32"),Tensor([2281701379],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([10, 20],"float32"),Tensor([2281701379],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:08:01.032892 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([10, 228170138],"float32"),Tensor([15],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([10, 228170138],"float32"),Tensor([15],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.65 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.30 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:08:02.808756 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([10, 228170138],"float32"),Tensor([20],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([10, 228170138],"float32"),Tensor([20],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:08:04.771419 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([10, 228170138],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([10, 228170138],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:08:06.734189 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([114085069, 20],"float32"),Tensor([20],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([114085069, 20],"float32"),Tensor([20],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:08:08.696147 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([1140850690, 2],"float32"),Tensor([2],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([1140850690, 2],"float32"),Tensor([2],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:08:10.732119 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([152113426, 15],"float32"),Tensor([15],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([152113426, 15],"float32"),Tensor([15],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:08:12.560229 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([16, 142606337],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([16, 142606337],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 7.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:08:14.934837 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([16, 8],"float32"),Tensor([142606337, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([16, 8],"float32"),Tensor([142606337, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:08:17.188441 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([16, 8],"float32"),Tensor([16, 142606337],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([16, 8],"float32"),Tensor([16, 142606337],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:08:19.522496 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([142606337, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([142606337, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:08:21.485053 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 142606337],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 142606337],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:08:23.467195 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([142606337, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([142606337, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:08:25.463641 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 142606337],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 142606337],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:08:27.676153 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([2281701379],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([2281701379],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 7.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:08:29.839463 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([2281701379],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([2281701379],"float32"),Tensor([16],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 7.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:08:31.983337 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([2281701379],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([2281701379],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 7.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:08:34.378989 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([2281701379],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([2281701379],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 7.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:08:36.170820 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([17825793, 2, 4, 4, 4],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([17825793, 2, 4, 4, 4],"float32"),Tensor([3],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:08:38.130015 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([2, 1140850690],"float32"),Tensor([2],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([2, 1140850690],"float32"),Tensor([2],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:08:40.060896 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([2, 2],"float32"),Tensor([2281701379],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([2, 2],"float32"),Tensor([2281701379],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:08:42.007396 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([285212673, 2, 4],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([285212673, 2, 4],"float32"),Tensor([3],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:08:43.968975 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([285212673, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([285212673, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 7.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:08:46.327943 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([285212673, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([285212673, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 7.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:08:48.097318 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([285212673, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([285212673, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 7.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:08:49.943648 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([3, 11883862, 4, 4, 4],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([3, 11883862, 4, 4, 4],"float32"),Tensor([3],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:08:52.344424 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([3, 190141782, 4],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([3, 190141782, 4],"float32"),Tensor([3],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:08:53.954111 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([3, 2, 23767723, 4, 4],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([3, 2, 23767723, 4, 4],"float32"),Tensor([3],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:08:56.508284 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([3, 2, 380283564],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([3, 2, 380283564],"float32"),Tensor([3],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:08:58.864411 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([3, 2, 4, 23767723, 4],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([3, 2, 4, 23767723, 4],"float32"),Tensor([3],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:09:01.664174 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([3, 2, 4, 4, 23767723],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([3, 2, 4, 4, 23767723],"float32"),Tensor([3],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:09:04.049958 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([3, 2, 4, 4, 4],"float32"),Tensor([2281701379],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([3, 2, 4, 4, 4],"float32"),Tensor([2281701379],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:09:06.453402 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([3, 2, 4, 4],"float32"),Tensor([2281701379],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([3, 2, 4, 4],"float32"),Tensor([2281701379],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:09:08.292133 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([3, 2, 4, 95070891],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([3, 2, 4, 95070891],"float32"),Tensor([3],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:09:10.236574 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([3, 2, 4],"float32"),Tensor([2281701379],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([3, 2, 4],"float32"),Tensor([2281701379],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:09:12.712082 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([3, 2, 95070891, 4],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([3, 2, 95070891, 4],"float32"),Tensor([3],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:09:14.885628 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([3, 47535446, 4, 4],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([3, 47535446, 4, 4],"float32"),Tensor([3],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:09:16.637300 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([48, 47535446],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([48, 47535446],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 7.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:09:18.735728 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([48, 8],"float32"),Tensor([142606337, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([48, 8],"float32"),Tensor([142606337, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 7.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:09:20.776041 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([2281701379],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([2281701379],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 7.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:09:22.814310 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([2281701379],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([2281701379],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 7.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:09:24.969440 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([142606337, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([142606337, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 7.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:09:27.128358 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([142606337, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([142606337, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 7.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:09:29.099877 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([2281701379],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([2281701379],"float32"),Tensor([48],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 7.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:09:31.246263 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([2281701379],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([2281701379],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 7.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:09:33.390397 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 47535446],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 47535446],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 7.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:09:35.804264 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 47535446],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 47535446],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 7.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:09:37.902315 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([48, 8],"float32"),Tensor([48, 47535446],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([48, 8],"float32"),Tensor([48, 47535446],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 7.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:09:39.799853 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([64, 35651585],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([64, 35651585],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 7.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:09:41.639605 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([64, 8],"float32"),Tensor([142606337, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([64, 8],"float32"),Tensor([142606337, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 7.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:09:44.106590 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([2281701379],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([2281701379],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 7.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:09:46.283388 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([2281701379],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([2281701379],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 7.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:09:48.925185 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([142606337, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([142606337, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 7.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:09:51.128858 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([142606337, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([142606337, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 7.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:09:52.645980 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([2281701379],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([2281701379],"float32"),Tensor([64],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 7.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:09:54.783980 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([2281701379],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([2281701379],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 7.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:09:56.931349 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 35651585],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 35651585],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 7.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:09:59.233978 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 35651585],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 35651585],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 7.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:10:01.523456 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([64, 8],"float32"),Tensor([64, 35651585],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([64, 8],"float32"),Tensor([64, 35651585],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 7.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:10:03.591789 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([71303169, 2, 4, 4],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([71303169, 2, 4, 4],"float32"),Tensor([3],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 8.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:10:06.080379 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([760567127, 3],"float32"),], )

[torch error] paddle.nn.utils.parameters_to_vector(list[Tensor([760567127, 3],"float32"),], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:10:07.872675 test begin: paddle.nn.utils.vector_to_parameters(Tensor([165],"float32"), list[Tensor([10, 15],"float32"),Tensor([2281701379],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([165],"float32"), list[Tensor([10, 15],"float32"),Tensor([2281701379],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 15
2025-03-18 12:10:09.877881 test begin: paddle.nn.utils.vector_to_parameters(Tensor([165],"float32"), list[Tensor([10, 228170138],"float32"),Tensor([15],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([165],"float32"), list[Tensor([10, 228170138],"float32"),Tensor([15],"float32"),], ) 
 shape '[10, 228170138]' is invalid for input of size 165
2025-03-18 12:10:12.374951 test begin: paddle.nn.utils.vector_to_parameters(Tensor([165],"float32"), list[Tensor([152113426, 15],"float32"),Tensor([15],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([165],"float32"), list[Tensor([152113426, 15],"float32"),Tensor([15],"float32"),], ) 
 shape '[152113426, 15]' is invalid for input of size 165
2025-03-18 12:10:13.775293 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([10, 15],"float32"),Tensor([15],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([10, 15],"float32"),Tensor([15],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [150, 15], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:165 != input_axis_dim:2281701379.] (at ../paddle/phi/infermeta/unary.cc:4438)

2025-03-18 12:10:19.380821 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([10, 3],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([10, 3],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [30, 0], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:30 != input_axis_dim:2281701379.] (at ../paddle/phi/infermeta/unary.cc:4438)

2025-03-18 12:10:25.172263 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [128, 256, 16, 16, 256, 256, 16, 16], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:960 != input_axis_dim:2281701379.] (at ../paddle/phi/infermeta/unary.cc:4438)

2025-03-18 12:10:29.536608 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([2, 2],"float32"),Tensor([2],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([2, 2],"float32"),Tensor([2],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [4, 2], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:6 != input_axis_dim:2281701379.] (at ../paddle/phi/infermeta/unary.cc:4438)

2025-03-18 12:10:33.510879 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([3, 2, 4, 4, 4],"float32"),Tensor([3],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([3, 2, 4, 4, 4],"float32"),Tensor([3],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [384, 3], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:387 != input_axis_dim:2281701379.] (at ../paddle/phi/infermeta/unary.cc:4438)

2025-03-18 12:10:38.897564 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([3, 2, 4, 4],"float32"),Tensor([3],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([3, 2, 4, 4],"float32"),Tensor([3],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [96, 3], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:99 != input_axis_dim:2281701379.] (at ../paddle/phi/infermeta/unary.cc:4438)

2025-03-18 12:10:43.964042 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([3, 2, 4],"float32"),Tensor([3],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([3, 2, 4],"float32"),Tensor([3],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [24, 3], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:27 != input_axis_dim:2281701379.] (at ../paddle/phi/infermeta/unary.cc:4438)

2025-03-18 12:10:48.672641 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [384, 768, 48, 48, 768, 768, 48, 48], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:2880 != input_axis_dim:2281701379.] (at ../paddle/phi/infermeta/unary.cc:4438)

2025-03-18 12:10:53.418676 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [512, 1024, 64, 64, 1024, 1024, 64, 64], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:3840 != input_axis_dim:2281701379.] (at ../paddle/phi/infermeta/unary.cc:4438)

2025-03-18 12:10:58.355774 test begin: paddle.nn.utils.vector_to_parameters(Tensor([27],"float32"), list[Tensor([285212673, 2, 4],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([27],"float32"), list[Tensor([285212673, 2, 4],"float32"),Tensor([3],"float32"),], ) 
 shape '[285212673, 2, 4]' is invalid for input of size 27
2025-03-18 12:11:00.725208 test begin: paddle.nn.utils.vector_to_parameters(Tensor([27],"float32"), list[Tensor([3, 190141782, 4],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([27],"float32"), list[Tensor([3, 190141782, 4],"float32"),Tensor([3],"float32"),], ) 
 shape '[3, 190141782, 4]' is invalid for input of size 27
2025-03-18 12:11:03.165978 test begin: paddle.nn.utils.vector_to_parameters(Tensor([27],"float32"), list[Tensor([3, 2, 380283564],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([27],"float32"), list[Tensor([3, 2, 380283564],"float32"),Tensor([3],"float32"),], ) 
 shape '[3, 2, 380283564]' is invalid for input of size 27
2025-03-18 12:11:05.325687 test begin: paddle.nn.utils.vector_to_parameters(Tensor([27],"float32"), list[Tensor([3, 2, 4],"float32"),Tensor([2281701379],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([27],"float32"), list[Tensor([3, 2, 4],"float32"),Tensor([2281701379],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 3
2025-03-18 12:11:07.187944 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([285212673, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([285212673, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 shape '[285212673, 8]' is invalid for input of size 2880
2025-03-18 12:11:09.259218 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 47535446],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 47535446],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 shape '[48, 47535446]' is invalid for input of size 2880
2025-03-18 12:11:11.700197 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([142606337, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([142606337, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 shape '[142606337, 16]' is invalid for input of size 2496
2025-03-18 12:11:13.314471 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([2281701379],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([2281701379],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 1728
2025-03-18 12:11:15.358818 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([2281701379],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([2281701379],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 1680
2025-03-18 12:11:18.028526 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([142606337, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([142606337, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 shape '[142606337, 16]' is invalid for input of size 1632
2025-03-18 12:11:20.647896 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([142606337, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([142606337, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 shape '[142606337, 16]' is invalid for input of size 864
2025-03-18 12:11:22.289579 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([2281701379],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([2281701379],"float32"),Tensor([48],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 96
2025-03-18 12:11:24.371859 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([2281701379],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([2281701379],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 48
2025-03-18 12:11:26.551406 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 47535446],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 47535446],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 shape '[48, 47535446]' is invalid for input of size 864
2025-03-18 12:11:28.637299 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 47535446],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 47535446],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 shape '[48, 47535446]' is invalid for input of size 1632
2025-03-18 12:11:30.895919 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 47535446],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 47535446],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 shape '[48, 47535446]' is invalid for input of size 2496
2025-03-18 12:11:33.070299 test begin: paddle.nn.utils.vector_to_parameters(Tensor([30],"float32"), list[Tensor([10, 228170138],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([30],"float32"), list[Tensor([10, 228170138],"float32"),], ) 
 shape '[10, 228170138]' is invalid for input of size 30
2025-03-18 12:11:35.576902 test begin: paddle.nn.utils.vector_to_parameters(Tensor([30],"float32"), list[Tensor([760567127, 3],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([30],"float32"), list[Tensor([760567127, 3],"float32"),], ) 
 shape '[760567127, 3]' is invalid for input of size 30
2025-03-18 12:11:37.512825 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([285212673, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([285212673, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 shape '[285212673, 8]' is invalid for input of size 3840
2025-03-18 12:11:39.676782 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 35651585],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 35651585],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 shape '[64, 35651585]' is invalid for input of size 3840
2025-03-18 12:11:42.081933 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([142606337, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([142606337, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 shape '[142606337, 16]' is invalid for input of size 3328
2025-03-18 12:11:43.583159 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([2281701379],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([2281701379],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 2304
2025-03-18 12:11:45.885492 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([2281701379],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([2281701379],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 2240
2025-03-18 12:11:47.768949 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([142606337, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([142606337, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 shape '[142606337, 16]' is invalid for input of size 2176
2025-03-18 12:11:49.897556 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([142606337, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([142606337, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 shape '[142606337, 16]' is invalid for input of size 1152
2025-03-18 12:11:51.841209 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([2281701379],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([2281701379],"float32"),Tensor([64],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 128
2025-03-18 12:11:54.026550 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([2281701379],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([2281701379],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 64
2025-03-18 12:11:55.793200 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 35651585],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 35651585],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 shape '[64, 35651585]' is invalid for input of size 1152
2025-03-18 12:11:58.030960 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 35651585],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 35651585],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 shape '[64, 35651585]' is invalid for input of size 2176
2025-03-18 12:12:00.620740 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 35651585],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 35651585],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 shape '[64, 35651585]' is invalid for input of size 3328
2025-03-18 12:12:02.734157 test begin: paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([17825793, 2, 4, 4, 4],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([17825793, 2, 4, 4, 4],"float32"),Tensor([3],"float32"),], ) 
 shape '[17825793, 2, 4, 4, 4]' is invalid for input of size 387
2025-03-18 12:12:05.049057 test begin: paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([3, 11883862, 4, 4, 4],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([3, 11883862, 4, 4, 4],"float32"),Tensor([3],"float32"),], ) 
 shape '[3, 11883862, 4, 4, 4]' is invalid for input of size 387
2025-03-18 12:12:07.315809 test begin: paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([3, 2, 23767723, 4, 4],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([3, 2, 23767723, 4, 4],"float32"),Tensor([3],"float32"),], ) 
 shape '[3, 2, 23767723, 4, 4]' is invalid for input of size 387
2025-03-18 12:12:09.444191 test begin: paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([3, 2, 4, 23767723, 4],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([3, 2, 4, 23767723, 4],"float32"),Tensor([3],"float32"),], ) 
 shape '[3, 2, 4, 23767723, 4]' is invalid for input of size 387
2025-03-18 12:12:11.549458 test begin: paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([3, 2, 4, 4, 23767723],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([3, 2, 4, 4, 23767723],"float32"),Tensor([3],"float32"),], ) 
 shape '[3, 2, 4, 4, 23767723]' is invalid for input of size 387
2025-03-18 12:12:13.698697 test begin: paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([3, 2, 4, 4, 4],"float32"),Tensor([2281701379],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([3, 2, 4, 4, 4],"float32"),Tensor([2281701379],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 3
2025-03-18 12:12:15.908279 test begin: paddle.nn.utils.vector_to_parameters(Tensor([6],"float32"), list[Tensor([1140850690, 2],"float32"),Tensor([2],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([6],"float32"), list[Tensor([1140850690, 2],"float32"),Tensor([2],"float32"),], ) 
 shape '[1140850690, 2]' is invalid for input of size 6
2025-03-18 12:12:18.270003 test begin: paddle.nn.utils.vector_to_parameters(Tensor([6],"float32"), list[Tensor([2, 1140850690],"float32"),Tensor([2],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([6],"float32"), list[Tensor([2, 1140850690],"float32"),Tensor([2],"float32"),], ) 
 shape '[2, 1140850690]' is invalid for input of size 6
2025-03-18 12:12:20.136150 test begin: paddle.nn.utils.vector_to_parameters(Tensor([6],"float32"), list[Tensor([2, 2],"float32"),Tensor([2281701379],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([6],"float32"), list[Tensor([2, 2],"float32"),Tensor([2281701379],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 2
2025-03-18 12:12:22.561087 test begin: paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 142606337],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 142606337],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 shape '[16, 142606337]' is invalid for input of size 960
2025-03-18 12:12:24.153560 test begin: paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([142606337, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([142606337, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 shape '[142606337, 16]' is invalid for input of size 832
2025-03-18 12:12:26.592297 test begin: paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 142606337],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 142606337],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 shape '[16, 142606337]' is invalid for input of size 832
2025-03-18 12:12:28.441613 test begin: paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([142606337, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([142606337, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 shape '[142606337, 16]' is invalid for input of size 544
2025-03-18 12:12:30.941389 test begin: paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 142606337],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 142606337],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 shape '[16, 142606337]' is invalid for input of size 544
2025-03-18 12:12:32.740210 test begin: paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([142606337, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([142606337, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 shape '[142606337, 16]' is invalid for input of size 288
2025-03-18 12:12:35.057407 test begin: paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 142606337],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 142606337],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 shape '[16, 142606337]' is invalid for input of size 288
2025-03-18 12:12:37.011606 test begin: paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([2281701379],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([2281701379],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 16
2025-03-18 12:12:39.394763 test begin: paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([2281701379],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([2281701379],"float32"),Tensor([16],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 32
2025-03-18 12:12:40.797980 test begin: paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([2281701379],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([2281701379],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 560
2025-03-18 12:12:42.751983 test begin: paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([2281701379],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([2281701379],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 576
2025-03-18 12:12:44.888509 test begin: paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([285212673, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([285212673, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 shape '[285212673, 8]' is invalid for input of size 960
2025-03-18 12:12:47.029379 test begin: paddle.nn.utils.vector_to_parameters(Tensor([99],"float32"), list[Tensor([3, 2, 4, 4],"float32"),Tensor([2281701379],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([99],"float32"), list[Tensor([3, 2, 4, 4],"float32"),Tensor([2281701379],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 3
2025-03-18 12:12:49.169060 test begin: paddle.nn.utils.vector_to_parameters(Tensor([99],"float32"), list[Tensor([3, 2, 4, 95070891],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([99],"float32"), list[Tensor([3, 2, 4, 95070891],"float32"),Tensor([3],"float32"),], ) 
 shape '[3, 2, 4, 95070891]' is invalid for input of size 99
2025-03-18 12:12:51.325878 test begin: paddle.nn.utils.vector_to_parameters(Tensor([99],"float32"), list[Tensor([3, 2, 95070891, 4],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([99],"float32"), list[Tensor([3, 2, 95070891, 4],"float32"),Tensor([3],"float32"),], ) 
 shape '[3, 2, 95070891, 4]' is invalid for input of size 99
2025-03-18 12:12:53.011098 test begin: paddle.nn.utils.vector_to_parameters(Tensor([99],"float32"), list[Tensor([3, 47535446, 4, 4],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([99],"float32"), list[Tensor([3, 47535446, 4, 4],"float32"),Tensor([3],"float32"),], ) 
 shape '[3, 47535446, 4, 4]' is invalid for input of size 99
2025-03-18 12:12:55.221386 test begin: paddle.nn.utils.vector_to_parameters(Tensor([99],"float32"), list[Tensor([71303169, 2, 4, 4],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([99],"float32"), list[Tensor([71303169, 2, 4, 4],"float32"),Tensor([3],"float32"),], ) 
 shape '[71303169, 2, 4, 4]' is invalid for input of size 99
2025-03-18 12:12:57.345534 test begin: paddle.ones_like(Tensor([1, 10, 114085069, 2],"float32"), )

[torch error] paddle.ones_like(Tensor([1, 10, 114085069, 2],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:12:59.727227 test begin: paddle.ones_like(Tensor([1, 10, 8, 28521268],"float32"), )

[torch error] paddle.ones_like(Tensor([1, 10, 8, 28521268],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:13:02.168436 test begin: paddle.ones_like(Tensor([1, 1024, 2228225],"float32"), )

[torch error] paddle.ones_like(Tensor([1, 1024, 2228225],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:13:04.377578 test begin: paddle.ones_like(Tensor([1, 12, 9, 21126865],"float32"), )

[torch error] paddle.ones_like(Tensor([1, 12, 9, 21126865],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:13:06.887269 test begin: paddle.ones_like(Tensor([1, 12, 95070891, 2],"float32"), )

[torch error] paddle.ones_like(Tensor([1, 12, 95070891, 2],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:13:09.067762 test begin: paddle.ones_like(Tensor([1, 126761188, 9, 2],"float32"), )

[torch error] paddle.ones_like(Tensor([1, 126761188, 9, 2],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:13:11.288439 test begin: paddle.ones_like(Tensor([1, 128, 1114113, 16],"float32"), )

[torch error] paddle.ones_like(Tensor([1, 128, 1114113, 16],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:13:12.898437 test begin: paddle.ones_like(Tensor([1, 128, 17825793],"float32"), )

[torch error] paddle.ones_like(Tensor([1, 128, 17825793],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:13:15.191494 test begin: paddle.ones_like(Tensor([1, 128, 8, 2228225],"float32"), )

[torch error] paddle.ones_like(Tensor([1, 128, 8, 2228225],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:13:17.356120 test begin: paddle.ones_like(Tensor([1, 142606337, 8, 2],"float32"), )

[torch error] paddle.ones_like(Tensor([1, 142606337, 8, 2],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:13:19.608411 test begin: paddle.ones_like(Tensor([1, 144, 200, 79226],"float32"), )

[torch error] paddle.ones_like(Tensor([1, 144, 200, 79226],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:13:22.020639 test begin: paddle.ones_like(Tensor([1, 144, 7922575, 2],"float32"), )

[torch error] paddle.ones_like(Tensor([1, 144, 7922575, 2],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:13:23.696841 test begin: paddle.ones_like(Tensor([1, 15, 15, 10140896],"float32"), )

[torch error] paddle.ones_like(Tensor([1, 15, 15, 10140896],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:13:25.759762 test begin: paddle.ones_like(Tensor([1, 15, 76056713, 2],"float32"), )

[torch error] paddle.ones_like(Tensor([1, 15, 76056713, 2],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:13:27.801299 test begin: paddle.ones_like(Tensor([1, 17825793, 128],"float32"), )

[torch error] paddle.ones_like(Tensor([1, 17825793, 128],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:13:29.823394 test begin: paddle.ones_like(Tensor([1, 17825793, 8, 16],"float32"), )

[torch error] paddle.ones_like(Tensor([1, 17825793, 8, 16],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:13:31.879928 test begin: paddle.ones_like(Tensor([1, 2281701379, 1],"float32"), )

[torch error] paddle.ones_like(Tensor([1, 2281701379, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:13:33.918697 test begin: paddle.ones_like(Tensor([1, 2281701379],"int32"), )

[torch error] paddle.ones_like(Tensor([1, 2281701379],"int32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:14:16.353375 test begin: paddle.ones_like(Tensor([1, 4096, 557057],"float32"), )

[torch error] paddle.ones_like(Tensor([1, 4096, 557057],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:14:18.626700 test begin: paddle.ones_like(Tensor([1, 5704254, 200, 2],"float32"), )

[torch error] paddle.ones_like(Tensor([1, 5704254, 200, 2],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:14:20.589382 test begin: paddle.ones_like(Tensor([1, 58, 39339679],"float32"), )

[torch error] paddle.ones_like(Tensor([1, 58, 39339679],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:14:22.526202 test begin: paddle.ones_like(Tensor([1, 76056713, 15, 2],"float32"), )

[torch error] paddle.ones_like(Tensor([1, 76056713, 15, 2],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:14:24.908533 test begin: paddle.ones_like(Tensor([1, 8912897, 256],"float32"), )

[torch error] paddle.ones_like(Tensor([1, 8912897, 256],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:14:26.618141 test begin: paddle.ones_like(Tensor([10563433, 12, 9, 2],"float32"), )

[torch error] paddle.ones_like(Tensor([10563433, 12, 9, 2],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:14:28.772567 test begin: paddle.ones_like(Tensor([139265, 128, 128],"float32"), )

[torch error] paddle.ones_like(Tensor([139265, 128, 128],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:14:30.719753 test begin: paddle.ones_like(Tensor([139265, 128, 8, 16],"float32"), )

[torch error] paddle.ones_like(Tensor([139265, 128, 8, 16],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:14:33.127043 test begin: paddle.ones_like(Tensor([14260634, 10, 8, 2],"float32"), )

[torch error] paddle.ones_like(Tensor([14260634, 10, 8, 2],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:14:34.614732 test begin: paddle.ones_like(Tensor([2228225, 1024, 1],"float32"), )

[torch error] paddle.ones_like(Tensor([2228225, 1024, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:14:36.836527 test begin: paddle.ones_like(Tensor([2270350, 1005],"int32"), )

[torch error] paddle.ones_like(Tensor([2270350, 1005],"int32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:14:38.445838 test begin: paddle.ones_like(Tensor([2272611, 1004],"int32"), )

[torch error] paddle.ones_like(Tensor([2272611, 1004],"int32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:14:40.853597 test begin: paddle.ones_like(Tensor([2274877, 1003],"int32"), )

[torch error] paddle.ones_like(Tensor([2274877, 1003],"int32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:14:43.558073 test begin: paddle.ones_like(Tensor([2277148, 1002],"int32"), )

[torch error] paddle.ones_like(Tensor([2277148, 1002],"int32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:14:46.278766 test begin: paddle.ones_like(Tensor([2279422, 1001],"int32"), )

[torch error] paddle.ones_like(Tensor([2279422, 1001],"int32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:14:47.839619 test begin: paddle.ones_like(Tensor([2281701379],"int32"), )

[torch error] paddle.ones_like(Tensor([2281701379],"int32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:14:50.207757 test begin: paddle.ones_like(Tensor([39339679, 58, 1],"float32"), )

[torch error] paddle.ones_like(Tensor([39339679, 58, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:14:52.105807 test begin: paddle.ones_like(Tensor([39613, 144, 200, 2],"float32"), )

[torch error] paddle.ones_like(Tensor([39613, 144, 200, 2],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 1.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:14:54.486005 test begin: paddle.ones_like(Tensor([4, 280, 376, 25, 217],"float32"), )

[torch error] paddle.ones_like(Tensor([4, 280, 376, 25, 217],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.58 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.51 GiB is allocated by PyTorch, and 1.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:14:56.502663 test begin: paddle.ones_like(Tensor([4, 280, 376, 5419, 1],"float32"), )

[torch error] paddle.ones_like(Tensor([4, 280, 376, 5419, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.58 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 10.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:14:58.771218 test begin: paddle.ones_like(Tensor([4, 280, 81490, 25, 1],"float32"), )

[torch error] paddle.ones_like(Tensor([4, 280, 81490, 25, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.58 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 11.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:15:00.895555 test begin: paddle.ones_like(Tensor([4, 60684, 376, 25, 1],"float32"), )

[torch error] paddle.ones_like(Tensor([4, 60684, 376, 25, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.58 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 11.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:15:03.045052 test begin: paddle.ones_like(Tensor([5070448, 15, 15, 2],"float32"), )

[torch error] paddle.ones_like(Tensor([5070448, 15, 15, 2],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.58 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:15:05.037326 test begin: paddle.ones_like(Tensor([557057, 4096, 1],"float32"), )

[torch error] paddle.ones_like(Tensor([557057, 4096, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.58 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 11.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:15:06.993750 test begin: paddle.ones_like(Tensor([69633, 128, 256],"float32"), )

[torch error] paddle.ones_like(Tensor([69633, 128, 256],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.58 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 11.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:15:08.939703 test begin: paddle.ones_like(Tensor([867, 280, 376, 25, 1],"float32"), )

[torch error] paddle.ones_like(Tensor([867, 280, 376, 25, 1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.58 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 11.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:15:11.079793 test begin: paddle.ones_like(x=Tensor([17674763, 3, 3, 3, 3, 3],"float16"), )

[torch error] paddle.ones_like(x=Tensor([17674763, 3, 3, 3, 3, 3],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.58 GiB is free. Process 13306 has 26.10 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 524.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:15:14.772542 test begin: paddle.ones_like(x=Tensor([253522376, 3, 3],"bool"), )

[accuracy error] paddle.ones_like(x=Tensor([253522376, 3, 3],"bool"), ) 
 
Arrays are not equal

Mismatched elements: 2281701384 / 2281701384 (100%)
 x: array([[[False, False, False],
        [False, False, False],
        [False, False, False]],...
 y: array([[[ True,  True,  True],
        [ True,  True,  True],
        [ True,  True,  True]],...
2025-03-18 12:16:42.446737 test begin: paddle.ones_like(x=Tensor([253522376, 3, 3],"float32"), )

[torch error] paddle.ones_like(x=Tensor([253522376, 3, 3],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:16:46.896823 test begin: paddle.ones_like(x=Tensor([253522376, 3, 3],"int32"), )

[torch error] paddle.ones_like(x=Tensor([253522376, 3, 3],"int32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:16:50.118459 test begin: paddle.ones_like(x=Tensor([3, 17674763, 3, 3, 3, 3],"float16"), )

[torch error] paddle.ones_like(x=Tensor([3, 17674763, 3, 3, 3, 3],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:16:52.632650 test begin: paddle.ones_like(x=Tensor([3, 253522376, 3],"bool"), )

[accuracy error] paddle.ones_like(x=Tensor([3, 253522376, 3],"bool"), ) 
 
Arrays are not equal

Mismatched elements: 2281701384 / 2281701384 (100%)
 x: array([[[False, False, False],
        [False, False, False],
        [False, False, False],...
 y: array([[[ True,  True,  True],
        [ True,  True,  True],
        [ True,  True,  True],...
2025-03-18 12:17:30.711751 test begin: paddle.ones_like(x=Tensor([3, 253522376, 3],"float32"), )

[torch error] paddle.ones_like(x=Tensor([3, 253522376, 3],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:17:35.075727 test begin: paddle.ones_like(x=Tensor([3, 253522376, 3],"int32"), )

[torch error] paddle.ones_like(x=Tensor([3, 253522376, 3],"int32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:17:39.042390 test begin: paddle.ones_like(x=Tensor([3, 3, 17674763, 3, 3, 3],"float16"), )

[torch error] paddle.ones_like(x=Tensor([3, 3, 17674763, 3, 3, 3],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:17:43.096343 test begin: paddle.ones_like(x=Tensor([3, 3, 253522376],"bool"), )

[accuracy error] paddle.ones_like(x=Tensor([3, 3, 253522376],"bool"), ) 
 
Arrays are not equal

Mismatched elements: 2281701384 / 2281701384 (100%)
 x: array([[[False, False, False, ..., False, False, False],
        [False, False, False, ..., False, False, False],
        [False, False, False, ..., False, False, False]],...
 y: array([[[ True,  True,  True, ...,  True,  True,  True],
        [ True,  True,  True, ...,  True,  True,  True],
        [ True,  True,  True, ...,  True,  True,  True]],...
2025-03-18 12:18:18.027272 test begin: paddle.ones_like(x=Tensor([3, 3, 253522376],"float32"), )

[torch error] paddle.ones_like(x=Tensor([3, 3, 253522376],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:18:22.389788 test begin: paddle.ones_like(x=Tensor([3, 3, 253522376],"int32"), )

[torch error] paddle.ones_like(x=Tensor([3, 3, 253522376],"int32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:18:23.910049 test begin: paddle.ones_like(x=Tensor([3, 3, 3, 17674763, 3, 3],"float16"), )

[torch error] paddle.ones_like(x=Tensor([3, 3, 3, 17674763, 3, 3],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:18:26.788697 test begin: paddle.ones_like(x=Tensor([3, 3, 3, 3, 17674763, 3],"float16"), )

[torch error] paddle.ones_like(x=Tensor([3, 3, 3, 3, 17674763, 3],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:18:28.549158 test begin: paddle.ones_like(x=Tensor([3, 3, 3, 3, 3, 17674763],"float16"), )

[torch error] paddle.ones_like(x=Tensor([3, 3, 3, 3, 3, 17674763],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:18:30.585186 test begin: paddle.ones_like(x=Tensor([3, 3, 477218589],"float16"), )

[torch error] paddle.ones_like(x=Tensor([3, 3, 477218589],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:18:32.350235 test begin: paddle.ones_like(x=Tensor([3, 477218589, 3],"float16"), )

[torch error] paddle.ones_like(x=Tensor([3, 477218589, 3],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:18:34.452326 test begin: paddle.ones_like(x=Tensor([477218589, 3, 3],"float16"), )

[torch error] paddle.ones_like(x=Tensor([477218589, 3, 3],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:18:36.673390 test begin: paddle.outer(Tensor([10],"float32"), Tensor([2281701379],"float32"), )

[torch error] paddle.outer(Tensor([10],"float32"), Tensor([2281701379],"float32"), ) 
 CUDA out of memory. Tried to allocate 85.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 4.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:18:38.590195 test begin: paddle.outer(Tensor([142],"float32"), Tensor([2281701379],"float32"), )

[torch error] paddle.outer(Tensor([142],"float32"), Tensor([2281701379],"float32"), ) 
 CUDA out of memory. Tried to allocate 1207.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 4.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:18:40.771675 test begin: paddle.outer(Tensor([16],"float32"), Tensor([2281701379],"float32"), )

[torch error] paddle.outer(Tensor([16],"float32"), Tensor([2281701379],"float32"), ) 
 CUDA out of memory. Tried to allocate 136.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 4.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:18:42.867030 test begin: paddle.outer(Tensor([2048],"float32"), Tensor([2281701379],"float32"), )

[torch error] paddle.outer(Tensor([2048],"float32"), Tensor([2281701379],"float32"), ) 
 CUDA out of memory. Tried to allocate 17408.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 3.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:18:44.904154 test begin: paddle.outer(Tensor([2281701379],"float32"), Tensor([1],"float32"), )

[torch error] paddle.outer(Tensor([2281701379],"float32"), Tensor([1],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 4.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:18:47.019806 test begin: paddle.outer(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), )

[torch error] paddle.outer(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:18:49.163128 test begin: paddle.outer(Tensor([2281701379],"float32"), Tensor([2],"float32"), )

[torch error] paddle.outer(Tensor([2281701379],"float32"), Tensor([2],"float32"), ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 4.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:18:51.542222 test begin: paddle.outer(Tensor([2281701379],"float32"), Tensor([32],"float32"), )

[torch error] paddle.outer(Tensor([2281701379],"float32"), Tensor([32],"float32"), ) 
 CUDA out of memory. Tried to allocate 272.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 4.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:18:53.271962 test begin: paddle.outer(Tensor([2281701379],"float32"), Tensor([4],"float32"), )

[torch error] paddle.outer(Tensor([2281701379],"float32"), Tensor([4],"float32"), ) 
 CUDA out of memory. Tried to allocate 34.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 4.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:18:55.227056 test begin: paddle.outer(x=Tensor([4294967297],"float16"), y=Tensor([4294967297],"float16"), )

[torch error] paddle.outer(x=Tensor([4294967297],"float16"), y=Tensor([4294967297],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:18:57.815794 test begin: paddle.outer(x=Tensor([4294967297],"float16"), y=Tensor([4],"float16"), )

[torch error] paddle.outer(x=Tensor([4294967297],"float16"), y=Tensor([4],"float16"), ) 
 CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 516.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:18:59.795311 test begin: paddle.outer(x=Tensor([4],"float16"), y=Tensor([4294967297],"float16"), )

[torch error] paddle.outer(x=Tensor([4],"float16"), y=Tensor([4294967297],"float16"), ) 
 CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 516.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:19:01.816120 test begin: paddle.pdist(Tensor([10, 228170138],"float32"), 0, )

CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.59 GiB is free. Process 13306 has 26.09 GiB memory in use. Process 135453 has 50.37 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 4.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:19:12.391104 test begin: paddle.pdist(Tensor([10, 228170138],"float32"), 1.0, )

CUDA out of memory. Tried to allocate 76.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 9.09 GiB is free. Process 135453 has 50.37 GiB memory in use. Process 109674 has 18.59 GiB memory in use. Of the allocated memory 17.00 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:20:42.375190 test begin: paddle.pdist(Tensor([10, 228170138],"float32"), 1.5, )

CUDA out of memory. Tried to allocate 76.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 9.09 GiB is free. Process 135453 has 50.37 GiB memory in use. Process 26519 has 18.59 GiB memory in use. Of the allocated memory 17.00 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:22:04.320695 test begin: paddle.pdist(Tensor([10, 228170138],"float32"), 2.0, )

CUDA out of memory. Tried to allocate 76.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 9.09 GiB is free. Process 135453 has 50.37 GiB memory in use. Process 96428 has 18.59 GiB memory in use. Of the allocated memory 17.00 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:23:23.915927 test begin: paddle.pdist(Tensor([10, 228170138],"float32"), 2.5, )

CUDA out of memory. Tried to allocate 76.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 9.09 GiB is free. Process 135453 has 50.37 GiB memory in use. Process 3787 has 18.59 GiB memory in use. Of the allocated memory 17.00 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:24:48.572229 test begin: paddle.pdist(Tensor([10, 228170138],"float32"), 3.0, )

CUDA out of memory. Tried to allocate 76.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 9.09 GiB is free. Process 135453 has 50.37 GiB memory in use. Process 77502 has 18.59 GiB memory in use. Of the allocated memory 17.00 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:26:20.305259 test begin: paddle.pdist(Tensor([10, 228170138],"float32"), math.inf, )

CUDA out of memory. Tried to allocate 76.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 9.09 GiB is free. Process 135453 has 50.37 GiB memory in use. Process 159560 has 18.59 GiB memory in use. Of the allocated memory 17.00 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:27:38.588147 test begin: paddle.pdist(Tensor([114085069, 20],"float32"), 0, )

W0318 12:28:44.008484 124372 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0318 12:28:44.009527 124372 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.pdist(Tensor([114085069, 20],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 24243076.99 GiB. GPU 0 has a total capacity of 79.18 GiB of which 18.19 GiB is free. Process 135453 has 50.37 GiB memory in use. Process 62859 has 9.49 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:28:48.727339 test begin: paddle.pdist(Tensor([114085069, 20],"float32"), 1.0, )

[torch error] paddle.pdist(Tensor([114085069, 20],"float32"), 1.0, ) 
 CUDA out of memory. Tried to allocate 24243076.99 GiB. GPU 0 has a total capacity of 79.18 GiB of which 17.60 GiB is free. Process 135453 has 50.37 GiB memory in use. Process 62859 has 10.09 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:28:51.760817 test begin: paddle.pdist(Tensor([114085069, 20],"float32"), 1.5, )

[torch error] paddle.pdist(Tensor([114085069, 20],"float32"), 1.5, ) 
 CUDA out of memory. Tried to allocate 24243076.99 GiB. GPU 0 has a total capacity of 79.18 GiB of which 17.60 GiB is free. Process 135453 has 50.37 GiB memory in use. Process 62859 has 10.09 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:28:56.718364 test begin: paddle.pdist(Tensor([114085069, 20],"float32"), 2.0, )

[torch error] paddle.pdist(Tensor([114085069, 20],"float32"), 2.0, ) 
 CUDA out of memory. Tried to allocate 24243076.99 GiB. GPU 0 has a total capacity of 79.18 GiB of which 17.60 GiB is free. Process 135453 has 50.37 GiB memory in use. Process 62859 has 10.09 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:28:58.358657 test begin: paddle.pdist(Tensor([114085069, 20],"float32"), 2.5, )

[torch error] paddle.pdist(Tensor([114085069, 20],"float32"), 2.5, ) 
 CUDA out of memory. Tried to allocate 24243076.99 GiB. GPU 0 has a total capacity of 79.18 GiB of which 17.60 GiB is free. Process 135453 has 50.37 GiB memory in use. Process 62859 has 10.09 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:29:00.128202 test begin: paddle.pdist(Tensor([114085069, 20],"float32"), 3.0, )

[torch error] paddle.pdist(Tensor([114085069, 20],"float32"), 3.0, ) 
 CUDA out of memory. Tried to allocate 24243076.99 GiB. GPU 0 has a total capacity of 79.18 GiB of which 17.60 GiB is free. Process 135453 has 50.37 GiB memory in use. Process 62859 has 10.09 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:29:01.975325 test begin: paddle.pdist(Tensor([114085069, 20],"float32"), math.inf, )

[torch error] paddle.pdist(Tensor([114085069, 20],"float32"), math.inf, ) 
 CUDA out of memory. Tried to allocate 24243076.99 GiB. GPU 0 has a total capacity of 79.18 GiB of which 17.60 GiB is free. Process 135453 has 50.37 GiB memory in use. Process 62859 has 10.09 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:29:04.347324 test begin: paddle.pdist(Tensor([214748365, 20],"float16"), 2.0, )

[torch error] paddle.pdist(Tensor([214748365, 20],"float16"), 2.0, ) 
 CUDA out of memory. Tried to allocate 42949672.84 GiB. GPU 0 has a total capacity of 79.18 GiB of which 17.60 GiB is free. Process 135453 has 50.37 GiB memory in use. Process 62859 has 10.09 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 514.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:30:32.691821 test begin: paddle.pdist(Tensor([50, 85899346],"float16"), 2.0, )

[torch error] paddle.pdist(Tensor([50, 85899346],"float16"), 2.0, ) 
 "pdist_cuda" not implemented for 'Half'
2025-03-18 12:30:34.772433 test begin: paddle.polygamma(Tensor([10, 20, 11408507],"float32"), 1, )

CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 606.38 MiB is free. Process 135453 has 50.37 GiB memory in use. Process 62859 has 27.09 GiB memory in use. Of the allocated memory 25.50 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:30:51.485820 test begin: paddle.polygamma(Tensor([10, 228170138, 1],"float32"), 1, )

CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 606.38 MiB is free. Process 135453 has 50.37 GiB memory in use. Process 78660 has 27.09 GiB memory in use. Of the allocated memory 25.50 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 12:32:11.783833 test begin: paddle.polygamma(Tensor([114085069, 20, 1],"float32"), 1, )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 86, in func_timeout
    thread.join(timeout)
  File "/usr/lib/python3.9/threading.py", line 1064, in join
    self._wait_for_tstate_lock(timeout=max(timeout, 0))
  File "/usr/lib/python3.9/threading.py", line 1080, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt
2025-03-18 14:06:54.756326 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=True, reduction="mean", name=None, )

W0318 14:08:17.136809 149441 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0318 14:08:17.137933 149441 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=True, reduction="mean", name=None, ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 1
2025-03-18 14:08:21.601987 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="mean", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="mean", name=None, ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 0
2025-03-18 14:08:24.088221 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="none", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="none", name=None, ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 0
2025-03-18 14:08:26.591831 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="sum", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="sum", name=None, ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 0
2025-03-18 14:08:28.562580 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=True, reduction="mean", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=True, reduction="mean", name=None, ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 0
2025-03-18 14:08:30.514045 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="mean", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="mean", name=None, ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 1
2025-03-18 14:08:32.479379 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="none", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="none", name=None, ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 1
2025-03-18 14:08:34.455440 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="sum", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="sum", name=None, ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 1
2025-03-18 14:08:35.817827 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=True, reduction="mean", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=True, reduction="mean", name=None, ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 1
2025-03-18 14:08:37.781346 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=False, reduction="mean", name=None, )

[paddle error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=False, reduction="mean", name=None, ) 
 The positive distance or negative distance should be greater than 0, The distance functions should be checked.
2025-03-18 14:20:58.126819 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=False, reduction="none", name=None, )

[paddle error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=False, reduction="none", name=None, ) 
 The positive distance or negative distance should be greater than 0, The distance functions should be checked.
2025-03-18 14:33:18.504646 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=False, reduction="sum", name=None, )

[paddle error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=False, reduction="sum", name=None, ) 
 The positive distance or negative distance should be greater than 0, The distance functions should be checked.
2025-03-18 14:45:38.186101 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=True, reduction="mean", name=None, )

[paddle error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), Tensor([5, 858993460],"float16"), margin=0.3, swap=True, reduction="mean", name=None, ) 
 The positive distance or negative distance should be greater than 0, The distance functions should be checked.
2025-03-18 15:03:49.171278 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="mean", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="mean", name=None, ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-18 15:03:53.960429 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="none", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="none", name=None, ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-18 15:03:57.548234 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="sum", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=False, reduction="sum", name=None, ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-18 15:03:59.354223 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=True, reduction="mean", name=None, )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([5, 5],"float16"), Tensor([5, 5],"float16"), margin=0.3, swap=True, reduction="mean", name=None, ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-18 15:04:01.663698 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), margin=0.3, swap=False, reduction="mean", name=None, )

CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 7.65 GiB is free. Process 26607 has 70.41 GiB memory in use. Of the allocated memory 60.00 GiB is allocated by PyTorch, and 8.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 15:04:21.027670 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), margin=0.3, swap=False, reduction="none", name=None, )

CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 4.45 GiB is free. Process 147881 has 73.61 GiB memory in use. Of the allocated memory 63.20 GiB is allocated by PyTorch, and 8.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 15:06:51.490764 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), margin=0.3, swap=False, reduction="sum", name=None, )

W0318 15:09:38.000716 39833 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0318 15:09:38.002909 39833 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), margin=0.3, swap=False, reduction="sum", name=None, ) 
 The positive distance or negative distance should be greater than 0, The distance functions should be checked.
2025-03-18 15:09:41.421965 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), Tensor([858993460, 5],"float16"), margin=0.3, swap=True, reduction="mean", name=None, )

CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 7.65 GiB is free. Process 147057 has 70.41 GiB memory in use. Of the allocated memory 66.40 GiB is allocated by PyTorch, and 2.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 15:10:02.912080 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", )

W0318 15:12:38.203235 71584 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0318 15:12:38.204710 71584 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 1
2025-03-18 15:12:41.310704 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 1
2025-03-18 15:12:44.042424 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 1
2025-03-18 15:12:47.624304 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 1
2025-03-18 15:12:50.851573 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 0
2025-03-18 15:12:54.156057 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 0
2025-03-18 15:12:57.323910 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 0
2025-03-18 15:13:00.194424 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 0
2025-03-18 15:13:03.371176 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 1
2025-03-18 15:13:06.296504 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 1
2025-03-18 15:13:09.355208 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 1
2025-03-18 15:13:13.252329 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 1
2025-03-18 15:13:15.725619 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 0
2025-03-18 15:13:17.866119 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 0
2025-03-18 15:13:20.039859 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 0
2025-03-18 15:13:22.395890 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", ) 
 The size of tensor a (5) must match the size of tensor b (858993460) at non-singleton dimension 0
2025-03-18 15:13:24.652457 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 1
2025-03-18 15:13:27.398374 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 1
2025-03-18 15:13:29.317838 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 1
2025-03-18 15:13:31.643810 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 1
2025-03-18 15:13:34.010268 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", )

[paddle error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", ) 
 The positive distance or negative distance should be greater than 0, The distance functions should be checked.
2025-03-18 15:26:00.569926 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", )

[paddle error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", ) 
 The positive distance or negative distance should be greater than 0, The distance functions should be checked.
2025-03-18 15:38:25.261927 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", )

[paddle error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", ) 
 The positive distance or negative distance should be greater than 0, The distance functions should be checked.
2025-03-18 15:50:47.107958 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", )

[paddle error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([5, 858993460],"float16"), positive=Tensor([5, 858993460],"float16"), negative=Tensor([5, 858993460],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", ) 
 The positive distance or negative distance should be greater than 0, The distance functions should be checked.
2025-03-18 16:08:54.463067 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-18 16:08:58.398455 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-18 16:08:59.857658 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-18 16:09:02.125467 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", )

[torch error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([5, 5],"float16"), negative=Tensor([5, 5],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", ) 
 The size of tensor a (858993460) must match the size of tensor b (5) at non-singleton dimension 0
2025-03-18 16:09:03.623405 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="mean", )

CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 7.65 GiB is free. Process 28461 has 70.41 GiB memory in use. Of the allocated memory 60.00 GiB is allocated by PyTorch, and 8.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:09:17.910168 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="none", )

CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 4.45 GiB is free. Process 88249 has 73.61 GiB memory in use. Of the allocated memory 63.20 GiB is allocated by PyTorch, and 8.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:10:55.422532 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", )

W0318 16:12:54.888128 143623 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0318 16:12:54.890297 143623 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[paddle error] paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=False, reduction="sum", ) 
 The positive distance or negative distance should be greater than 0, The distance functions should be checked.
2025-03-18 16:12:56.600330 test begin: paddle.nn.functional.triplet_margin_with_distance_loss(input=Tensor([858993460, 5],"float16"), positive=Tensor([858993460, 5],"float16"), negative=Tensor([858993460, 5],"float16"), distance_function=None, margin=0.3, swap=True, reduction="mean", )

CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 7.65 GiB is free. Process 2298 has 70.41 GiB memory in use. Of the allocated memory 66.40 GiB is allocated by PyTorch, and 2.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:13:10.505105 test begin: paddle.nn.functional.unfold(Tensor([10, 3, 1188387, 64],"float32"), 3, 1, 1, tuple(1,1,), )

W0318 16:14:25.858834 148336 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0318 16:14:25.859814 148336 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.nn.functional.unfold(Tensor([10, 3, 1188387, 64],"float32"), 3, 1, 1, tuple(1,1,), ) 
 CUDA out of memory. Tried to allocate 76.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 60.06 GiB is free. Process 118416 has 17.99 GiB memory in use. Of the allocated memory 17.00 GiB is allocated by PyTorch, and 3.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:14:27.875629 test begin: paddle.nn.functional.unfold(Tensor([10, 3, 1188387, 64],"float32"), 3, 1, tuple(1,1,), 1, )

[torch error] paddle.nn.functional.unfold(Tensor([10, 3, 1188387, 64],"float32"), 3, 1, tuple(1,1,), 1, ) 
 CUDA out of memory. Tried to allocate 76.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 59.47 GiB is free. Process 118416 has 18.59 GiB memory in use. Of the allocated memory 17.00 GiB is allocated by PyTorch, and 3.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:14:30.493558 test begin: paddle.nn.functional.unfold(Tensor([10, 3, 1188387, 64],"float32"), 3, tuple(1,1,), 1, 1, )

[torch error] paddle.nn.functional.unfold(Tensor([10, 3, 1188387, 64],"float32"), 3, tuple(1,1,), 1, 1, ) 
 CUDA out of memory. Tried to allocate 76.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 59.47 GiB is free. Process 118416 has 18.59 GiB memory in use. Of the allocated memory 17.00 GiB is allocated by PyTorch, and 3.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:14:33.192493 test begin: paddle.nn.functional.unfold(Tensor([10, 3, 1188387, 64],"float32"), 3, tuple(1,1,), tuple(1,1,), tuple(1,1,), )

[torch error] paddle.nn.functional.unfold(Tensor([10, 3, 1188387, 64],"float32"), 3, tuple(1,1,), tuple(1,1,), tuple(1,1,), ) 
 CUDA out of memory. Tried to allocate 76.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 59.47 GiB is free. Process 118416 has 18.59 GiB memory in use. Of the allocated memory 17.00 GiB is allocated by PyTorch, and 3.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:14:35.049898 test begin: paddle.nn.functional.unfold(Tensor([10, 3, 1188387, 64],"float32"), tuple(3,3,), tuple(1,1,), tuple(1,1,), tuple(1,1,), )

[torch error] paddle.nn.functional.unfold(Tensor([10, 3, 1188387, 64],"float32"), tuple(3,3,), tuple(1,1,), tuple(1,1,), tuple(1,1,), ) 
 CUDA out of memory. Tried to allocate 76.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 59.47 GiB is free. Process 118416 has 18.59 GiB memory in use. Of the allocated memory 17.00 GiB is allocated by PyTorch, and 3.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:14:37.464752 test begin: paddle.nn.functional.unfold(Tensor([10, 3, 64, 1188387],"float32"), 3, 1, 1, tuple(1,1,), )

[torch error] paddle.nn.functional.unfold(Tensor([10, 3, 64, 1188387],"float32"), 3, 1, 1, tuple(1,1,), ) 
 CUDA out of memory. Tried to allocate 76.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 59.47 GiB is free. Process 118416 has 18.59 GiB memory in use. Of the allocated memory 17.00 GiB is allocated by PyTorch, and 3.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:14:39.612521 test begin: paddle.nn.functional.unfold(Tensor([10, 3, 64, 1188387],"float32"), 3, 1, tuple(1,1,), 1, )

[torch error] paddle.nn.functional.unfold(Tensor([10, 3, 64, 1188387],"float32"), 3, 1, tuple(1,1,), 1, ) 
 CUDA out of memory. Tried to allocate 76.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 59.47 GiB is free. Process 118416 has 18.59 GiB memory in use. Of the allocated memory 17.00 GiB is allocated by PyTorch, and 3.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:14:42.311843 test begin: paddle.nn.functional.unfold(Tensor([10, 3, 64, 1188387],"float32"), 3, tuple(1,1,), 1, 1, )

[torch error] paddle.nn.functional.unfold(Tensor([10, 3, 64, 1188387],"float32"), 3, tuple(1,1,), 1, 1, ) 
 CUDA out of memory. Tried to allocate 76.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 59.47 GiB is free. Process 118416 has 18.59 GiB memory in use. Of the allocated memory 17.00 GiB is allocated by PyTorch, and 3.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:14:44.995524 test begin: paddle.nn.functional.unfold(Tensor([10, 3, 64, 1188387],"float32"), 3, tuple(1,1,), tuple(1,1,), tuple(1,1,), )

[torch error] paddle.nn.functional.unfold(Tensor([10, 3, 64, 1188387],"float32"), 3, tuple(1,1,), tuple(1,1,), tuple(1,1,), ) 
 CUDA out of memory. Tried to allocate 76.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 59.47 GiB is free. Process 118416 has 18.59 GiB memory in use. Of the allocated memory 17.00 GiB is allocated by PyTorch, and 3.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:14:47.722139 test begin: paddle.nn.functional.unfold(Tensor([10, 3, 64, 1188387],"float32"), tuple(3,3,), tuple(1,1,), tuple(1,1,), tuple(1,1,), )

[torch error] paddle.nn.functional.unfold(Tensor([10, 3, 64, 1188387],"float32"), tuple(3,3,), tuple(1,1,), tuple(1,1,), tuple(1,1,), ) 
 CUDA out of memory. Tried to allocate 76.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 59.47 GiB is free. Process 118416 has 18.59 GiB memory in use. Of the allocated memory 17.00 GiB is allocated by PyTorch, and 3.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:14:49.843197 test begin: paddle.nn.functional.unfold(Tensor([10, 55706, 64, 64],"float32"), 3, 1, 1, tuple(1,1,), )

[torch error] paddle.nn.functional.unfold(Tensor([10, 55706, 64, 64],"float32"), 3, 1, 1, tuple(1,1,), ) 
 CUDA out of memory. Tried to allocate 76.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 59.47 GiB is free. Process 118416 has 18.59 GiB memory in use. Of the allocated memory 17.00 GiB is allocated by PyTorch, and 3.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:14:52.561504 test begin: paddle.nn.functional.unfold(Tensor([10, 55706, 64, 64],"float32"), 3, 1, tuple(1,1,), 1, )

[torch error] paddle.nn.functional.unfold(Tensor([10, 55706, 64, 64],"float32"), 3, 1, tuple(1,1,), 1, ) 
 CUDA out of memory. Tried to allocate 76.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 59.47 GiB is free. Process 118416 has 18.59 GiB memory in use. Of the allocated memory 17.00 GiB is allocated by PyTorch, and 3.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:14:55.286438 test begin: paddle.nn.functional.unfold(Tensor([10, 55706, 64, 64],"float32"), 3, tuple(1,1,), 1, 1, )

[torch error] paddle.nn.functional.unfold(Tensor([10, 55706, 64, 64],"float32"), 3, tuple(1,1,), 1, 1, ) 
 CUDA out of memory. Tried to allocate 76.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 59.47 GiB is free. Process 118416 has 18.59 GiB memory in use. Of the allocated memory 17.00 GiB is allocated by PyTorch, and 3.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:14:57.442167 test begin: paddle.nn.functional.unfold(Tensor([10, 55706, 64, 64],"float32"), 3, tuple(1,1,), tuple(1,1,), tuple(1,1,), )

[torch error] paddle.nn.functional.unfold(Tensor([10, 55706, 64, 64],"float32"), 3, tuple(1,1,), tuple(1,1,), tuple(1,1,), ) 
 CUDA out of memory. Tried to allocate 76.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 59.47 GiB is free. Process 118416 has 18.59 GiB memory in use. Of the allocated memory 17.00 GiB is allocated by PyTorch, and 3.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:14:59.562572 test begin: paddle.nn.functional.unfold(Tensor([10, 55706, 64, 64],"float32"), tuple(3,3,), tuple(1,1,), tuple(1,1,), tuple(1,1,), )

[torch error] paddle.nn.functional.unfold(Tensor([10, 55706, 64, 64],"float32"), tuple(3,3,), tuple(1,1,), tuple(1,1,), tuple(1,1,), ) 
 CUDA out of memory. Tried to allocate 76.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 59.47 GiB is free. Process 118416 has 18.59 GiB memory in use. Of the allocated memory 17.00 GiB is allocated by PyTorch, and 3.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:15:01.422670 test begin: paddle.nn.functional.unfold(Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=3, strides=1, paddings=0, dilations=1, name=None, )

2025-03-18 16:17:38.047038 test begin: paddle.nn.functional.unfold(Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=list[2,4,], strides=1, paddings=1, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=list[2,4,], strides=1, paddings=1, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 63.36 GiB. GPU 0 has a total capacity of 79.18 GiB of which 5.88 GiB is free. Process 118416 has 72.17 GiB memory in use. Of the allocated memory 70.08 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:17:41.554422 test begin: paddle.nn.functional.unfold(Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=list[2,4,], strides=2, paddings=1, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=list[2,4,], strides=2, paddings=1, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 19.20 GiB. GPU 0 has a total capacity of 79.18 GiB of which 5.88 GiB is free. Process 118416 has 72.17 GiB memory in use. Of the allocated memory 70.08 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:17:43.058350 test begin: paddle.nn.functional.unfold(Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=list[3,3,], strides=1, paddings=0, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=list[3,3,], strides=1, paddings=0, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 46.08 GiB. GPU 0 has a total capacity of 79.18 GiB of which 5.88 GiB is free. Process 118416 has 72.17 GiB memory in use. Of the allocated memory 70.08 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:17:44.791355 test begin: paddle.nn.functional.unfold(Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=list[3,3,], strides=1, paddings=1, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=list[3,3,], strides=1, paddings=1, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 72.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 5.88 GiB is free. Process 118416 has 72.17 GiB memory in use. Of the allocated memory 70.08 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:17:45.990088 test begin: paddle.nn.functional.unfold(Tensor([15158, 3, 224, 224],"float32"), 16, 16, )

2025-03-18 16:17:58.110883 test begin: paddle.nn.functional.unfold(Tensor([185686, 3, 64, 64],"float32"), 3, 1, 1, tuple(1,1,), )

W0318 16:19:03.821635 163703 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0318 16:19:03.822657 163703 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.nn.functional.unfold(Tensor([185686, 3, 64, 64],"float32"), 3, 1, 1, tuple(1,1,), ) 
 CUDA out of memory. Tried to allocate 76.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 60.06 GiB is free. Process 26772 has 17.99 GiB memory in use. Of the allocated memory 17.00 GiB is allocated by PyTorch, and 3.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:19:05.867524 test begin: paddle.nn.functional.unfold(Tensor([185686, 3, 64, 64],"float32"), 3, 1, tuple(1,1,), 1, )

[torch error] paddle.nn.functional.unfold(Tensor([185686, 3, 64, 64],"float32"), 3, 1, tuple(1,1,), 1, ) 
 CUDA out of memory. Tried to allocate 76.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 59.47 GiB is free. Process 26772 has 18.59 GiB memory in use. Of the allocated memory 17.00 GiB is allocated by PyTorch, and 3.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:19:07.607022 test begin: paddle.nn.functional.unfold(Tensor([185686, 3, 64, 64],"float32"), 3, tuple(1,1,), 1, 1, )

[torch error] paddle.nn.functional.unfold(Tensor([185686, 3, 64, 64],"float32"), 3, tuple(1,1,), 1, 1, ) 
 CUDA out of memory. Tried to allocate 76.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 59.47 GiB is free. Process 26772 has 18.59 GiB memory in use. Of the allocated memory 17.00 GiB is allocated by PyTorch, and 3.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:19:10.376387 test begin: paddle.nn.functional.unfold(Tensor([185686, 3, 64, 64],"float32"), 3, tuple(1,1,), tuple(1,1,), tuple(1,1,), )

[torch error] paddle.nn.functional.unfold(Tensor([185686, 3, 64, 64],"float32"), 3, tuple(1,1,), tuple(1,1,), tuple(1,1,), ) 
 CUDA out of memory. Tried to allocate 76.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 59.47 GiB is free. Process 26772 has 18.59 GiB memory in use. Of the allocated memory 17.00 GiB is allocated by PyTorch, and 3.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:19:13.100359 test begin: paddle.nn.functional.unfold(Tensor([185686, 3, 64, 64],"float32"), tuple(3,3,), tuple(1,1,), tuple(1,1,), tuple(1,1,), )

[torch error] paddle.nn.functional.unfold(Tensor([185686, 3, 64, 64],"float32"), tuple(3,3,), tuple(1,1,), tuple(1,1,), tuple(1,1,), ) 
 CUDA out of memory. Tried to allocate 76.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 59.47 GiB is free. Process 26772 has 18.59 GiB memory in use. Of the allocated memory 17.00 GiB is allocated by PyTorch, and 3.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:19:15.345812 test begin: paddle.nn.functional.unfold(Tensor([2, 11408507, 10, 10],"float32"), kernel_sizes=3, strides=1, paddings=0, dilations=1, name=None, )

2025-03-18 16:19:18.507096 test begin: paddle.nn.functional.unfold(Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=3, strides=1, paddings=0, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=3, strides=1, paddings=0, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 46.08 GiB. GPU 0 has a total capacity of 79.18 GiB of which 3.00 GiB is free. Process 26772 has 75.05 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:20:36.201041 test begin: paddle.nn.functional.unfold(Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=list[2,4,], strides=1, paddings=1, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=list[2,4,], strides=1, paddings=1, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 63.36 GiB. GPU 0 has a total capacity of 79.18 GiB of which 3.00 GiB is free. Process 26772 has 75.05 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:20:37.886118 test begin: paddle.nn.functional.unfold(Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=list[2,4,], strides=2, paddings=1, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=list[2,4,], strides=2, paddings=1, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 19.20 GiB. GPU 0 has a total capacity of 79.18 GiB of which 3.00 GiB is free. Process 26772 has 75.05 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:20:40.907207 test begin: paddle.nn.functional.unfold(Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=list[3,3,], strides=1, paddings=0, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=list[3,3,], strides=1, paddings=0, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 46.08 GiB. GPU 0 has a total capacity of 79.18 GiB of which 3.00 GiB is free. Process 26772 has 75.05 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:20:43.600939 test begin: paddle.nn.functional.unfold(Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=list[3,3,], strides=1, paddings=1, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=list[3,3,], strides=1, paddings=1, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 72.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 3.00 GiB is free. Process 26772 has 75.05 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:20:45.576904 test begin: paddle.nn.functional.unfold(Tensor([2, 3, 10, 38028357],"float32"), kernel_sizes=3, strides=1, paddings=0, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 3, 10, 38028357],"float32"), kernel_sizes=3, strides=1, paddings=0, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 61.20 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.00 GiB is free. Process 26772 has 76.05 GiB memory in use. Of the allocated memory 74.46 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:20:52.882855 test begin: paddle.nn.functional.unfold(Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=3, strides=1, paddings=0, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=3, strides=1, paddings=0, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 57.60 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.50 GiB is free. Process 26772 has 75.55 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:20:56.810334 test begin: paddle.nn.functional.unfold(Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=list[2,4,], strides=1, paddings=1, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=list[2,4,], strides=1, paddings=1, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 70.40 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.50 GiB is free. Process 26772 has 75.55 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:20:59.245718 test begin: paddle.nn.functional.unfold(Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=list[2,4,], strides=2, paddings=1, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=list[2,4,], strides=2, paddings=1, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 19.20 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.50 GiB is free. Process 26772 has 75.55 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:21:01.686063 test begin: paddle.nn.functional.unfold(Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=list[3,3,], strides=1, paddings=0, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=list[3,3,], strides=1, paddings=0, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 57.60 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.50 GiB is free. Process 26772 has 75.55 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:21:03.753459 test begin: paddle.nn.functional.unfold(Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=list[3,3,], strides=1, paddings=1, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=list[3,3,], strides=1, paddings=1, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 72.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.50 GiB is free. Process 26772 has 75.55 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:21:05.715252 test begin: paddle.nn.functional.unfold(Tensor([2, 3, 38028357, 10],"float32"), kernel_sizes=3, strides=1, paddings=0, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 3, 38028357, 10],"float32"), kernel_sizes=3, strides=1, paddings=0, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 61.20 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.00 GiB is free. Process 26772 has 76.05 GiB memory in use. Of the allocated memory 74.46 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:21:08.160917 test begin: paddle.nn.functional.unfold(Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=3, strides=1, paddings=0, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=3, strides=1, paddings=0, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 57.60 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.50 GiB is free. Process 26772 has 75.55 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:21:10.673185 test begin: paddle.nn.functional.unfold(Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=list[2,4,], strides=1, paddings=1, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=list[2,4,], strides=1, paddings=1, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 57.60 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.50 GiB is free. Process 26772 has 75.55 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:21:12.717807 test begin: paddle.nn.functional.unfold(Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=list[2,4,], strides=2, paddings=1, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=list[2,4,], strides=2, paddings=1, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.50 GiB is free. Process 26772 has 75.55 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:21:14.580243 test begin: paddle.nn.functional.unfold(Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=list[3,3,], strides=1, paddings=0, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=list[3,3,], strides=1, paddings=0, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 57.60 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.50 GiB is free. Process 26772 has 75.55 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:21:16.190488 test begin: paddle.nn.functional.unfold(Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=list[3,3,], strides=1, paddings=1, dilations=1, name=None, )

[torch error] paddle.nn.functional.unfold(Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=list[3,3,], strides=1, paddings=1, dilations=1, name=None, ) 
 CUDA out of memory. Tried to allocate 72.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.50 GiB is free. Process 26772 has 75.55 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:21:18.036816 test begin: paddle.nn.functional.unfold(Tensor([64, 3, 224, 53053],"float32"), 16, 16, )

2025-03-18 16:21:30.886498 test begin: paddle.nn.functional.unfold(Tensor([64, 3, 53053, 224],"float32"), 16, 16, )

W0318 16:22:58.186923  8051 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0318 16:22:58.188119  8051 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.nn.functional.unfold(Tensor([64, 3, 53053, 224],"float32"), 16, 16, )
2025-03-18 16:25:59.320273 test begin: paddle.nn.functional.unfold(Tensor([64, 711, 224, 224],"float32"), 16, 16, )

[Pass] paddle.nn.functional.unfold(Tensor([64, 711, 224, 224],"float32"), 16, 16, )
2025-03-18 16:29:17.496524 test begin: paddle.nn.functional.unfold(Tensor([7605672, 3, 10, 10],"float32"), kernel_sizes=3, strides=1, paddings=0, dilations=1, name=None, )

2025-03-18 16:29:57.439584 test begin: paddle.nn.functional.unfold(x=Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=3, )

[torch error] paddle.nn.functional.unfold(x=Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=3, ) 
 CUDA out of memory. Tried to allocate 46.08 GiB. GPU 0 has a total capacity of 79.18 GiB of which 3.00 GiB is free. Process 49007 has 75.05 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:31:28.865516 test begin: paddle.nn.functional.unfold(x=Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=list[2,4,], paddings=1, )

[torch error] paddle.nn.functional.unfold(x=Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=list[2,4,], paddings=1, ) 
 CUDA out of memory. Tried to allocate 63.36 GiB. GPU 0 has a total capacity of 79.18 GiB of which 3.00 GiB is free. Process 49007 has 75.05 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:31:31.008345 test begin: paddle.nn.functional.unfold(x=Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=list[2,4,], paddings=1, strides=2, )

[torch error] paddle.nn.functional.unfold(x=Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=list[2,4,], paddings=1, strides=2, ) 
 CUDA out of memory. Tried to allocate 19.20 GiB. GPU 0 has a total capacity of 79.18 GiB of which 3.00 GiB is free. Process 49007 has 75.05 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:31:33.621044 test begin: paddle.nn.functional.unfold(x=Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=list[3,3,], )

[torch error] paddle.nn.functional.unfold(x=Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=list[3,3,], ) 
 CUDA out of memory. Tried to allocate 46.08 GiB. GPU 0 has a total capacity of 79.18 GiB of which 3.00 GiB is free. Process 49007 has 75.05 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:31:35.933877 test begin: paddle.nn.functional.unfold(x=Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=list[3,3,], paddings=1, )

[torch error] paddle.nn.functional.unfold(x=Tensor([14316558, 3, 10, 10],"float16"), kernel_sizes=list[3,3,], paddings=1, ) 
 CUDA out of memory. Tried to allocate 72.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 3.00 GiB is free. Process 49007 has 75.05 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:31:38.496480 test begin: paddle.nn.functional.unfold(x=Tensor([2, 11408507, 10, 10],"float32"), kernel_sizes=3, )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 11408507, 10, 10],"float32"), kernel_sizes=3, ) 
 CUDA out of memory. Tried to allocate 48.96 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.00 GiB is free. Process 49007 has 76.05 GiB memory in use. Of the allocated memory 74.46 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:31:43.212210 test begin: paddle.nn.functional.unfold(x=Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=3, )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=3, ) 
 CUDA out of memory. Tried to allocate 46.08 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.50 GiB is free. Process 49007 has 75.55 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:31:48.853581 test begin: paddle.nn.functional.unfold(x=Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=list[2,4,], paddings=1, )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=list[2,4,], paddings=1, ) 
 CUDA out of memory. Tried to allocate 63.36 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.50 GiB is free. Process 49007 has 75.55 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:31:50.489109 test begin: paddle.nn.functional.unfold(x=Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=list[2,4,], paddings=1, strides=2, )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=list[2,4,], paddings=1, strides=2, ) 
 CUDA out of memory. Tried to allocate 19.20 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.50 GiB is free. Process 49007 has 75.55 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:31:52.322798 test begin: paddle.nn.functional.unfold(x=Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=list[3,3,], )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=list[3,3,], ) 
 CUDA out of memory. Tried to allocate 46.08 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.50 GiB is free. Process 49007 has 75.55 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:31:54.777176 test begin: paddle.nn.functional.unfold(x=Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=list[3,3,], paddings=1, )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 21474837, 10, 10],"float16"), kernel_sizes=list[3,3,], paddings=1, ) 
 CUDA out of memory. Tried to allocate 72.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.50 GiB is free. Process 49007 has 75.55 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:31:56.507584 test begin: paddle.nn.functional.unfold(x=Tensor([2, 3, 10, 38028357],"float32"), kernel_sizes=3, )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 3, 10, 38028357],"float32"), kernel_sizes=3, ) 
 CUDA out of memory. Tried to allocate 61.20 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.00 GiB is free. Process 49007 has 76.05 GiB memory in use. Of the allocated memory 74.46 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:32:00.399694 test begin: paddle.nn.functional.unfold(x=Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=3, )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=3, ) 
 CUDA out of memory. Tried to allocate 57.60 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.50 GiB is free. Process 49007 has 75.55 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:32:02.863740 test begin: paddle.nn.functional.unfold(x=Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=list[2,4,], paddings=1, )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=list[2,4,], paddings=1, ) 
 CUDA out of memory. Tried to allocate 70.40 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.50 GiB is free. Process 49007 has 75.55 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:32:05.301331 test begin: paddle.nn.functional.unfold(x=Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=list[2,4,], paddings=1, strides=2, )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=list[2,4,], paddings=1, strides=2, ) 
 CUDA out of memory. Tried to allocate 19.20 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.50 GiB is free. Process 49007 has 75.55 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:32:07.720177 test begin: paddle.nn.functional.unfold(x=Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=list[3,3,], )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=list[3,3,], ) 
 CUDA out of memory. Tried to allocate 57.60 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.50 GiB is free. Process 49007 has 75.55 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:32:10.161234 test begin: paddle.nn.functional.unfold(x=Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=list[3,3,], paddings=1, )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 3, 10, 71582789],"float16"), kernel_sizes=list[3,3,], paddings=1, ) 
 CUDA out of memory. Tried to allocate 72.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.50 GiB is free. Process 49007 has 75.55 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:32:12.577401 test begin: paddle.nn.functional.unfold(x=Tensor([2, 3, 38028357, 10],"float32"), kernel_sizes=3, )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 3, 38028357, 10],"float32"), kernel_sizes=3, ) 
 CUDA out of memory. Tried to allocate 61.20 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.00 GiB is free. Process 49007 has 76.05 GiB memory in use. Of the allocated memory 74.46 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:32:16.160877 test begin: paddle.nn.functional.unfold(x=Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=3, )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=3, ) 
 CUDA out of memory. Tried to allocate 57.60 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.50 GiB is free. Process 49007 has 75.55 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:32:18.696436 test begin: paddle.nn.functional.unfold(x=Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=list[2,4,], paddings=1, )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=list[2,4,], paddings=1, ) 
 CUDA out of memory. Tried to allocate 57.60 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.50 GiB is free. Process 49007 has 75.55 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:32:21.239964 test begin: paddle.nn.functional.unfold(x=Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=list[2,4,], paddings=1, strides=2, )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=list[2,4,], paddings=1, strides=2, ) 
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.50 GiB is free. Process 49007 has 75.55 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:32:23.785830 test begin: paddle.nn.functional.unfold(x=Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=list[3,3,], )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=list[3,3,], ) 
 CUDA out of memory. Tried to allocate 57.60 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.50 GiB is free. Process 49007 has 75.55 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:32:26.190689 test begin: paddle.nn.functional.unfold(x=Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=list[3,3,], paddings=1, )

[torch error] paddle.nn.functional.unfold(x=Tensor([2, 3, 71582789, 10],"float16"), kernel_sizes=list[3,3,], paddings=1, ) 
 CUDA out of memory. Tried to allocate 72.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.50 GiB is free. Process 49007 has 75.55 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 518.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:32:28.330354 test begin: paddle.nn.functional.unfold(x=Tensor([7605672, 3, 10, 10],"float32"), kernel_sizes=3, )

[torch error] paddle.nn.functional.unfold(x=Tensor([7605672, 3, 10, 10],"float32"), kernel_sizes=3, ) 
 CUDA out of memory. Tried to allocate 48.96 GiB. GPU 0 has a total capacity of 79.18 GiB of which 2.00 GiB is free. Process 49007 has 76.05 GiB memory in use. Of the allocated memory 74.46 GiB is allocated by PyTorch, and 6.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 16:32:30.911463 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([10, 15],"float32"),Tensor([2281701379],"float32"),], )

2025-03-18 16:32:41.159933 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([10, 20],"float32"),Tensor([2281701379],"float32"),], )

W0318 16:34:05.640775 30415 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0318 16:34:05.642053 30415 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[not compare]  None tensor([[ 0.2875, -0.1946, -0.1165,  0.4244, -0.1176, -0.4629,  0.0265,  0.3536,
          0.0649, -0.3683,  0.3050,  0.4756,  0.3001,  0.1689, -0.0235, -0.0235,
          0.1789,  0.4801,  0.1033, -0.1554],
        [ 0.0231, -0.2152,  0.4408,  0.0466,  0.3926, -0.3491, -0.0331,  0.0219,
         -0.3193, -0.2319,  0.2782, -0.4603, -0.4482, -0.1966, -0.1616,  0.0587,
         -0.1872,  0.4569, -0.0200, -0.4112],
        [-0.0453,  0.4144, -0.0663,  0.3710,  0.0426, -0.2206,  0.3631, -0.0510,
          0.2890,  0.1706,  0.4911, -0.0753,  0.1452,  0.4071,  0.3176, -0.3029,
         -0.4047, -0.0140, -0.4078, -0.3644],
        [ 0.2642,  0.3934,  0.4974,  0.1647, -0.1109,  0.3092, -0.2864,  0.1853,
         -0.3991, -0.0783, -0.2104,  0.2593,  0.3514, -0.1257, -0.1534, -0.3946,
          0.1975, -0.0074,  0.1793,  0.3940],
        [ 0.4700, -0.0381, -0.3039, -0.1678,  0.0584,  0.2394,  0.1346, -0.3845,
          0.4594, -0.3825,  0.0206, -0.0858, -0.3933,  0.0744, -0.4802,  0.2029,
         -0.0756, -0.3595, -0.3356, -0.3808],
        [-0.1126, -0.0641, -0.0801,  0.4253,  0.3473, -0.2821,  0.3355,  0.3130,
         -0.2274,  0.0640, -0.0345, -0.1173,  0.0390, -0.0335,  0.4002, -0.0304,
          0.1637, -0.0447, -0.1177,  0.3111],
        [-0.0337,  0.3897,  0.1190,  0.3907,  0.2743, -0.4548,  0.1147,  0.0197,
         -0.3022,  0.2534, -0.2110,  0.4957, -0.1040,  0.0171, -0.4394,  0.3198,
         -0.1376,  0.4309,  0.4314, -0.1596],
        [ 0.1371, -0.4695,  0.4254,  0.3177, -0.0484,  0.0779, -0.3046,  0.4593,
          0.0927,  0.1183, -0.0163, -0.3480, -0.3183,  0.3175, -0.2496,  0.1948,
         -0.0085,  0.0994, -0.3906,  0.1411],
        [ 0.2268,  0.2850, -0.1130,  0.1568,  0.4658,  0.2857, -0.1205, -0.3240,
         -0.2335, -0.0637, -0.2786, -0.2461, -0.0629,  0.3375, -0.4819, -0.0416,
          0.3308,  0.4079,  0.0424,  0.1809],
        [ 0.2393,  0.1381, -0.3741,  0.4222, -0.4597, -0.4661, -0.4237, -0.4959,
          0.3501, -0.1275, -0.2300,  0.1288,  0.2410,  0.1676, -0.1464,  0.1379,
          0.0801,  0.1966, -0.0632, -0.3358]])
2025-03-18 16:35:36.701836 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([10, 228170138],"float32"),Tensor([15],"float32"),], )

[not compare]  None tensor([[-0.4597, -0.4661, -0.4237,  ...,  0.1910,  0.4820,  0.2383],
        [-0.1172, -0.1261, -0.2841,  ...,  0.2214, -0.1934, -0.4385],
        [-0.1753,  0.3211,  0.4041,  ...,  0.1901, -0.3915,  0.4627],
        ...,
        [ 0.1224,  0.2581, -0.0319,  ...,  0.4226, -0.3568,  0.0288],
        [-0.1550, -0.3580,  0.0994,  ..., -0.3709,  0.3351,  0.3238],
        [ 0.2141, -0.3926,  0.3077,  ...,  0.2297, -0.4348, -0.0478]])
2025-03-18 16:37:51.353677 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([10, 228170138],"float32"),Tensor([20],"float32"),], )

[not compare]  None tensor([[ 0.1809,  0.2393,  0.1381,  ...,  0.4079, -0.0500,  0.2496],
        [ 0.0206, -0.0464,  0.1910,  ...,  0.2659,  0.0596, -0.2063],
        [-0.1544, -0.2546,  0.2214,  ..., -0.2290,  0.4045,  0.5000],
        ...,
        [ 0.2626,  0.3741, -0.4092,  ..., -0.4096, -0.0872,  0.0035],
        [-0.1549, -0.1233,  0.4226,  ...,  0.2303,  0.0903,  0.2488],
        [-0.0858, -0.1611, -0.3709,  ..., -0.1803, -0.4281, -0.4262]])
2025-03-18 16:39:35.328288 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([10, 228170138],"float32"),], )

[not compare]  None tensor([[-0.3358,  0.1606, -0.4382,  ..., -0.0090,  0.1347, -0.4348],
        [ 0.2284, -0.2941,  0.0176,  ...,  0.1004,  0.4161,  0.2516],
        [ 0.4008,  0.0283,  0.4764,  ..., -0.0965, -0.0904,  0.1603],
        ...,
        [-0.2313,  0.0324,  0.2318,  ..., -0.1862,  0.2681, -0.0982],
        [-0.0348, -0.2739,  0.1996,  ...,  0.0156,  0.1850,  0.1784],
        [ 0.3997,  0.1421,  0.4640,  ..., -0.3204, -0.1164,  0.3856]])
2025-03-18 16:41:15.401650 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([114085069, 20],"float32"),Tensor([20],"float32"),], )

[not compare]  None tensor([[ 1.8086e-01,  2.3930e-01,  1.3805e-01,  ...,  8.0144e-02,
          1.9655e-01, -6.3195e-02],
        [-3.3585e-01,  1.6061e-01, -4.3819e-01,  ...,  4.3723e-02,
          9.6416e-02, -2.6700e-01],
        [ 4.5616e-01,  3.4744e-01,  4.9839e-01,  ..., -3.7589e-01,
          4.9191e-01, -2.5112e-01],
        ...,
        [-2.9547e-01, -5.0516e-02, -1.7126e-01,  ..., -6.5547e-02,
         -2.5866e-01, -3.2855e-04],
        [ 9.0284e-02, -3.4100e-01,  3.6056e-01,  ..., -1.3775e-01,
         -2.2515e-01,  3.9518e-01],
        [ 1.0166e-01,  2.8747e-01, -2.0259e-01,  ..., -1.8029e-01,
         -4.2807e-01, -4.2619e-01]])
2025-03-18 16:43:18.853059 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([1140850690, 2],"float32"),Tensor([2],"float32"),], )

[not compare]  None tensor([[ 0.1966, -0.0632],
        [-0.3358,  0.1606],
        [-0.4382, -0.4168],
        ...,
        [-0.3715,  0.0596],
        [-0.4350, -0.0614],
        [-0.2201, -0.3204]])
2025-03-18 16:45:31.884981 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([152113426, 15],"float32"),Tensor([15],"float32"),], )

[not compare]  None tensor([[-0.4819, -0.0416,  0.3308,  ..., -0.4237, -0.4959,  0.3501],
        [-0.1275, -0.2300,  0.1288,  ..., -0.4382, -0.4168,  0.4929],
        [ 0.0852,  0.0394, -0.1083,  ...,  0.0437,  0.0964, -0.2670],
        ...,
        [ 0.0903, -0.3410,  0.3606,  ..., -0.2019,  0.2277,  0.1478],
        [ 0.4968,  0.0258, -0.1378,  ..., -0.3686, -0.4866, -0.0496],
        [-0.0555, -0.2256,  0.4151,  ...,  0.2297, -0.4348, -0.0478]])
2025-03-18 16:47:17.531183 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([16, 142606337],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[not compare]  None tensor([[ 0.3995, -0.1923, -0.1373,  ...,  0.0070, -0.2767,  0.3804],
        [ 0.2931,  0.2880, -0.2299,  ...,  0.3045,  0.1906,  0.3410],
        [-0.0413, -0.1237,  0.4060,  ...,  0.4173, -0.1207,  0.2891],
        ...,
        [-0.4074,  0.0947, -0.4106,  ..., -0.1355, -0.1790, -0.3694],
        [ 0.3528,  0.0192, -0.3420,  ..., -0.0905,  0.0520, -0.1557],
        [ 0.1536,  0.1267,  0.2930,  ...,  0.4449, -0.3111,  0.0638]])
2025-03-18 16:48:52.435866 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([16, 8],"float32"),Tensor([142606337, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[not compare]  None tensor([[-1.2451e-02, -2.7838e-01, -4.2607e-01,  4.8197e-01, -2.1846e-01,
          1.7663e-01,  3.6719e-01,  3.6299e-01],
        [ 4.7350e-01,  2.8193e-04, -3.7339e-02,  3.5190e-01,  4.0786e-01,
         -3.4015e-01,  3.4031e-01,  1.4745e-01],
        [ 6.9910e-02, -3.6688e-01,  1.5750e-01,  2.5046e-01, -2.9324e-01,
         -2.6735e-01,  3.4158e-01,  5.3812e-02],
        [ 1.5930e-01, -2.4374e-01, -3.1114e-02,  2.8018e-01, -4.4728e-01,
         -8.8391e-02, -3.1163e-01,  4.0308e-01],
        [ 2.6545e-01,  2.4889e-01,  6.0295e-02,  4.4613e-01,  4.8515e-01,
          1.9753e-01, -1.5667e-01, -4.6595e-01],
        [-3.6905e-01,  1.0378e-01, -3.4765e-03,  4.9926e-01, -2.7782e-01,
          5.5071e-03, -6.2612e-02, -2.5720e-01],
        [-1.7514e-01,  3.4010e-01,  4.1858e-01,  1.8246e-02,  4.0934e-01,
         -3.4341e-01, -3.6385e-01, -4.2737e-02],
        [ 2.9666e-01,  3.7941e-01,  1.8893e-01, -1.3143e-02, -2.2150e-01,
          3.9345e-01, -2.5587e-01, -3.5953e-01],
        [-1.8947e-01,  7.9232e-02,  2.1343e-02, -2.9419e-01,  4.7702e-02,
         -2.8152e-01, -3.4940e-01, -4.4225e-02],
        [ 1.3757e-01, -1.4379e-01, -4.5595e-01, -1.9878e-01, -2.8178e-01,
          2.1714e-01,  2.2799e-02,  3.3300e-01],
        [ 4.7477e-01, -4.7534e-01, -4.3817e-01, -2.5536e-01, -3.2329e-03,
         -1.6206e-01,  4.3321e-01,  2.8410e-01],
        [-4.2161e-01, -8.6324e-03, -3.2431e-01, -3.9876e-01, -1.2667e-01,
         -1.1684e-01,  2.4960e-01,  2.8027e-01],
        [-1.7121e-01, -4.5725e-01,  3.2917e-01,  1.1095e-01,  1.3811e-01,
          9.6645e-02, -4.3655e-01, -4.2003e-01],
        [ 4.6680e-01, -3.2080e-01,  4.5577e-01,  3.9400e-01,  1.6898e-02,
         -2.3727e-01, -2.0581e-01, -2.6656e-01],
        [-2.6652e-01, -1.5232e-01,  3.1556e-02, -8.7108e-03, -3.8514e-01,
         -6.0122e-02,  1.5924e-01, -3.7850e-01],
        [-4.3933e-01, -3.5672e-01,  3.5412e-01,  2.0119e-01, -3.9162e-01,
         -1.2559e-01, -1.9816e-01,  4.1927e-01]])
2025-03-18 16:50:35.794928 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([16, 8],"float32"),Tensor([16, 142606337],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[not compare]  None tensor([[-1.2451e-02, -2.7838e-01, -4.2607e-01,  4.8197e-01, -2.1846e-01,
          1.7663e-01,  3.6719e-01,  3.6299e-01],
        [ 4.7350e-01,  2.8193e-04, -3.7339e-02,  3.5190e-01,  4.0786e-01,
         -3.4015e-01,  3.4031e-01,  1.4745e-01],
        [ 6.9910e-02, -3.6688e-01,  1.5750e-01,  2.5046e-01, -2.9324e-01,
         -2.6735e-01,  3.4158e-01,  5.3812e-02],
        [ 1.5930e-01, -2.4374e-01, -3.1114e-02,  2.8018e-01, -4.4728e-01,
         -8.8391e-02, -3.1163e-01,  4.0308e-01],
        [ 2.6545e-01,  2.4889e-01,  6.0295e-02,  4.4613e-01,  4.8515e-01,
          1.9753e-01, -1.5667e-01, -4.6595e-01],
        [-3.6905e-01,  1.0378e-01, -3.4765e-03,  4.9926e-01, -2.7782e-01,
          5.5071e-03, -6.2612e-02, -2.5720e-01],
        [-1.7514e-01,  3.4010e-01,  4.1858e-01,  1.8246e-02,  4.0934e-01,
         -3.4341e-01, -3.6385e-01, -4.2737e-02],
        [ 2.9666e-01,  3.7941e-01,  1.8893e-01, -1.3143e-02, -2.2150e-01,
          3.9345e-01, -2.5587e-01, -3.5953e-01],
        [-1.8947e-01,  7.9232e-02,  2.1343e-02, -2.9419e-01,  4.7702e-02,
         -2.8152e-01, -3.4940e-01, -4.4225e-02],
        [ 1.3757e-01, -1.4379e-01, -4.5595e-01, -1.9878e-01, -2.8178e-01,
          2.1714e-01,  2.2799e-02,  3.3300e-01],
        [ 4.7477e-01, -4.7534e-01, -4.3817e-01, -2.5536e-01, -3.2329e-03,
         -1.6206e-01,  4.3321e-01,  2.8410e-01],
        [-4.2161e-01, -8.6324e-03, -3.2431e-01, -3.9876e-01, -1.2667e-01,
         -1.1684e-01,  2.4960e-01,  2.8027e-01],
        [-1.7121e-01, -4.5725e-01,  3.2917e-01,  1.1095e-01,  1.3811e-01,
          9.6645e-02, -4.3655e-01, -4.2003e-01],
        [ 4.6680e-01, -3.2080e-01,  4.5577e-01,  3.9400e-01,  1.6898e-02,
         -2.3727e-01, -2.0581e-01, -2.6656e-01],
        [-2.6652e-01, -1.5232e-01,  3.1556e-02, -8.7108e-03, -3.8514e-01,
         -6.0122e-02,  1.5924e-01, -3.7850e-01],
        [-4.3933e-01, -3.5672e-01,  3.5412e-01,  2.0119e-01, -3.9162e-01,
         -1.2559e-01, -1.9816e-01,  4.1927e-01]])
2025-03-18 16:52:32.236164 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([142606337, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[not compare]  None tensor([[-1.2451e-02, -2.7838e-01, -4.2607e-01,  4.8197e-01, -2.1846e-01,
          1.7663e-01,  3.6719e-01,  3.6299e-01],
        [ 4.7350e-01,  2.8193e-04, -3.7339e-02,  3.5190e-01,  4.0786e-01,
         -3.4015e-01,  3.4031e-01,  1.4745e-01],
        [ 6.9910e-02, -3.6688e-01,  1.5750e-01,  2.5046e-01, -2.9324e-01,
         -2.6735e-01,  3.4158e-01,  5.3812e-02],
        [ 1.5930e-01, -2.4374e-01, -3.1114e-02,  2.8018e-01, -4.4728e-01,
         -8.8391e-02, -3.1163e-01,  4.0308e-01],
        [ 2.6545e-01,  2.4889e-01,  6.0295e-02,  4.4613e-01,  4.8515e-01,
          1.9753e-01, -1.5667e-01, -4.6595e-01],
        [-3.6905e-01,  1.0378e-01, -3.4765e-03,  4.9926e-01, -2.7782e-01,
          5.5071e-03, -6.2612e-02, -2.5720e-01],
        [-1.7514e-01,  3.4010e-01,  4.1858e-01,  1.8246e-02,  4.0934e-01,
         -3.4341e-01, -3.6385e-01, -4.2737e-02],
        [ 2.9666e-01,  3.7941e-01,  1.8893e-01, -1.3143e-02, -2.2150e-01,
          3.9345e-01, -2.5587e-01, -3.5953e-01],
        [-1.8947e-01,  7.9232e-02,  2.1343e-02, -2.9419e-01,  4.7702e-02,
         -2.8152e-01, -3.4940e-01, -4.4225e-02],
        [ 1.3757e-01, -1.4379e-01, -4.5595e-01, -1.9878e-01, -2.8178e-01,
          2.1714e-01,  2.2799e-02,  3.3300e-01],
        [ 4.7477e-01, -4.7534e-01, -4.3817e-01, -2.5536e-01, -3.2329e-03,
         -1.6206e-01,  4.3321e-01,  2.8410e-01],
        [-4.2161e-01, -8.6324e-03, -3.2431e-01, -3.9876e-01, -1.2667e-01,
         -1.1684e-01,  2.4960e-01,  2.8027e-01],
        [-1.7121e-01, -4.5725e-01,  3.2917e-01,  1.1095e-01,  1.3811e-01,
          9.6645e-02, -4.3655e-01, -4.2003e-01],
        [ 4.6680e-01, -3.2080e-01,  4.5577e-01,  3.9400e-01,  1.6898e-02,
         -2.3727e-01, -2.0581e-01, -2.6656e-01],
        [-2.6652e-01, -1.5232e-01,  3.1556e-02, -8.7108e-03, -3.8514e-01,
         -6.0122e-02,  1.5924e-01, -3.7850e-01],
        [-4.3933e-01, -3.5672e-01,  3.5412e-01,  2.0119e-01, -3.9162e-01,
         -1.2559e-01, -1.9816e-01,  4.1927e-01]])
2025-03-18 16:54:17.822206 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 142606337],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[not compare]  None tensor([[-1.2451e-02, -2.7838e-01, -4.2607e-01,  4.8197e-01, -2.1846e-01,
          1.7663e-01,  3.6719e-01,  3.6299e-01],
        [ 4.7350e-01,  2.8193e-04, -3.7339e-02,  3.5190e-01,  4.0786e-01,
         -3.4015e-01,  3.4031e-01,  1.4745e-01],
        [ 6.9910e-02, -3.6688e-01,  1.5750e-01,  2.5046e-01, -2.9324e-01,
         -2.6735e-01,  3.4158e-01,  5.3812e-02],
        [ 1.5930e-01, -2.4374e-01, -3.1114e-02,  2.8018e-01, -4.4728e-01,
         -8.8391e-02, -3.1163e-01,  4.0308e-01],
        [ 2.6545e-01,  2.4889e-01,  6.0295e-02,  4.4613e-01,  4.8515e-01,
          1.9753e-01, -1.5667e-01, -4.6595e-01],
        [-3.6905e-01,  1.0378e-01, -3.4765e-03,  4.9926e-01, -2.7782e-01,
          5.5071e-03, -6.2612e-02, -2.5720e-01],
        [-1.7514e-01,  3.4010e-01,  4.1858e-01,  1.8246e-02,  4.0934e-01,
         -3.4341e-01, -3.6385e-01, -4.2737e-02],
        [ 2.9666e-01,  3.7941e-01,  1.8893e-01, -1.3143e-02, -2.2150e-01,
          3.9345e-01, -2.5587e-01, -3.5953e-01],
        [-1.8947e-01,  7.9232e-02,  2.1343e-02, -2.9419e-01,  4.7702e-02,
         -2.8152e-01, -3.4940e-01, -4.4225e-02],
        [ 1.3757e-01, -1.4379e-01, -4.5595e-01, -1.9878e-01, -2.8178e-01,
          2.1714e-01,  2.2799e-02,  3.3300e-01],
        [ 4.7477e-01, -4.7534e-01, -4.3817e-01, -2.5536e-01, -3.2329e-03,
         -1.6206e-01,  4.3321e-01,  2.8410e-01],
        [-4.2161e-01, -8.6324e-03, -3.2431e-01, -3.9876e-01, -1.2667e-01,
         -1.1684e-01,  2.4960e-01,  2.8027e-01],
        [-1.7121e-01, -4.5725e-01,  3.2917e-01,  1.1095e-01,  1.3811e-01,
          9.6645e-02, -4.3655e-01, -4.2003e-01],
        [ 4.6680e-01, -3.2080e-01,  4.5577e-01,  3.9400e-01,  1.6898e-02,
         -2.3727e-01, -2.0581e-01, -2.6656e-01],
        [-2.6652e-01, -1.5232e-01,  3.1556e-02, -8.7108e-03, -3.8514e-01,
         -6.0122e-02,  1.5924e-01, -3.7850e-01],
        [-4.3933e-01, -3.5672e-01,  3.5412e-01,  2.0119e-01, -3.9162e-01,
         -1.2559e-01, -1.9816e-01,  4.1927e-01]])
2025-03-18 16:56:11.380443 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([142606337, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[not compare]  None tensor([[-1.2451e-02, -2.7838e-01, -4.2607e-01,  4.8197e-01, -2.1846e-01,
          1.7663e-01,  3.6719e-01,  3.6299e-01],
        [ 4.7350e-01,  2.8193e-04, -3.7339e-02,  3.5190e-01,  4.0786e-01,
         -3.4015e-01,  3.4031e-01,  1.4745e-01],
        [ 6.9910e-02, -3.6688e-01,  1.5750e-01,  2.5046e-01, -2.9324e-01,
         -2.6735e-01,  3.4158e-01,  5.3812e-02],
        [ 1.5930e-01, -2.4374e-01, -3.1114e-02,  2.8018e-01, -4.4728e-01,
         -8.8391e-02, -3.1163e-01,  4.0308e-01],
        [ 2.6545e-01,  2.4889e-01,  6.0295e-02,  4.4613e-01,  4.8515e-01,
          1.9753e-01, -1.5667e-01, -4.6595e-01],
        [-3.6905e-01,  1.0378e-01, -3.4765e-03,  4.9926e-01, -2.7782e-01,
          5.5071e-03, -6.2612e-02, -2.5720e-01],
        [-1.7514e-01,  3.4010e-01,  4.1858e-01,  1.8246e-02,  4.0934e-01,
         -3.4341e-01, -3.6385e-01, -4.2737e-02],
        [ 2.9666e-01,  3.7941e-01,  1.8893e-01, -1.3143e-02, -2.2150e-01,
          3.9345e-01, -2.5587e-01, -3.5953e-01],
        [-1.8947e-01,  7.9232e-02,  2.1343e-02, -2.9419e-01,  4.7702e-02,
         -2.8152e-01, -3.4940e-01, -4.4225e-02],
        [ 1.3757e-01, -1.4379e-01, -4.5595e-01, -1.9878e-01, -2.8178e-01,
          2.1714e-01,  2.2799e-02,  3.3300e-01],
        [ 4.7477e-01, -4.7534e-01, -4.3817e-01, -2.5536e-01, -3.2329e-03,
         -1.6206e-01,  4.3321e-01,  2.8410e-01],
        [-4.2161e-01, -8.6324e-03, -3.2431e-01, -3.9876e-01, -1.2667e-01,
         -1.1684e-01,  2.4960e-01,  2.8027e-01],
        [-1.7121e-01, -4.5725e-01,  3.2917e-01,  1.1095e-01,  1.3811e-01,
          9.6645e-02, -4.3655e-01, -4.2003e-01],
        [ 4.6680e-01, -3.2080e-01,  4.5577e-01,  3.9400e-01,  1.6898e-02,
         -2.3727e-01, -2.0581e-01, -2.6656e-01],
        [-2.6652e-01, -1.5232e-01,  3.1556e-02, -8.7108e-03, -3.8514e-01,
         -6.0122e-02,  1.5924e-01, -3.7850e-01],
        [-4.3933e-01, -3.5672e-01,  3.5412e-01,  2.0119e-01, -3.9162e-01,
         -1.2559e-01, -1.9816e-01,  4.1927e-01]])
2025-03-18 16:57:54.831329 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 142606337],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[not compare]  None tensor([[-1.2451e-02, -2.7838e-01, -4.2607e-01,  4.8197e-01, -2.1846e-01,
          1.7663e-01,  3.6719e-01,  3.6299e-01],
        [ 4.7350e-01,  2.8193e-04, -3.7339e-02,  3.5190e-01,  4.0786e-01,
         -3.4015e-01,  3.4031e-01,  1.4745e-01],
        [ 6.9910e-02, -3.6688e-01,  1.5750e-01,  2.5046e-01, -2.9324e-01,
         -2.6735e-01,  3.4158e-01,  5.3812e-02],
        [ 1.5930e-01, -2.4374e-01, -3.1114e-02,  2.8018e-01, -4.4728e-01,
         -8.8391e-02, -3.1163e-01,  4.0308e-01],
        [ 2.6545e-01,  2.4889e-01,  6.0295e-02,  4.4613e-01,  4.8515e-01,
          1.9753e-01, -1.5667e-01, -4.6595e-01],
        [-3.6905e-01,  1.0378e-01, -3.4765e-03,  4.9926e-01, -2.7782e-01,
          5.5071e-03, -6.2612e-02, -2.5720e-01],
        [-1.7514e-01,  3.4010e-01,  4.1858e-01,  1.8246e-02,  4.0934e-01,
         -3.4341e-01, -3.6385e-01, -4.2737e-02],
        [ 2.9666e-01,  3.7941e-01,  1.8893e-01, -1.3143e-02, -2.2150e-01,
          3.9345e-01, -2.5587e-01, -3.5953e-01],
        [-1.8947e-01,  7.9232e-02,  2.1343e-02, -2.9419e-01,  4.7702e-02,
         -2.8152e-01, -3.4940e-01, -4.4225e-02],
        [ 1.3757e-01, -1.4379e-01, -4.5595e-01, -1.9878e-01, -2.8178e-01,
          2.1714e-01,  2.2799e-02,  3.3300e-01],
        [ 4.7477e-01, -4.7534e-01, -4.3817e-01, -2.5536e-01, -3.2329e-03,
         -1.6206e-01,  4.3321e-01,  2.8410e-01],
        [-4.2161e-01, -8.6324e-03, -3.2431e-01, -3.9876e-01, -1.2667e-01,
         -1.1684e-01,  2.4960e-01,  2.8027e-01],
        [-1.7121e-01, -4.5725e-01,  3.2917e-01,  1.1095e-01,  1.3811e-01,
          9.6645e-02, -4.3655e-01, -4.2003e-01],
        [ 4.6680e-01, -3.2080e-01,  4.5577e-01,  3.9400e-01,  1.6898e-02,
         -2.3727e-01, -2.0581e-01, -2.6656e-01],
        [-2.6652e-01, -1.5232e-01,  3.1556e-02, -8.7108e-03, -3.8514e-01,
         -6.0122e-02,  1.5924e-01, -3.7850e-01],
        [-4.3933e-01, -3.5672e-01,  3.5412e-01,  2.0119e-01, -3.9162e-01,
         -1.2559e-01, -1.9816e-01,  4.1927e-01]])
2025-03-18 16:59:35.754703 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([2281701379],"float32"),], )

[not compare]  None tensor([[ 0.2657, -0.0288, -0.4235, -0.1430, -0.4994, -0.4993,  0.0708,  0.0944],
        [ 0.4359,  0.1413,  0.3728, -0.4614, -0.3049,  0.1559, -0.4176,  0.3221],
        [-0.2245,  0.3896,  0.0341,  0.0930, -0.4123,  0.0392,  0.0889, -0.0021],
        [-0.1625,  0.3470,  0.3696, -0.3783,  0.4642,  0.2639,  0.3233, -0.2173],
        [-0.3721,  0.2116, -0.2388,  0.4133,  0.1711,  0.4396, -0.4147, -0.0313],
        [ 0.1651, -0.4288, -0.2024, -0.4104, -0.2723,  0.1608, -0.3439, -0.3961],
        [-0.1223, -0.1366, -0.4461, -0.2626,  0.0086,  0.2286, -0.0963, -0.0649],
        [-0.4404, -0.0165,  0.2570,  0.2090, -0.4808, -0.4108, -0.1738, -0.2833],
        [-0.3926,  0.3695,  0.2005,  0.1446, -0.0168,  0.3175, -0.1737, -0.0243],
        [ 0.0896, -0.0844, -0.1308,  0.1956, -0.1953, -0.3003, -0.2153, -0.3999],
        [ 0.1880,  0.4211, -0.4962,  0.2120, -0.4914, -0.0678,  0.1609,  0.0219],
        [ 0.1001,  0.0515,  0.3803,  0.4713,  0.3727,  0.1305, -0.1372, -0.2705],
        [ 0.1972, -0.4227, -0.2236,  0.3995, -0.1923, -0.1373,  0.4155, -0.1340],
        [ 0.4819, -0.2365, -0.1303,  0.1790, -0.4286,  0.0100,  0.2825,  0.0348],
        [-0.3235, -0.4553, -0.0646, -0.2300, -0.2286, -0.0056,  0.2526, -0.1354],
        [ 0.4090, -0.2003, -0.3155,  0.4139, -0.0518,  0.2145,  0.2531,  0.1831]])
2025-03-18 17:01:24.656285 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([2281701379],"float32"),Tensor([16],"float32"),], )

[not compare]  None tensor([[ 0.2657, -0.0288, -0.4235, -0.1430, -0.4994, -0.4993,  0.0708,  0.0944],
        [ 0.4359,  0.1413,  0.3728, -0.4614, -0.3049,  0.1559, -0.4176,  0.3221],
        [-0.2245,  0.3896,  0.0341,  0.0930, -0.4123,  0.0392,  0.0889, -0.0021],
        [-0.1625,  0.3470,  0.3696, -0.3783,  0.4642,  0.2639,  0.3233, -0.2173],
        [-0.3721,  0.2116, -0.2388,  0.4133,  0.1711,  0.4396, -0.4147, -0.0313],
        [ 0.1651, -0.4288, -0.2024, -0.4104, -0.2723,  0.1608, -0.3439, -0.3961],
        [-0.1223, -0.1366, -0.4461, -0.2626,  0.0086,  0.2286, -0.0963, -0.0649],
        [-0.4404, -0.0165,  0.2570,  0.2090, -0.4808, -0.4108, -0.1738, -0.2833],
        [-0.3926,  0.3695,  0.2005,  0.1446, -0.0168,  0.3175, -0.1737, -0.0243],
        [ 0.0896, -0.0844, -0.1308,  0.1956, -0.1953, -0.3003, -0.2153, -0.3999],
        [ 0.1880,  0.4211, -0.4962,  0.2120, -0.4914, -0.0678,  0.1609,  0.0219],
        [ 0.1001,  0.0515,  0.3803,  0.4713,  0.3727,  0.1305, -0.1372, -0.2705],
        [ 0.1972, -0.4227, -0.2236,  0.3995, -0.1923, -0.1373,  0.4155, -0.1340],
        [ 0.4819, -0.2365, -0.1303,  0.1790, -0.4286,  0.0100,  0.2825,  0.0348],
        [-0.3235, -0.4553, -0.0646, -0.2300, -0.2286, -0.0056,  0.2526, -0.1354],
        [ 0.4090, -0.2003, -0.3155,  0.4139, -0.0518,  0.2145,  0.2531,  0.1831]])
2025-03-18 17:02:57.262090 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([2281701379],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[not compare]  None tensor([[ 0.2657, -0.0288, -0.4235, -0.1430, -0.4994, -0.4993,  0.0708,  0.0944],
        [ 0.4359,  0.1413,  0.3728, -0.4614, -0.3049,  0.1559, -0.4176,  0.3221],
        [-0.2245,  0.3896,  0.0341,  0.0930, -0.4123,  0.0392,  0.0889, -0.0021],
        [-0.1625,  0.3470,  0.3696, -0.3783,  0.4642,  0.2639,  0.3233, -0.2173],
        [-0.3721,  0.2116, -0.2388,  0.4133,  0.1711,  0.4396, -0.4147, -0.0313],
        [ 0.1651, -0.4288, -0.2024, -0.4104, -0.2723,  0.1608, -0.3439, -0.3961],
        [-0.1223, -0.1366, -0.4461, -0.2626,  0.0086,  0.2286, -0.0963, -0.0649],
        [-0.4404, -0.0165,  0.2570,  0.2090, -0.4808, -0.4108, -0.1738, -0.2833],
        [-0.3926,  0.3695,  0.2005,  0.1446, -0.0168,  0.3175, -0.1737, -0.0243],
        [ 0.0896, -0.0844, -0.1308,  0.1956, -0.1953, -0.3003, -0.2153, -0.3999],
        [ 0.1880,  0.4211, -0.4962,  0.2120, -0.4914, -0.0678,  0.1609,  0.0219],
        [ 0.1001,  0.0515,  0.3803,  0.4713,  0.3727,  0.1305, -0.1372, -0.2705],
        [ 0.1972, -0.4227, -0.2236,  0.3995, -0.1923, -0.1373,  0.4155, -0.1340],
        [ 0.4819, -0.2365, -0.1303,  0.1790, -0.4286,  0.0100,  0.2825,  0.0348],
        [-0.3235, -0.4553, -0.0646, -0.2300, -0.2286, -0.0056,  0.2526, -0.1354],
        [ 0.4090, -0.2003, -0.3155,  0.4139, -0.0518,  0.2145,  0.2531,  0.1831]])
2025-03-18 17:04:29.398235 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([2281701379],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[not compare]  None tensor([[ 0.2657, -0.0288, -0.4235, -0.1430, -0.4994, -0.4993,  0.0708,  0.0944],
        [ 0.4359,  0.1413,  0.3728, -0.4614, -0.3049,  0.1559, -0.4176,  0.3221],
        [-0.2245,  0.3896,  0.0341,  0.0930, -0.4123,  0.0392,  0.0889, -0.0021],
        [-0.1625,  0.3470,  0.3696, -0.3783,  0.4642,  0.2639,  0.3233, -0.2173],
        [-0.3721,  0.2116, -0.2388,  0.4133,  0.1711,  0.4396, -0.4147, -0.0313],
        [ 0.1651, -0.4288, -0.2024, -0.4104, -0.2723,  0.1608, -0.3439, -0.3961],
        [-0.1223, -0.1366, -0.4461, -0.2626,  0.0086,  0.2286, -0.0963, -0.0649],
        [-0.4404, -0.0165,  0.2570,  0.2090, -0.4808, -0.4108, -0.1738, -0.2833],
        [-0.3926,  0.3695,  0.2005,  0.1446, -0.0168,  0.3175, -0.1737, -0.0243],
        [ 0.0896, -0.0844, -0.1308,  0.1956, -0.1953, -0.3003, -0.2153, -0.3999],
        [ 0.1880,  0.4211, -0.4962,  0.2120, -0.4914, -0.0678,  0.1609,  0.0219],
        [ 0.1001,  0.0515,  0.3803,  0.4713,  0.3727,  0.1305, -0.1372, -0.2705],
        [ 0.1972, -0.4227, -0.2236,  0.3995, -0.1923, -0.1373,  0.4155, -0.1340],
        [ 0.4819, -0.2365, -0.1303,  0.1790, -0.4286,  0.0100,  0.2825,  0.0348],
        [-0.3235, -0.4553, -0.0646, -0.2300, -0.2286, -0.0056,  0.2526, -0.1354],
        [ 0.4090, -0.2003, -0.3155,  0.4139, -0.0518,  0.2145,  0.2531,  0.1831]])
2025-03-18 17:06:01.521579 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([17825793, 2, 4, 4, 4],"float32"),Tensor([3],"float32"),], )

[not compare]  None tensor([[[[[ 3.5144e-01, -1.2571e-01, -1.5342e-01, -3.9465e-01],
           [ 1.9747e-01, -7.3926e-03,  1.7934e-01,  3.9395e-01],
           [ 4.7002e-01, -3.8056e-02, -3.0394e-01, -1.6783e-01],
           [ 5.8365e-02,  2.3940e-01,  1.3459e-01, -3.8453e-01]],

          [[ 4.5945e-01, -3.8248e-01,  2.0600e-02, -8.5770e-02],
           [-3.9327e-01,  7.4436e-02, -4.8018e-01,  2.0290e-01],
           [-7.5615e-02, -3.5952e-01, -3.3563e-01, -3.8077e-01],
           [-1.1261e-01, -6.4128e-02, -8.0100e-02,  4.2535e-01]],

          [[ 3.4731e-01, -2.8212e-01,  3.3554e-01,  3.1296e-01],
           [-2.2736e-01,  6.4030e-02, -3.4530e-02, -1.1729e-01],
           [ 3.9012e-02, -3.3549e-02,  4.0021e-01, -3.0403e-02],
           [ 1.6371e-01, -4.4657e-02, -1.1770e-01,  3.1106e-01]],

          [[-3.3684e-02,  3.8969e-01,  1.1902e-01,  3.9068e-01],
           [ 2.7433e-01, -4.5485e-01,  1.1469e-01,  1.9737e-02],
           [-3.0224e-01,  2.5338e-01, -2.1104e-01,  4.9572e-01],
           [-1.0401e-01,  1.7092e-02, -4.3936e-01,  3.1979e-01]]],


         [[[-1.3757e-01,  4.3092e-01,  4.3143e-01, -1.5961e-01],
           [ 1.3707e-01, -4.6952e-01,  4.2536e-01,  3.1769e-01],
           [-4.8401e-02,  7.7898e-02, -3.0457e-01,  4.5931e-01],
           [ 9.2681e-02,  1.1829e-01, -1.6293e-02, -3.4799e-01]],

          [[-3.1828e-01,  3.1749e-01, -2.4956e-01,  1.9479e-01],
           [-8.5299e-03,  9.9425e-02, -3.9064e-01,  1.4114e-01],
           [ 2.2681e-01,  2.8502e-01, -1.1303e-01,  1.5675e-01],
           [ 4.6583e-01,  2.8569e-01, -1.2045e-01, -3.2402e-01]],

          [[-2.3352e-01, -6.3720e-02, -2.7855e-01, -2.4608e-01],
           [-6.2878e-02,  3.3746e-01, -4.8186e-01, -4.1635e-02],
           [ 3.3076e-01,  4.0787e-01,  4.2444e-02,  1.8086e-01],
           [ 2.3930e-01,  1.3805e-01, -3.7408e-01,  4.2218e-01]],

          [[-4.5966e-01, -4.6613e-01, -4.2367e-01, -4.9590e-01],
           [ 3.5007e-01, -1.2753e-01, -2.2997e-01,  1.2884e-01],
           [ 2.4104e-01,  1.6759e-01, -1.4640e-01,  1.3786e-01],
           [ 8.0144e-02,  1.9655e-01, -6.3195e-02, -3.3585e-01]]]],



        [[[[ 1.6061e-01, -4.3819e-01, -4.1684e-01,  4.9294e-01],
           [ 8.5180e-02,  3.9437e-02, -1.0828e-01, -4.7164e-01],
           [ 2.5115e-02, -3.3898e-02,  3.2395e-01, -1.7367e-01],
           [-1.4246e-02,  3.5848e-01, -3.8309e-01,  1.8092e-01]],

          [[ 4.3723e-02,  9.6416e-02, -2.6700e-01,  4.5616e-01],
           [ 3.4744e-01,  4.9839e-01,  8.1663e-02,  4.1400e-02],
           [ 2.7579e-01,  1.2553e-01, -4.7814e-01,  1.0214e-01],
           [ 5.0357e-02,  1.6999e-02,  3.3284e-01,  7.0862e-02]],

          [[-9.5449e-02, -2.3550e-01, -4.0060e-01,  3.2601e-01],
           [-3.7589e-01,  4.9191e-01, -2.5112e-01,  2.4502e-01],
           [ 3.4502e-01,  7.0843e-02,  1.2678e-01, -4.9709e-01],
           [ 1.3311e-01,  1.3725e-01, -4.2097e-02, -1.3387e-01]],

          [[ 1.2105e-01,  4.4664e-01,  4.6134e-01, -4.1379e-01],
           [ 2.4025e-01, -2.9732e-01,  1.0366e-01,  1.6342e-01],
           [ 5.9875e-02, -2.7612e-01, -2.0925e-02, -4.9011e-01],
           [ 2.1154e-01,  4.5075e-01,  1.2705e-01,  1.6395e-01]]],


         [[[-5.4271e-02,  4.0733e-02,  3.6224e-01,  1.6406e-01],
           [-3.7303e-01,  3.0607e-01,  8.4399e-02, -2.2604e-01],
           [ 1.6649e-01, -1.4697e-01, -8.0751e-02, -1.1033e-01],
           [-7.7340e-03,  2.7927e-01,  4.0034e-01, -7.7384e-02]],

          [[ 1.5263e-01,  1.3169e-01,  3.0483e-01, -4.4336e-02],
           [-6.1224e-02,  4.3649e-02,  3.6100e-01,  2.4419e-01],
           [-2.8381e-01, -1.1008e-01, -8.9767e-02,  1.6089e-01],
           [-1.5920e-01,  4.5387e-01,  2.3405e-02,  3.5536e-01]],

          [[ 4.0123e-01,  2.5160e-01, -6.0872e-02,  1.4234e-01],
           [-2.8006e-01,  4.6641e-02, -4.4652e-01,  4.9765e-01],
           [ 1.4606e-01, -3.8710e-01, -1.1713e-01,  2.3867e-01],
           [-4.7361e-01, -2.4599e-01,  4.1470e-01, -4.0013e-01]],

          [[ 4.6562e-01, -2.9565e-01, -7.7943e-02,  2.1920e-01],
           [ 1.9508e-01, -1.9097e-01, -2.0760e-01,  4.3465e-01],
           [ 4.6712e-01,  2.1056e-01, -3.4866e-01, -1.6676e-01],
           [ 4.7712e-01, -2.2055e-01, -4.4851e-01, -4.6661e-01]]]],



        [[[[ 1.5383e-01,  3.9203e-01, -2.3741e-01, -2.3528e-01],
           [ 1.4292e-01,  1.9434e-01, -1.6190e-01,  3.4162e-01],
           [-2.9191e-01,  2.6969e-01,  2.4015e-02, -4.0111e-03],
           [ 1.3206e-01, -2.0555e-01,  2.6088e-01, -1.7798e-01]],

          [[-2.9524e-01,  1.6399e-01,  2.1017e-01, -4.7747e-02],
           [-3.7395e-01, -1.1334e-01, -8.6264e-02, -3.5735e-01],
           [-4.0236e-01, -8.5178e-02, -8.8944e-02, -3.1975e-01],
           [ 2.8325e-02,  3.6913e-02, -1.8236e-02,  4.2590e-02]],

          [[-3.3927e-02,  4.3456e-01, -3.1109e-01,  4.9117e-01],
           [-3.8707e-01, -4.6294e-01,  1.3732e-01,  4.6219e-01],
           [-8.9071e-02, -4.5157e-01,  1.2918e-01, -1.7599e-01],
           [-4.0772e-01,  7.8367e-04, -1.6254e-01, -1.7828e-01]],

          [[-3.4674e-01,  2.7406e-01, -1.7179e-01, -1.6532e-01],
           [-1.4539e-01,  4.2215e-01,  4.1917e-02,  3.3972e-01],
           [ 5.8193e-02,  4.8611e-01,  2.6586e-01,  4.4897e-01],
           [ 3.6564e-01, -4.4588e-01,  2.1463e-01,  3.0652e-01]]],


         [[[-4.8142e-01,  2.5881e-01, -1.8139e-01, -4.4266e-01],
           [ 2.7684e-01, -4.1965e-01,  1.2146e-01,  1.7555e-01],
           [ 1.3867e-01, -4.1840e-01,  3.2384e-01, -2.3856e-01],
           [ 2.2296e-01,  3.7622e-01,  1.3217e-01,  1.9559e-01]],

          [[-1.7183e-01,  1.3701e-01,  4.4185e-01,  9.9544e-02],
           [-4.0814e-01,  8.7473e-02,  2.3315e-01, -8.0853e-02],
           [ 2.7772e-04,  4.8122e-03,  4.6561e-01, -8.9487e-02],
           [-4.2634e-01, -3.0390e-01,  2.3855e-01,  6.3584e-02]],

          [[ 1.8141e-01,  7.7182e-02,  3.9147e-01,  9.4696e-02],
           [ 1.4736e-01, -1.2468e-01, -8.2179e-02,  3.7744e-01],
           [ 2.4648e-01, -4.4286e-01, -4.7567e-02, -3.1483e-01],
           [ 3.6877e-01,  3.9732e-01, -3.3362e-01,  1.9141e-01]],

          [[ 4.8714e-01,  2.8521e-01, -2.5271e-01,  5.6697e-02],
           [ 8.7483e-02,  3.1908e-01, -4.5309e-01, -2.8543e-01],
           [ 3.1498e-01,  2.1343e-01,  1.0796e-01,  4.8419e-01],
           [-1.7137e-01,  2.7826e-01, -1.8145e-01,  4.0404e-01]]]],



        ...,



        [[[[ 2.8086e-01, -2.7588e-01, -3.0859e-01,  4.8108e-01],
           [ 2.0079e-01, -4.5966e-01, -3.1102e-01,  1.3809e-01],
           [ 3.5932e-01, -1.5407e-01,  3.1607e-01, -3.7668e-01],
           [-2.2322e-01, -1.4743e-01,  3.1628e-01, -4.6319e-01]],

          [[-3.2106e-01, -1.7700e-01,  4.4214e-02,  3.6071e-01],
           [-3.3472e-01,  2.6313e-01,  3.5746e-01, -4.9333e-01],
           [-2.7170e-01, -5.2128e-02, -2.1835e-01,  4.8694e-01],
           [ 4.5295e-01,  4.3516e-01, -4.7690e-01, -5.5743e-02]],

          [[-4.9922e-01,  4.6064e-01,  1.1624e-04,  9.1603e-02],
           [-2.7557e-01,  5.9064e-02, -4.2038e-01, -9.4105e-02],
           [ 3.5170e-01,  2.9541e-01, -2.6031e-01, -2.4968e-01],
           [-1.0236e-01,  2.8061e-01,  1.1864e-01, -8.7367e-02]],

          [[ 1.0765e-01, -1.2762e-01,  2.5976e-01,  1.8249e-01],
           [ 9.8614e-02, -4.8949e-01,  3.8858e-01,  3.3862e-01],
           [-4.5602e-01,  1.6575e-01,  2.0414e-02,  4.2920e-01],
           [ 2.1585e-01, -4.8073e-03,  4.5981e-02,  3.2296e-01]]],


         [[[ 3.3840e-01,  8.0536e-02, -2.7410e-01,  3.7279e-01],
           [ 4.7672e-01, -1.2494e-01,  2.0320e-02,  3.0568e-03],
           [-5.5913e-02,  4.5669e-01, -2.2775e-01, -3.4452e-01],
           [-2.2549e-01,  1.6992e-01,  8.5883e-02,  3.4356e-01]],

          [[-2.0298e-01,  1.9804e-01, -1.5484e-01,  4.9925e-02],
           [-2.3765e-01, -1.8012e-01,  1.2589e-01, -4.8846e-02],
           [ 2.0567e-01, -1.6863e-01, -1.9948e-01, -4.0428e-01],
           [-8.3040e-02, -2.4482e-01, -4.9591e-01, -1.9400e-01]],

          [[-3.9186e-01, -4.8752e-01, -2.7329e-01,  1.6071e-02],
           [-1.8922e-02, -1.3809e-01,  4.6153e-01,  2.9270e-01],
           [-4.6335e-01, -1.6070e-01,  3.6513e-01,  3.8460e-01],
           [-3.2778e-01,  4.7692e-01,  3.0745e-01, -2.6745e-02]],

          [[ 3.3330e-01,  1.2631e-02, -2.2241e-01,  4.8426e-01],
           [ 2.3052e-01, -3.3921e-01, -3.5078e-01, -3.7525e-01],
           [ 6.9480e-02, -4.9810e-01,  4.9527e-01, -3.7115e-01],
           [-1.0791e-01,  2.4829e-01, -3.3094e-01, -2.2688e-01]]]],



        [[[[-2.4061e-01,  4.5435e-01, -3.7394e-01,  1.9085e-01],
           [-8.6658e-02, -1.6430e-01,  2.1348e-02,  3.5604e-01],
           [-1.3852e-01,  1.9997e-01,  7.6294e-02,  4.4412e-01],
           [-3.7771e-01, -4.4375e-01,  2.8993e-01,  7.7275e-02]],

          [[-3.3832e-01, -2.6006e-01, -4.0766e-01, -3.9057e-01],
           [-3.2042e-01,  2.2326e-01,  1.1509e-01,  1.1788e-01],
           [ 1.4212e-01, -3.3509e-01,  8.9830e-02, -7.8664e-03],
           [ 3.2394e-01,  2.3442e-01,  9.0924e-02,  3.8985e-01]],

          [[-1.5192e-01,  9.8865e-02, -2.0407e-01, -1.6411e-01],
           [-4.4458e-01, -4.4471e-01, -3.0670e-01, -1.2718e-01],
           [-3.0323e-01, -3.3472e-01, -2.3230e-01,  7.4590e-02],
           [-4.3256e-02, -5.0262e-02,  4.9378e-02, -3.1306e-01]],

          [[-3.5188e-01,  3.4500e-01, -3.2952e-01, -9.6692e-02],
           [-5.1698e-02,  3.9409e-01, -3.5993e-01, -3.0199e-02],
           [ 3.7695e-01, -2.7462e-01,  9.8409e-02, -3.9800e-01],
           [-3.5754e-01,  6.5079e-02,  1.2543e-01, -2.3244e-01]]],


         [[[ 1.9857e-01, -9.5937e-02,  3.3412e-01, -1.2772e-01],
           [ 8.6486e-02,  4.6402e-01, -2.3256e-02,  2.1303e-02],
           [ 4.3504e-01, -1.1970e-01,  4.8578e-02, -6.5762e-03],
           [-8.7822e-02,  4.6831e-01, -9.5298e-02, -2.1194e-01]],

          [[-1.0597e-01, -1.5088e-01, -1.1912e-01,  3.6522e-01],
           [-4.0145e-01, -6.7555e-02, -6.9014e-02, -3.1679e-01],
           [ 2.7278e-02,  4.8325e-01, -4.4871e-01,  4.8587e-01],
           [ 3.9116e-01, -3.2562e-01,  1.7314e-01,  1.9583e-01]],

          [[-2.2782e-01,  4.1914e-02, -3.3366e-01, -3.3355e-01],
           [-3.2039e-01, -9.2077e-02, -4.8696e-01, -7.0993e-03],
           [-2.8493e-01,  2.2438e-02, -3.7493e-01,  4.9625e-01],
           [ 4.7445e-01, -5.1710e-02, -4.0384e-01, -2.5566e-01]],

          [[ 2.3566e-01, -3.4773e-01,  1.9294e-01,  3.1481e-02],
           [-2.1355e-02, -2.3888e-02,  2.2486e-01,  2.3863e-01],
           [ 1.3576e-01,  1.9869e-02, -2.8743e-01,  3.6928e-01],
           [ 3.6208e-01, -4.3088e-02, -8.4406e-02, -1.9930e-01]]]],



        [[[[ 2.1795e-01, -5.5542e-03,  1.9383e-01, -2.2813e-01],
           [ 2.0530e-01,  1.7700e-01, -4.2001e-01,  1.8859e-01],
           [ 1.5293e-01, -4.9775e-01,  1.0345e-03, -2.7647e-01],
           [-4.8442e-01,  1.9470e-01, -4.1397e-01, -4.7029e-01]],

          [[-2.5898e-01,  5.7166e-02, -2.3349e-01, -2.8119e-01],
           [-4.1507e-01,  2.7310e-01, -4.6539e-01,  4.3261e-01],
           [-3.6406e-01,  3.7303e-01,  3.4351e-01,  7.4748e-03],
           [ 2.1111e-01, -4.6966e-01, -4.1660e-01,  2.4501e-01]],

          [[ 3.3308e-01,  2.0528e-01,  3.3004e-01, -3.2347e-01],
           [-2.4480e-01, -4.6874e-02,  8.0368e-02, -3.1465e-02],
           [-1.5781e-01, -1.1402e-01,  3.7851e-01, -1.0087e-01],
           [ 4.9838e-01,  1.1901e-01,  2.5920e-01, -3.0172e-01]],

          [[-3.4181e-01,  2.3603e-01,  4.3538e-01, -2.9547e-01],
           [-5.0516e-02, -1.7126e-01, -2.4809e-01, -1.5076e-01],
           [-2.4838e-01, -4.7253e-01,  5.5365e-02,  2.8138e-01],
           [ 1.9273e-01, -5.7875e-02, -1.0314e-01, -2.1692e-01]]],


         [[[ 1.6946e-01,  2.0824e-01,  2.7541e-02,  4.0874e-01],
           [-6.5547e-02, -2.5866e-01, -3.2855e-04,  9.0284e-02],
           [-3.4100e-01,  3.6056e-01, -1.1090e-01,  1.6948e-01],
           [-1.8190e-01, -1.0407e-01, -4.4904e-02,  6.2364e-02]],

          [[ 4.5383e-02, -4.0807e-01,  4.4441e-01, -2.0194e-01],
           [ 2.2768e-01,  1.4779e-01,  4.9682e-01,  2.5804e-02],
           [-1.3775e-01, -2.2515e-01,  3.9518e-01,  1.0166e-01],
           [ 2.8747e-01, -2.0259e-01, -5.2554e-02, -3.0652e-01]],

          [[-1.9806e-01, -4.4153e-01, -3.6861e-01, -4.8663e-01],
           [-4.9562e-02, -5.5540e-02, -2.2558e-01,  4.1513e-01],
           [ 3.5180e-01, -3.5563e-01, -1.9798e-01,  8.4594e-02],
           [-1.8029e-01, -4.2807e-01, -4.2619e-01,  2.9464e-01]],

          [[-1.6814e-01,  2.2974e-01, -4.3483e-01, -4.7804e-02],
           [ 3.2146e-01, -3.6948e-04, -3.2227e-01, -2.8674e-01],
           [-2.8261e-01,  1.4893e-01,  4.1823e-01, -3.7154e-01],
           [ 5.9622e-02, -4.3501e-01, -6.1434e-02, -2.2014e-01]]]]])
2025-03-18 17:07:42.420759 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([2, 1140850690],"float32"),Tensor([2],"float32"),], )

[not compare]  None tensor([[ 0.1966, -0.0632, -0.3358,  ..., -0.0844, -0.4728, -0.1584],
        [ 0.1717, -0.2820, -0.4466,  ..., -0.0614, -0.2201, -0.3204]])
2025-03-18 17:09:32.988149 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([2, 2],"float32"),Tensor([2281701379],"float32"),], )

[not compare]  None tensor([[ 0.0801,  0.1966],
        [-0.0632, -0.3358]])
2025-03-18 17:11:13.803539 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([285212673, 2, 4],"float32"),Tensor([3],"float32"),], )

[not compare]  None tensor([[[ 2.4104e-01,  1.6759e-01, -1.4640e-01,  1.3786e-01],
         [ 8.0144e-02,  1.9655e-01, -6.3195e-02, -3.3585e-01]],

        [[ 1.6061e-01, -4.3819e-01, -4.1684e-01,  4.9294e-01],
         [ 8.5180e-02,  3.9437e-02, -1.0828e-01, -4.7164e-01]],

        [[ 2.5115e-02, -3.3898e-02,  3.2395e-01, -1.7367e-01],
         [-1.4246e-02,  3.5848e-01, -3.8309e-01,  1.8092e-01]],

        ...,

        [[ 3.5180e-01, -3.5563e-01, -1.9798e-01,  8.4594e-02],
         [-1.8029e-01, -4.2807e-01, -4.2619e-01,  2.9464e-01]],

        [[-1.6814e-01,  2.2974e-01, -4.3483e-01, -4.7804e-02],
         [ 3.2146e-01, -3.6948e-04, -3.2227e-01, -2.8674e-01]],

        [[-2.8261e-01,  1.4893e-01,  4.1823e-01, -3.7154e-01],
         [ 5.9622e-02, -4.3501e-01, -6.1434e-02, -2.2014e-01]]])
2025-03-18 17:12:54.917756 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([285212673, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[not compare]  None tensor([[ 0.1790, -0.4286,  0.0100,  ..., -0.3235, -0.4553, -0.0646],
        [-0.2300, -0.2286, -0.0056,  ...,  0.4090, -0.2003, -0.3155],
        [ 0.4139, -0.0518,  0.2145,  ..., -0.4746, -0.4986,  0.2901],
        ...,
        [-0.3925,  0.3978, -0.3244,  ...,  0.3188, -0.2097,  0.2837],
        [-0.0843,  0.4070,  0.4913,  ..., -0.2056, -0.1355,  0.3952],
        [-0.4696, -0.3655,  0.3425,  ...,  0.4449, -0.3111,  0.0638]])
2025-03-18 17:14:32.325497 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([285212673, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[not compare]  None tensor([[-0.1004, -0.2077,  0.4216,  ..., -0.0467, -0.1527, -0.4243],
        [ 0.0620, -0.1061,  0.2113,  ..., -0.1655,  0.3286,  0.0771],
        [ 0.4784, -0.0048, -0.0387,  ..., -0.4257,  0.3465,  0.3090],
        ...,
        [-0.2671,  0.1996, -0.3666,  ..., -0.3112,  0.3154, -0.1567],
        [ 0.0158, -0.0553,  0.3487,  ...,  0.0056, -0.3326,  0.4372],
        [ 0.0195, -0.2819, -0.1620,  ..., -0.4287, -0.4389, -0.0568]])
2025-03-18 17:16:13.199674 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([285212673, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[not compare]  None tensor([[-0.3192, -0.3879,  0.1156,  ...,  0.3868,  0.4448, -0.2183],
        [-0.4870, -0.4654,  0.2346,  ..., -0.4909, -0.3741,  0.2582],
        [-0.2665,  0.3944,  0.4137,  ...,  0.0700,  0.1928, -0.1743],
        ...,
        [-0.0561,  0.0048, -0.1104,  ..., -0.3973, -0.3658,  0.3162],
        [-0.0634, -0.4561, -0.3978,  ..., -0.2908, -0.2403, -0.2158],
        [-0.4294, -0.3341, -0.2588,  ..., -0.4944,  0.4895,  0.3995]])
2025-03-18 17:17:52.359145 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([3, 11883862, 4, 4, 4],"float32"),Tensor([3],"float32"),], )

[not compare]  None tensor([[[[[ 3.5144e-01, -1.2571e-01, -1.5342e-01, -3.9465e-01],
           [ 1.9747e-01, -7.3926e-03,  1.7934e-01,  3.9395e-01],
           [ 4.7002e-01, -3.8056e-02, -3.0394e-01, -1.6783e-01],
           [ 5.8365e-02,  2.3940e-01,  1.3459e-01, -3.8453e-01]],

          [[ 4.5945e-01, -3.8248e-01,  2.0600e-02, -8.5770e-02],
           [-3.9327e-01,  7.4436e-02, -4.8018e-01,  2.0290e-01],
           [-7.5615e-02, -3.5952e-01, -3.3563e-01, -3.8077e-01],
           [-1.1261e-01, -6.4128e-02, -8.0100e-02,  4.2535e-01]],

          [[ 3.4731e-01, -2.8212e-01,  3.3554e-01,  3.1296e-01],
           [-2.2736e-01,  6.4030e-02, -3.4530e-02, -1.1729e-01],
           [ 3.9012e-02, -3.3549e-02,  4.0021e-01, -3.0403e-02],
           [ 1.6371e-01, -4.4657e-02, -1.1770e-01,  3.1106e-01]],

          [[-3.3684e-02,  3.8969e-01,  1.1902e-01,  3.9068e-01],
           [ 2.7433e-01, -4.5485e-01,  1.1469e-01,  1.9737e-02],
           [-3.0224e-01,  2.5338e-01, -2.1104e-01,  4.9572e-01],
           [-1.0401e-01,  1.7092e-02, -4.3936e-01,  3.1979e-01]]],


         [[[-1.3757e-01,  4.3092e-01,  4.3143e-01, -1.5961e-01],
           [ 1.3707e-01, -4.6952e-01,  4.2536e-01,  3.1769e-01],
           [-4.8401e-02,  7.7898e-02, -3.0457e-01,  4.5931e-01],
           [ 9.2681e-02,  1.1829e-01, -1.6293e-02, -3.4799e-01]],

          [[-3.1828e-01,  3.1749e-01, -2.4956e-01,  1.9479e-01],
           [-8.5299e-03,  9.9425e-02, -3.9064e-01,  1.4114e-01],
           [ 2.2681e-01,  2.8502e-01, -1.1303e-01,  1.5675e-01],
           [ 4.6583e-01,  2.8569e-01, -1.2045e-01, -3.2402e-01]],

          [[-2.3352e-01, -6.3720e-02, -2.7855e-01, -2.4608e-01],
           [-6.2878e-02,  3.3746e-01, -4.8186e-01, -4.1635e-02],
           [ 3.3076e-01,  4.0787e-01,  4.2444e-02,  1.8086e-01],
           [ 2.3930e-01,  1.3805e-01, -3.7408e-01,  4.2218e-01]],

          [[-4.5966e-01, -4.6613e-01, -4.2367e-01, -4.9590e-01],
           [ 3.5007e-01, -1.2753e-01, -2.2997e-01,  1.2884e-01],
           [ 2.4104e-01,  1.6759e-01, -1.4640e-01,  1.3786e-01],
           [ 8.0144e-02,  1.9655e-01, -6.3195e-02, -3.3585e-01]]],


         [[[ 1.6061e-01, -4.3819e-01, -4.1684e-01,  4.9294e-01],
           [ 8.5180e-02,  3.9437e-02, -1.0828e-01, -4.7164e-01],
           [ 2.5115e-02, -3.3898e-02,  3.2395e-01, -1.7367e-01],
           [-1.4246e-02,  3.5848e-01, -3.8309e-01,  1.8092e-01]],

          [[ 4.3723e-02,  9.6416e-02, -2.6700e-01,  4.5616e-01],
           [ 3.4744e-01,  4.9839e-01,  8.1663e-02,  4.1400e-02],
           [ 2.7579e-01,  1.2553e-01, -4.7814e-01,  1.0214e-01],
           [ 5.0357e-02,  1.6999e-02,  3.3284e-01,  7.0862e-02]],

          [[-9.5449e-02, -2.3550e-01, -4.0060e-01,  3.2601e-01],
           [-3.7589e-01,  4.9191e-01, -2.5112e-01,  2.4502e-01],
           [ 3.4502e-01,  7.0843e-02,  1.2678e-01, -4.9709e-01],
           [ 1.3311e-01,  1.3725e-01, -4.2097e-02, -1.3387e-01]],

          [[ 1.2105e-01,  4.4664e-01,  4.6134e-01, -4.1379e-01],
           [ 2.4025e-01, -2.9732e-01,  1.0366e-01,  1.6342e-01],
           [ 5.9875e-02, -2.7612e-01, -2.0925e-02, -4.9011e-01],
           [ 2.1154e-01,  4.5075e-01,  1.2705e-01,  1.6395e-01]]],


         ...,


         [[[ 2.7930e-01,  4.7610e-01, -4.6562e-01,  2.9315e-01],
           [-4.4453e-01,  3.3401e-01, -4.3846e-01, -7.0649e-02],
           [-9.1617e-02, -2.5938e-01,  8.9362e-02, -1.4665e-01],
           [-6.4674e-02,  3.5626e-01, -1.9200e-01,  3.5072e-01]],

          [[-1.4461e-01, -4.3731e-01, -1.7561e-01, -2.7768e-01],
           [ 1.6600e-01, -2.7638e-01,  4.9637e-01,  3.0381e-02],
           [-3.4688e-01, -3.0656e-01, -1.0986e-01, -3.2515e-01],
           [ 2.4709e-01,  5.2347e-02,  1.8868e-01,  4.9086e-01]],

          [[ 2.2985e-01,  3.6911e-01, -1.8982e-01,  2.1368e-01],
           [ 4.0956e-01,  4.7742e-02,  4.1667e-01, -3.1817e-01],
           [-9.0251e-02,  4.9788e-01,  1.2696e-01, -2.8007e-01],
           [-1.6603e-01, -3.5782e-01, -4.5540e-01,  7.1310e-03]],

          [[ 4.7626e-01, -1.4551e-01,  1.3021e-01, -3.8854e-01],
           [ 3.5686e-01, -2.2723e-01,  8.2154e-02,  1.9319e-01],
           [-4.1749e-01, -3.8761e-01,  8.5916e-02, -3.6300e-02],
           [ 1.6162e-01, -1.8110e-01, -2.4225e-03, -4.1171e-02]]],


         [[[-1.6227e-01, -2.1171e-01, -1.0694e-01, -1.4849e-01],
           [-4.5761e-01, -4.6743e-01, -5.4064e-03,  2.6487e-02],
           [ 2.2667e-01,  5.7132e-02,  2.2563e-01,  1.3656e-01],
           [-1.2983e-02,  4.0945e-01,  1.7918e-01, -1.7349e-01]],

          [[-8.4332e-02,  2.6927e-01,  4.0768e-01,  7.8170e-02],
           [-4.1891e-01, -4.1701e-01,  2.7788e-02,  2.5712e-01],
           [ 3.5274e-01,  1.5971e-02,  2.7093e-01,  4.6502e-01],
           [-1.8930e-01, -2.8016e-01,  2.1774e-02, -1.7948e-01]],

          [[ 1.4591e-01, -2.0716e-01,  9.5091e-02,  4.7349e-02],
           [-3.1988e-01, -2.6703e-01,  2.4583e-01,  2.9140e-02],
           [-2.9419e-01, -2.3560e-01,  1.1279e-01, -1.3443e-01],
           [-4.5924e-01, -4.8840e-01, -4.9615e-01, -9.2352e-02]],

          [[-1.4658e-01,  3.5579e-01,  4.2239e-02, -4.2116e-01],
           [-2.4557e-01, -3.3473e-01, -9.0837e-02, -3.1741e-01],
           [-4.1647e-01, -2.2542e-01,  3.9151e-01, -1.4769e-01],
           [-2.0054e-01,  5.0370e-02, -3.3633e-01,  3.3023e-01]]],


         [[[ 3.6552e-01, -9.4539e-02,  4.2648e-01,  1.2335e-02],
           [ 1.7010e-02,  3.6129e-02,  1.8066e-02, -4.4969e-01],
           [-2.2458e-01, -1.0862e-01,  3.5347e-01,  2.5798e-01],
           [-2.0658e-01,  1.1844e-02,  4.2370e-01,  3.5786e-01]],

          [[ 1.4231e-01,  2.3544e-01,  4.4793e-01,  3.2080e-01],
           [-4.7200e-01, -9.5391e-02, -4.6050e-02,  7.8916e-02],
           [-1.3964e-01, -3.5328e-02, -4.9251e-01, -4.9325e-01],
           [ 6.1998e-02, -3.2632e-01, -1.4668e-01, -3.6472e-01]],

          [[ 1.5573e-01,  2.3322e-01, -2.6116e-01, -3.8545e-01],
           [-4.2075e-01,  4.6169e-01, -2.8832e-01,  2.7113e-01],
           [-2.4882e-01, -1.4325e-01,  1.7037e-01,  8.4794e-02],
           [-4.6842e-01,  3.9896e-01, -3.1540e-01, -4.1675e-01]],

          [[-2.2072e-01, -3.9232e-01,  5.0288e-02,  2.6218e-01],
           [ 4.8488e-01,  4.4707e-01,  1.1613e-01,  2.2171e-01],
           [-2.8704e-01,  2.5125e-01, -3.0371e-01,  4.6934e-01],
           [ 3.4986e-01,  4.3437e-01,  3.0120e-01,  1.3604e-01]]]],



        [[[[-1.5066e-01, -5.1299e-02, -3.5045e-02,  1.1066e-01],
           [ 4.7750e-01,  2.4254e-01, -3.3810e-01, -2.2047e-02],
           [ 3.8078e-01,  1.1651e-01,  3.2102e-01, -1.0523e-01],
           [ 1.1880e-01, -2.9428e-01, -1.0485e-02,  5.7354e-03]],

          [[-5.8249e-02, -3.9252e-01, -5.7541e-02, -2.2755e-01],
           [-1.0274e-01,  2.2919e-02, -3.2786e-01, -1.4478e-01],
           [-4.5627e-01, -3.7104e-01, -4.9889e-01,  2.5705e-03],
           [ 3.8165e-01, -4.2521e-01,  7.1672e-02, -1.3438e-01]],

          [[-7.8715e-02,  4.5516e-01,  2.8498e-01, -3.0749e-01],
           [-4.6217e-01, -4.6866e-01, -8.2155e-02, -6.3671e-02],
           [ 4.6678e-01,  4.6853e-01, -3.0515e-01,  3.8544e-01],
           [-2.1352e-01,  2.3147e-01,  3.0656e-01, -3.7105e-01]],

          [[-2.5187e-02,  2.2679e-01,  1.7154e-01,  1.3388e-01],
           [-3.3866e-01,  3.1385e-01,  9.4947e-02, -9.0630e-02],
           [-4.7973e-01, -2.2178e-01, -1.6818e-02, -1.2409e-01],
           [-3.2081e-01, -1.5254e-01,  2.7752e-01,  3.8550e-01]]],


         [[[ 1.9694e-01, -4.1135e-01,  1.6358e-01, -3.5123e-01],
           [-6.7197e-04,  3.1875e-01,  4.9769e-01, -4.8492e-01],
           [ 3.5163e-01,  4.0087e-01,  3.4764e-01,  4.9687e-01],
           [-1.1039e-01, -3.8959e-01,  2.4091e-01,  3.4829e-01]],

          [[-2.9171e-01, -8.6592e-02, -3.7584e-01, -1.5459e-01],
           [-3.7782e-02,  2.4303e-01,  2.3525e-02, -4.9869e-01],
           [-2.0824e-01, -4.4638e-01, -9.1700e-02,  4.6943e-01],
           [ 4.3443e-01,  8.7745e-02,  3.3991e-01, -3.4918e-01]],

          [[ 3.1012e-01,  3.5128e-01, -3.3426e-01,  9.4020e-02],
           [-4.6863e-01,  2.1891e-01,  4.7435e-01,  1.2338e-04],
           [ 1.1010e-01,  4.7055e-01,  4.4302e-01,  3.2367e-01],
           [-3.5106e-01,  1.1032e-01,  2.2755e-01, -4.9951e-01]],

          [[ 2.1382e-01, -4.8177e-01, -3.9956e-01,  3.8951e-01],
           [-3.2261e-01, -4.0619e-01, -2.9779e-01, -8.3063e-02],
           [ 3.9822e-01,  5.4428e-02, -1.0091e-01,  2.3169e-01],
           [ 4.7407e-01,  6.0467e-02,  1.0961e-02, -3.0191e-01]]],


         [[[ 1.3997e-01,  4.1871e-01,  4.7899e-01, -1.7793e-01],
           [ 1.8823e-02, -1.5512e-01, -4.4899e-01, -2.2899e-01],
           [ 3.0582e-01, -7.5833e-02, -2.8038e-01,  4.2966e-01],
           [-2.0449e-03,  2.9316e-01,  4.7054e-01, -1.8411e-02]],

          [[-2.2010e-01,  4.2419e-01, -2.8844e-01,  2.1177e-01],
           [-1.5343e-01,  4.7119e-03, -3.9576e-01,  3.3129e-01],
           [ 4.4572e-01,  4.7807e-01, -1.4542e-01, -2.1110e-01],
           [-3.3656e-01, -2.6729e-01,  1.0956e-02,  4.2709e-01]],

          [[ 1.6071e-01,  3.4681e-01, -4.3221e-01,  2.0349e-01],
           [-3.6430e-01, -3.0388e-01,  2.4990e-01,  9.4021e-02],
           [-3.5598e-01, -2.2000e-02, -2.4897e-01, -2.2418e-01],
           [-4.2920e-02, -2.0332e-03,  4.8144e-01, -3.7718e-01]],

          [[-1.3841e-01,  2.4403e-02, -3.9273e-01, -4.8275e-01],
           [-1.0572e-01, -4.0391e-01,  3.6615e-01, -2.1984e-01],
           [ 2.8508e-01,  3.3074e-01,  4.9172e-01,  3.5883e-01],
           [-4.3572e-01,  3.9224e-01, -2.3603e-01, -3.0420e-01]]],


         ...,


         [[[-1.9845e-01, -5.5546e-02,  5.4854e-02,  4.1468e-01],
           [-3.6928e-01, -1.9120e-01,  3.4733e-02, -2.4360e-01],
           [-5.4089e-02, -4.7699e-01,  1.5196e-01,  2.6117e-01],
           [-2.7536e-01,  2.4692e-01,  1.4464e-01,  1.1477e-02]],

          [[ 2.2225e-01,  1.2069e-01, -2.8798e-01,  2.0196e-01],
           [-1.2402e-01,  1.4528e-01,  4.0175e-01, -7.9874e-02],
           [-1.4688e-01, -2.3004e-01, -3.2740e-01,  4.5027e-01],
           [ 2.1323e-01,  2.1571e-01, -3.5640e-01,  8.3771e-02]],

          [[-4.6605e-01, -4.2535e-01, -2.4088e-01, -3.8634e-01],
           [-4.6947e-01,  2.8139e-01,  4.1194e-01, -3.0772e-01],
           [-1.5835e-01, -2.2807e-01,  3.9311e-01, -1.0885e-02],
           [ 2.9748e-01, -3.5815e-01, -3.2636e-01,  2.3916e-01]],

          [[ 3.4086e-01, -4.1405e-01,  3.3418e-01,  2.0386e-02],
           [ 2.8184e-01, -3.8261e-01, -3.5976e-01, -1.3481e-01],
           [ 4.1339e-01, -3.8853e-01,  4.6282e-01, -1.0291e-01],
           [ 4.4951e-01, -3.7958e-01,  1.6531e-01,  2.9996e-02]]],


         [[[-3.4647e-01, -4.2480e-01, -4.2822e-01, -1.7661e-01],
           [-2.1656e-01,  1.6715e-01, -2.3543e-01,  2.7385e-01],
           [-4.2461e-02, -7.2269e-03,  4.8826e-01,  1.4102e-01],
           [ 3.6875e-01, -5.7301e-02,  2.3862e-01, -1.4458e-01]],

          [[ 2.0352e-01,  1.2681e-01,  2.4113e-01,  3.7598e-01],
           [ 8.5044e-02, -4.3194e-01,  3.8919e-01,  3.8780e-01],
           [-3.4688e-01, -2.9408e-01, -2.0379e-01, -9.6456e-02],
           [-1.9468e-01,  1.8362e-01,  1.5631e-01,  2.0796e-03]],

          [[-2.5647e-01, -2.6871e-01,  5.2322e-02, -4.9895e-01],
           [-4.9611e-01, -4.5972e-01, -4.7127e-01,  2.9976e-01],
           [-2.1775e-01, -2.5133e-01,  2.9191e-01,  2.3898e-01],
           [-3.8951e-01,  2.9867e-01,  4.2655e-01, -9.6939e-02]],

          [[-9.9978e-02,  2.6513e-01, -4.5834e-01,  3.0635e-01],
           [-2.2155e-01, -2.2308e-01,  1.4811e-01,  4.6071e-01],
           [-1.8149e-01, -4.1261e-01,  2.1125e-01, -1.3522e-01],
           [-4.7712e-01, -2.8359e-02,  1.2203e-01,  2.3472e-01]]],


         [[[-1.6839e-01,  2.0728e-01,  3.1655e-01,  4.7667e-01],
           [ 3.2446e-01, -8.6889e-02,  4.0705e-01, -4.1433e-01],
           [-2.6415e-02,  4.3521e-01, -2.2591e-01,  1.3448e-01],
           [ 1.5870e-01, -2.5469e-01, -2.5411e-01,  2.2043e-02]],

          [[-2.3783e-01,  4.6297e-01, -3.9179e-01, -2.9943e-01],
           [ 7.2092e-02, -3.8342e-01,  2.8372e-02,  3.8814e-01],
           [ 3.9793e-01, -2.6900e-02,  2.8772e-01, -3.2515e-01],
           [-4.4190e-01,  3.5540e-01, -2.5746e-01,  3.4934e-01]],

          [[-1.0276e-01,  4.0814e-01,  1.7602e-01,  3.4203e-01],
           [-3.7585e-01, -3.3249e-01, -4.2466e-01,  4.4133e-01],
           [ 5.8685e-02, -8.8108e-02, -3.9516e-01,  3.8296e-01],
           [ 3.1956e-01, -1.6429e-01, -3.0813e-01,  1.0821e-02]],

          [[ 1.0819e-01, -1.3469e-01, -4.7016e-01,  4.8897e-01],
           [ 1.9809e-01, -3.1948e-01, -2.6128e-01,  3.9779e-01],
           [-4.5074e-01,  3.3029e-02,  1.6245e-01, -3.3097e-01],
           [ 3.6720e-01, -3.8066e-01,  6.0682e-02, -2.0752e-01]]]],



        [[[[ 4.3178e-01,  4.8681e-01, -1.1679e-01,  8.1086e-02],
           [ 3.7312e-01,  3.6753e-01,  2.2301e-01, -1.5109e-01],
           [-8.8730e-02, -3.2674e-01, -1.6343e-01,  3.1051e-01],
           [-7.6643e-02,  4.0792e-01,  6.5141e-02, -1.7577e-02]],

          [[ 3.0762e-01,  2.6640e-01,  2.5333e-01,  3.3109e-01],
           [-8.5369e-03,  2.5417e-01,  3.8495e-01,  3.8020e-01],
           [ 1.8479e-01,  1.2777e-02,  1.8449e-01, -2.6130e-01],
           [ 4.0294e-01, -1.2486e-01,  3.0586e-01,  2.6442e-01]],

          [[-1.1135e-01,  7.4095e-02, -2.0682e-01,  4.5205e-01],
           [ 2.9411e-01, -2.5577e-02,  2.1258e-01, -4.3491e-01],
           [-5.8497e-02, -4.2731e-01,  1.7408e-01, -3.2637e-01],
           [-3.8298e-01,  1.3352e-01, -1.2106e-01,  3.0946e-01]],

          [[-1.3547e-01,  3.2543e-01, -1.6291e-01,  1.5496e-01],
           [ 2.8618e-01,  5.4538e-02, -2.4109e-01, -3.6821e-02],
           [-4.6692e-01, -3.6228e-01,  4.6054e-01,  3.7113e-01],
           [ 3.2793e-01,  2.5790e-01, -1.7884e-01,  4.0224e-01]]],


         [[[-5.2521e-02,  4.2979e-01,  2.2459e-03,  4.0358e-01],
           [ 3.5386e-01,  6.0339e-02,  3.3523e-01, -1.2252e-01],
           [ 2.2666e-01, -2.3444e-01, -4.7742e-01, -3.6950e-01],
           [-3.8800e-01,  3.3065e-01,  2.9815e-02,  2.6448e-01]],

          [[ 1.1196e-01, -8.1999e-02, -3.3210e-01,  2.2194e-02],
           [ 1.7916e-01,  4.7002e-01,  2.4068e-01, -3.3501e-01],
           [ 2.1695e-01, -3.7629e-01, -3.3819e-01,  1.3866e-01],
           [ 6.8669e-02,  1.5772e-01,  2.0286e-01,  2.7807e-01]],

          [[-4.8461e-01,  3.0931e-01, -3.8593e-01,  4.2804e-01],
           [-2.9289e-01,  2.7776e-01, -4.5114e-01,  2.5956e-01],
           [-6.6718e-03, -1.9910e-01,  2.7000e-01, -2.1088e-02],
           [-3.0362e-01,  2.3773e-01,  6.2899e-03, -4.2433e-01]],

          [[-1.6785e-01,  2.9596e-01,  3.7540e-01, -1.9380e-01],
           [-4.6958e-01,  1.3283e-03,  2.9167e-01,  1.4634e-01],
           [-3.7448e-02, -1.1187e-02,  4.5196e-01,  2.8988e-02],
           [-2.7301e-01,  2.8281e-01, -3.1847e-01, -2.2760e-01]]],


         [[[-4.3733e-01,  2.0897e-01,  8.0926e-02,  4.9392e-01],
           [-2.3104e-01, -2.2611e-01,  4.3877e-01, -4.0833e-01],
           [ 1.2335e-01, -7.2186e-02, -1.3512e-01,  1.7755e-01],
           [-1.4166e-01, -1.6104e-01,  1.3572e-01,  1.0872e-01]],

          [[-4.1464e-02, -1.3985e-01,  4.5638e-01, -3.3024e-03],
           [ 1.0625e-02, -4.1054e-03,  1.6516e-01, -9.4825e-02],
           [ 2.2481e-01, -3.2859e-01, -4.4745e-01,  3.8031e-01],
           [ 2.5641e-01,  1.3776e-01, -1.1690e-01,  3.4571e-01]],

          [[ 1.2545e-01, -4.7312e-01, -1.5836e-01, -2.5882e-01],
           [-3.3584e-01, -2.6337e-01, -1.3534e-01, -5.7001e-02],
           [-1.3825e-01, -6.8215e-02, -3.3278e-01,  2.7089e-02],
           [-3.7697e-01,  2.3815e-01, -2.3184e-01,  8.0891e-02]],

          [[-3.3978e-01, -4.9492e-01,  2.6296e-01,  1.1629e-01],
           [-3.3435e-01,  1.1671e-01, -1.2859e-01, -2.8526e-02],
           [-1.5222e-01, -1.8459e-01, -4.4323e-01,  2.4147e-01],
           [-1.6111e-01,  4.3216e-01,  2.1367e-01,  2.3871e-01]]],


         ...,


         [[[ 1.9857e-01, -9.5937e-02,  3.3412e-01, -1.2772e-01],
           [ 8.6486e-02,  4.6402e-01, -2.3256e-02,  2.1303e-02],
           [ 4.3504e-01, -1.1970e-01,  4.8578e-02, -6.5762e-03],
           [-8.7822e-02,  4.6831e-01, -9.5298e-02, -2.1194e-01]],

          [[-1.0597e-01, -1.5088e-01, -1.1912e-01,  3.6522e-01],
           [-4.0145e-01, -6.7555e-02, -6.9014e-02, -3.1679e-01],
           [ 2.7278e-02,  4.8325e-01, -4.4871e-01,  4.8587e-01],
           [ 3.9116e-01, -3.2562e-01,  1.7314e-01,  1.9583e-01]],

          [[-2.2782e-01,  4.1914e-02, -3.3366e-01, -3.3355e-01],
           [-3.2039e-01, -9.2077e-02, -4.8696e-01, -7.0993e-03],
           [-2.8493e-01,  2.2438e-02, -3.7493e-01,  4.9625e-01],
           [ 4.7445e-01, -5.1710e-02, -4.0384e-01, -2.5566e-01]],

          [[ 2.3566e-01, -3.4773e-01,  1.9294e-01,  3.1481e-02],
           [-2.1355e-02, -2.3888e-02,  2.2486e-01,  2.3863e-01],
           [ 1.3576e-01,  1.9869e-02, -2.8743e-01,  3.6928e-01],
           [ 3.6208e-01, -4.3088e-02, -8.4406e-02, -1.9930e-01]]],


         [[[ 2.1795e-01, -5.5542e-03,  1.9383e-01, -2.2813e-01],
           [ 2.0530e-01,  1.7700e-01, -4.2001e-01,  1.8859e-01],
           [ 1.5293e-01, -4.9775e-01,  1.0345e-03, -2.7647e-01],
           [-4.8442e-01,  1.9470e-01, -4.1397e-01, -4.7029e-01]],

          [[-2.5898e-01,  5.7166e-02, -2.3349e-01, -2.8119e-01],
           [-4.1507e-01,  2.7310e-01, -4.6539e-01,  4.3261e-01],
           [-3.6406e-01,  3.7303e-01,  3.4351e-01,  7.4748e-03],
           [ 2.1111e-01, -4.6966e-01, -4.1660e-01,  2.4501e-01]],

          [[ 3.3308e-01,  2.0528e-01,  3.3004e-01, -3.2347e-01],
           [-2.4480e-01, -4.6874e-02,  8.0368e-02, -3.1465e-02],
           [-1.5781e-01, -1.1402e-01,  3.7851e-01, -1.0087e-01],
           [ 4.9838e-01,  1.1901e-01,  2.5920e-01, -3.0172e-01]],

          [[-3.4181e-01,  2.3603e-01,  4.3538e-01, -2.9547e-01],
           [-5.0516e-02, -1.7126e-01, -2.4809e-01, -1.5076e-01],
           [-2.4838e-01, -4.7253e-01,  5.5365e-02,  2.8138e-01],
           [ 1.9273e-01, -5.7875e-02, -1.0314e-01, -2.1692e-01]]],


         [[[ 1.6946e-01,  2.0824e-01,  2.7541e-02,  4.0874e-01],
           [-6.5547e-02, -2.5866e-01, -3.2855e-04,  9.0284e-02],
           [-3.4100e-01,  3.6056e-01, -1.1090e-01,  1.6948e-01],
           [-1.8190e-01, -1.0407e-01, -4.4904e-02,  6.2364e-02]],

          [[ 4.5383e-02, -4.0807e-01,  4.4441e-01, -2.0194e-01],
           [ 2.2768e-01,  1.4779e-01,  4.9682e-01,  2.5804e-02],
           [-1.3775e-01, -2.2515e-01,  3.9518e-01,  1.0166e-01],
           [ 2.8747e-01, -2.0259e-01, -5.2554e-02, -3.0652e-01]],

          [[-1.9806e-01, -4.4153e-01, -3.6861e-01, -4.8663e-01],
           [-4.9562e-02, -5.5540e-02, -2.2558e-01,  4.1513e-01],
           [ 3.5180e-01, -3.5563e-01, -1.9798e-01,  8.4594e-02],
           [-1.8029e-01, -4.2807e-01, -4.2619e-01,  2.9464e-01]],

          [[-1.6814e-01,  2.2974e-01, -4.3483e-01, -4.7804e-02],
           [ 3.2146e-01, -3.6948e-04, -3.2227e-01, -2.8674e-01],
           [-2.8261e-01,  1.4893e-01,  4.1823e-01, -3.7154e-01],
           [ 5.9622e-02, -4.3501e-01, -6.1434e-02, -2.2014e-01]]]]])
2025-03-18 17:19:25.149349 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([3, 190141782, 4],"float32"),Tensor([3],"float32"),], )

[not compare]  None tensor([[[ 2.4104e-01,  1.6759e-01, -1.4640e-01,  1.3786e-01],
         [ 8.0144e-02,  1.9655e-01, -6.3195e-02, -3.3585e-01],
         [ 1.6061e-01, -4.3819e-01, -4.1684e-01,  4.9294e-01],
         ...,
         [-6.7197e-04,  3.1875e-01,  4.9769e-01, -4.8492e-01],
         [ 3.5163e-01,  4.0087e-01,  3.4764e-01,  4.9687e-01],
         [-1.1039e-01, -3.8959e-01,  2.4091e-01,  3.4829e-01]],

        [[-2.9171e-01, -8.6592e-02, -3.7584e-01, -1.5459e-01],
         [-3.7782e-02,  2.4303e-01,  2.3525e-02, -4.9869e-01],
         [-2.0824e-01, -4.4638e-01, -9.1700e-02,  4.6943e-01],
         ...,
         [ 4.0294e-01, -1.2486e-01,  3.0586e-01,  2.6442e-01],
         [-1.1135e-01,  7.4095e-02, -2.0682e-01,  4.5205e-01],
         [ 2.9411e-01, -2.5577e-02,  2.1258e-01, -4.3491e-01]],

        [[-5.8497e-02, -4.2731e-01,  1.7408e-01, -3.2637e-01],
         [-3.8298e-01,  1.3352e-01, -1.2106e-01,  3.0946e-01],
         [-1.3547e-01,  3.2543e-01, -1.6291e-01,  1.5496e-01],
         ...,
         [ 3.2146e-01, -3.6948e-04, -3.2227e-01, -2.8674e-01],
         [-2.8261e-01,  1.4893e-01,  4.1823e-01, -3.7154e-01],
         [ 5.9622e-02, -4.3501e-01, -6.1434e-02, -2.2014e-01]]])
2025-03-18 17:20:53.374122 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([3, 2, 23767723, 4, 4],"float32"),Tensor([3],"float32"),], )

[not compare]  None tensor([[[[[-2.3352e-01, -6.3720e-02, -2.7855e-01, -2.4608e-01],
           [-6.2878e-02,  3.3746e-01, -4.8186e-01, -4.1635e-02],
           [ 3.3076e-01,  4.0787e-01,  4.2444e-02,  1.8086e-01],
           [ 2.3930e-01,  1.3805e-01, -3.7408e-01,  4.2218e-01]],

          [[-4.5966e-01, -4.6613e-01, -4.2367e-01, -4.9590e-01],
           [ 3.5007e-01, -1.2753e-01, -2.2997e-01,  1.2884e-01],
           [ 2.4104e-01,  1.6759e-01, -1.4640e-01,  1.3786e-01],
           [ 8.0144e-02,  1.9655e-01, -6.3195e-02, -3.3585e-01]],

          [[ 1.6061e-01, -4.3819e-01, -4.1684e-01,  4.9294e-01],
           [ 8.5180e-02,  3.9437e-02, -1.0828e-01, -4.7164e-01],
           [ 2.5115e-02, -3.3898e-02,  3.2395e-01, -1.7367e-01],
           [-1.4246e-02,  3.5848e-01, -3.8309e-01,  1.8092e-01]],

          ...,

          [[-3.2252e-01, -2.1098e-01,  3.9369e-01, -1.8387e-01],
           [ 2.9384e-01, -4.8144e-01,  5.8906e-02, -4.0853e-01],
           [ 4.5010e-01,  9.5514e-02,  3.4681e-01,  4.1149e-01],
           [-3.2832e-01,  9.4980e-02,  2.3406e-01, -4.7036e-01]],

          [[-1.9815e-01,  3.9388e-01, -4.6987e-01,  1.7560e-01],
           [ 3.3998e-01, -4.7447e-02,  4.9636e-02, -3.5862e-01],
           [-1.4320e-02,  3.1382e-01, -5.1463e-02,  3.3562e-01],
           [ 3.5159e-01, -2.4213e-01, -2.4241e-02,  8.3230e-02]],

          [[-2.8115e-01, -2.9788e-01,  4.1748e-01,  2.7733e-01],
           [-3.6687e-01,  9.0988e-02, -2.2044e-01,  4.6553e-01],
           [ 6.4475e-02, -1.3035e-01,  4.2799e-01, -2.9141e-01],
           [-1.3423e-01,  2.3817e-01, -2.7738e-01,  3.9020e-01]]],


         [[[ 2.3615e-01,  4.7938e-01,  4.7439e-01, -9.3976e-02],
           [-6.0929e-02, -4.2354e-01,  3.1991e-01, -4.0593e-01],
           [ 2.6328e-01,  2.9519e-01, -2.5643e-01, -4.5884e-01],
           [ 2.6757e-01, -2.6039e-01, -4.7349e-01,  3.8436e-01]],

          [[-2.1791e-01, -2.5958e-01, -1.1694e-01,  7.5750e-02],
           [-5.0659e-02, -5.4979e-02, -4.9553e-01, -1.3351e-01],
           [-3.7316e-01, -8.9571e-02,  3.6248e-01,  1.2136e-02],
           [-1.3494e-01, -2.9755e-02, -1.8883e-01,  4.2585e-01]],

          [[-8.7383e-02,  4.1891e-01, -1.8991e-01,  3.2116e-01],
           [ 4.9670e-01,  7.0222e-02, -1.8266e-01, -3.7689e-01],
           [ 4.3342e-01,  4.6828e-02,  9.8104e-02,  2.2805e-01],
           [ 3.2417e-01, -3.6478e-01,  3.5740e-01, -3.1127e-01]],

          ...,

          [[-5.8249e-02, -3.9252e-01, -5.7541e-02, -2.2755e-01],
           [-1.0274e-01,  2.2919e-02, -3.2786e-01, -1.4478e-01],
           [-4.5627e-01, -3.7104e-01, -4.9889e-01,  2.5705e-03],
           [ 3.8165e-01, -4.2521e-01,  7.1672e-02, -1.3438e-01]],

          [[-7.8715e-02,  4.5516e-01,  2.8498e-01, -3.0749e-01],
           [-4.6217e-01, -4.6866e-01, -8.2155e-02, -6.3671e-02],
           [ 4.6678e-01,  4.6853e-01, -3.0515e-01,  3.8544e-01],
           [-2.1352e-01,  2.3147e-01,  3.0656e-01, -3.7105e-01]],

          [[-2.5187e-02,  2.2679e-01,  1.7154e-01,  1.3388e-01],
           [-3.3866e-01,  3.1385e-01,  9.4947e-02, -9.0630e-02],
           [-4.7973e-01, -2.2178e-01, -1.6818e-02, -1.2409e-01],
           [-3.2081e-01, -1.5254e-01,  2.7752e-01,  3.8550e-01]]]],



        [[[[ 1.9694e-01, -4.1135e-01,  1.6358e-01, -3.5123e-01],
           [-6.7197e-04,  3.1875e-01,  4.9769e-01, -4.8492e-01],
           [ 3.5163e-01,  4.0087e-01,  3.4764e-01,  4.9687e-01],
           [-1.1039e-01, -3.8959e-01,  2.4091e-01,  3.4829e-01]],

          [[-2.9171e-01, -8.6592e-02, -3.7584e-01, -1.5459e-01],
           [-3.7782e-02,  2.4303e-01,  2.3525e-02, -4.9869e-01],
           [-2.0824e-01, -4.4638e-01, -9.1700e-02,  4.6943e-01],
           [ 4.3443e-01,  8.7745e-02,  3.3991e-01, -3.4918e-01]],

          [[ 3.1012e-01,  3.5128e-01, -3.3426e-01,  9.4020e-02],
           [-4.6863e-01,  2.1891e-01,  4.7435e-01,  1.2338e-04],
           [ 1.1010e-01,  4.7055e-01,  4.4302e-01,  3.2367e-01],
           [-3.5106e-01,  1.1032e-01,  2.2755e-01, -4.9951e-01]],

          ...,

          [[ 2.1812e-01,  2.2638e-01,  1.7183e-01,  1.1343e-01],
           [ 4.3249e-01, -2.8970e-01,  1.5096e-02,  2.8985e-02],
           [-4.0101e-01, -4.3028e-01, -3.8874e-01, -1.4332e-01],
           [-3.1612e-01, -2.5949e-01,  2.3133e-01, -3.4325e-01]],

          [[-3.5464e-01,  4.1340e-01,  1.8982e-01,  4.2884e-01],
           [ 7.3195e-02,  4.6575e-02, -1.5745e-01, -1.7280e-01],
           [ 3.3225e-01,  8.9656e-03,  2.1010e-01, -4.5011e-01],
           [-2.8773e-01, -1.1487e-01, -1.4810e-01,  3.7808e-01]],

          [[ 3.9944e-02,  1.7753e-01, -2.0827e-01,  3.2573e-01],
           [-1.7838e-01, -3.5648e-01,  8.5767e-02, -1.6785e-01],
           [-1.6988e-01, -8.8992e-02, -3.0193e-02, -2.6318e-01],
           [ 2.0751e-01, -2.1498e-01,  3.4147e-01,  7.9000e-02]]],


         [[[-2.7694e-01,  2.7294e-01,  4.0491e-01,  3.6934e-01],
           [-4.2421e-01,  1.3181e-01, -4.2334e-01,  4.6433e-01],
           [-1.4680e-01, -7.5331e-02, -2.6091e-01, -4.4154e-01],
           [-8.4447e-02, -4.7281e-01, -1.5840e-01,  1.7175e-01]],

          [[-2.8202e-01, -4.4658e-01,  1.7639e-02,  3.3849e-01],
           [ 1.2025e-01,  4.2554e-01,  4.0232e-01,  4.0375e-01],
           [-3.8826e-01,  1.1977e-01, -1.3365e-01, -8.0118e-02],
           [-7.9696e-02, -2.2480e-01,  3.8518e-01,  3.7710e-01]],

          [[-2.5970e-01,  2.1949e-01,  4.1136e-01,  4.9574e-01],
           [-4.1697e-01,  2.4400e-01,  3.4019e-01,  3.3939e-01],
           [-2.2126e-01, -2.5667e-01, -1.2170e-01,  7.4479e-02],
           [ 3.1793e-01, -1.1419e-01, -3.6877e-01,  1.0629e-01]],

          ...,

          [[ 1.0819e-01, -1.3469e-01, -4.7016e-01,  4.8897e-01],
           [ 1.9809e-01, -3.1948e-01, -2.6128e-01,  3.9779e-01],
           [-4.5074e-01,  3.3029e-02,  1.6245e-01, -3.3097e-01],
           [ 3.6720e-01, -3.8066e-01,  6.0682e-02, -2.0752e-01]],

          [[ 4.3178e-01,  4.8681e-01, -1.1679e-01,  8.1086e-02],
           [ 3.7312e-01,  3.6753e-01,  2.2301e-01, -1.5109e-01],
           [-8.8730e-02, -3.2674e-01, -1.6343e-01,  3.1051e-01],
           [-7.6643e-02,  4.0792e-01,  6.5141e-02, -1.7577e-02]],

          [[ 3.0762e-01,  2.6640e-01,  2.5333e-01,  3.3109e-01],
           [-8.5369e-03,  2.5417e-01,  3.8495e-01,  3.8020e-01],
           [ 1.8479e-01,  1.2777e-02,  1.8449e-01, -2.6130e-01],
           [ 4.0294e-01, -1.2486e-01,  3.0586e-01,  2.6442e-01]]]],



        [[[[-1.1135e-01,  7.4095e-02, -2.0682e-01,  4.5205e-01],
           [ 2.9411e-01, -2.5577e-02,  2.1258e-01, -4.3491e-01],
           [-5.8497e-02, -4.2731e-01,  1.7408e-01, -3.2637e-01],
           [-3.8298e-01,  1.3352e-01, -1.2106e-01,  3.0946e-01]],

          [[-1.3547e-01,  3.2543e-01, -1.6291e-01,  1.5496e-01],
           [ 2.8618e-01,  5.4538e-02, -2.4109e-01, -3.6821e-02],
           [-4.6692e-01, -3.6228e-01,  4.6054e-01,  3.7113e-01],
           [ 3.2793e-01,  2.5790e-01, -1.7884e-01,  4.0224e-01]],

          [[-5.2521e-02,  4.2979e-01,  2.2459e-03,  4.0358e-01],
           [ 3.5386e-01,  6.0339e-02,  3.3523e-01, -1.2252e-01],
           [ 2.2666e-01, -2.3444e-01, -4.7742e-01, -3.6950e-01],
           [-3.8800e-01,  3.3065e-01,  2.9815e-02,  2.6448e-01]],

          ...,

          [[ 3.3409e-01, -4.4765e-01, -4.9690e-01,  2.1146e-01],
           [ 2.6991e-01,  4.6775e-01,  1.2609e-01, -2.7036e-01],
           [-5.8210e-02, -1.4428e-01, -4.6055e-01,  1.9270e-01],
           [-3.6997e-01, -4.1492e-01, -3.3280e-01,  4.5259e-01]],

          [[ 4.3143e-01, -4.0582e-01,  4.9780e-01,  2.9526e-01],
           [ 2.7333e-01, -6.3083e-02, -1.6450e-01, -2.1509e-01],
           [ 1.8045e-01, -1.1835e-01, -1.5085e-01, -3.5776e-01],
           [-8.5408e-02, -3.3481e-01, -1.1793e-01, -2.0769e-01]],

          [[-4.3247e-01, -7.8126e-02, -2.6531e-01, -1.0977e-01],
           [-3.0826e-01, -4.2305e-01,  4.9129e-01, -5.6472e-02],
           [ 3.4139e-01,  3.0228e-01, -4.7056e-02,  4.5161e-01],
           [ 9.7663e-02, -2.6745e-01,  1.7448e-01,  4.6760e-01]]],


         [[[ 2.6617e-01, -9.8389e-02, -2.4920e-01, -1.1584e-01],
           [-6.3132e-02,  3.2793e-01, -2.0208e-02,  8.5292e-02],
           [ 2.0709e-01, -4.0282e-01,  1.3591e-03,  7.6865e-02],
           [-1.0579e-01, -2.0682e-01,  2.7525e-01, -3.1466e-01]],

          [[-2.3289e-01, -3.8341e-01, -3.4958e-01, -1.7280e-01],
           [-5.1922e-02, -2.0633e-01,  9.1162e-02,  3.0802e-01],
           [-1.5755e-01,  2.9203e-01,  2.2243e-01, -2.9103e-02],
           [ 1.8961e-02,  1.2052e-01, -1.1887e-01, -2.4587e-01]],

          [[ 3.1733e-01, -4.1195e-01,  6.6769e-02, -2.8803e-01],
           [-1.4865e-01, -2.6837e-01,  3.7434e-01,  4.2154e-01],
           [ 2.2768e-01, -4.0885e-02,  1.9878e-01, -3.6060e-01],
           [-2.3594e-02,  1.2264e-01, -3.1318e-01, -1.6189e-02]],

          ...,

          [[ 4.5383e-02, -4.0807e-01,  4.4441e-01, -2.0194e-01],
           [ 2.2768e-01,  1.4779e-01,  4.9682e-01,  2.5804e-02],
           [-1.3775e-01, -2.2515e-01,  3.9518e-01,  1.0166e-01],
           [ 2.8747e-01, -2.0259e-01, -5.2554e-02, -3.0652e-01]],

          [[-1.9806e-01, -4.4153e-01, -3.6861e-01, -4.8663e-01],
           [-4.9562e-02, -5.5540e-02, -2.2558e-01,  4.1513e-01],
           [ 3.5180e-01, -3.5563e-01, -1.9798e-01,  8.4594e-02],
           [-1.8029e-01, -4.2807e-01, -4.2619e-01,  2.9464e-01]],

          [[-1.6814e-01,  2.2974e-01, -4.3483e-01, -4.7804e-02],
           [ 3.2146e-01, -3.6948e-04, -3.2227e-01, -2.8674e-01],
           [-2.8261e-01,  1.4893e-01,  4.1823e-01, -3.7154e-01],
           [ 5.9622e-02, -4.3501e-01, -6.1434e-02, -2.2014e-01]]]]])
2025-03-18 17:22:32.200249 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([3, 2, 380283564],"float32"),Tensor([3],"float32"),], )

[not compare]  None tensor([[[ 0.2410,  0.1676, -0.1464,  ..., -0.2596, -0.1169,  0.0757],
         [-0.0507, -0.0550, -0.4955,  ..., -0.3896,  0.2409,  0.3483]],

        [[-0.2917, -0.0866, -0.3758,  ..., -0.0753, -0.2609, -0.4415],
         [-0.0844, -0.4728, -0.1584,  ..., -0.0256,  0.2126, -0.4349]],

        [[-0.0585, -0.4273,  0.1741,  ..., -0.0984, -0.2492, -0.1158],
         [-0.0631,  0.3279, -0.0202,  ..., -0.4350, -0.0614, -0.2201]]])
2025-03-18 17:24:03.489909 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([3, 2, 4, 23767723, 4],"float32"),Tensor([3],"float32"),], )

[not compare]  None tensor([[[[[-2.3352e-01, -6.3720e-02, -2.7855e-01, -2.4608e-01],
           [-6.2878e-02,  3.3746e-01, -4.8186e-01, -4.1635e-02],
           [ 3.3076e-01,  4.0787e-01,  4.2444e-02,  1.8086e-01],
           ...,
           [ 1.8530e-01,  4.0453e-01,  3.2694e-02, -2.0818e-01],
           [-1.9548e-01,  4.0659e-01,  3.5200e-01,  3.9368e-01],
           [ 1.8418e-01,  4.8290e-01, -3.0534e-01, -4.4975e-01]],

          [[ 2.8528e-01,  2.3828e-01,  4.6654e-01,  4.1005e-01],
           [ 4.4193e-01, -2.4847e-01, -3.6896e-01,  7.0484e-02],
           [-5.8294e-02,  2.9111e-01,  5.8583e-02,  3.4602e-01],
           ...,
           [-3.1968e-01, -5.1983e-02, -2.4611e-02,  1.1424e-01],
           [-5.3621e-02,  3.4703e-01,  2.5590e-01, -4.0049e-01],
           [-4.7868e-01, -2.3547e-02, -3.7555e-01,  1.0457e-01]],

          [[-3.1036e-01,  4.4016e-02, -3.9274e-02,  4.9937e-01],
           [ 1.7594e-01, -2.7651e-01, -2.4659e-01, -4.8292e-02],
           [-4.2828e-01,  1.9083e-02,  1.5829e-01,  9.2213e-02],
           ...,
           [ 2.4630e-01, -1.6123e-01,  1.8962e-01, -1.3006e-01],
           [ 1.5024e-01,  1.4694e-02,  3.0313e-01,  3.8562e-01],
           [-3.7787e-01,  1.1653e-01, -2.1860e-01,  3.0877e-01]],

          [[ 4.0099e-01, -1.3755e-02,  1.8847e-01,  7.8337e-02],
           [ 1.8307e-01, -4.2194e-01, -2.1608e-03,  4.8649e-01],
           [ 2.7075e-01,  5.6825e-02, -2.3268e-01,  2.4785e-01],
           ...,
           [-3.6687e-01,  9.0988e-02, -2.2044e-01,  4.6553e-01],
           [ 6.4475e-02, -1.3035e-01,  4.2799e-01, -2.9141e-01],
           [-1.3423e-01,  2.3817e-01, -2.7738e-01,  3.9020e-01]]],


         [[[ 2.3615e-01,  4.7938e-01,  4.7439e-01, -9.3976e-02],
           [-6.0929e-02, -4.2354e-01,  3.1991e-01, -4.0593e-01],
           [ 2.6328e-01,  2.9519e-01, -2.5643e-01, -4.5884e-01],
           ...,
           [ 4.4545e-01,  3.5577e-01, -2.0923e-01, -3.5852e-01],
           [-1.9692e-02,  3.2664e-01, -3.4830e-01, -1.9055e-01],
           [ 4.2370e-01, -3.7902e-01,  3.8683e-01, -1.0482e-01]],

          [[ 3.4069e-01, -6.9581e-02,  4.5718e-01,  1.6840e-01],
           [-4.3482e-01, -4.3153e-01, -2.3368e-01,  3.2244e-01],
           [ 3.5953e-01, -1.9458e-02,  1.8450e-01,  1.3435e-01],
           ...,
           [-8.2462e-02,  6.1182e-02,  1.9015e-01,  2.4144e-01],
           [-4.1438e-02, -2.9962e-01, -2.5147e-01, -2.0044e-01],
           [ 4.5236e-01,  4.9696e-01, -2.0056e-01,  4.0542e-01]],

          [[ 1.1769e-01,  4.1108e-01,  1.0153e-01,  3.1307e-01],
           [-4.6737e-01,  1.2073e-01, -3.7849e-02, -3.3150e-01],
           [ 1.9544e-01, -3.1373e-01,  9.6292e-04, -8.6889e-02],
           ...,
           [ 4.4670e-01, -5.1837e-02,  1.7082e-01, -4.2867e-01],
           [-1.7785e-04, -9.3491e-02,  4.9634e-01,  3.3056e-01],
           [-4.1766e-01, -6.4472e-03,  4.7382e-01, -4.5088e-01]],

          [[-8.9677e-02, -2.6597e-01,  7.2653e-02,  4.9869e-01],
           [ 2.6564e-01,  3.1928e-01, -2.6837e-02,  4.0466e-01],
           [-7.6436e-02, -3.7791e-02, -3.1318e-01, -4.5914e-01],
           ...,
           [-3.3866e-01,  3.1385e-01,  9.4947e-02, -9.0630e-02],
           [-4.7973e-01, -2.2178e-01, -1.6818e-02, -1.2409e-01],
           [-3.2081e-01, -1.5254e-01,  2.7752e-01,  3.8550e-01]]]],



        [[[[ 1.9694e-01, -4.1135e-01,  1.6358e-01, -3.5123e-01],
           [-6.7197e-04,  3.1875e-01,  4.9769e-01, -4.8492e-01],
           [ 3.5163e-01,  4.0087e-01,  3.4764e-01,  4.9687e-01],
           ...,
           [ 3.2840e-01,  3.9504e-01, -1.1676e-01,  2.1657e-01],
           [ 4.0297e-01, -4.8323e-01, -2.5150e-01, -3.4985e-01],
           [-1.4623e-01, -2.0241e-01, -4.6195e-01, -3.3040e-01]],

          [[ 4.5806e-01,  2.0725e-01,  1.9098e-01, -6.7609e-02],
           [-2.9973e-01,  2.4962e-01, -2.6398e-03, -4.6404e-02],
           [ 4.2138e-01,  2.1536e-01,  2.6555e-01,  1.3813e-01],
           ...,
           [ 4.6008e-01, -4.5705e-01, -2.0374e-01,  7.8297e-02],
           [ 4.8161e-01, -2.4026e-01,  7.6177e-02, -2.4298e-01],
           [ 1.9884e-01,  3.0748e-01,  1.4136e-01,  1.2952e-01]],

          [[ 4.1713e-01,  4.7983e-01, -4.4454e-01,  4.3383e-01],
           [ 2.1885e-01, -4.4984e-02,  4.7329e-01, -4.5564e-01],
           [ 3.9501e-01, -2.6570e-01,  1.8414e-01,  1.7679e-01],
           ...,
           [-4.6572e-01, -4.4671e-01,  8.0775e-02, -2.4805e-01],
           [ 2.0626e-01,  2.3669e-01,  4.4188e-01,  4.4798e-01],
           [ 2.8712e-01, -4.6473e-01, -7.0202e-02,  2.9566e-01]],

          [[ 7.8515e-02, -2.2273e-01, -2.1677e-02,  2.8136e-01],
           [ 3.8088e-04,  3.7945e-02, -2.8641e-01,  3.0749e-01],
           [ 4.1795e-01,  4.3897e-01, -1.1181e-01, -4.3833e-01],
           ...,
           [-1.7838e-01, -3.5648e-01,  8.5767e-02, -1.6785e-01],
           [-1.6988e-01, -8.8992e-02, -3.0193e-02, -2.6318e-01],
           [ 2.0751e-01, -2.1498e-01,  3.4147e-01,  7.9000e-02]]],


         [[[-2.7694e-01,  2.7294e-01,  4.0491e-01,  3.6934e-01],
           [-4.2421e-01,  1.3181e-01, -4.2334e-01,  4.6433e-01],
           [-1.4680e-01, -7.5331e-02, -2.6091e-01, -4.4154e-01],
           ...,
           [ 2.6855e-01, -1.9237e-01,  7.4131e-03, -4.3783e-01],
           [-1.0030e-02,  3.3563e-01,  4.5691e-01, -6.5208e-02],
           [-2.9352e-01, -9.2882e-02, -2.0367e-01,  1.4609e-01]],

          [[ 1.1019e-01, -1.9201e-01, -1.1596e-01, -4.6586e-01],
           [ 2.4966e-01, -3.0403e-01,  1.9362e-01,  3.5267e-01],
           [ 4.9007e-01,  3.8442e-01,  3.2010e-02, -2.9353e-01],
           ...,
           [ 3.4913e-01,  2.7063e-01, -4.8025e-01, -3.1439e-01],
           [-5.8405e-02, -3.0270e-01,  3.6260e-01,  3.6368e-01],
           [ 4.5430e-01,  3.1040e-01, -6.3589e-02,  1.3871e-01]],

          [[-1.9606e-02,  6.7400e-02, -1.6966e-01, -2.1000e-01],
           [-3.1111e-01,  1.5380e-02,  2.3467e-01, -1.2763e-01],
           [-3.0582e-01,  2.4925e-01,  4.8702e-01, -4.4992e-01],
           ...,
           [-1.1610e-01, -1.6200e-01, -4.7549e-01,  8.9151e-02],
           [ 1.5033e-01, -4.8713e-02, -1.4336e-01,  2.8296e-01],
           [-3.5814e-01, -1.6737e-01,  4.4932e-01,  1.4721e-01]],

          [[ 6.7595e-02,  7.2363e-02,  1.1888e-01, -3.6419e-01],
           [-1.2532e-01, -1.9914e-02, -2.6624e-01,  2.0623e-01],
           [-2.5034e-01,  3.4046e-01,  3.2030e-01, -4.3409e-01],
           ...,
           [-8.5369e-03,  2.5417e-01,  3.8495e-01,  3.8020e-01],
           [ 1.8479e-01,  1.2777e-02,  1.8449e-01, -2.6130e-01],
           [ 4.0294e-01, -1.2486e-01,  3.0586e-01,  2.6442e-01]]]],



        [[[[-1.1135e-01,  7.4095e-02, -2.0682e-01,  4.5205e-01],
           [ 2.9411e-01, -2.5577e-02,  2.1258e-01, -4.3491e-01],
           [-5.8497e-02, -4.2731e-01,  1.7408e-01, -3.2637e-01],
           ...,
           [-9.8543e-02,  4.1169e-01, -4.0741e-01,  3.2693e-01],
           [-2.6714e-03, -3.9928e-01, -2.3878e-01,  3.6683e-02],
           [ 1.7659e-01,  1.3670e-01, -3.5766e-01,  1.7664e-01]],

          [[ 1.7966e-01,  3.5823e-01, -2.3945e-01,  2.7801e-01],
           [-1.0330e-01, -1.5889e-01, -4.9188e-01,  2.1226e-01],
           [-4.3724e-01,  3.9311e-01,  2.8325e-01,  3.5167e-01],
           ...,
           [-4.6369e-02,  1.4992e-01,  2.9758e-01,  2.0837e-01],
           [-3.1835e-01, -4.9335e-01, -4.8992e-01, -1.9571e-01],
           [ 2.3690e-01,  7.8812e-02,  1.5803e-01,  2.6222e-01]],

          [[-2.2772e-02,  4.5875e-01,  4.0617e-01, -4.3027e-01],
           [ 4.7365e-02, -1.6281e-01, -4.1852e-01,  2.0722e-01],
           [ 4.2712e-01,  1.3922e-01,  8.4329e-02, -4.7124e-01],
           ...,
           [-2.2310e-01,  1.1547e-01,  3.0552e-01,  1.9944e-01],
           [-4.6808e-01,  1.6002e-01, -3.1176e-01,  2.8225e-01],
           [ 4.7122e-01, -2.7705e-01,  2.5946e-01,  2.1694e-01]],

          [[ 4.3609e-01,  3.8275e-01,  3.3827e-01,  4.8830e-01],
           [-4.6144e-01, -4.4676e-01, -4.3222e-01,  3.6377e-01],
           [ 1.1727e-01,  2.4087e-01, -4.1233e-01,  1.5770e-01],
           ...,
           [-3.0826e-01, -4.2305e-01,  4.9129e-01, -5.6472e-02],
           [ 3.4139e-01,  3.0228e-01, -4.7056e-02,  4.5161e-01],
           [ 9.7663e-02, -2.6745e-01,  1.7448e-01,  4.6760e-01]]],


         [[[ 2.6617e-01, -9.8389e-02, -2.4920e-01, -1.1584e-01],
           [-6.3132e-02,  3.2793e-01, -2.0208e-02,  8.5292e-02],
           [ 2.0709e-01, -4.0282e-01,  1.3591e-03,  7.6865e-02],
           ...,
           [-4.4424e-01,  1.6309e-01,  5.0525e-02,  3.9535e-01],
           [ 2.1721e-01, -2.1407e-01, -1.3390e-01, -7.3107e-02],
           [ 1.7367e-01, -3.8014e-01, -8.2779e-02, -2.9195e-01]],

          [[ 1.0688e-01, -2.1881e-01,  1.2613e-01, -3.8391e-01],
           [ 3.5552e-01,  3.4812e-01,  2.4114e-01,  2.4596e-01],
           [-2.5495e-01,  4.7769e-01,  8.7711e-02, -4.1319e-01],
           ...,
           [-1.1514e-01,  3.8978e-01,  2.0586e-01,  1.9088e-01],
           [ 3.1481e-01, -4.7513e-01, -2.8165e-01, -3.9230e-01],
           [-3.2375e-01, -4.8128e-01, -4.5948e-01,  2.3199e-01]],

          [[ 4.0427e-02,  2.1648e-01,  3.3029e-01,  1.5252e-01],
           [-1.3338e-01,  3.9922e-01,  2.8263e-01, -1.7844e-01],
           [-4.8546e-01,  3.4348e-01,  8.1075e-03, -1.2428e-01],
           ...,
           [ 1.9687e-01, -7.7673e-02, -1.5561e-02,  3.0946e-01],
           [ 1.0162e-01,  4.3816e-01,  2.3026e-02, -1.9276e-01],
           [-4.1786e-01, -3.6672e-01,  4.4994e-01,  4.4634e-01]],

          [[ 4.6950e-02, -1.8818e-01,  1.0445e-01,  3.6778e-01],
           [-4.5083e-01,  8.9148e-02,  7.5920e-02,  1.5012e-01],
           [ 1.0108e-01, -2.4293e-01,  4.4786e-01,  3.7503e-02],
           ...,
           [ 3.2146e-01, -3.6948e-04, -3.2227e-01, -2.8674e-01],
           [-2.8261e-01,  1.4893e-01,  4.1823e-01, -3.7154e-01],
           [ 5.9622e-02, -4.3501e-01, -6.1434e-02, -2.2014e-01]]]]])
2025-03-18 17:25:31.368417 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([3, 2, 4, 4, 23767723],"float32"),Tensor([3],"float32"),], )

[not compare]  None tensor([[[[[-2.3352e-01, -6.3720e-02, -2.7855e-01,  ...,  4.8091e-01,
            -2.8683e-02,  8.6674e-03],
           [ 2.3670e-02, -2.1155e-01,  2.6178e-01,  ..., -3.4683e-01,
            -2.7804e-01, -3.8840e-01],
           [ 2.7782e-01, -2.8731e-01,  3.1331e-01,  ...,  1.0823e-01,
             2.7952e-01, -2.5868e-01],
           [-1.6801e-01, -1.8530e-01,  1.3325e-01,  ...,  4.8290e-01,
            -3.0534e-01, -4.4975e-01]],

          [[ 2.8528e-01,  2.3828e-01,  4.6654e-01,  ...,  2.5180e-01,
             2.9066e-01,  1.1463e-01],
           [ 2.2239e-01,  4.1166e-01, -1.7279e-01,  ...,  3.5801e-01,
             4.1602e-01,  4.0417e-01],
           [-4.2383e-01, -5.4112e-02,  3.6740e-01,  ..., -4.1191e-02,
            -3.8200e-01,  4.7428e-01],
           [ 1.7955e-01, -4.8323e-01, -2.2059e-01,  ..., -2.3547e-02,
            -3.7555e-01,  1.0457e-01]],

          [[-3.1036e-01,  4.4016e-02, -3.9274e-02,  ...,  4.4438e-01,
             1.9675e-01,  3.7870e-01],
           [ 4.0027e-02,  7.2719e-02, -2.7042e-01,  ...,  5.5164e-02,
             1.5749e-01, -2.1409e-02],
           [-1.9421e-02, -2.9130e-01,  2.7422e-01,  ..., -4.1625e-01,
             2.7375e-01, -4.8209e-01],
           [ 2.4996e-01, -1.0908e-01, -2.5570e-01,  ...,  1.1653e-01,
            -2.1860e-01,  3.0877e-01]],

          [[ 4.0099e-01, -1.3755e-02,  1.8847e-01,  ...,  2.5844e-01,
             4.0339e-01, -2.6495e-02],
           [-4.9220e-01, -2.8441e-01, -3.1280e-01,  ...,  8.4695e-02,
             4.2311e-01,  4.6586e-01],
           [ 4.8165e-01,  2.2448e-01,  4.1721e-01,  ...,  1.8910e-02,
            -2.8938e-01, -3.2912e-01],
           [ 3.2163e-01,  5.6806e-02,  3.2965e-01,  ...,  2.3817e-01,
            -2.7738e-01,  3.9020e-01]]],


         [[[ 2.3615e-01,  4.7938e-01,  4.7439e-01,  ...,  4.5524e-01,
             2.1289e-01,  2.8726e-01],
           [-3.6309e-01,  4.7796e-01,  1.7465e-01,  ..., -1.8861e-01,
            -4.4736e-01,  2.9997e-01],
           [-1.0562e-01, -1.7613e-01, -1.9013e-01,  ..., -3.8287e-01,
            -4.9487e-01, -2.5357e-01],
           [ 1.9019e-02,  1.6272e-01,  4.0989e-01,  ..., -3.7902e-01,
             3.8683e-01, -1.0482e-01]],

          [[ 3.4069e-01, -6.9581e-02,  4.5718e-01,  ..., -6.3243e-02,
            -3.4989e-01, -2.5553e-01],
           [ 2.7170e-01, -3.8116e-01, -2.7302e-01,  ...,  4.9277e-02,
             7.9119e-02,  4.8674e-01],
           [-4.3768e-01, -4.5447e-01,  1.0801e-01,  ...,  3.5663e-01,
             4.3374e-01, -2.6036e-01],
           [ 7.2743e-02, -1.6271e-01, -7.4674e-02,  ...,  4.9696e-01,
            -2.0056e-01,  4.0542e-01]],

          [[ 1.1769e-01,  4.1108e-01,  1.0153e-01,  ..., -5.2229e-02,
             2.8872e-01,  3.8939e-01],
           [ 9.2570e-02,  2.6833e-01,  4.6329e-02,  ...,  3.7796e-01,
            -7.5866e-02,  2.2155e-01],
           [ 3.6762e-01, -4.1486e-01,  9.4188e-02,  ..., -4.7056e-01,
            -1.1620e-01,  1.9343e-01],
           [-4.7384e-01, -4.5159e-01, -3.5392e-01,  ..., -6.4472e-03,
             4.7382e-01, -4.5088e-01]],

          [[-8.9677e-02, -2.6597e-01,  7.2653e-02,  ..., -3.2779e-01,
            -9.2851e-02, -7.3140e-03],
           [ 4.5452e-01, -4.3273e-01,  2.2287e-01,  ...,  2.6812e-01,
            -2.9171e-01,  1.2738e-01],
           [ 2.7832e-01, -2.9012e-01,  1.6077e-02,  ...,  4.2371e-01,
            -1.2601e-02, -4.0266e-01],
           [ 1.4418e-01, -2.5576e-01,  3.6610e-01,  ..., -1.5254e-01,
             2.7752e-01,  3.8550e-01]]]],



        [[[[ 1.9694e-01, -4.1135e-01,  1.6358e-01,  ...,  2.8961e-01,
            -3.9774e-01,  1.7328e-01],
           [-4.1839e-01, -1.3970e-01,  2.6819e-01,  ...,  2.1546e-01,
            -7.7347e-02, -2.6556e-01],
           [ 4.2342e-01,  3.2341e-01,  3.2704e-01,  ...,  3.8299e-01,
             4.8453e-01,  4.5289e-01],
           [ 2.5109e-02, -2.1071e-01,  1.1215e-02,  ..., -2.0241e-01,
            -4.6195e-01, -3.3040e-01]],

          [[ 4.5806e-01,  2.0725e-01,  1.9098e-01,  ..., -2.6491e-01,
            -3.4159e-02,  1.0130e-01],
           [ 1.7866e-01,  4.9279e-01,  2.1433e-01,  ..., -3.4282e-02,
            -4.3661e-01,  3.3301e-01],
           [-2.7294e-01, -2.5973e-01,  2.8528e-01,  ...,  4.7803e-02,
            -1.3268e-01, -2.0523e-01],
           [-3.1109e-01,  6.5299e-02,  2.7052e-01,  ...,  3.0748e-01,
             1.4136e-01,  1.2952e-01]],

          [[ 4.1713e-01,  4.7983e-01, -4.4454e-01,  ..., -2.0248e-01,
            -3.3361e-01,  3.3423e-01],
           [ 4.9795e-01,  3.4063e-01,  6.2951e-02,  ...,  3.5008e-01,
             2.0690e-01, -2.5708e-02],
           [ 1.9828e-01,  2.9753e-01,  3.5638e-02,  ..., -2.3447e-01,
             1.5439e-02, -4.1182e-01],
           [-5.7457e-02, -2.3361e-01, -3.1611e-01,  ..., -4.6473e-01,
            -7.0202e-02,  2.9566e-01]],

          [[ 7.8515e-02, -2.2273e-01, -2.1677e-02,  ...,  2.0398e-01,
             3.1372e-01,  2.5360e-01],
           [-9.5246e-05, -4.8623e-01, -4.1954e-01,  ..., -3.8320e-01,
             3.9213e-01,  3.6037e-01],
           [-3.8566e-01, -1.0246e-01, -1.4095e-01,  ...,  4.2348e-01,
            -1.8378e-01,  3.6198e-01],
           [ 3.2248e-01,  4.8295e-01, -2.6381e-02,  ..., -2.1498e-01,
             3.4147e-01,  7.9000e-02]]],


         [[[-2.7694e-01,  2.7294e-01,  4.0491e-01,  ...,  3.7222e-01,
             1.6118e-01,  4.4885e-01],
           [-6.7418e-02,  4.5718e-01,  1.9686e-01,  ...,  1.1185e-01,
             4.0674e-01,  3.2345e-01],
           [-1.6907e-01, -2.2598e-02, -1.6305e-01,  ...,  2.0043e-02,
             4.4828e-01, -1.3792e-01],
           [-1.5307e-01, -3.7864e-01, -4.2265e-01,  ..., -9.2882e-02,
            -2.0367e-01,  1.4609e-01]],

          [[ 1.1019e-01, -1.9201e-01, -1.1596e-01,  ...,  4.9034e-01,
             1.8093e-01, -3.0067e-01],
           [-4.1434e-01,  1.6017e-01, -3.4546e-01,  ...,  2.4493e-01,
            -4.9116e-02,  2.7544e-01],
           [-1.4124e-01,  9.4544e-02, -4.5134e-01,  ..., -4.6956e-01,
            -9.3619e-03, -4.6057e-02],
           [ 1.5337e-01, -3.5805e-01, -3.1939e-01,  ...,  3.1040e-01,
            -6.3589e-02,  1.3871e-01]],

          [[-1.9606e-02,  6.7400e-02, -1.6966e-01,  ...,  4.0573e-01,
             1.2524e-01, -6.4901e-02],
           [ 2.8227e-01,  4.3990e-02,  3.7427e-01,  ..., -2.7563e-01,
             4.0300e-01,  4.5182e-01],
           [-2.4220e-02, -1.2430e-02, -8.0956e-02,  ...,  3.6355e-01,
            -3.3193e-01,  3.7624e-02],
           [ 6.3383e-02, -2.3853e-01, -4.2146e-01,  ..., -1.6737e-01,
             4.4932e-01,  1.4721e-01]],

          [[ 6.7595e-02,  7.2363e-02,  1.1888e-01,  ..., -1.3202e-01,
            -2.0040e-01,  2.2553e-01],
           [ 2.8258e-01, -1.7279e-01,  3.1606e-01,  ..., -2.2218e-01,
             9.8949e-02,  4.1683e-01],
           [ 3.8768e-01, -3.0768e-01,  4.2869e-02,  ...,  1.7058e-02,
             3.7062e-01,  7.5353e-02],
           [-2.1126e-01,  4.8442e-01, -3.6725e-01,  ..., -1.2486e-01,
             3.0586e-01,  2.6442e-01]]]],



        [[[[-1.1135e-01,  7.4095e-02, -2.0682e-01,  ...,  1.6868e-01,
            -3.6126e-01,  1.1931e-01],
           [ 2.8627e-01, -1.8901e-01,  3.7422e-01,  ...,  3.7492e-01,
             1.4400e-01, -1.0466e-01],
           [ 5.6760e-02,  4.3966e-01,  2.9760e-01,  ..., -1.4502e-01,
            -2.3889e-01, -1.4578e-01],
           [ 2.9600e-01,  1.5243e-02, -5.4198e-02,  ...,  1.3670e-01,
            -3.5766e-01,  1.7664e-01]],

          [[ 1.7966e-01,  3.5823e-01, -2.3945e-01,  ...,  1.1709e-01,
             4.1587e-01, -1.2644e-01],
           [-1.9060e-01, -3.4191e-01,  4.0559e-01,  ...,  6.4003e-02,
             4.0262e-01,  5.2295e-02],
           [ 3.0560e-01, -3.6189e-01,  1.1028e-01,  ...,  2.1333e-01,
            -1.8519e-02,  1.5350e-01],
           [ 4.3072e-01, -2.9946e-01,  4.8816e-01,  ...,  7.8812e-02,
             1.5803e-01,  2.6222e-01]],

          [[-2.2772e-02,  4.5875e-01,  4.0617e-01,  ..., -3.4103e-01,
             1.2096e-01,  3.1972e-01],
           [-1.4474e-01, -8.8946e-02, -1.5879e-01,  ...,  1.7598e-01,
             1.9035e-01,  4.8597e-01],
           [ 3.8128e-01, -4.2639e-01,  1.2870e-01,  ..., -3.7958e-01,
            -4.8185e-01,  7.4592e-02],
           [-1.2072e-01, -6.7207e-02, -6.2249e-03,  ..., -2.7705e-01,
             2.5946e-01,  2.1694e-01]],

          [[ 4.3609e-01,  3.8275e-01,  3.3827e-01,  ...,  1.5113e-01,
            -2.6370e-01, -2.8160e-01],
           [ 4.9998e-01,  1.7817e-01,  3.7536e-01,  ...,  1.1056e-01,
             1.3408e-01,  2.9471e-01],
           [-4.8976e-01,  3.0995e-01,  4.0227e-01,  ...,  4.4883e-01,
            -2.6356e-01, -1.0082e-01],
           [ 1.9607e-01, -4.8531e-01,  1.5612e-01,  ..., -2.6745e-01,
             1.7448e-01,  4.6760e-01]]],


         [[[ 2.6617e-01, -9.8389e-02, -2.4920e-01,  ...,  3.8301e-01,
            -1.2562e-01,  4.5959e-01],
           [ 4.8349e-01,  1.1657e-02,  4.9442e-03,  ..., -4.6992e-01,
            -5.8805e-02,  2.8826e-04],
           [-4.2520e-01, -7.7481e-02, -2.7296e-01,  ...,  2.7640e-01,
             2.1825e-01,  2.6864e-01],
           [-1.1660e-01, -1.4789e-01, -3.5937e-01,  ..., -3.8014e-01,
            -8.2779e-02, -2.9195e-01]],

          [[ 1.0688e-01, -2.1881e-01,  1.2613e-01,  ..., -4.0004e-01,
            -2.9270e-01, -4.5403e-01],
           [-1.3428e-01, -2.7191e-01,  1.6688e-01,  ..., -1.1462e-01,
            -3.9849e-01,  6.8117e-02],
           [ 3.7877e-01,  2.1162e-01, -2.5340e-01,  ...,  4.1534e-01,
             2.0512e-01, -2.9103e-01],
           [-4.3866e-01, -2.2739e-01,  4.9077e-01,  ..., -4.8128e-01,
            -4.5948e-01,  2.3199e-01]],

          [[ 4.0427e-02,  2.1648e-01,  3.3029e-01,  ...,  4.2045e-01,
            -1.4242e-01,  2.4662e-01],
           [ 4.2627e-01,  1.8477e-01,  1.7970e-01,  ..., -3.8430e-01,
             3.7499e-01, -2.0656e-01],
           [ 1.7430e-01,  3.1898e-01,  2.1201e-01,  ..., -3.4649e-01,
             4.4795e-01, -1.2658e-01],
           [-5.4051e-02, -2.7920e-01,  2.2786e-01,  ..., -3.6672e-01,
             4.4994e-01,  4.4634e-01]],

          [[ 4.6950e-02, -1.8818e-01,  1.0445e-01,  ..., -1.3702e-01,
             4.2278e-01, -2.4494e-01],
           [ 4.0492e-01, -4.5300e-01, -1.3408e-01,  ...,  6.0043e-02,
             1.9615e-01, -1.3532e-01],
           [ 3.8618e-01,  1.1506e-01, -4.0852e-01,  ..., -2.6082e-01,
             4.4195e-01,  1.8528e-01],
           [ 1.3419e-01, -1.8027e-01,  1.0619e-02,  ..., -4.3501e-01,
            -6.1434e-02, -2.2014e-01]]]]])
2025-03-18 17:27:03.836961 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([3, 2, 4, 4, 4],"float32"),Tensor([2281701379],"float32"),], )

[not compare]  None tensor([[[[[ 0.3165,  0.4218,  0.3068, -0.0585],
           [ 0.1443,  0.3811, -0.3744, -0.2923],
           [ 0.0341,  0.0284, -0.1428,  0.1501],
           [-0.1187, -0.1696, -0.4729, -0.3101]],

          [[-0.2584,  0.3570,  0.4708,  0.1457],
           [-0.2800,  0.4879, -0.1119,  0.3189],
           [-0.2786,  0.0510, -0.2233, -0.0226],
           [-0.1858, -0.3360, -0.1109, -0.4660]],

          [[-0.3405,  0.3145, -0.0023,  0.1274],
           [ 0.2545,  0.3772,  0.0729, -0.2852],
           [ 0.1520,  0.3310,  0.1164,  0.0715],
           [ 0.4632,  0.2793, -0.0974, -0.2275]],

          [[-0.4165, -0.1875,  0.3416, -0.3229],
           [-0.2515, -0.4851,  0.4461, -0.3526],
           [ 0.2093,  0.4862, -0.3061, -0.0508],
           [-0.0510,  0.4162,  0.4735, -0.3667]]],


         [[[ 0.4159, -0.2952,  0.3444, -0.4542],
           [-0.3394,  0.2441, -0.3936,  0.4985],
           [ 0.1010,  0.4144,  0.0037,  0.4906],
           [-0.3887, -0.1126,  0.1303, -0.3685]],

          [[-0.0524, -0.1674, -0.3173, -0.0696],
           [ 0.4235,  0.3791,  0.2374, -0.2694],
           [ 0.3309,  0.2937, -0.3626, -0.3178],
           [-0.3435, -0.0205,  0.1807, -0.3061]],

          [[-0.0766, -0.1076, -0.3231, -0.0079],
           [ 0.1490,  0.2528, -0.3576, -0.0639],
           [ 0.0558,  0.1560, -0.2722,  0.1592],
           [-0.4321,  0.4129, -0.2853, -0.3078]],

          [[-0.2345, -0.1007, -0.3146,  0.2496],
           [-0.0673, -0.2223, -0.1163, -0.0917],
           [ 0.4960, -0.1371,  0.2319, -0.3249],
           [ 0.3735, -0.2087,  0.1657, -0.0976]]]],



        [[[[-0.1731,  0.1807,  0.4456, -0.4425],
           [-0.1853,  0.4923,  0.4489, -0.1643],
           [-0.3436, -0.4781,  0.3275, -0.4946],
           [ 0.4564,  0.2664, -0.1130, -0.4314]],

          [[ 0.3592,  0.2607,  0.1385,  0.2482],
           [-0.3936,  0.2581,  0.0024, -0.4727],
           [-0.0287,  0.3687, -0.4422, -0.3074],
           [ 0.2716, -0.2252, -0.3858, -0.0463]],

          [[-0.4644, -0.4251,  0.2686, -0.4870],
           [-0.3652,  0.1310, -0.4968,  0.0475],
           [-0.4594, -0.3620,  0.4630,  0.4510],
           [-0.3961, -0.1753, -0.2736,  0.3839]],

          [[-0.4630,  0.0676,  0.2054, -0.0020],
           [ 0.1546, -0.4424, -0.1614,  0.4167],
           [ 0.2875, -0.1946, -0.1165,  0.4244],
           [-0.1176, -0.4629,  0.0265,  0.3536]]],


         [[[ 0.0649, -0.3683,  0.3050,  0.4756],
           [ 0.3001,  0.1689, -0.0235, -0.0235],
           [ 0.1789,  0.4801,  0.1033, -0.1554],
           [ 0.0231, -0.2152,  0.4408,  0.0466]],

          [[ 0.3926, -0.3491, -0.0331,  0.0219],
           [-0.3193, -0.2319,  0.2782, -0.4603],
           [-0.4482, -0.1966, -0.1616,  0.0587],
           [-0.1872,  0.4569, -0.0200, -0.4112]],

          [[-0.0453,  0.4144, -0.0663,  0.3710],
           [ 0.0426, -0.2206,  0.3631, -0.0510],
           [ 0.2890,  0.1706,  0.4911, -0.0753],
           [ 0.1452,  0.4071,  0.3176, -0.3029]],

          [[-0.4047, -0.0140, -0.4078, -0.3644],
           [ 0.2642,  0.3934,  0.4974,  0.1647],
           [-0.1109,  0.3092, -0.2864,  0.1853],
           [-0.3991, -0.0783, -0.2104,  0.2593]]]],



        [[[[ 0.3514, -0.1257, -0.1534, -0.3946],
           [ 0.1975, -0.0074,  0.1793,  0.3940],
           [ 0.4700, -0.0381, -0.3039, -0.1678],
           [ 0.0584,  0.2394,  0.1346, -0.3845]],

          [[ 0.4594, -0.3825,  0.0206, -0.0858],
           [-0.3933,  0.0744, -0.4802,  0.2029],
           [-0.0756, -0.3595, -0.3356, -0.3808],
           [-0.1126, -0.0641, -0.0801,  0.4253]],

          [[ 0.3473, -0.2821,  0.3355,  0.3130],
           [-0.2274,  0.0640, -0.0345, -0.1173],
           [ 0.0390, -0.0335,  0.4002, -0.0304],
           [ 0.1637, -0.0447, -0.1177,  0.3111]],

          [[-0.0337,  0.3897,  0.1190,  0.3907],
           [ 0.2743, -0.4548,  0.1147,  0.0197],
           [-0.3022,  0.2534, -0.2110,  0.4957],
           [-0.1040,  0.0171, -0.4394,  0.3198]]],


         [[[-0.1376,  0.4309,  0.4314, -0.1596],
           [ 0.1371, -0.4695,  0.4254,  0.3177],
           [-0.0484,  0.0779, -0.3046,  0.4593],
           [ 0.0927,  0.1183, -0.0163, -0.3480]],

          [[-0.3183,  0.3175, -0.2496,  0.1948],
           [-0.0085,  0.0994, -0.3906,  0.1411],
           [ 0.2268,  0.2850, -0.1130,  0.1568],
           [ 0.4658,  0.2857, -0.1205, -0.3240]],

          [[-0.2335, -0.0637, -0.2786, -0.2461],
           [-0.0629,  0.3375, -0.4819, -0.0416],
           [ 0.3308,  0.4079,  0.0424,  0.1809],
           [ 0.2393,  0.1381, -0.3741,  0.4222]],

          [[-0.4597, -0.4661, -0.4237, -0.4959],
           [ 0.3501, -0.1275, -0.2300,  0.1288],
           [ 0.2410,  0.1676, -0.1464,  0.1379],
           [ 0.0801,  0.1966, -0.0632, -0.3358]]]]])
2025-03-18 17:28:33.754740 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([3, 2, 4, 4],"float32"),Tensor([2281701379],"float32"),], )

[not compare]  None tensor([[[[ 0.3473, -0.2821,  0.3355,  0.3130],
          [-0.2274,  0.0640, -0.0345, -0.1173],
          [ 0.0390, -0.0335,  0.4002, -0.0304],
          [ 0.1637, -0.0447, -0.1177,  0.3111]],

         [[-0.0337,  0.3897,  0.1190,  0.3907],
          [ 0.2743, -0.4548,  0.1147,  0.0197],
          [-0.3022,  0.2534, -0.2110,  0.4957],
          [-0.1040,  0.0171, -0.4394,  0.3198]]],


        [[[-0.1376,  0.4309,  0.4314, -0.1596],
          [ 0.1371, -0.4695,  0.4254,  0.3177],
          [-0.0484,  0.0779, -0.3046,  0.4593],
          [ 0.0927,  0.1183, -0.0163, -0.3480]],

         [[-0.3183,  0.3175, -0.2496,  0.1948],
          [-0.0085,  0.0994, -0.3906,  0.1411],
          [ 0.2268,  0.2850, -0.1130,  0.1568],
          [ 0.4658,  0.2857, -0.1205, -0.3240]]],


        [[[-0.2335, -0.0637, -0.2786, -0.2461],
          [-0.0629,  0.3375, -0.4819, -0.0416],
          [ 0.3308,  0.4079,  0.0424,  0.1809],
          [ 0.2393,  0.1381, -0.3741,  0.4222]],

         [[-0.4597, -0.4661, -0.4237, -0.4959],
          [ 0.3501, -0.1275, -0.2300,  0.1288],
          [ 0.2410,  0.1676, -0.1464,  0.1379],
          [ 0.0801,  0.1966, -0.0632, -0.3358]]]])
2025-03-18 17:30:04.777690 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([3, 2, 4, 95070891],"float32"),Tensor([3],"float32"),], )

[not compare]  None tensor([[[[ 0.2410,  0.1676, -0.1464,  ..., -0.0712, -0.3564,  0.3858],
          [-0.1008,  0.1308,  0.4327,  ...,  0.1602,  0.0993, -0.0580],
          [-0.0502, -0.4136,  0.1217,  ...,  0.3582, -0.2652, -0.3458],
          [ 0.2055,  0.1940, -0.0846,  ..., -0.2596, -0.1169,  0.0757]],

         [[-0.0507, -0.0550, -0.4955,  ...,  0.4189,  0.4019, -0.2047],
          [ 0.2601,  0.1850,  0.2345,  ...,  0.2757, -0.2872,  0.0985],
          [-0.1940, -0.2643,  0.0776,  ..., -0.1255,  0.3925,  0.4347],
          [ 0.0870, -0.0878, -0.0840,  ..., -0.3896,  0.2409,  0.3483]]],


        [[[-0.2917, -0.0866, -0.3758,  ...,  0.3851,  0.0629, -0.0590],
          [ 0.2727,  0.1233,  0.1775,  ...,  0.1768,  0.4345, -0.3935],
          [ 0.2378, -0.1763,  0.0225,  ..., -0.1118, -0.4383,  0.0728],
          [ 0.0352,  0.4979, -0.1841,  ..., -0.0753, -0.2609, -0.4415]],

         [[-0.0844, -0.4728, -0.1584,  ...,  0.4901,  0.3844,  0.0320],
          [-0.2935,  0.2790,  0.4086,  ..., -0.1276, -0.3058,  0.2492],
          [ 0.4870, -0.4499, -0.4498,  ..., -0.2662,  0.2062, -0.2503],
          [ 0.3405,  0.3203, -0.4341,  ..., -0.0256,  0.2126, -0.4349]]],


        [[[-0.0585, -0.4273,  0.1741,  ..., -0.1033, -0.1589, -0.4919],
          [ 0.2123, -0.4372,  0.3931,  ..., -0.4303,  0.0474, -0.1628],
          [-0.4185,  0.2072,  0.4271,  ...,  0.3383,  0.4883, -0.4614],
          [-0.4468, -0.4322,  0.3638,  ..., -0.0984, -0.2492, -0.1158]],

         [[-0.0631,  0.3279, -0.0202,  ...,  0.1069, -0.2188,  0.1261],
          [-0.3839,  0.3555,  0.3481,  ...,  0.2320,  0.0404,  0.2165],
          [ 0.3303,  0.1525, -0.1334,  ...,  0.4499,  0.4463,  0.0470],
          [-0.1882,  0.1045,  0.3678,  ..., -0.4350, -0.0614, -0.2201]]]])
2025-03-18 17:31:40.353552 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([3, 2, 4],"float32"),Tensor([2281701379],"float32"),], )

[not compare]  None tensor([[[ 0.3308,  0.4079,  0.0424,  0.1809],
         [ 0.2393,  0.1381, -0.3741,  0.4222]],

        [[-0.4597, -0.4661, -0.4237, -0.4959],
         [ 0.3501, -0.1275, -0.2300,  0.1288]],

        [[ 0.2410,  0.1676, -0.1464,  0.1379],
         [ 0.0801,  0.1966, -0.0632, -0.3358]]])
2025-03-18 17:33:23.472768 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([3, 2, 95070891, 4],"float32"),Tensor([3],"float32"),], )

[not compare]  None tensor([[[[ 2.4104e-01,  1.6759e-01, -1.4640e-01,  1.3786e-01],
          [ 8.0144e-02,  1.9655e-01, -6.3195e-02, -3.3585e-01],
          [ 1.6061e-01, -4.3819e-01, -4.1684e-01,  4.9294e-01],
          ...,
          [ 2.6328e-01,  2.9519e-01, -2.5643e-01, -4.5884e-01],
          [ 2.6757e-01, -2.6039e-01, -4.7349e-01,  3.8436e-01],
          [-2.1791e-01, -2.5958e-01, -1.1694e-01,  7.5750e-02]],

         [[-5.0659e-02, -5.4979e-02, -4.9553e-01, -1.3351e-01],
          [-3.7316e-01, -8.9571e-02,  3.6248e-01,  1.2136e-02],
          [-1.3494e-01, -2.9755e-02, -1.8883e-01,  4.2585e-01],
          ...,
          [-6.7197e-04,  3.1875e-01,  4.9769e-01, -4.8492e-01],
          [ 3.5163e-01,  4.0087e-01,  3.4764e-01,  4.9687e-01],
          [-1.1039e-01, -3.8959e-01,  2.4091e-01,  3.4829e-01]]],


        [[[-2.9171e-01, -8.6592e-02, -3.7584e-01, -1.5459e-01],
          [-3.7782e-02,  2.4303e-01,  2.3525e-02, -4.9869e-01],
          [-2.0824e-01, -4.4638e-01, -9.1700e-02,  4.6943e-01],
          ...,
          [-2.7694e-01,  2.7294e-01,  4.0491e-01,  3.6934e-01],
          [-4.2421e-01,  1.3181e-01, -4.2334e-01,  4.6433e-01],
          [-1.4680e-01, -7.5331e-02, -2.6091e-01, -4.4154e-01]],

         [[-8.4447e-02, -4.7281e-01, -1.5840e-01,  1.7175e-01],
          [-2.8202e-01, -4.4658e-01,  1.7639e-02,  3.3849e-01],
          [ 1.2025e-01,  4.2554e-01,  4.0232e-01,  4.0375e-01],
          ...,
          [ 4.0294e-01, -1.2486e-01,  3.0586e-01,  2.6442e-01],
          [-1.1135e-01,  7.4095e-02, -2.0682e-01,  4.5205e-01],
          [ 2.9411e-01, -2.5577e-02,  2.1258e-01, -4.3491e-01]]],


        [[[-5.8497e-02, -4.2731e-01,  1.7408e-01, -3.2637e-01],
          [-3.8298e-01,  1.3352e-01, -1.2106e-01,  3.0946e-01],
          [-1.3547e-01,  3.2543e-01, -1.6291e-01,  1.5496e-01],
          ...,
          [ 3.4139e-01,  3.0228e-01, -4.7056e-02,  4.5161e-01],
          [ 9.7663e-02, -2.6745e-01,  1.7448e-01,  4.6760e-01],
          [ 2.6617e-01, -9.8389e-02, -2.4920e-01, -1.1584e-01]],

         [[-6.3132e-02,  3.2793e-01, -2.0208e-02,  8.5292e-02],
          [ 2.0709e-01, -4.0282e-01,  1.3591e-03,  7.6865e-02],
          [-1.0579e-01, -2.0682e-01,  2.7525e-01, -3.1466e-01],
          ...,
          [ 3.2146e-01, -3.6948e-04, -3.2227e-01, -2.8674e-01],
          [-2.8261e-01,  1.4893e-01,  4.1823e-01, -3.7154e-01],
          [ 5.9622e-02, -4.3501e-01, -6.1434e-02, -2.2014e-01]]]])
2025-03-18 17:35:06.976194 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([3, 47535446, 4, 4],"float32"),Tensor([3],"float32"),], )

[not compare]  None tensor([[[[-2.3352e-01, -6.3720e-02, -2.7855e-01, -2.4608e-01],
          [-6.2878e-02,  3.3746e-01, -4.8186e-01, -4.1635e-02],
          [ 3.3076e-01,  4.0787e-01,  4.2444e-02,  1.8086e-01],
          [ 2.3930e-01,  1.3805e-01, -3.7408e-01,  4.2218e-01]],

         [[-4.5966e-01, -4.6613e-01, -4.2367e-01, -4.9590e-01],
          [ 3.5007e-01, -1.2753e-01, -2.2997e-01,  1.2884e-01],
          [ 2.4104e-01,  1.6759e-01, -1.4640e-01,  1.3786e-01],
          [ 8.0144e-02,  1.9655e-01, -6.3195e-02, -3.3585e-01]],

         [[ 1.6061e-01, -4.3819e-01, -4.1684e-01,  4.9294e-01],
          [ 8.5180e-02,  3.9437e-02, -1.0828e-01, -4.7164e-01],
          [ 2.5115e-02, -3.3898e-02,  3.2395e-01, -1.7367e-01],
          [-1.4246e-02,  3.5848e-01, -3.8309e-01,  1.8092e-01]],

         ...,

         [[-5.8249e-02, -3.9252e-01, -5.7541e-02, -2.2755e-01],
          [-1.0274e-01,  2.2919e-02, -3.2786e-01, -1.4478e-01],
          [-4.5627e-01, -3.7104e-01, -4.9889e-01,  2.5705e-03],
          [ 3.8165e-01, -4.2521e-01,  7.1672e-02, -1.3438e-01]],

         [[-7.8715e-02,  4.5516e-01,  2.8498e-01, -3.0749e-01],
          [-4.6217e-01, -4.6866e-01, -8.2155e-02, -6.3671e-02],
          [ 4.6678e-01,  4.6853e-01, -3.0515e-01,  3.8544e-01],
          [-2.1352e-01,  2.3147e-01,  3.0656e-01, -3.7105e-01]],

         [[-2.5187e-02,  2.2679e-01,  1.7154e-01,  1.3388e-01],
          [-3.3866e-01,  3.1385e-01,  9.4947e-02, -9.0630e-02],
          [-4.7973e-01, -2.2178e-01, -1.6818e-02, -1.2409e-01],
          [-3.2081e-01, -1.5254e-01,  2.7752e-01,  3.8550e-01]]],


        [[[ 1.9694e-01, -4.1135e-01,  1.6358e-01, -3.5123e-01],
          [-6.7197e-04,  3.1875e-01,  4.9769e-01, -4.8492e-01],
          [ 3.5163e-01,  4.0087e-01,  3.4764e-01,  4.9687e-01],
          [-1.1039e-01, -3.8959e-01,  2.4091e-01,  3.4829e-01]],

         [[-2.9171e-01, -8.6592e-02, -3.7584e-01, -1.5459e-01],
          [-3.7782e-02,  2.4303e-01,  2.3525e-02, -4.9869e-01],
          [-2.0824e-01, -4.4638e-01, -9.1700e-02,  4.6943e-01],
          [ 4.3443e-01,  8.7745e-02,  3.3991e-01, -3.4918e-01]],

         [[ 3.1012e-01,  3.5128e-01, -3.3426e-01,  9.4020e-02],
          [-4.6863e-01,  2.1891e-01,  4.7435e-01,  1.2338e-04],
          [ 1.1010e-01,  4.7055e-01,  4.4302e-01,  3.2367e-01],
          [-3.5106e-01,  1.1032e-01,  2.2755e-01, -4.9951e-01]],

         ...,

         [[ 1.0819e-01, -1.3469e-01, -4.7016e-01,  4.8897e-01],
          [ 1.9809e-01, -3.1948e-01, -2.6128e-01,  3.9779e-01],
          [-4.5074e-01,  3.3029e-02,  1.6245e-01, -3.3097e-01],
          [ 3.6720e-01, -3.8066e-01,  6.0682e-02, -2.0752e-01]],

         [[ 4.3178e-01,  4.8681e-01, -1.1679e-01,  8.1086e-02],
          [ 3.7312e-01,  3.6753e-01,  2.2301e-01, -1.5109e-01],
          [-8.8730e-02, -3.2674e-01, -1.6343e-01,  3.1051e-01],
          [-7.6643e-02,  4.0792e-01,  6.5141e-02, -1.7577e-02]],

         [[ 3.0762e-01,  2.6640e-01,  2.5333e-01,  3.3109e-01],
          [-8.5369e-03,  2.5417e-01,  3.8495e-01,  3.8020e-01],
          [ 1.8479e-01,  1.2777e-02,  1.8449e-01, -2.6130e-01],
          [ 4.0294e-01, -1.2486e-01,  3.0586e-01,  2.6442e-01]]],


        [[[-1.1135e-01,  7.4095e-02, -2.0682e-01,  4.5205e-01],
          [ 2.9411e-01, -2.5577e-02,  2.1258e-01, -4.3491e-01],
          [-5.8497e-02, -4.2731e-01,  1.7408e-01, -3.2637e-01],
          [-3.8298e-01,  1.3352e-01, -1.2106e-01,  3.0946e-01]],

         [[-1.3547e-01,  3.2543e-01, -1.6291e-01,  1.5496e-01],
          [ 2.8618e-01,  5.4538e-02, -2.4109e-01, -3.6821e-02],
          [-4.6692e-01, -3.6228e-01,  4.6054e-01,  3.7113e-01],
          [ 3.2793e-01,  2.5790e-01, -1.7884e-01,  4.0224e-01]],

         [[-5.2521e-02,  4.2979e-01,  2.2459e-03,  4.0358e-01],
          [ 3.5386e-01,  6.0339e-02,  3.3523e-01, -1.2252e-01],
          [ 2.2666e-01, -2.3444e-01, -4.7742e-01, -3.6950e-01],
          [-3.8800e-01,  3.3065e-01,  2.9815e-02,  2.6448e-01]],

         ...,

         [[ 4.5383e-02, -4.0807e-01,  4.4441e-01, -2.0194e-01],
          [ 2.2768e-01,  1.4779e-01,  4.9682e-01,  2.5804e-02],
          [-1.3775e-01, -2.2515e-01,  3.9518e-01,  1.0166e-01],
          [ 2.8747e-01, -2.0259e-01, -5.2554e-02, -3.0652e-01]],

         [[-1.9806e-01, -4.4153e-01, -3.6861e-01, -4.8663e-01],
          [-4.9562e-02, -5.5540e-02, -2.2558e-01,  4.1513e-01],
          [ 3.5180e-01, -3.5563e-01, -1.9798e-01,  8.4594e-02],
          [-1.8029e-01, -4.2807e-01, -4.2619e-01,  2.9464e-01]],

         [[-1.6814e-01,  2.2974e-01, -4.3483e-01, -4.7804e-02],
          [ 3.2146e-01, -3.6948e-04, -3.2227e-01, -2.8674e-01],
          [-2.8261e-01,  1.4893e-01,  4.1823e-01, -3.7154e-01],
          [ 5.9622e-02, -4.3501e-01, -6.1434e-02, -2.2014e-01]]]])
2025-03-18 17:36:47.763569 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([48, 47535446],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[not compare]  None tensor([[ 0.0516, -0.4857, -0.1165,  ...,  0.3138, -0.2301, -0.4487],
        [ 0.1896, -0.3067, -0.1745,  ...,  0.3864, -0.4189,  0.0359],
        [-0.2119, -0.3607,  0.3859,  ...,  0.4162,  0.0755,  0.4010],
        ...,
        [ 0.1599,  0.1439,  0.0705,  ..., -0.2264, -0.1810,  0.0025],
        [ 0.3445,  0.3731, -0.1246,  ...,  0.0269,  0.0977, -0.4974],
        [ 0.0812,  0.1912, -0.3957,  ..., -0.4287, -0.4389, -0.0568]])
2025-03-18 17:38:31.591520 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([48, 8],"float32"),Tensor([142606337, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[not compare]  None tensor([[ 0.4578, -0.1037,  0.4622, -0.3497,  0.1096, -0.3959, -0.2043,  0.3085],
        [ 0.1580, -0.0985, -0.3931,  0.4471, -0.2505,  0.3916, -0.0197,  0.1271],
        [-0.4143, -0.1687,  0.1942,  0.4851,  0.4403,  0.3838, -0.1531, -0.1247],
        [-0.4961, -0.4127, -0.0802,  0.1241,  0.3849, -0.4658, -0.0874,  0.1340],
        [ 0.0155,  0.3528,  0.1664, -0.1271, -0.0976, -0.1577, -0.3023, -0.4323],
        [ 0.0523, -0.2699, -0.1617, -0.4707, -0.3297, -0.4891,  0.2864, -0.3375],
        [-0.4816,  0.1767,  0.1200,  0.2820,  0.4766, -0.4815, -0.3220, -0.4353],
        [ 0.4183, -0.3823,  0.3825,  0.4679, -0.2940,  0.0906, -0.1810, -0.2490],
        [ 0.3050,  0.3440, -0.2218,  0.4162, -0.3939, -0.3226, -0.2174,  0.0893],
        [ 0.1766, -0.1116, -0.2683,  0.2699, -0.4031, -0.3656,  0.0431,  0.2696],
        [ 0.3855, -0.1065, -0.3909,  0.3784,  0.3302, -0.3792, -0.4497,  0.2620],
        [ 0.0785, -0.4673, -0.2081,  0.3840, -0.0495, -0.2937, -0.3219, -0.0219],
        [-0.2248, -0.0694, -0.2595, -0.1061, -0.1763,  0.4850, -0.0061,  0.4196],
        [-0.2074,  0.3409, -0.1221,  0.2268, -0.4060, -0.4904, -0.0438, -0.4406],
        [ 0.4226, -0.4894, -0.4023,  0.4843,  0.2943,  0.0602,  0.2924, -0.3022],
        [ 0.4798,  0.1477, -0.0323,  0.4511, -0.2382,  0.1157,  0.2862,  0.2114],
        [ 0.3811,  0.4219, -0.3472,  0.3415,  0.3151, -0.0637,  0.0405, -0.0501],
        [ 0.1021, -0.4367, -0.1885,  0.2110, -0.4300, -0.2573, -0.4771,  0.0343],
        [-0.2568,  0.0808, -0.4020,  0.4706, -0.4080,  0.2926,  0.1629,  0.0625],
        [-0.3563, -0.4548, -0.3033,  0.1399,  0.2135,  0.1628, -0.4540,  0.2617],
        [ 0.3071, -0.0313,  0.4496,  0.2198, -0.3120, -0.2146,  0.1683,  0.1731],
        [ 0.4509, -0.0628, -0.3913, -0.0501, -0.1874,  0.1260,  0.3439,  0.4647],
        [-0.0335, -0.1991,  0.0266, -0.3324,  0.3668, -0.1406, -0.4826,  0.3528],
        [ 0.1817,  0.2238,  0.1592, -0.3516, -0.1199, -0.1376, -0.1106, -0.4238],
        [ 0.3973, -0.1878, -0.2352,  0.0580, -0.3406,  0.3911,  0.0430, -0.1678],
        [ 0.4974, -0.0835, -0.1006, -0.2600, -0.4489, -0.1236, -0.1854,  0.1365],
        [-0.3078,  0.3592, -0.1524, -0.4135, -0.0318, -0.1125,  0.0524, -0.4706],
        [ 0.4918,  0.2554, -0.0036, -0.0450,  0.2225, -0.1228,  0.2551, -0.4864],
        [ 0.4857, -0.2388,  0.2820,  0.0188,  0.1230,  0.0854, -0.1833,  0.3588],
        [ 0.2176,  0.1391,  0.3359, -0.1934, -0.2191, -0.3311, -0.4560,  0.0518],
        [ 0.0076, -0.1766, -0.0204, -0.1558,  0.1202, -0.1774, -0.1179, -0.1083],
        [ 0.2909,  0.3455,  0.2782, -0.3135,  0.4290, -0.2964,  0.0728, -0.2967],
        [ 0.0806,  0.3625, -0.4605, -0.4280,  0.3781,  0.2210,  0.3288, -0.3144],
        [-0.0228,  0.2920,  0.1622, -0.3877, -0.1832, -0.3444,  0.0203,  0.3810],
        [ 0.3164, -0.0751, -0.0516,  0.4027, -0.2697, -0.4086,  0.0899, -0.2546],
        [ 0.1857, -0.0353,  0.4027, -0.0254, -0.2364,  0.1646,  0.4239,  0.2072],
        [-0.2543, -0.1614, -0.1072,  0.2229, -0.4212,  0.0509, -0.2956, -0.2826],
        [-0.1663, -0.4290, -0.3465, -0.0357,  0.2751,  0.2445,  0.0971, -0.1872],
        [-0.4698,  0.3407,  0.1605,  0.2837,  0.1058,  0.1352, -0.0582,  0.1943],
        [-0.0607,  0.1697,  0.0699,  0.3292, -0.4934, -0.0476, -0.2101, -0.3174],
        [ 0.2621,  0.1162, -0.2791, -0.2568,  0.0500, -0.2562, -0.0380,  0.0756],
        [-0.0822,  0.0556,  0.0015,  0.0940,  0.4283,  0.0880,  0.0201, -0.2758],
        [ 0.0244,  0.3077,  0.2039,  0.3785, -0.3409, -0.2637, -0.2795, -0.3423],
        [ 0.4753,  0.2763,  0.0659, -0.3247,  0.0012, -0.0723,  0.1390, -0.1850],
        [ 0.0234, -0.2491, -0.4229,  0.0630, -0.0596, -0.1567,  0.2292,  0.2517],
        [-0.0885,  0.3063, -0.3435, -0.1879,  0.2093,  0.3252, -0.1042,  0.3412],
        [ 0.2620, -0.1519, -0.0180, -0.2640, -0.3331, -0.0609, -0.0044,  0.0969],
        [-0.3671,  0.1189,  0.0099,  0.4776,  0.0431, -0.3673,  0.1640, -0.0456]])
2025-03-18 17:40:22.524473 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([2281701379],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[not compare]  None tensor([[-0.0372, -0.4361, -0.3755,  0.2082,  0.0346,  0.1829, -0.2872,  0.2302],
        [-0.2721, -0.2252, -0.0805,  0.1670,  0.2779,  0.4490, -0.1109, -0.1896],
        [ 0.4440, -0.0911,  0.3861,  0.1138,  0.2960,  0.0955,  0.4091,  0.4683],
        [ 0.4966, -0.4560, -0.2645,  0.0455, -0.4307, -0.1242,  0.1781, -0.1539],
        [-0.0828,  0.3690,  0.3446, -0.1205, -0.1902, -0.1266, -0.1974, -0.3179],
        [ 0.0779, -0.1539,  0.1904,  0.4242, -0.4555, -0.2741, -0.3668, -0.2246],
        [-0.3389, -0.3306, -0.3351, -0.3904, -0.4631, -0.4981, -0.2836,  0.0795],
        [ 0.1997,  0.3252, -0.2195,  0.4920,  0.1590, -0.1666,  0.1603, -0.0590],
        [ 0.4110,  0.3595,  0.1949, -0.3568, -0.0082, -0.3976,  0.1620, -0.3262],
        [-0.4269,  0.2231,  0.4087, -0.2084, -0.0739, -0.3792,  0.0627, -0.1287],
        [-0.1398,  0.2823,  0.1534,  0.2706,  0.2614,  0.1931, -0.4682, -0.2043],
        [ 0.2567,  0.0005,  0.2640,  0.1807, -0.2019, -0.4829, -0.4538,  0.0216],
        [-0.4008, -0.2951,  0.3589,  0.1860, -0.3273, -0.3131,  0.1339, -0.4619],
        [ 0.2490,  0.2637,  0.1881,  0.4550,  0.0007,  0.4825,  0.1627, -0.0712],
        [-0.3970,  0.1599,  0.3025, -0.1037, -0.3589,  0.3129,  0.3726, -0.2790],
        [ 0.0907, -0.3790,  0.2606, -0.3659,  0.4499, -0.0732,  0.4978, -0.0607],
        [ 0.0887, -0.1145,  0.3577,  0.2392,  0.0412, -0.0357, -0.2875, -0.2923],
        [ 0.3180,  0.1469,  0.2894,  0.2007, -0.1344, -0.3721,  0.1521, -0.1810],
        [ 0.1652, -0.4759,  0.1323,  0.0850, -0.0501,  0.4218, -0.1979,  0.0983],
        [ 0.1556,  0.2690, -0.0533, -0.0802, -0.0493,  0.2728,  0.3394,  0.2200],
        [ 0.4126, -0.3183,  0.1013, -0.4693,  0.1854,  0.3110,  0.1743, -0.2411],
        [-0.0987,  0.4821,  0.4218,  0.1600, -0.3737,  0.0856,  0.3606,  0.0279],
        [-0.3711, -0.2190,  0.2529, -0.4708,  0.2660,  0.2054, -0.3908,  0.4011],
        [-0.2795,  0.2463,  0.0465, -0.2408, -0.1926,  0.3313,  0.0363,  0.2083],
        [-0.1764, -0.1444,  0.4457,  0.3767,  0.1743, -0.3119, -0.2285, -0.3084],
        [-0.3070,  0.1567, -0.3245, -0.4091, -0.1182,  0.0833, -0.0107, -0.1737],
        [-0.0137, -0.0621,  0.2921,  0.2913,  0.1191, -0.1671,  0.3269,  0.3537],
        [ 0.3134, -0.1665,  0.4791, -0.3418,  0.4152, -0.0060,  0.1076,  0.4867],
        [-0.3185, -0.2656,  0.0564, -0.3084, -0.3664,  0.0411, -0.0787,  0.3425],
        [ 0.3226, -0.2670, -0.3910, -0.1752,  0.3057,  0.1430, -0.1469, -0.3863],
        [ 0.4202, -0.4832, -0.4889, -0.3943, -0.0184,  0.4235,  0.3025, -0.4843],
        [ 0.3772,  0.2081,  0.0857,  0.2520,  0.1115, -0.2595,  0.2708,  0.3600],
        [ 0.3390, -0.1593, -0.4807, -0.0704, -0.4454,  0.2271, -0.1281,  0.1581],
        [-0.0647,  0.4230,  0.3777,  0.4369,  0.2871,  0.3171,  0.0577,  0.2174],
        [-0.2672, -0.3422,  0.0847,  0.1245, -0.2525,  0.3133,  0.0283,  0.1959],
        [-0.3637,  0.4368,  0.2714, -0.3598,  0.0012, -0.1758, -0.2610, -0.2156],
        [ 0.4413,  0.0265,  0.2671,  0.3995,  0.1018, -0.4748,  0.4709,  0.0195],
        [ 0.0813,  0.4882,  0.2632, -0.0532, -0.0115,  0.0263, -0.1886, -0.3101],
        [-0.1762,  0.2320, -0.1651,  0.0516, -0.4857, -0.1165,  0.1032, -0.1769],
        [-0.0786, -0.2890,  0.3183, -0.0375, -0.1844,  0.1287, -0.3335,  0.1180],
        [ 0.3273,  0.2581, -0.1516,  0.1070, -0.1246, -0.4044,  0.4655, -0.4782],
        [ 0.1274,  0.3500, -0.2248, -0.1004, -0.2077,  0.4216,  0.0053,  0.2730],
        [-0.0467, -0.1527, -0.4243,  0.0620, -0.1061,  0.2113,  0.4427, -0.0711],
        [-0.1655,  0.3286,  0.0771,  0.4784, -0.0048, -0.0387,  0.3593, -0.3564],
        [-0.4257,  0.3465,  0.3090, -0.4407,  0.2289, -0.2786,  0.2981,  0.0301],
        [-0.1827,  0.0478, -0.2370,  0.1656, -0.3935, -0.1562, -0.0741, -0.4759],
        [-0.2951, -0.0472, -0.3158,  0.4825,  0.1639, -0.2768, -0.3074,  0.3422],
        [ 0.2809,  0.0079,  0.0416, -0.0406, -0.0541,  0.0148, -0.0902, -0.2984]])
2025-03-18 17:42:05.334099 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([2281701379],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[not compare]  None tensor([[-0.0372, -0.4361, -0.3755,  0.2082,  0.0346,  0.1829, -0.2872,  0.2302],
        [-0.2721, -0.2252, -0.0805,  0.1670,  0.2779,  0.4490, -0.1109, -0.1896],
        [ 0.4440, -0.0911,  0.3861,  0.1138,  0.2960,  0.0955,  0.4091,  0.4683],
        [ 0.4966, -0.4560, -0.2645,  0.0455, -0.4307, -0.1242,  0.1781, -0.1539],
        [-0.0828,  0.3690,  0.3446, -0.1205, -0.1902, -0.1266, -0.1974, -0.3179],
        [ 0.0779, -0.1539,  0.1904,  0.4242, -0.4555, -0.2741, -0.3668, -0.2246],
        [-0.3389, -0.3306, -0.3351, -0.3904, -0.4631, -0.4981, -0.2836,  0.0795],
        [ 0.1997,  0.3252, -0.2195,  0.4920,  0.1590, -0.1666,  0.1603, -0.0590],
        [ 0.4110,  0.3595,  0.1949, -0.3568, -0.0082, -0.3976,  0.1620, -0.3262],
        [-0.4269,  0.2231,  0.4087, -0.2084, -0.0739, -0.3792,  0.0627, -0.1287],
        [-0.1398,  0.2823,  0.1534,  0.2706,  0.2614,  0.1931, -0.4682, -0.2043],
        [ 0.2567,  0.0005,  0.2640,  0.1807, -0.2019, -0.4829, -0.4538,  0.0216],
        [-0.4008, -0.2951,  0.3589,  0.1860, -0.3273, -0.3131,  0.1339, -0.4619],
        [ 0.2490,  0.2637,  0.1881,  0.4550,  0.0007,  0.4825,  0.1627, -0.0712],
        [-0.3970,  0.1599,  0.3025, -0.1037, -0.3589,  0.3129,  0.3726, -0.2790],
        [ 0.0907, -0.3790,  0.2606, -0.3659,  0.4499, -0.0732,  0.4978, -0.0607],
        [ 0.0887, -0.1145,  0.3577,  0.2392,  0.0412, -0.0357, -0.2875, -0.2923],
        [ 0.3180,  0.1469,  0.2894,  0.2007, -0.1344, -0.3721,  0.1521, -0.1810],
        [ 0.1652, -0.4759,  0.1323,  0.0850, -0.0501,  0.4218, -0.1979,  0.0983],
        [ 0.1556,  0.2690, -0.0533, -0.0802, -0.0493,  0.2728,  0.3394,  0.2200],
        [ 0.4126, -0.3183,  0.1013, -0.4693,  0.1854,  0.3110,  0.1743, -0.2411],
        [-0.0987,  0.4821,  0.4218,  0.1600, -0.3737,  0.0856,  0.3606,  0.0279],
        [-0.3711, -0.2190,  0.2529, -0.4708,  0.2660,  0.2054, -0.3908,  0.4011],
        [-0.2795,  0.2463,  0.0465, -0.2408, -0.1926,  0.3313,  0.0363,  0.2083],
        [-0.1764, -0.1444,  0.4457,  0.3767,  0.1743, -0.3119, -0.2285, -0.3084],
        [-0.3070,  0.1567, -0.3245, -0.4091, -0.1182,  0.0833, -0.0107, -0.1737],
        [-0.0137, -0.0621,  0.2921,  0.2913,  0.1191, -0.1671,  0.3269,  0.3537],
        [ 0.3134, -0.1665,  0.4791, -0.3418,  0.4152, -0.0060,  0.1076,  0.4867],
        [-0.3185, -0.2656,  0.0564, -0.3084, -0.3664,  0.0411, -0.0787,  0.3425],
        [ 0.3226, -0.2670, -0.3910, -0.1752,  0.3057,  0.1430, -0.1469, -0.3863],
        [ 0.4202, -0.4832, -0.4889, -0.3943, -0.0184,  0.4235,  0.3025, -0.4843],
        [ 0.3772,  0.2081,  0.0857,  0.2520,  0.1115, -0.2595,  0.2708,  0.3600],
        [ 0.3390, -0.1593, -0.4807, -0.0704, -0.4454,  0.2271, -0.1281,  0.1581],
        [-0.0647,  0.4230,  0.3777,  0.4369,  0.2871,  0.3171,  0.0577,  0.2174],
        [-0.2672, -0.3422,  0.0847,  0.1245, -0.2525,  0.3133,  0.0283,  0.1959],
        [-0.3637,  0.4368,  0.2714, -0.3598,  0.0012, -0.1758, -0.2610, -0.2156],
        [ 0.4413,  0.0265,  0.2671,  0.3995,  0.1018, -0.4748,  0.4709,  0.0195],
        [ 0.0813,  0.4882,  0.2632, -0.0532, -0.0115,  0.0263, -0.1886, -0.3101],
        [-0.1762,  0.2320, -0.1651,  0.0516, -0.4857, -0.1165,  0.1032, -0.1769],
        [-0.0786, -0.2890,  0.3183, -0.0375, -0.1844,  0.1287, -0.3335,  0.1180],
        [ 0.3273,  0.2581, -0.1516,  0.1070, -0.1246, -0.4044,  0.4655, -0.4782],
        [ 0.1274,  0.3500, -0.2248, -0.1004, -0.2077,  0.4216,  0.0053,  0.2730],
        [-0.0467, -0.1527, -0.4243,  0.0620, -0.1061,  0.2113,  0.4427, -0.0711],
        [-0.1655,  0.3286,  0.0771,  0.4784, -0.0048, -0.0387,  0.3593, -0.3564],
        [-0.4257,  0.3465,  0.3090, -0.4407,  0.2289, -0.2786,  0.2981,  0.0301],
        [-0.1827,  0.0478, -0.2370,  0.1656, -0.3935, -0.1562, -0.0741, -0.4759],
        [-0.2951, -0.0472, -0.3158,  0.4825,  0.1639, -0.2768, -0.3074,  0.3422],
        [ 0.2809,  0.0079,  0.0416, -0.0406, -0.0541,  0.0148, -0.0902, -0.2984]])
2025-03-18 17:43:39.894442 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([142606337, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[not compare]  None tensor([[ 0.4578, -0.1037,  0.4622, -0.3497,  0.1096, -0.3959, -0.2043,  0.3085],
        [ 0.1580, -0.0985, -0.3931,  0.4471, -0.2505,  0.3916, -0.0197,  0.1271],
        [-0.4143, -0.1687,  0.1942,  0.4851,  0.4403,  0.3838, -0.1531, -0.1247],
        [-0.4961, -0.4127, -0.0802,  0.1241,  0.3849, -0.4658, -0.0874,  0.1340],
        [ 0.0155,  0.3528,  0.1664, -0.1271, -0.0976, -0.1577, -0.3023, -0.4323],
        [ 0.0523, -0.2699, -0.1617, -0.4707, -0.3297, -0.4891,  0.2864, -0.3375],
        [-0.4816,  0.1767,  0.1200,  0.2820,  0.4766, -0.4815, -0.3220, -0.4353],
        [ 0.4183, -0.3823,  0.3825,  0.4679, -0.2940,  0.0906, -0.1810, -0.2490],
        [ 0.3050,  0.3440, -0.2218,  0.4162, -0.3939, -0.3226, -0.2174,  0.0893],
        [ 0.1766, -0.1116, -0.2683,  0.2699, -0.4031, -0.3656,  0.0431,  0.2696],
        [ 0.3855, -0.1065, -0.3909,  0.3784,  0.3302, -0.3792, -0.4497,  0.2620],
        [ 0.0785, -0.4673, -0.2081,  0.3840, -0.0495, -0.2937, -0.3219, -0.0219],
        [-0.2248, -0.0694, -0.2595, -0.1061, -0.1763,  0.4850, -0.0061,  0.4196],
        [-0.2074,  0.3409, -0.1221,  0.2268, -0.4060, -0.4904, -0.0438, -0.4406],
        [ 0.4226, -0.4894, -0.4023,  0.4843,  0.2943,  0.0602,  0.2924, -0.3022],
        [ 0.4798,  0.1477, -0.0323,  0.4511, -0.2382,  0.1157,  0.2862,  0.2114],
        [ 0.3811,  0.4219, -0.3472,  0.3415,  0.3151, -0.0637,  0.0405, -0.0501],
        [ 0.1021, -0.4367, -0.1885,  0.2110, -0.4300, -0.2573, -0.4771,  0.0343],
        [-0.2568,  0.0808, -0.4020,  0.4706, -0.4080,  0.2926,  0.1629,  0.0625],
        [-0.3563, -0.4548, -0.3033,  0.1399,  0.2135,  0.1628, -0.4540,  0.2617],
        [ 0.3071, -0.0313,  0.4496,  0.2198, -0.3120, -0.2146,  0.1683,  0.1731],
        [ 0.4509, -0.0628, -0.3913, -0.0501, -0.1874,  0.1260,  0.3439,  0.4647],
        [-0.0335, -0.1991,  0.0266, -0.3324,  0.3668, -0.1406, -0.4826,  0.3528],
        [ 0.1817,  0.2238,  0.1592, -0.3516, -0.1199, -0.1376, -0.1106, -0.4238],
        [ 0.3973, -0.1878, -0.2352,  0.0580, -0.3406,  0.3911,  0.0430, -0.1678],
        [ 0.4974, -0.0835, -0.1006, -0.2600, -0.4489, -0.1236, -0.1854,  0.1365],
        [-0.3078,  0.3592, -0.1524, -0.4135, -0.0318, -0.1125,  0.0524, -0.4706],
        [ 0.4918,  0.2554, -0.0036, -0.0450,  0.2225, -0.1228,  0.2551, -0.4864],
        [ 0.4857, -0.2388,  0.2820,  0.0188,  0.1230,  0.0854, -0.1833,  0.3588],
        [ 0.2176,  0.1391,  0.3359, -0.1934, -0.2191, -0.3311, -0.4560,  0.0518],
        [ 0.0076, -0.1766, -0.0204, -0.1558,  0.1202, -0.1774, -0.1179, -0.1083],
        [ 0.2909,  0.3455,  0.2782, -0.3135,  0.4290, -0.2964,  0.0728, -0.2967],
        [ 0.0806,  0.3625, -0.4605, -0.4280,  0.3781,  0.2210,  0.3288, -0.3144],
        [-0.0228,  0.2920,  0.1622, -0.3877, -0.1832, -0.3444,  0.0203,  0.3810],
        [ 0.3164, -0.0751, -0.0516,  0.4027, -0.2697, -0.4086,  0.0899, -0.2546],
        [ 0.1857, -0.0353,  0.4027, -0.0254, -0.2364,  0.1646,  0.4239,  0.2072],
        [-0.2543, -0.1614, -0.1072,  0.2229, -0.4212,  0.0509, -0.2956, -0.2826],
        [-0.1663, -0.4290, -0.3465, -0.0357,  0.2751,  0.2445,  0.0971, -0.1872],
        [-0.4698,  0.3407,  0.1605,  0.2837,  0.1058,  0.1352, -0.0582,  0.1943],
        [-0.0607,  0.1697,  0.0699,  0.3292, -0.4934, -0.0476, -0.2101, -0.3174],
        [ 0.2621,  0.1162, -0.2791, -0.2568,  0.0500, -0.2562, -0.0380,  0.0756],
        [-0.0822,  0.0556,  0.0015,  0.0940,  0.4283,  0.0880,  0.0201, -0.2758],
        [ 0.0244,  0.3077,  0.2039,  0.3785, -0.3409, -0.2637, -0.2795, -0.3423],
        [ 0.4753,  0.2763,  0.0659, -0.3247,  0.0012, -0.0723,  0.1390, -0.1850],
        [ 0.0234, -0.2491, -0.4229,  0.0630, -0.0596, -0.1567,  0.2292,  0.2517],
        [-0.0885,  0.3063, -0.3435, -0.1879,  0.2093,  0.3252, -0.1042,  0.3412],
        [ 0.2620, -0.1519, -0.0180, -0.2640, -0.3331, -0.0609, -0.0044,  0.0969],
        [-0.3671,  0.1189,  0.0099,  0.4776,  0.0431, -0.3673,  0.1640, -0.0456]])
2025-03-18 17:45:23.929622 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([142606337, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[not compare]  None tensor([[ 0.4578, -0.1037,  0.4622, -0.3497,  0.1096, -0.3959, -0.2043,  0.3085],
        [ 0.1580, -0.0985, -0.3931,  0.4471, -0.2505,  0.3916, -0.0197,  0.1271],
        [-0.4143, -0.1687,  0.1942,  0.4851,  0.4403,  0.3838, -0.1531, -0.1247],
        [-0.4961, -0.4127, -0.0802,  0.1241,  0.3849, -0.4658, -0.0874,  0.1340],
        [ 0.0155,  0.3528,  0.1664, -0.1271, -0.0976, -0.1577, -0.3023, -0.4323],
        [ 0.0523, -0.2699, -0.1617, -0.4707, -0.3297, -0.4891,  0.2864, -0.3375],
        [-0.4816,  0.1767,  0.1200,  0.2820,  0.4766, -0.4815, -0.3220, -0.4353],
        [ 0.4183, -0.3823,  0.3825,  0.4679, -0.2940,  0.0906, -0.1810, -0.2490],
        [ 0.3050,  0.3440, -0.2218,  0.4162, -0.3939, -0.3226, -0.2174,  0.0893],
        [ 0.1766, -0.1116, -0.2683,  0.2699, -0.4031, -0.3656,  0.0431,  0.2696],
        [ 0.3855, -0.1065, -0.3909,  0.3784,  0.3302, -0.3792, -0.4497,  0.2620],
        [ 0.0785, -0.4673, -0.2081,  0.3840, -0.0495, -0.2937, -0.3219, -0.0219],
        [-0.2248, -0.0694, -0.2595, -0.1061, -0.1763,  0.4850, -0.0061,  0.4196],
        [-0.2074,  0.3409, -0.1221,  0.2268, -0.4060, -0.4904, -0.0438, -0.4406],
        [ 0.4226, -0.4894, -0.4023,  0.4843,  0.2943,  0.0602,  0.2924, -0.3022],
        [ 0.4798,  0.1477, -0.0323,  0.4511, -0.2382,  0.1157,  0.2862,  0.2114],
        [ 0.3811,  0.4219, -0.3472,  0.3415,  0.3151, -0.0637,  0.0405, -0.0501],
        [ 0.1021, -0.4367, -0.1885,  0.2110, -0.4300, -0.2573, -0.4771,  0.0343],
        [-0.2568,  0.0808, -0.4020,  0.4706, -0.4080,  0.2926,  0.1629,  0.0625],
        [-0.3563, -0.4548, -0.3033,  0.1399,  0.2135,  0.1628, -0.4540,  0.2617],
        [ 0.3071, -0.0313,  0.4496,  0.2198, -0.3120, -0.2146,  0.1683,  0.1731],
        [ 0.4509, -0.0628, -0.3913, -0.0501, -0.1874,  0.1260,  0.3439,  0.4647],
        [-0.0335, -0.1991,  0.0266, -0.3324,  0.3668, -0.1406, -0.4826,  0.3528],
        [ 0.1817,  0.2238,  0.1592, -0.3516, -0.1199, -0.1376, -0.1106, -0.4238],
        [ 0.3973, -0.1878, -0.2352,  0.0580, -0.3406,  0.3911,  0.0430, -0.1678],
        [ 0.4974, -0.0835, -0.1006, -0.2600, -0.4489, -0.1236, -0.1854,  0.1365],
        [-0.3078,  0.3592, -0.1524, -0.4135, -0.0318, -0.1125,  0.0524, -0.4706],
        [ 0.4918,  0.2554, -0.0036, -0.0450,  0.2225, -0.1228,  0.2551, -0.4864],
        [ 0.4857, -0.2388,  0.2820,  0.0188,  0.1230,  0.0854, -0.1833,  0.3588],
        [ 0.2176,  0.1391,  0.3359, -0.1934, -0.2191, -0.3311, -0.4560,  0.0518],
        [ 0.0076, -0.1766, -0.0204, -0.1558,  0.1202, -0.1774, -0.1179, -0.1083],
        [ 0.2909,  0.3455,  0.2782, -0.3135,  0.4290, -0.2964,  0.0728, -0.2967],
        [ 0.0806,  0.3625, -0.4605, -0.4280,  0.3781,  0.2210,  0.3288, -0.3144],
        [-0.0228,  0.2920,  0.1622, -0.3877, -0.1832, -0.3444,  0.0203,  0.3810],
        [ 0.3164, -0.0751, -0.0516,  0.4027, -0.2697, -0.4086,  0.0899, -0.2546],
        [ 0.1857, -0.0353,  0.4027, -0.0254, -0.2364,  0.1646,  0.4239,  0.2072],
        [-0.2543, -0.1614, -0.1072,  0.2229, -0.4212,  0.0509, -0.2956, -0.2826],
        [-0.1663, -0.4290, -0.3465, -0.0357,  0.2751,  0.2445,  0.0971, -0.1872],
        [-0.4698,  0.3407,  0.1605,  0.2837,  0.1058,  0.1352, -0.0582,  0.1943],
        [-0.0607,  0.1697,  0.0699,  0.3292, -0.4934, -0.0476, -0.2101, -0.3174],
        [ 0.2621,  0.1162, -0.2791, -0.2568,  0.0500, -0.2562, -0.0380,  0.0756],
        [-0.0822,  0.0556,  0.0015,  0.0940,  0.4283,  0.0880,  0.0201, -0.2758],
        [ 0.0244,  0.3077,  0.2039,  0.3785, -0.3409, -0.2637, -0.2795, -0.3423],
        [ 0.4753,  0.2763,  0.0659, -0.3247,  0.0012, -0.0723,  0.1390, -0.1850],
        [ 0.0234, -0.2491, -0.4229,  0.0630, -0.0596, -0.1567,  0.2292,  0.2517],
        [-0.0885,  0.3063, -0.3435, -0.1879,  0.2093,  0.3252, -0.1042,  0.3412],
        [ 0.2620, -0.1519, -0.0180, -0.2640, -0.3331, -0.0609, -0.0044,  0.0969],
        [-0.3671,  0.1189,  0.0099,  0.4776,  0.0431, -0.3673,  0.1640, -0.0456]])
2025-03-18 17:47:11.631009 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([2281701379],"float32"),Tensor([48],"float32"),], )

[not compare]  None tensor([[-0.0372, -0.4361, -0.3755,  0.2082,  0.0346,  0.1829, -0.2872,  0.2302],
        [-0.2721, -0.2252, -0.0805,  0.1670,  0.2779,  0.4490, -0.1109, -0.1896],
        [ 0.4440, -0.0911,  0.3861,  0.1138,  0.2960,  0.0955,  0.4091,  0.4683],
        [ 0.4966, -0.4560, -0.2645,  0.0455, -0.4307, -0.1242,  0.1781, -0.1539],
        [-0.0828,  0.3690,  0.3446, -0.1205, -0.1902, -0.1266, -0.1974, -0.3179],
        [ 0.0779, -0.1539,  0.1904,  0.4242, -0.4555, -0.2741, -0.3668, -0.2246],
        [-0.3389, -0.3306, -0.3351, -0.3904, -0.4631, -0.4981, -0.2836,  0.0795],
        [ 0.1997,  0.3252, -0.2195,  0.4920,  0.1590, -0.1666,  0.1603, -0.0590],
        [ 0.4110,  0.3595,  0.1949, -0.3568, -0.0082, -0.3976,  0.1620, -0.3262],
        [-0.4269,  0.2231,  0.4087, -0.2084, -0.0739, -0.3792,  0.0627, -0.1287],
        [-0.1398,  0.2823,  0.1534,  0.2706,  0.2614,  0.1931, -0.4682, -0.2043],
        [ 0.2567,  0.0005,  0.2640,  0.1807, -0.2019, -0.4829, -0.4538,  0.0216],
        [-0.4008, -0.2951,  0.3589,  0.1860, -0.3273, -0.3131,  0.1339, -0.4619],
        [ 0.2490,  0.2637,  0.1881,  0.4550,  0.0007,  0.4825,  0.1627, -0.0712],
        [-0.3970,  0.1599,  0.3025, -0.1037, -0.3589,  0.3129,  0.3726, -0.2790],
        [ 0.0907, -0.3790,  0.2606, -0.3659,  0.4499, -0.0732,  0.4978, -0.0607],
        [ 0.0887, -0.1145,  0.3577,  0.2392,  0.0412, -0.0357, -0.2875, -0.2923],
        [ 0.3180,  0.1469,  0.2894,  0.2007, -0.1344, -0.3721,  0.1521, -0.1810],
        [ 0.1652, -0.4759,  0.1323,  0.0850, -0.0501,  0.4218, -0.1979,  0.0983],
        [ 0.1556,  0.2690, -0.0533, -0.0802, -0.0493,  0.2728,  0.3394,  0.2200],
        [ 0.4126, -0.3183,  0.1013, -0.4693,  0.1854,  0.3110,  0.1743, -0.2411],
        [-0.0987,  0.4821,  0.4218,  0.1600, -0.3737,  0.0856,  0.3606,  0.0279],
        [-0.3711, -0.2190,  0.2529, -0.4708,  0.2660,  0.2054, -0.3908,  0.4011],
        [-0.2795,  0.2463,  0.0465, -0.2408, -0.1926,  0.3313,  0.0363,  0.2083],
        [-0.1764, -0.1444,  0.4457,  0.3767,  0.1743, -0.3119, -0.2285, -0.3084],
        [-0.3070,  0.1567, -0.3245, -0.4091, -0.1182,  0.0833, -0.0107, -0.1737],
        [-0.0137, -0.0621,  0.2921,  0.2913,  0.1191, -0.1671,  0.3269,  0.3537],
        [ 0.3134, -0.1665,  0.4791, -0.3418,  0.4152, -0.0060,  0.1076,  0.4867],
        [-0.3185, -0.2656,  0.0564, -0.3084, -0.3664,  0.0411, -0.0787,  0.3425],
        [ 0.3226, -0.2670, -0.3910, -0.1752,  0.3057,  0.1430, -0.1469, -0.3863],
        [ 0.4202, -0.4832, -0.4889, -0.3943, -0.0184,  0.4235,  0.3025, -0.4843],
        [ 0.3772,  0.2081,  0.0857,  0.2520,  0.1115, -0.2595,  0.2708,  0.3600],
        [ 0.3390, -0.1593, -0.4807, -0.0704, -0.4454,  0.2271, -0.1281,  0.1581],
        [-0.0647,  0.4230,  0.3777,  0.4369,  0.2871,  0.3171,  0.0577,  0.2174],
        [-0.2672, -0.3422,  0.0847,  0.1245, -0.2525,  0.3133,  0.0283,  0.1959],
        [-0.3637,  0.4368,  0.2714, -0.3598,  0.0012, -0.1758, -0.2610, -0.2156],
        [ 0.4413,  0.0265,  0.2671,  0.3995,  0.1018, -0.4748,  0.4709,  0.0195],
        [ 0.0813,  0.4882,  0.2632, -0.0532, -0.0115,  0.0263, -0.1886, -0.3101],
        [-0.1762,  0.2320, -0.1651,  0.0516, -0.4857, -0.1165,  0.1032, -0.1769],
        [-0.0786, -0.2890,  0.3183, -0.0375, -0.1844,  0.1287, -0.3335,  0.1180],
        [ 0.3273,  0.2581, -0.1516,  0.1070, -0.1246, -0.4044,  0.4655, -0.4782],
        [ 0.1274,  0.3500, -0.2248, -0.1004, -0.2077,  0.4216,  0.0053,  0.2730],
        [-0.0467, -0.1527, -0.4243,  0.0620, -0.1061,  0.2113,  0.4427, -0.0711],
        [-0.1655,  0.3286,  0.0771,  0.4784, -0.0048, -0.0387,  0.3593, -0.3564],
        [-0.4257,  0.3465,  0.3090, -0.4407,  0.2289, -0.2786,  0.2981,  0.0301],
        [-0.1827,  0.0478, -0.2370,  0.1656, -0.3935, -0.1562, -0.0741, -0.4759],
        [-0.2951, -0.0472, -0.3158,  0.4825,  0.1639, -0.2768, -0.3074,  0.3422],
        [ 0.2809,  0.0079,  0.0416, -0.0406, -0.0541,  0.0148, -0.0902, -0.2984]])
2025-03-18 17:49:05.904750 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([2281701379],"float32"),], )

[not compare]  None tensor([[-0.0372, -0.4361, -0.3755,  0.2082,  0.0346,  0.1829, -0.2872,  0.2302],
        [-0.2721, -0.2252, -0.0805,  0.1670,  0.2779,  0.4490, -0.1109, -0.1896],
        [ 0.4440, -0.0911,  0.3861,  0.1138,  0.2960,  0.0955,  0.4091,  0.4683],
        [ 0.4966, -0.4560, -0.2645,  0.0455, -0.4307, -0.1242,  0.1781, -0.1539],
        [-0.0828,  0.3690,  0.3446, -0.1205, -0.1902, -0.1266, -0.1974, -0.3179],
        [ 0.0779, -0.1539,  0.1904,  0.4242, -0.4555, -0.2741, -0.3668, -0.2246],
        [-0.3389, -0.3306, -0.3351, -0.3904, -0.4631, -0.4981, -0.2836,  0.0795],
        [ 0.1997,  0.3252, -0.2195,  0.4920,  0.1590, -0.1666,  0.1603, -0.0590],
        [ 0.4110,  0.3595,  0.1949, -0.3568, -0.0082, -0.3976,  0.1620, -0.3262],
        [-0.4269,  0.2231,  0.4087, -0.2084, -0.0739, -0.3792,  0.0627, -0.1287],
        [-0.1398,  0.2823,  0.1534,  0.2706,  0.2614,  0.1931, -0.4682, -0.2043],
        [ 0.2567,  0.0005,  0.2640,  0.1807, -0.2019, -0.4829, -0.4538,  0.0216],
        [-0.4008, -0.2951,  0.3589,  0.1860, -0.3273, -0.3131,  0.1339, -0.4619],
        [ 0.2490,  0.2637,  0.1881,  0.4550,  0.0007,  0.4825,  0.1627, -0.0712],
        [-0.3970,  0.1599,  0.3025, -0.1037, -0.3589,  0.3129,  0.3726, -0.2790],
        [ 0.0907, -0.3790,  0.2606, -0.3659,  0.4499, -0.0732,  0.4978, -0.0607],
        [ 0.0887, -0.1145,  0.3577,  0.2392,  0.0412, -0.0357, -0.2875, -0.2923],
        [ 0.3180,  0.1469,  0.2894,  0.2007, -0.1344, -0.3721,  0.1521, -0.1810],
        [ 0.1652, -0.4759,  0.1323,  0.0850, -0.0501,  0.4218, -0.1979,  0.0983],
        [ 0.1556,  0.2690, -0.0533, -0.0802, -0.0493,  0.2728,  0.3394,  0.2200],
        [ 0.4126, -0.3183,  0.1013, -0.4693,  0.1854,  0.3110,  0.1743, -0.2411],
        [-0.0987,  0.4821,  0.4218,  0.1600, -0.3737,  0.0856,  0.3606,  0.0279],
        [-0.3711, -0.2190,  0.2529, -0.4708,  0.2660,  0.2054, -0.3908,  0.4011],
        [-0.2795,  0.2463,  0.0465, -0.2408, -0.1926,  0.3313,  0.0363,  0.2083],
        [-0.1764, -0.1444,  0.4457,  0.3767,  0.1743, -0.3119, -0.2285, -0.3084],
        [-0.3070,  0.1567, -0.3245, -0.4091, -0.1182,  0.0833, -0.0107, -0.1737],
        [-0.0137, -0.0621,  0.2921,  0.2913,  0.1191, -0.1671,  0.3269,  0.3537],
        [ 0.3134, -0.1665,  0.4791, -0.3418,  0.4152, -0.0060,  0.1076,  0.4867],
        [-0.3185, -0.2656,  0.0564, -0.3084, -0.3664,  0.0411, -0.0787,  0.3425],
        [ 0.3226, -0.2670, -0.3910, -0.1752,  0.3057,  0.1430, -0.1469, -0.3863],
        [ 0.4202, -0.4832, -0.4889, -0.3943, -0.0184,  0.4235,  0.3025, -0.4843],
        [ 0.3772,  0.2081,  0.0857,  0.2520,  0.1115, -0.2595,  0.2708,  0.3600],
        [ 0.3390, -0.1593, -0.4807, -0.0704, -0.4454,  0.2271, -0.1281,  0.1581],
        [-0.0647,  0.4230,  0.3777,  0.4369,  0.2871,  0.3171,  0.0577,  0.2174],
        [-0.2672, -0.3422,  0.0847,  0.1245, -0.2525,  0.3133,  0.0283,  0.1959],
        [-0.3637,  0.4368,  0.2714, -0.3598,  0.0012, -0.1758, -0.2610, -0.2156],
        [ 0.4413,  0.0265,  0.2671,  0.3995,  0.1018, -0.4748,  0.4709,  0.0195],
        [ 0.0813,  0.4882,  0.2632, -0.0532, -0.0115,  0.0263, -0.1886, -0.3101],
        [-0.1762,  0.2320, -0.1651,  0.0516, -0.4857, -0.1165,  0.1032, -0.1769],
        [-0.0786, -0.2890,  0.3183, -0.0375, -0.1844,  0.1287, -0.3335,  0.1180],
        [ 0.3273,  0.2581, -0.1516,  0.1070, -0.1246, -0.4044,  0.4655, -0.4782],
        [ 0.1274,  0.3500, -0.2248, -0.1004, -0.2077,  0.4216,  0.0053,  0.2730],
        [-0.0467, -0.1527, -0.4243,  0.0620, -0.1061,  0.2113,  0.4427, -0.0711],
        [-0.1655,  0.3286,  0.0771,  0.4784, -0.0048, -0.0387,  0.3593, -0.3564],
        [-0.4257,  0.3465,  0.3090, -0.4407,  0.2289, -0.2786,  0.2981,  0.0301],
        [-0.1827,  0.0478, -0.2370,  0.1656, -0.3935, -0.1562, -0.0741, -0.4759],
        [-0.2951, -0.0472, -0.3158,  0.4825,  0.1639, -0.2768, -0.3074,  0.3422],
        [ 0.2809,  0.0079,  0.0416, -0.0406, -0.0541,  0.0148, -0.0902, -0.2984]])
2025-03-18 17:50:47.117273 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 47535446],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[not compare]  None tensor([[-0.3848,  0.4089, -0.4006,  0.2819, -0.4521,  0.3809, -0.1737,  0.2175],
        [-0.4538, -0.4868,  0.4465,  0.0745, -0.1096,  0.4945, -0.1126, -0.1533],
        [ 0.4578, -0.1037,  0.4622, -0.3497,  0.1096, -0.3959, -0.2043,  0.3085],
        [ 0.1580, -0.0985, -0.3931,  0.4471, -0.2505,  0.3916, -0.0197,  0.1271],
        [-0.4143, -0.1687,  0.1942,  0.4851,  0.4403,  0.3838, -0.1531, -0.1247],
        [-0.4961, -0.4127, -0.0802,  0.1241,  0.3849, -0.4658, -0.0874,  0.1340],
        [ 0.0155,  0.3528,  0.1664, -0.1271, -0.0976, -0.1577, -0.3023, -0.4323],
        [ 0.0523, -0.2699, -0.1617, -0.4707, -0.3297, -0.4891,  0.2864, -0.3375],
        [-0.4816,  0.1767,  0.1200,  0.2820,  0.4766, -0.4815, -0.3220, -0.4353],
        [ 0.4183, -0.3823,  0.3825,  0.4679, -0.2940,  0.0906, -0.1810, -0.2490],
        [ 0.3050,  0.3440, -0.2218,  0.4162, -0.3939, -0.3226, -0.2174,  0.0893],
        [ 0.1766, -0.1116, -0.2683,  0.2699, -0.4031, -0.3656,  0.0431,  0.2696],
        [ 0.3855, -0.1065, -0.3909,  0.3784,  0.3302, -0.3792, -0.4497,  0.2620],
        [ 0.0785, -0.4673, -0.2081,  0.3840, -0.0495, -0.2937, -0.3219, -0.0219],
        [-0.2248, -0.0694, -0.2595, -0.1061, -0.1763,  0.4850, -0.0061,  0.4196],
        [-0.2074,  0.3409, -0.1221,  0.2268, -0.4060, -0.4904, -0.0438, -0.4406],
        [ 0.4226, -0.4894, -0.4023,  0.4843,  0.2943,  0.0602,  0.2924, -0.3022],
        [ 0.4798,  0.1477, -0.0323,  0.4511, -0.2382,  0.1157,  0.2862,  0.2114],
        [ 0.3811,  0.4219, -0.3472,  0.3415,  0.3151, -0.0637,  0.0405, -0.0501],
        [ 0.1021, -0.4367, -0.1885,  0.2110, -0.4300, -0.2573, -0.4771,  0.0343],
        [-0.2568,  0.0808, -0.4020,  0.4706, -0.4080,  0.2926,  0.1629,  0.0625],
        [-0.3563, -0.4548, -0.3033,  0.1399,  0.2135,  0.1628, -0.4540,  0.2617],
        [ 0.3071, -0.0313,  0.4496,  0.2198, -0.3120, -0.2146,  0.1683,  0.1731],
        [ 0.4509, -0.0628, -0.3913, -0.0501, -0.1874,  0.1260,  0.3439,  0.4647],
        [-0.0335, -0.1991,  0.0266, -0.3324,  0.3668, -0.1406, -0.4826,  0.3528],
        [ 0.1817,  0.2238,  0.1592, -0.3516, -0.1199, -0.1376, -0.1106, -0.4238],
        [ 0.3973, -0.1878, -0.2352,  0.0580, -0.3406,  0.3911,  0.0430, -0.1678],
        [ 0.4974, -0.0835, -0.1006, -0.2600, -0.4489, -0.1236, -0.1854,  0.1365],
        [-0.3078,  0.3592, -0.1524, -0.4135, -0.0318, -0.1125,  0.0524, -0.4706],
        [ 0.4918,  0.2554, -0.0036, -0.0450,  0.2225, -0.1228,  0.2551, -0.4864],
        [ 0.4857, -0.2388,  0.2820,  0.0188,  0.1230,  0.0854, -0.1833,  0.3588],
        [ 0.2176,  0.1391,  0.3359, -0.1934, -0.2191, -0.3311, -0.4560,  0.0518],
        [ 0.0076, -0.1766, -0.0204, -0.1558,  0.1202, -0.1774, -0.1179, -0.1083],
        [ 0.2909,  0.3455,  0.2782, -0.3135,  0.4290, -0.2964,  0.0728, -0.2967],
        [ 0.0806,  0.3625, -0.4605, -0.4280,  0.3781,  0.2210,  0.3288, -0.3144],
        [-0.0228,  0.2920,  0.1622, -0.3877, -0.1832, -0.3444,  0.0203,  0.3810],
        [ 0.3164, -0.0751, -0.0516,  0.4027, -0.2697, -0.4086,  0.0899, -0.2546],
        [ 0.1857, -0.0353,  0.4027, -0.0254, -0.2364,  0.1646,  0.4239,  0.2072],
        [-0.2543, -0.1614, -0.1072,  0.2229, -0.4212,  0.0509, -0.2956, -0.2826],
        [-0.1663, -0.4290, -0.3465, -0.0357,  0.2751,  0.2445,  0.0971, -0.1872],
        [-0.4698,  0.3407,  0.1605,  0.2837,  0.1058,  0.1352, -0.0582,  0.1943],
        [-0.0607,  0.1697,  0.0699,  0.3292, -0.4934, -0.0476, -0.2101, -0.3174],
        [ 0.2621,  0.1162, -0.2791, -0.2568,  0.0500, -0.2562, -0.0380,  0.0756],
        [-0.0822,  0.0556,  0.0015,  0.0940,  0.4283,  0.0880,  0.0201, -0.2758],
        [ 0.0244,  0.3077,  0.2039,  0.3785, -0.3409, -0.2637, -0.2795, -0.3423],
        [ 0.4753,  0.2763,  0.0659, -0.3247,  0.0012, -0.0723,  0.1390, -0.1850],
        [ 0.0234, -0.2491, -0.4229,  0.0630, -0.0596, -0.1567,  0.2292,  0.2517],
        [-0.0885,  0.3063, -0.3435, -0.1879,  0.2093,  0.3252, -0.1042,  0.3412]])
2025-03-18 17:52:39.526001 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 47535446],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[not compare]  None tensor([[-0.3848,  0.4089, -0.4006,  0.2819, -0.4521,  0.3809, -0.1737,  0.2175],
        [-0.4538, -0.4868,  0.4465,  0.0745, -0.1096,  0.4945, -0.1126, -0.1533],
        [ 0.4578, -0.1037,  0.4622, -0.3497,  0.1096, -0.3959, -0.2043,  0.3085],
        [ 0.1580, -0.0985, -0.3931,  0.4471, -0.2505,  0.3916, -0.0197,  0.1271],
        [-0.4143, -0.1687,  0.1942,  0.4851,  0.4403,  0.3838, -0.1531, -0.1247],
        [-0.4961, -0.4127, -0.0802,  0.1241,  0.3849, -0.4658, -0.0874,  0.1340],
        [ 0.0155,  0.3528,  0.1664, -0.1271, -0.0976, -0.1577, -0.3023, -0.4323],
        [ 0.0523, -0.2699, -0.1617, -0.4707, -0.3297, -0.4891,  0.2864, -0.3375],
        [-0.4816,  0.1767,  0.1200,  0.2820,  0.4766, -0.4815, -0.3220, -0.4353],
        [ 0.4183, -0.3823,  0.3825,  0.4679, -0.2940,  0.0906, -0.1810, -0.2490],
        [ 0.3050,  0.3440, -0.2218,  0.4162, -0.3939, -0.3226, -0.2174,  0.0893],
        [ 0.1766, -0.1116, -0.2683,  0.2699, -0.4031, -0.3656,  0.0431,  0.2696],
        [ 0.3855, -0.1065, -0.3909,  0.3784,  0.3302, -0.3792, -0.4497,  0.2620],
        [ 0.0785, -0.4673, -0.2081,  0.3840, -0.0495, -0.2937, -0.3219, -0.0219],
        [-0.2248, -0.0694, -0.2595, -0.1061, -0.1763,  0.4850, -0.0061,  0.4196],
        [-0.2074,  0.3409, -0.1221,  0.2268, -0.4060, -0.4904, -0.0438, -0.4406],
        [ 0.4226, -0.4894, -0.4023,  0.4843,  0.2943,  0.0602,  0.2924, -0.3022],
        [ 0.4798,  0.1477, -0.0323,  0.4511, -0.2382,  0.1157,  0.2862,  0.2114],
        [ 0.3811,  0.4219, -0.3472,  0.3415,  0.3151, -0.0637,  0.0405, -0.0501],
        [ 0.1021, -0.4367, -0.1885,  0.2110, -0.4300, -0.2573, -0.4771,  0.0343],
        [-0.2568,  0.0808, -0.4020,  0.4706, -0.4080,  0.2926,  0.1629,  0.0625],
        [-0.3563, -0.4548, -0.3033,  0.1399,  0.2135,  0.1628, -0.4540,  0.2617],
        [ 0.3071, -0.0313,  0.4496,  0.2198, -0.3120, -0.2146,  0.1683,  0.1731],
        [ 0.4509, -0.0628, -0.3913, -0.0501, -0.1874,  0.1260,  0.3439,  0.4647],
        [-0.0335, -0.1991,  0.0266, -0.3324,  0.3668, -0.1406, -0.4826,  0.3528],
        [ 0.1817,  0.2238,  0.1592, -0.3516, -0.1199, -0.1376, -0.1106, -0.4238],
        [ 0.3973, -0.1878, -0.2352,  0.0580, -0.3406,  0.3911,  0.0430, -0.1678],
        [ 0.4974, -0.0835, -0.1006, -0.2600, -0.4489, -0.1236, -0.1854,  0.1365],
        [-0.3078,  0.3592, -0.1524, -0.4135, -0.0318, -0.1125,  0.0524, -0.4706],
        [ 0.4918,  0.2554, -0.0036, -0.0450,  0.2225, -0.1228,  0.2551, -0.4864],
        [ 0.4857, -0.2388,  0.2820,  0.0188,  0.1230,  0.0854, -0.1833,  0.3588],
        [ 0.2176,  0.1391,  0.3359, -0.1934, -0.2191, -0.3311, -0.4560,  0.0518],
        [ 0.0076, -0.1766, -0.0204, -0.1558,  0.1202, -0.1774, -0.1179, -0.1083],
        [ 0.2909,  0.3455,  0.2782, -0.3135,  0.4290, -0.2964,  0.0728, -0.2967],
        [ 0.0806,  0.3625, -0.4605, -0.4280,  0.3781,  0.2210,  0.3288, -0.3144],
        [-0.0228,  0.2920,  0.1622, -0.3877, -0.1832, -0.3444,  0.0203,  0.3810],
        [ 0.3164, -0.0751, -0.0516,  0.4027, -0.2697, -0.4086,  0.0899, -0.2546],
        [ 0.1857, -0.0353,  0.4027, -0.0254, -0.2364,  0.1646,  0.4239,  0.2072],
        [-0.2543, -0.1614, -0.1072,  0.2229, -0.4212,  0.0509, -0.2956, -0.2826],
        [-0.1663, -0.4290, -0.3465, -0.0357,  0.2751,  0.2445,  0.0971, -0.1872],
        [-0.4698,  0.3407,  0.1605,  0.2837,  0.1058,  0.1352, -0.0582,  0.1943],
        [-0.0607,  0.1697,  0.0699,  0.3292, -0.4934, -0.0476, -0.2101, -0.3174],
        [ 0.2621,  0.1162, -0.2791, -0.2568,  0.0500, -0.2562, -0.0380,  0.0756],
        [-0.0822,  0.0556,  0.0015,  0.0940,  0.4283,  0.0880,  0.0201, -0.2758],
        [ 0.0244,  0.3077,  0.2039,  0.3785, -0.3409, -0.2637, -0.2795, -0.3423],
        [ 0.4753,  0.2763,  0.0659, -0.3247,  0.0012, -0.0723,  0.1390, -0.1850],
        [ 0.0234, -0.2491, -0.4229,  0.0630, -0.0596, -0.1567,  0.2292,  0.2517],
        [-0.0885,  0.3063, -0.3435, -0.1879,  0.2093,  0.3252, -0.1042,  0.3412]])
2025-03-18 17:54:21.624526 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([48, 8],"float32"),Tensor([48, 47535446],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[not compare]  None tensor([[-0.3848,  0.4089, -0.4006,  0.2819, -0.4521,  0.3809, -0.1737,  0.2175],
        [-0.4538, -0.4868,  0.4465,  0.0745, -0.1096,  0.4945, -0.1126, -0.1533],
        [ 0.4578, -0.1037,  0.4622, -0.3497,  0.1096, -0.3959, -0.2043,  0.3085],
        [ 0.1580, -0.0985, -0.3931,  0.4471, -0.2505,  0.3916, -0.0197,  0.1271],
        [-0.4143, -0.1687,  0.1942,  0.4851,  0.4403,  0.3838, -0.1531, -0.1247],
        [-0.4961, -0.4127, -0.0802,  0.1241,  0.3849, -0.4658, -0.0874,  0.1340],
        [ 0.0155,  0.3528,  0.1664, -0.1271, -0.0976, -0.1577, -0.3023, -0.4323],
        [ 0.0523, -0.2699, -0.1617, -0.4707, -0.3297, -0.4891,  0.2864, -0.3375],
        [-0.4816,  0.1767,  0.1200,  0.2820,  0.4766, -0.4815, -0.3220, -0.4353],
        [ 0.4183, -0.3823,  0.3825,  0.4679, -0.2940,  0.0906, -0.1810, -0.2490],
        [ 0.3050,  0.3440, -0.2218,  0.4162, -0.3939, -0.3226, -0.2174,  0.0893],
        [ 0.1766, -0.1116, -0.2683,  0.2699, -0.4031, -0.3656,  0.0431,  0.2696],
        [ 0.3855, -0.1065, -0.3909,  0.3784,  0.3302, -0.3792, -0.4497,  0.2620],
        [ 0.0785, -0.4673, -0.2081,  0.3840, -0.0495, -0.2937, -0.3219, -0.0219],
        [-0.2248, -0.0694, -0.2595, -0.1061, -0.1763,  0.4850, -0.0061,  0.4196],
        [-0.2074,  0.3409, -0.1221,  0.2268, -0.4060, -0.4904, -0.0438, -0.4406],
        [ 0.4226, -0.4894, -0.4023,  0.4843,  0.2943,  0.0602,  0.2924, -0.3022],
        [ 0.4798,  0.1477, -0.0323,  0.4511, -0.2382,  0.1157,  0.2862,  0.2114],
        [ 0.3811,  0.4219, -0.3472,  0.3415,  0.3151, -0.0637,  0.0405, -0.0501],
        [ 0.1021, -0.4367, -0.1885,  0.2110, -0.4300, -0.2573, -0.4771,  0.0343],
        [-0.2568,  0.0808, -0.4020,  0.4706, -0.4080,  0.2926,  0.1629,  0.0625],
        [-0.3563, -0.4548, -0.3033,  0.1399,  0.2135,  0.1628, -0.4540,  0.2617],
        [ 0.3071, -0.0313,  0.4496,  0.2198, -0.3120, -0.2146,  0.1683,  0.1731],
        [ 0.4509, -0.0628, -0.3913, -0.0501, -0.1874,  0.1260,  0.3439,  0.4647],
        [-0.0335, -0.1991,  0.0266, -0.3324,  0.3668, -0.1406, -0.4826,  0.3528],
        [ 0.1817,  0.2238,  0.1592, -0.3516, -0.1199, -0.1376, -0.1106, -0.4238],
        [ 0.3973, -0.1878, -0.2352,  0.0580, -0.3406,  0.3911,  0.0430, -0.1678],
        [ 0.4974, -0.0835, -0.1006, -0.2600, -0.4489, -0.1236, -0.1854,  0.1365],
        [-0.3078,  0.3592, -0.1524, -0.4135, -0.0318, -0.1125,  0.0524, -0.4706],
        [ 0.4918,  0.2554, -0.0036, -0.0450,  0.2225, -0.1228,  0.2551, -0.4864],
        [ 0.4857, -0.2388,  0.2820,  0.0188,  0.1230,  0.0854, -0.1833,  0.3588],
        [ 0.2176,  0.1391,  0.3359, -0.1934, -0.2191, -0.3311, -0.4560,  0.0518],
        [ 0.0076, -0.1766, -0.0204, -0.1558,  0.1202, -0.1774, -0.1179, -0.1083],
        [ 0.2909,  0.3455,  0.2782, -0.3135,  0.4290, -0.2964,  0.0728, -0.2967],
        [ 0.0806,  0.3625, -0.4605, -0.4280,  0.3781,  0.2210,  0.3288, -0.3144],
        [-0.0228,  0.2920,  0.1622, -0.3877, -0.1832, -0.3444,  0.0203,  0.3810],
        [ 0.3164, -0.0751, -0.0516,  0.4027, -0.2697, -0.4086,  0.0899, -0.2546],
        [ 0.1857, -0.0353,  0.4027, -0.0254, -0.2364,  0.1646,  0.4239,  0.2072],
        [-0.2543, -0.1614, -0.1072,  0.2229, -0.4212,  0.0509, -0.2956, -0.2826],
        [-0.1663, -0.4290, -0.3465, -0.0357,  0.2751,  0.2445,  0.0971, -0.1872],
        [-0.4698,  0.3407,  0.1605,  0.2837,  0.1058,  0.1352, -0.0582,  0.1943],
        [-0.0607,  0.1697,  0.0699,  0.3292, -0.4934, -0.0476, -0.2101, -0.3174],
        [ 0.2621,  0.1162, -0.2791, -0.2568,  0.0500, -0.2562, -0.0380,  0.0756],
        [-0.0822,  0.0556,  0.0015,  0.0940,  0.4283,  0.0880,  0.0201, -0.2758],
        [ 0.0244,  0.3077,  0.2039,  0.3785, -0.3409, -0.2637, -0.2795, -0.3423],
        [ 0.4753,  0.2763,  0.0659, -0.3247,  0.0012, -0.0723,  0.1390, -0.1850],
        [ 0.0234, -0.2491, -0.4229,  0.0630, -0.0596, -0.1567,  0.2292,  0.2517],
        [-0.0885,  0.3063, -0.3435, -0.1879,  0.2093,  0.3252, -0.1042,  0.3412]])
2025-03-18 17:56:07.182988 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([64, 35651585],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[not compare]  None tensor([[-0.3649, -0.0898,  0.1966,  ..., -0.0428, -0.1841, -0.1611],
        [ 0.3517,  0.0094,  0.4315,  ..., -0.4443, -0.4176,  0.4455],
        [ 0.2643, -0.1932, -0.3217,  ..., -0.0417,  0.0501,  0.4010],
        ...,
        [-0.1549, -0.1694,  0.0886,  ...,  0.3451,  0.1932, -0.2683],
        [ 0.3623,  0.1455,  0.4046,  ..., -0.2225,  0.1601,  0.3156],
        [-0.3722,  0.2792, -0.4961,  ..., -0.4944,  0.4895,  0.3995]])
2025-03-18 17:57:46.801910 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([64, 8],"float32"),Tensor([142606337, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[not compare]  None tensor([[ 2.0819e-01,  3.4650e-02,  1.8292e-01, -2.8721e-01,  2.3018e-01,
         -2.7206e-01, -2.2519e-01, -8.0517e-02],
        [ 1.6704e-01,  2.7792e-01,  4.4896e-01, -1.1095e-01, -1.8964e-01,
          4.4402e-01, -9.1072e-02,  3.8606e-01],
        [ 1.1383e-01,  2.9602e-01,  9.5458e-02,  4.0912e-01,  4.6825e-01,
          4.9661e-01, -4.5595e-01, -2.6453e-01],
        [ 4.5521e-02, -4.3069e-01, -1.2416e-01,  1.7815e-01, -1.5392e-01,
         -8.2781e-02,  3.6905e-01,  3.4463e-01],
        [-1.2047e-01, -1.9021e-01, -1.2656e-01, -1.9735e-01, -3.1795e-01,
          7.7858e-02, -1.5394e-01,  1.9040e-01],
        [ 4.2418e-01, -4.5550e-01, -2.7407e-01, -3.6682e-01, -2.2456e-01,
         -3.3893e-01, -3.3062e-01, -3.3508e-01],
        [-3.9040e-01, -4.6307e-01, -4.9807e-01, -2.8355e-01,  7.9494e-02,
          1.9968e-01,  3.2524e-01, -2.1946e-01],
        [ 4.9202e-01,  1.5897e-01, -1.6661e-01,  1.6033e-01, -5.8961e-02,
          4.1099e-01,  3.5952e-01,  1.9490e-01],
        [-3.5677e-01, -8.1904e-03, -3.9764e-01,  1.6201e-01, -3.2615e-01,
         -4.2689e-01,  2.2314e-01,  4.0871e-01],
        [-2.0836e-01, -7.3891e-02, -3.7918e-01,  6.2680e-02, -1.2871e-01,
         -1.3983e-01,  2.8235e-01,  1.5336e-01],
        [ 2.7063e-01,  2.6137e-01,  1.9312e-01, -4.6818e-01, -2.0427e-01,
          2.5668e-01,  5.1453e-04,  2.6400e-01],
        [ 1.8068e-01, -2.0190e-01, -4.8288e-01, -4.5376e-01,  2.1634e-02,
         -4.0084e-01, -2.9506e-01,  3.5891e-01],
        [ 1.8597e-01, -3.2727e-01, -3.1309e-01,  1.3388e-01, -4.6187e-01,
          2.4899e-01,  2.6373e-01,  1.8808e-01],
        [ 4.5496e-01,  7.4547e-04,  4.8250e-01,  1.6268e-01, -7.1244e-02,
         -3.9697e-01,  1.5990e-01,  3.0246e-01],
        [-1.0365e-01, -3.5892e-01,  3.1288e-01,  3.7257e-01, -2.7899e-01,
          9.0670e-02, -3.7897e-01,  2.6059e-01],
        [-3.6591e-01,  4.4985e-01, -7.3246e-02,  4.9778e-01, -6.0728e-02,
          8.8742e-02, -1.1455e-01,  3.5773e-01],
        [ 2.3919e-01,  4.1178e-02, -3.5698e-02, -2.8753e-01, -2.9226e-01,
          3.1802e-01,  1.4694e-01,  2.8941e-01],
        [ 2.0070e-01, -1.3439e-01, -3.7211e-01,  1.5207e-01, -1.8104e-01,
          1.6520e-01, -4.7590e-01,  1.3231e-01],
        [ 8.5047e-02, -5.0129e-02,  4.2176e-01, -1.9794e-01,  9.8317e-02,
          1.5557e-01,  2.6898e-01, -5.3308e-02],
        [-8.0174e-02, -4.9304e-02,  2.7283e-01,  3.3938e-01,  2.2002e-01,
          4.1258e-01, -3.1825e-01,  1.0130e-01],
        [-4.6932e-01,  1.8535e-01,  3.1103e-01,  1.7427e-01, -2.4109e-01,
         -9.8682e-02,  4.8210e-01,  4.2181e-01],
        [ 1.6004e-01, -3.7367e-01,  8.5593e-02,  3.6062e-01,  2.7894e-02,
         -3.7111e-01, -2.1903e-01,  2.5290e-01],
        [-4.7081e-01,  2.6604e-01,  2.0545e-01, -3.9083e-01,  4.0114e-01,
         -2.7946e-01,  2.4625e-01,  4.6450e-02],
        [-2.4076e-01, -1.9259e-01,  3.3132e-01,  3.6346e-02,  2.0832e-01,
         -1.7643e-01, -1.4441e-01,  4.4574e-01],
        [ 3.7667e-01,  1.7433e-01, -3.1192e-01, -2.2852e-01, -3.0836e-01,
         -3.0696e-01,  1.5672e-01, -3.2449e-01],
        [-4.0908e-01, -1.1816e-01,  8.3295e-02, -1.0676e-02, -1.7365e-01,
         -1.3693e-02, -6.2095e-02,  2.9211e-01],
        [ 2.9129e-01,  1.1912e-01, -1.6710e-01,  3.2692e-01,  3.5365e-01,
          3.1340e-01, -1.6647e-01,  4.7914e-01],
        [-3.4178e-01,  4.1522e-01, -5.9854e-03,  1.0764e-01,  4.8675e-01,
         -3.1853e-01, -2.6564e-01,  5.6410e-02],
        [-3.0844e-01, -3.6636e-01,  4.1146e-02, -7.8689e-02,  3.4251e-01,
          3.2263e-01, -2.6700e-01, -3.9097e-01],
        [-1.7519e-01,  3.0574e-01,  1.4299e-01, -1.4686e-01, -3.8632e-01,
          4.2015e-01, -4.8322e-01, -4.8887e-01],
        [-3.9432e-01, -1.8439e-02,  4.2351e-01,  3.0249e-01, -4.8434e-01,
          3.7722e-01,  2.0810e-01,  8.5713e-02],
        [ 2.5196e-01,  1.1153e-01, -2.5949e-01,  2.7079e-01,  3.6000e-01,
          3.3896e-01, -1.5930e-01, -4.8073e-01],
        [-7.0415e-02, -4.4540e-01,  2.2714e-01, -1.2808e-01,  1.5808e-01,
         -6.4718e-02,  4.2304e-01,  3.7765e-01],
        [ 4.3692e-01,  2.8713e-01,  3.1711e-01,  5.7701e-02,  2.1744e-01,
         -2.6719e-01, -3.4222e-01,  8.4679e-02],
        [ 1.2451e-01, -2.5249e-01,  3.1327e-01,  2.8328e-02,  1.9587e-01,
         -3.6372e-01,  4.3680e-01,  2.7137e-01],
        [-3.5984e-01,  1.2020e-03, -1.7577e-01, -2.6097e-01, -2.1561e-01,
          4.4127e-01,  2.6533e-02,  2.6708e-01],
        [ 3.9949e-01,  1.0179e-01, -4.7481e-01,  4.7094e-01,  1.9533e-02,
          8.1277e-02,  4.8824e-01,  2.6319e-01],
        [-5.3213e-02, -1.1470e-02,  2.6261e-02, -1.8858e-01, -3.1008e-01,
         -1.7623e-01,  2.3204e-01, -1.6513e-01],
        [ 5.1607e-02, -4.8571e-01, -1.1655e-01,  1.0322e-01, -1.7686e-01,
         -7.8604e-02, -2.8905e-01,  3.1830e-01],
        [-3.7516e-02, -1.8444e-01,  1.2874e-01, -3.3350e-01,  1.1800e-01,
          3.2729e-01,  2.5811e-01, -1.5164e-01],
        [ 1.0696e-01, -1.2461e-01, -4.0443e-01,  4.6550e-01, -4.7822e-01,
          1.2745e-01,  3.5004e-01, -2.2483e-01],
        [-1.0036e-01, -2.0767e-01,  4.2157e-01,  5.3280e-03,  2.7303e-01,
         -4.6740e-02, -1.5271e-01, -4.2430e-01],
        [ 6.1966e-02, -1.0609e-01,  2.1125e-01,  4.4265e-01, -7.1083e-02,
         -1.6545e-01,  3.2856e-01,  7.7141e-02],
        [ 4.7835e-01, -4.8314e-03, -3.8695e-02,  3.5930e-01, -3.5636e-01,
         -4.2568e-01,  3.4648e-01,  3.0902e-01],
        [-4.4069e-01,  2.2887e-01, -2.7858e-01,  2.9805e-01,  3.0120e-02,
         -1.8267e-01,  4.7830e-02, -2.3703e-01],
        [ 1.6560e-01, -3.9347e-01, -1.5621e-01, -7.4060e-02, -4.7593e-01,
         -2.9508e-01, -4.7205e-02, -3.1585e-01],
        [ 4.8253e-01,  1.6393e-01, -2.7680e-01, -3.0742e-01,  3.4217e-01,
          2.8088e-01,  7.9270e-03,  4.1555e-02],
        [-4.0581e-02, -5.4133e-02,  1.4810e-02, -9.0167e-02, -2.9838e-01,
          1.0859e-01, -2.4039e-02, -2.5320e-01],
        [-3.8352e-01,  4.7281e-01, -2.0216e-01, -4.2485e-01,  3.5160e-01,
         -3.6317e-01, -1.8242e-01, -1.6885e-01],
        [-3.4973e-01, -1.3115e-01, -4.9215e-01, -3.0751e-01, -3.0007e-01,
         -3.9759e-01,  3.7730e-01,  2.9348e-01],
        [ 1.8084e-01, -4.3669e-01, -1.7375e-01,  3.2108e-01,  3.1741e-01,
          1.5175e-01, -4.1881e-01, -3.0161e-01],
        [ 3.2600e-01, -6.7508e-02,  1.1908e-01,  4.9437e-01,  2.5420e-01,
         -3.1964e-02, -5.5197e-02,  1.5475e-01],
        [-1.2575e-01, -2.2463e-01,  1.8694e-01,  2.5097e-01, -1.2909e-01,
          1.5817e-01, -6.7702e-02, -1.6119e-01],
        [ 3.7534e-01,  3.6966e-01,  4.3056e-01, -3.4284e-01,  2.4259e-01,
         -9.6645e-02, -4.7147e-02, -2.4931e-01],
        [-1.9530e-01, -4.1402e-01, -4.0880e-01, -3.6032e-01,  5.3089e-03,
          1.7237e-01,  1.3188e-01,  1.7112e-01],
        [ 2.0171e-01,  3.0668e-01,  1.2067e-02, -1.2997e-01, -4.6105e-01,
          9.1161e-02,  3.0934e-02, -9.0938e-02],
        [-9.1223e-02, -1.2751e-01, -3.1900e-01, -2.2704e-01, -1.7249e-01,
          3.3192e-01, -1.3609e-01,  2.0413e-01],
        [-3.0823e-01,  4.8354e-01, -4.0271e-01,  2.7635e-01, -7.1984e-02,
          3.8145e-02,  3.3917e-01,  4.9370e-01],
        [ 4.8517e-01, -3.1871e-01, -1.6700e-01, -3.8554e-01, -3.4095e-01,
         -3.5571e-01, -4.9841e-01, -3.5627e-01],
        [-1.0353e-01,  4.5409e-01, -1.9339e-04, -4.0730e-01,  3.3234e-01,
          8.0021e-02,  3.5921e-01,  2.8297e-01],
        [ 5.3276e-02, -3.5597e-01, -6.6298e-02,  3.1664e-01, -2.2763e-01,
         -4.9504e-01,  1.6677e-01, -1.8754e-02],
        [ 4.0065e-01,  3.3636e-01, -2.7679e-01, -2.0668e-01,  4.4267e-01,
         -1.8897e-02, -2.4028e-01,  3.9419e-01],
        [-2.3750e-01, -1.7520e-01, -3.3269e-01, -4.9066e-01, -1.5546e-01,
         -2.6583e-01, -4.3690e-01, -8.9235e-02],
        [ 2.9683e-01,  3.7650e-01, -1.8502e-01, -4.5714e-01, -2.9762e-01,
          1.6978e-01, -3.3652e-01, -3.5343e-01]])
2025-03-18 17:59:30.336108 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([2281701379],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[not compare]  None tensor([[ 0.0484,  0.0281,  0.4848, -0.0823, -0.4193,  0.0938, -0.3171,  0.3393],
        [ 0.0511, -0.2967,  0.3439, -0.4212, -0.3133,  0.0268, -0.1807, -0.0925],
        [-0.0037, -0.2223, -0.3443, -0.1512,  0.1677, -0.1244, -0.2737, -0.0465],
        [-0.0645, -0.2428, -0.2034,  0.0895,  0.2852, -0.4458,  0.2768,  0.0377],
        [-0.4529, -0.2788, -0.2556, -0.4018, -0.4018, -0.4285,  0.0876,  0.0899],
        [-0.2690,  0.1765,  0.0108,  0.0275, -0.0992, -0.2401,  0.1633,  0.3845],
        [-0.2912,  0.4370, -0.0304, -0.4121,  0.3065, -0.0720, -0.0408,  0.2474],
        [-0.3877,  0.0487,  0.3768,  0.0740, -0.0110, -0.2156,  0.1263,  0.1000],
        [ 0.0952, -0.0632,  0.4541,  0.3298, -0.0306, -0.4336, -0.3849,  0.3670],
        [ 0.0492,  0.4617,  0.1594,  0.0136, -0.3000,  0.3054, -0.2758, -0.1845],
        [ 0.1690,  0.1869, -0.4699,  0.2972,  0.1792, -0.0884, -0.3819, -0.0929],
        [ 0.4620,  0.0031,  0.0006,  0.0840, -0.4168, -0.0244,  0.1268,  0.0438],
        [ 0.1872,  0.1066, -0.1214,  0.3996, -0.4279,  0.2485,  0.4721, -0.3411],
        [ 0.4908,  0.3959, -0.3138, -0.0048, -0.2946,  0.3133, -0.2034,  0.4568],
        [-0.0391, -0.4032, -0.0915,  0.4351,  0.1699, -0.0348, -0.2171,  0.3340],
        [ 0.2714, -0.4197,  0.4182, -0.2406, -0.0463,  0.1279, -0.3931,  0.4416],
        [ 0.4735,  0.0931,  0.2053, -0.4340, -0.3020,  0.0616, -0.2981, -0.4571],
        [ 0.3038, -0.2089, -0.3432,  0.2915,  0.3432, -0.2437, -0.3871, -0.4010],
        [ 0.2476, -0.0188, -0.0904, -0.2635, -0.2521, -0.0813, -0.1598,  0.1720],
        [ 0.3897, -0.4393,  0.1769, -0.0807, -0.0796, -0.3808,  0.2053,  0.4491],
        [ 0.1310, -0.1263,  0.3298, -0.3903,  0.4158,  0.3685, -0.3886,  0.3485],
        [ 0.4611,  0.2004, -0.3821,  0.1484, -0.2185,  0.0728, -0.3881, -0.0473],
        [-0.0684,  0.2234, -0.0432, -0.4509, -0.3426,  0.1713,  0.4235,  0.4872],
        [-0.4578,  0.1196,  0.3732, -0.2135, -0.3189, -0.3496, -0.0678, -0.2504],
        [ 0.4072,  0.4311,  0.3120,  0.1567,  0.2731, -0.3681,  0.4249, -0.0081],
        [-0.0885, -0.0801,  0.4117,  0.3465,  0.0609,  0.4675, -0.2372,  0.0151],
        [ 0.4155, -0.4434, -0.3104, -0.0373, -0.0309, -0.1794, -0.3408, -0.4078],
        [-0.1882, -0.2508, -0.4529, -0.3509, -0.2879, -0.3444, -0.1852,  0.1615],
        [ 0.2833, -0.4100,  0.2345, -0.2945, -0.2654, -0.3231, -0.0526, -0.1874],
        [ 0.2057, -0.1900,  0.4153, -0.2166, -0.4624,  0.3620, -0.3664,  0.3834],
        [ 0.0831, -0.2485, -0.1811, -0.1992,  0.0619,  0.1468, -0.0554,  0.4171],
        [ 0.4627,  0.2674,  0.1010,  0.4793,  0.0583,  0.1749, -0.0764,  0.4447],
        [-0.4045, -0.2482,  0.1692, -0.4222,  0.1645, -0.3963,  0.4584, -0.4814],
        [ 0.0219,  0.2733, -0.0345,  0.2788,  0.0546, -0.4789,  0.2071,  0.3227],
        [ 0.3341, -0.4302,  0.1611,  0.3230, -0.1126,  0.3693,  0.4078,  0.3942],
        [ 0.1986, -0.3042, -0.1310,  0.0209, -0.4932, -0.1722,  0.3226, -0.4065],
        [-0.3192,  0.1801,  0.3844, -0.1755, -0.3510,  0.0006,  0.1494, -0.3149],
        [ 0.2617,  0.2831,  0.1154,  0.3832,  0.4216,  0.1614,  0.2396, -0.3041],
        [-0.4898, -0.3428, -0.1433, -0.1431,  0.3236,  0.4465, -0.4018,  0.2070],
        [ 0.2644, -0.2715, -0.2988, -0.0062,  0.4077, -0.1788, -0.3469, -0.1315],
        [-0.0797,  0.2082,  0.3957,  0.2769,  0.4804,  0.0075, -0.2175,  0.0728],
        [ 0.1514,  0.0999,  0.2309,  0.4697, -0.1997,  0.4845,  0.1972,  0.2727],
        [-0.4444, -0.1034,  0.3220,  0.3217, -0.3285,  0.3297,  0.0187, -0.1905],
        [ 0.0386,  0.0222,  0.0146,  0.2265,  0.3433, -0.2324, -0.3379, -0.3655],
        [ 0.1398,  0.0100,  0.0319,  0.4771,  0.3792, -0.2047, -0.0432,  0.4152],
        [ 0.1764,  0.2256,  0.0946,  0.0659, -0.4122,  0.1064, -0.2190, -0.4973],
        [-0.4895,  0.4696, -0.1786, -0.2320,  0.0515, -0.0591, -0.4939,  0.1803],
        [-0.2361,  0.3905, -0.2794, -0.1901, -0.1845,  0.1833,  0.0210,  0.2258],
        [-0.0852, -0.4885, -0.0798, -0.3649, -0.0898,  0.1966,  0.1483, -0.1092],
        [-0.3413,  0.3677, -0.4923,  0.1847, -0.3271, -0.4958,  0.2175,  0.0819],
        [ 0.0276,  0.2426,  0.1521, -0.3781,  0.1799,  0.4497,  0.1167,  0.1008],
        [-0.3544, -0.4906,  0.1791, -0.1431,  0.2752,  0.1275,  0.2872, -0.0951],
        [-0.3635, -0.2387,  0.3172,  0.3078,  0.3437, -0.4825, -0.2914, -0.2958],
        [-0.3017,  0.2449, -0.2864,  0.3556,  0.2643, -0.1189,  0.4108, -0.2328],
        [ 0.1696, -0.3831, -0.1946, -0.0861,  0.4633,  0.2293,  0.1139,  0.0629],
        [-0.4153,  0.4917,  0.2332, -0.3192, -0.3879,  0.1156,  0.2559, -0.1805],
        [ 0.3868,  0.4448, -0.2183, -0.4870, -0.4654,  0.2346,  0.4006,  0.3014],
        [-0.4909, -0.3741,  0.2582, -0.2665,  0.3944,  0.4137,  0.0950, -0.4809],
        [ 0.0700,  0.1928, -0.1743, -0.0955,  0.3018, -0.2648,  0.2804,  0.2612],
        [-0.1509, -0.1870,  0.3609, -0.0725, -0.4939,  0.2522, -0.3936, -0.3682],
        [ 0.2358, -0.3674, -0.4952, -0.1037, -0.0790, -0.4860, -0.1878,  0.2472],
        [ 0.1218,  0.0856,  0.3096,  0.2121,  0.4666,  0.3289, -0.1356,  0.1884],
        [ 0.2232,  0.1750,  0.2961,  0.0658, -0.4525, -0.2007, -0.1615,  0.2962],
        [ 0.2784,  0.1854,  0.0553,  0.2008, -0.1206,  0.0012, -0.1181, -0.3035]])
2025-03-18 18:01:28.858379 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([2281701379],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[not compare]  None tensor([[ 0.0484,  0.0281,  0.4848, -0.0823, -0.4193,  0.0938, -0.3171,  0.3393],
        [ 0.0511, -0.2967,  0.3439, -0.4212, -0.3133,  0.0268, -0.1807, -0.0925],
        [-0.0037, -0.2223, -0.3443, -0.1512,  0.1677, -0.1244, -0.2737, -0.0465],
        [-0.0645, -0.2428, -0.2034,  0.0895,  0.2852, -0.4458,  0.2768,  0.0377],
        [-0.4529, -0.2788, -0.2556, -0.4018, -0.4018, -0.4285,  0.0876,  0.0899],
        [-0.2690,  0.1765,  0.0108,  0.0275, -0.0992, -0.2401,  0.1633,  0.3845],
        [-0.2912,  0.4370, -0.0304, -0.4121,  0.3065, -0.0720, -0.0408,  0.2474],
        [-0.3877,  0.0487,  0.3768,  0.0740, -0.0110, -0.2156,  0.1263,  0.1000],
        [ 0.0952, -0.0632,  0.4541,  0.3298, -0.0306, -0.4336, -0.3849,  0.3670],
        [ 0.0492,  0.4617,  0.1594,  0.0136, -0.3000,  0.3054, -0.2758, -0.1845],
        [ 0.1690,  0.1869, -0.4699,  0.2972,  0.1792, -0.0884, -0.3819, -0.0929],
        [ 0.4620,  0.0031,  0.0006,  0.0840, -0.4168, -0.0244,  0.1268,  0.0438],
        [ 0.1872,  0.1066, -0.1214,  0.3996, -0.4279,  0.2485,  0.4721, -0.3411],
        [ 0.4908,  0.3959, -0.3138, -0.0048, -0.2946,  0.3133, -0.2034,  0.4568],
        [-0.0391, -0.4032, -0.0915,  0.4351,  0.1699, -0.0348, -0.2171,  0.3340],
        [ 0.2714, -0.4197,  0.4182, -0.2406, -0.0463,  0.1279, -0.3931,  0.4416],
        [ 0.4735,  0.0931,  0.2053, -0.4340, -0.3020,  0.0616, -0.2981, -0.4571],
        [ 0.3038, -0.2089, -0.3432,  0.2915,  0.3432, -0.2437, -0.3871, -0.4010],
        [ 0.2476, -0.0188, -0.0904, -0.2635, -0.2521, -0.0813, -0.1598,  0.1720],
        [ 0.3897, -0.4393,  0.1769, -0.0807, -0.0796, -0.3808,  0.2053,  0.4491],
        [ 0.1310, -0.1263,  0.3298, -0.3903,  0.4158,  0.3685, -0.3886,  0.3485],
        [ 0.4611,  0.2004, -0.3821,  0.1484, -0.2185,  0.0728, -0.3881, -0.0473],
        [-0.0684,  0.2234, -0.0432, -0.4509, -0.3426,  0.1713,  0.4235,  0.4872],
        [-0.4578,  0.1196,  0.3732, -0.2135, -0.3189, -0.3496, -0.0678, -0.2504],
        [ 0.4072,  0.4311,  0.3120,  0.1567,  0.2731, -0.3681,  0.4249, -0.0081],
        [-0.0885, -0.0801,  0.4117,  0.3465,  0.0609,  0.4675, -0.2372,  0.0151],
        [ 0.4155, -0.4434, -0.3104, -0.0373, -0.0309, -0.1794, -0.3408, -0.4078],
        [-0.1882, -0.2508, -0.4529, -0.3509, -0.2879, -0.3444, -0.1852,  0.1615],
        [ 0.2833, -0.4100,  0.2345, -0.2945, -0.2654, -0.3231, -0.0526, -0.1874],
        [ 0.2057, -0.1900,  0.4153, -0.2166, -0.4624,  0.3620, -0.3664,  0.3834],
        [ 0.0831, -0.2485, -0.1811, -0.1992,  0.0619,  0.1468, -0.0554,  0.4171],
        [ 0.4627,  0.2674,  0.1010,  0.4793,  0.0583,  0.1749, -0.0764,  0.4447],
        [-0.4045, -0.2482,  0.1692, -0.4222,  0.1645, -0.3963,  0.4584, -0.4814],
        [ 0.0219,  0.2733, -0.0345,  0.2788,  0.0546, -0.4789,  0.2071,  0.3227],
        [ 0.3341, -0.4302,  0.1611,  0.3230, -0.1126,  0.3693,  0.4078,  0.3942],
        [ 0.1986, -0.3042, -0.1310,  0.0209, -0.4932, -0.1722,  0.3226, -0.4065],
        [-0.3192,  0.1801,  0.3844, -0.1755, -0.3510,  0.0006,  0.1494, -0.3149],
        [ 0.2617,  0.2831,  0.1154,  0.3832,  0.4216,  0.1614,  0.2396, -0.3041],
        [-0.4898, -0.3428, -0.1433, -0.1431,  0.3236,  0.4465, -0.4018,  0.2070],
        [ 0.2644, -0.2715, -0.2988, -0.0062,  0.4077, -0.1788, -0.3469, -0.1315],
        [-0.0797,  0.2082,  0.3957,  0.2769,  0.4804,  0.0075, -0.2175,  0.0728],
        [ 0.1514,  0.0999,  0.2309,  0.4697, -0.1997,  0.4845,  0.1972,  0.2727],
        [-0.4444, -0.1034,  0.3220,  0.3217, -0.3285,  0.3297,  0.0187, -0.1905],
        [ 0.0386,  0.0222,  0.0146,  0.2265,  0.3433, -0.2324, -0.3379, -0.3655],
        [ 0.1398,  0.0100,  0.0319,  0.4771,  0.3792, -0.2047, -0.0432,  0.4152],
        [ 0.1764,  0.2256,  0.0946,  0.0659, -0.4122,  0.1064, -0.2190, -0.4973],
        [-0.4895,  0.4696, -0.1786, -0.2320,  0.0515, -0.0591, -0.4939,  0.1803],
        [-0.2361,  0.3905, -0.2794, -0.1901, -0.1845,  0.1833,  0.0210,  0.2258],
        [-0.0852, -0.4885, -0.0798, -0.3649, -0.0898,  0.1966,  0.1483, -0.1092],
        [-0.3413,  0.3677, -0.4923,  0.1847, -0.3271, -0.4958,  0.2175,  0.0819],
        [ 0.0276,  0.2426,  0.1521, -0.3781,  0.1799,  0.4497,  0.1167,  0.1008],
        [-0.3544, -0.4906,  0.1791, -0.1431,  0.2752,  0.1275,  0.2872, -0.0951],
        [-0.3635, -0.2387,  0.3172,  0.3078,  0.3437, -0.4825, -0.2914, -0.2958],
        [-0.3017,  0.2449, -0.2864,  0.3556,  0.2643, -0.1189,  0.4108, -0.2328],
        [ 0.1696, -0.3831, -0.1946, -0.0861,  0.4633,  0.2293,  0.1139,  0.0629],
        [-0.4153,  0.4917,  0.2332, -0.3192, -0.3879,  0.1156,  0.2559, -0.1805],
        [ 0.3868,  0.4448, -0.2183, -0.4870, -0.4654,  0.2346,  0.4006,  0.3014],
        [-0.4909, -0.3741,  0.2582, -0.2665,  0.3944,  0.4137,  0.0950, -0.4809],
        [ 0.0700,  0.1928, -0.1743, -0.0955,  0.3018, -0.2648,  0.2804,  0.2612],
        [-0.1509, -0.1870,  0.3609, -0.0725, -0.4939,  0.2522, -0.3936, -0.3682],
        [ 0.2358, -0.3674, -0.4952, -0.1037, -0.0790, -0.4860, -0.1878,  0.2472],
        [ 0.1218,  0.0856,  0.3096,  0.2121,  0.4666,  0.3289, -0.1356,  0.1884],
        [ 0.2232,  0.1750,  0.2961,  0.0658, -0.4525, -0.2007, -0.1615,  0.2962],
        [ 0.2784,  0.1854,  0.0553,  0.2008, -0.1206,  0.0012, -0.1181, -0.3035]])
2025-03-18 18:03:12.034697 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([142606337, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[not compare]  None tensor([[ 2.0819e-01,  3.4650e-02,  1.8292e-01, -2.8721e-01,  2.3018e-01,
         -2.7206e-01, -2.2519e-01, -8.0517e-02],
        [ 1.6704e-01,  2.7792e-01,  4.4896e-01, -1.1095e-01, -1.8964e-01,
          4.4402e-01, -9.1072e-02,  3.8606e-01],
        [ 1.1383e-01,  2.9602e-01,  9.5458e-02,  4.0912e-01,  4.6825e-01,
          4.9661e-01, -4.5595e-01, -2.6453e-01],
        [ 4.5521e-02, -4.3069e-01, -1.2416e-01,  1.7815e-01, -1.5392e-01,
         -8.2781e-02,  3.6905e-01,  3.4463e-01],
        [-1.2047e-01, -1.9021e-01, -1.2656e-01, -1.9735e-01, -3.1795e-01,
          7.7858e-02, -1.5394e-01,  1.9040e-01],
        [ 4.2418e-01, -4.5550e-01, -2.7407e-01, -3.6682e-01, -2.2456e-01,
         -3.3893e-01, -3.3062e-01, -3.3508e-01],
        [-3.9040e-01, -4.6307e-01, -4.9807e-01, -2.8355e-01,  7.9494e-02,
          1.9968e-01,  3.2524e-01, -2.1946e-01],
        [ 4.9202e-01,  1.5897e-01, -1.6661e-01,  1.6033e-01, -5.8961e-02,
          4.1099e-01,  3.5952e-01,  1.9490e-01],
        [-3.5677e-01, -8.1904e-03, -3.9764e-01,  1.6201e-01, -3.2615e-01,
         -4.2689e-01,  2.2314e-01,  4.0871e-01],
        [-2.0836e-01, -7.3891e-02, -3.7918e-01,  6.2680e-02, -1.2871e-01,
         -1.3983e-01,  2.8235e-01,  1.5336e-01],
        [ 2.7063e-01,  2.6137e-01,  1.9312e-01, -4.6818e-01, -2.0427e-01,
          2.5668e-01,  5.1453e-04,  2.6400e-01],
        [ 1.8068e-01, -2.0190e-01, -4.8288e-01, -4.5376e-01,  2.1634e-02,
         -4.0084e-01, -2.9506e-01,  3.5891e-01],
        [ 1.8597e-01, -3.2727e-01, -3.1309e-01,  1.3388e-01, -4.6187e-01,
          2.4899e-01,  2.6373e-01,  1.8808e-01],
        [ 4.5496e-01,  7.4547e-04,  4.8250e-01,  1.6268e-01, -7.1244e-02,
         -3.9697e-01,  1.5990e-01,  3.0246e-01],
        [-1.0365e-01, -3.5892e-01,  3.1288e-01,  3.7257e-01, -2.7899e-01,
          9.0670e-02, -3.7897e-01,  2.6059e-01],
        [-3.6591e-01,  4.4985e-01, -7.3246e-02,  4.9778e-01, -6.0728e-02,
          8.8742e-02, -1.1455e-01,  3.5773e-01],
        [ 2.3919e-01,  4.1178e-02, -3.5698e-02, -2.8753e-01, -2.9226e-01,
          3.1802e-01,  1.4694e-01,  2.8941e-01],
        [ 2.0070e-01, -1.3439e-01, -3.7211e-01,  1.5207e-01, -1.8104e-01,
          1.6520e-01, -4.7590e-01,  1.3231e-01],
        [ 8.5047e-02, -5.0129e-02,  4.2176e-01, -1.9794e-01,  9.8317e-02,
          1.5557e-01,  2.6898e-01, -5.3308e-02],
        [-8.0174e-02, -4.9304e-02,  2.7283e-01,  3.3938e-01,  2.2002e-01,
          4.1258e-01, -3.1825e-01,  1.0130e-01],
        [-4.6932e-01,  1.8535e-01,  3.1103e-01,  1.7427e-01, -2.4109e-01,
         -9.8682e-02,  4.8210e-01,  4.2181e-01],
        [ 1.6004e-01, -3.7367e-01,  8.5593e-02,  3.6062e-01,  2.7894e-02,
         -3.7111e-01, -2.1903e-01,  2.5290e-01],
        [-4.7081e-01,  2.6604e-01,  2.0545e-01, -3.9083e-01,  4.0114e-01,
         -2.7946e-01,  2.4625e-01,  4.6450e-02],
        [-2.4076e-01, -1.9259e-01,  3.3132e-01,  3.6346e-02,  2.0832e-01,
         -1.7643e-01, -1.4441e-01,  4.4574e-01],
        [ 3.7667e-01,  1.7433e-01, -3.1192e-01, -2.2852e-01, -3.0836e-01,
         -3.0696e-01,  1.5672e-01, -3.2449e-01],
        [-4.0908e-01, -1.1816e-01,  8.3295e-02, -1.0676e-02, -1.7365e-01,
         -1.3693e-02, -6.2095e-02,  2.9211e-01],
        [ 2.9129e-01,  1.1912e-01, -1.6710e-01,  3.2692e-01,  3.5365e-01,
          3.1340e-01, -1.6647e-01,  4.7914e-01],
        [-3.4178e-01,  4.1522e-01, -5.9854e-03,  1.0764e-01,  4.8675e-01,
         -3.1853e-01, -2.6564e-01,  5.6410e-02],
        [-3.0844e-01, -3.6636e-01,  4.1146e-02, -7.8689e-02,  3.4251e-01,
          3.2263e-01, -2.6700e-01, -3.9097e-01],
        [-1.7519e-01,  3.0574e-01,  1.4299e-01, -1.4686e-01, -3.8632e-01,
          4.2015e-01, -4.8322e-01, -4.8887e-01],
        [-3.9432e-01, -1.8439e-02,  4.2351e-01,  3.0249e-01, -4.8434e-01,
          3.7722e-01,  2.0810e-01,  8.5713e-02],
        [ 2.5196e-01,  1.1153e-01, -2.5949e-01,  2.7079e-01,  3.6000e-01,
          3.3896e-01, -1.5930e-01, -4.8073e-01],
        [-7.0415e-02, -4.4540e-01,  2.2714e-01, -1.2808e-01,  1.5808e-01,
         -6.4718e-02,  4.2304e-01,  3.7765e-01],
        [ 4.3692e-01,  2.8713e-01,  3.1711e-01,  5.7701e-02,  2.1744e-01,
         -2.6719e-01, -3.4222e-01,  8.4679e-02],
        [ 1.2451e-01, -2.5249e-01,  3.1327e-01,  2.8328e-02,  1.9587e-01,
         -3.6372e-01,  4.3680e-01,  2.7137e-01],
        [-3.5984e-01,  1.2020e-03, -1.7577e-01, -2.6097e-01, -2.1561e-01,
          4.4127e-01,  2.6533e-02,  2.6708e-01],
        [ 3.9949e-01,  1.0179e-01, -4.7481e-01,  4.7094e-01,  1.9533e-02,
          8.1277e-02,  4.8824e-01,  2.6319e-01],
        [-5.3213e-02, -1.1470e-02,  2.6261e-02, -1.8858e-01, -3.1008e-01,
         -1.7623e-01,  2.3204e-01, -1.6513e-01],
        [ 5.1607e-02, -4.8571e-01, -1.1655e-01,  1.0322e-01, -1.7686e-01,
         -7.8604e-02, -2.8905e-01,  3.1830e-01],
        [-3.7516e-02, -1.8444e-01,  1.2874e-01, -3.3350e-01,  1.1800e-01,
          3.2729e-01,  2.5811e-01, -1.5164e-01],
        [ 1.0696e-01, -1.2461e-01, -4.0443e-01,  4.6550e-01, -4.7822e-01,
          1.2745e-01,  3.5004e-01, -2.2483e-01],
        [-1.0036e-01, -2.0767e-01,  4.2157e-01,  5.3280e-03,  2.7303e-01,
         -4.6740e-02, -1.5271e-01, -4.2430e-01],
        [ 6.1966e-02, -1.0609e-01,  2.1125e-01,  4.4265e-01, -7.1083e-02,
         -1.6545e-01,  3.2856e-01,  7.7141e-02],
        [ 4.7835e-01, -4.8314e-03, -3.8695e-02,  3.5930e-01, -3.5636e-01,
         -4.2568e-01,  3.4648e-01,  3.0902e-01],
        [-4.4069e-01,  2.2887e-01, -2.7858e-01,  2.9805e-01,  3.0120e-02,
         -1.8267e-01,  4.7830e-02, -2.3703e-01],
        [ 1.6560e-01, -3.9347e-01, -1.5621e-01, -7.4060e-02, -4.7593e-01,
         -2.9508e-01, -4.7205e-02, -3.1585e-01],
        [ 4.8253e-01,  1.6393e-01, -2.7680e-01, -3.0742e-01,  3.4217e-01,
          2.8088e-01,  7.9270e-03,  4.1555e-02],
        [-4.0581e-02, -5.4133e-02,  1.4810e-02, -9.0167e-02, -2.9838e-01,
          1.0859e-01, -2.4039e-02, -2.5320e-01],
        [-3.8352e-01,  4.7281e-01, -2.0216e-01, -4.2485e-01,  3.5160e-01,
         -3.6317e-01, -1.8242e-01, -1.6885e-01],
        [-3.4973e-01, -1.3115e-01, -4.9215e-01, -3.0751e-01, -3.0007e-01,
         -3.9759e-01,  3.7730e-01,  2.9348e-01],
        [ 1.8084e-01, -4.3669e-01, -1.7375e-01,  3.2108e-01,  3.1741e-01,
          1.5175e-01, -4.1881e-01, -3.0161e-01],
        [ 3.2600e-01, -6.7508e-02,  1.1908e-01,  4.9437e-01,  2.5420e-01,
         -3.1964e-02, -5.5197e-02,  1.5475e-01],
        [-1.2575e-01, -2.2463e-01,  1.8694e-01,  2.5097e-01, -1.2909e-01,
          1.5817e-01, -6.7702e-02, -1.6119e-01],
        [ 3.7534e-01,  3.6966e-01,  4.3056e-01, -3.4284e-01,  2.4259e-01,
         -9.6645e-02, -4.7147e-02, -2.4931e-01],
        [-1.9530e-01, -4.1402e-01, -4.0880e-01, -3.6032e-01,  5.3089e-03,
          1.7237e-01,  1.3188e-01,  1.7112e-01],
        [ 2.0171e-01,  3.0668e-01,  1.2067e-02, -1.2997e-01, -4.6105e-01,
          9.1161e-02,  3.0934e-02, -9.0938e-02],
        [-9.1223e-02, -1.2751e-01, -3.1900e-01, -2.2704e-01, -1.7249e-01,
          3.3192e-01, -1.3609e-01,  2.0413e-01],
        [-3.0823e-01,  4.8354e-01, -4.0271e-01,  2.7635e-01, -7.1984e-02,
          3.8145e-02,  3.3917e-01,  4.9370e-01],
        [ 4.8517e-01, -3.1871e-01, -1.6700e-01, -3.8554e-01, -3.4095e-01,
         -3.5571e-01, -4.9841e-01, -3.5627e-01],
        [-1.0353e-01,  4.5409e-01, -1.9339e-04, -4.0730e-01,  3.3234e-01,
          8.0021e-02,  3.5921e-01,  2.8297e-01],
        [ 5.3276e-02, -3.5597e-01, -6.6298e-02,  3.1664e-01, -2.2763e-01,
         -4.9504e-01,  1.6677e-01, -1.8754e-02],
        [ 4.0065e-01,  3.3636e-01, -2.7679e-01, -2.0668e-01,  4.4267e-01,
         -1.8897e-02, -2.4028e-01,  3.9419e-01],
        [-2.3750e-01, -1.7520e-01, -3.3269e-01, -4.9066e-01, -1.5546e-01,
         -2.6583e-01, -4.3690e-01, -8.9235e-02],
        [ 2.9683e-01,  3.7650e-01, -1.8502e-01, -4.5714e-01, -2.9762e-01,
          1.6978e-01, -3.3652e-01, -3.5343e-01]])
2025-03-18 18:04:47.906412 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([142606337, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[not compare]  None tensor([[ 2.0819e-01,  3.4650e-02,  1.8292e-01, -2.8721e-01,  2.3018e-01,
         -2.7206e-01, -2.2519e-01, -8.0517e-02],
        [ 1.6704e-01,  2.7792e-01,  4.4896e-01, -1.1095e-01, -1.8964e-01,
          4.4402e-01, -9.1072e-02,  3.8606e-01],
        [ 1.1383e-01,  2.9602e-01,  9.5458e-02,  4.0912e-01,  4.6825e-01,
          4.9661e-01, -4.5595e-01, -2.6453e-01],
        [ 4.5521e-02, -4.3069e-01, -1.2416e-01,  1.7815e-01, -1.5392e-01,
         -8.2781e-02,  3.6905e-01,  3.4463e-01],
        [-1.2047e-01, -1.9021e-01, -1.2656e-01, -1.9735e-01, -3.1795e-01,
          7.7858e-02, -1.5394e-01,  1.9040e-01],
        [ 4.2418e-01, -4.5550e-01, -2.7407e-01, -3.6682e-01, -2.2456e-01,
         -3.3893e-01, -3.3062e-01, -3.3508e-01],
        [-3.9040e-01, -4.6307e-01, -4.9807e-01, -2.8355e-01,  7.9494e-02,
          1.9968e-01,  3.2524e-01, -2.1946e-01],
        [ 4.9202e-01,  1.5897e-01, -1.6661e-01,  1.6033e-01, -5.8961e-02,
          4.1099e-01,  3.5952e-01,  1.9490e-01],
        [-3.5677e-01, -8.1904e-03, -3.9764e-01,  1.6201e-01, -3.2615e-01,
         -4.2689e-01,  2.2314e-01,  4.0871e-01],
        [-2.0836e-01, -7.3891e-02, -3.7918e-01,  6.2680e-02, -1.2871e-01,
         -1.3983e-01,  2.8235e-01,  1.5336e-01],
        [ 2.7063e-01,  2.6137e-01,  1.9312e-01, -4.6818e-01, -2.0427e-01,
          2.5668e-01,  5.1453e-04,  2.6400e-01],
        [ 1.8068e-01, -2.0190e-01, -4.8288e-01, -4.5376e-01,  2.1634e-02,
         -4.0084e-01, -2.9506e-01,  3.5891e-01],
        [ 1.8597e-01, -3.2727e-01, -3.1309e-01,  1.3388e-01, -4.6187e-01,
          2.4899e-01,  2.6373e-01,  1.8808e-01],
        [ 4.5496e-01,  7.4547e-04,  4.8250e-01,  1.6268e-01, -7.1244e-02,
         -3.9697e-01,  1.5990e-01,  3.0246e-01],
        [-1.0365e-01, -3.5892e-01,  3.1288e-01,  3.7257e-01, -2.7899e-01,
          9.0670e-02, -3.7897e-01,  2.6059e-01],
        [-3.6591e-01,  4.4985e-01, -7.3246e-02,  4.9778e-01, -6.0728e-02,
          8.8742e-02, -1.1455e-01,  3.5773e-01],
        [ 2.3919e-01,  4.1178e-02, -3.5698e-02, -2.8753e-01, -2.9226e-01,
          3.1802e-01,  1.4694e-01,  2.8941e-01],
        [ 2.0070e-01, -1.3439e-01, -3.7211e-01,  1.5207e-01, -1.8104e-01,
          1.6520e-01, -4.7590e-01,  1.3231e-01],
        [ 8.5047e-02, -5.0129e-02,  4.2176e-01, -1.9794e-01,  9.8317e-02,
          1.5557e-01,  2.6898e-01, -5.3308e-02],
        [-8.0174e-02, -4.9304e-02,  2.7283e-01,  3.3938e-01,  2.2002e-01,
          4.1258e-01, -3.1825e-01,  1.0130e-01],
        [-4.6932e-01,  1.8535e-01,  3.1103e-01,  1.7427e-01, -2.4109e-01,
         -9.8682e-02,  4.8210e-01,  4.2181e-01],
        [ 1.6004e-01, -3.7367e-01,  8.5593e-02,  3.6062e-01,  2.7894e-02,
         -3.7111e-01, -2.1903e-01,  2.5290e-01],
        [-4.7081e-01,  2.6604e-01,  2.0545e-01, -3.9083e-01,  4.0114e-01,
         -2.7946e-01,  2.4625e-01,  4.6450e-02],
        [-2.4076e-01, -1.9259e-01,  3.3132e-01,  3.6346e-02,  2.0832e-01,
         -1.7643e-01, -1.4441e-01,  4.4574e-01],
        [ 3.7667e-01,  1.7433e-01, -3.1192e-01, -2.2852e-01, -3.0836e-01,
         -3.0696e-01,  1.5672e-01, -3.2449e-01],
        [-4.0908e-01, -1.1816e-01,  8.3295e-02, -1.0676e-02, -1.7365e-01,
         -1.3693e-02, -6.2095e-02,  2.9211e-01],
        [ 2.9129e-01,  1.1912e-01, -1.6710e-01,  3.2692e-01,  3.5365e-01,
          3.1340e-01, -1.6647e-01,  4.7914e-01],
        [-3.4178e-01,  4.1522e-01, -5.9854e-03,  1.0764e-01,  4.8675e-01,
         -3.1853e-01, -2.6564e-01,  5.6410e-02],
        [-3.0844e-01, -3.6636e-01,  4.1146e-02, -7.8689e-02,  3.4251e-01,
          3.2263e-01, -2.6700e-01, -3.9097e-01],
        [-1.7519e-01,  3.0574e-01,  1.4299e-01, -1.4686e-01, -3.8632e-01,
          4.2015e-01, -4.8322e-01, -4.8887e-01],
        [-3.9432e-01, -1.8439e-02,  4.2351e-01,  3.0249e-01, -4.8434e-01,
          3.7722e-01,  2.0810e-01,  8.5713e-02],
        [ 2.5196e-01,  1.1153e-01, -2.5949e-01,  2.7079e-01,  3.6000e-01,
          3.3896e-01, -1.5930e-01, -4.8073e-01],
        [-7.0415e-02, -4.4540e-01,  2.2714e-01, -1.2808e-01,  1.5808e-01,
         -6.4718e-02,  4.2304e-01,  3.7765e-01],
        [ 4.3692e-01,  2.8713e-01,  3.1711e-01,  5.7701e-02,  2.1744e-01,
         -2.6719e-01, -3.4222e-01,  8.4679e-02],
        [ 1.2451e-01, -2.5249e-01,  3.1327e-01,  2.8328e-02,  1.9587e-01,
         -3.6372e-01,  4.3680e-01,  2.7137e-01],
        [-3.5984e-01,  1.2020e-03, -1.7577e-01, -2.6097e-01, -2.1561e-01,
          4.4127e-01,  2.6533e-02,  2.6708e-01],
        [ 3.9949e-01,  1.0179e-01, -4.7481e-01,  4.7094e-01,  1.9533e-02,
          8.1277e-02,  4.8824e-01,  2.6319e-01],
        [-5.3213e-02, -1.1470e-02,  2.6261e-02, -1.8858e-01, -3.1008e-01,
         -1.7623e-01,  2.3204e-01, -1.6513e-01],
        [ 5.1607e-02, -4.8571e-01, -1.1655e-01,  1.0322e-01, -1.7686e-01,
         -7.8604e-02, -2.8905e-01,  3.1830e-01],
        [-3.7516e-02, -1.8444e-01,  1.2874e-01, -3.3350e-01,  1.1800e-01,
          3.2729e-01,  2.5811e-01, -1.5164e-01],
        [ 1.0696e-01, -1.2461e-01, -4.0443e-01,  4.6550e-01, -4.7822e-01,
          1.2745e-01,  3.5004e-01, -2.2483e-01],
        [-1.0036e-01, -2.0767e-01,  4.2157e-01,  5.3280e-03,  2.7303e-01,
         -4.6740e-02, -1.5271e-01, -4.2430e-01],
        [ 6.1966e-02, -1.0609e-01,  2.1125e-01,  4.4265e-01, -7.1083e-02,
         -1.6545e-01,  3.2856e-01,  7.7141e-02],
        [ 4.7835e-01, -4.8314e-03, -3.8695e-02,  3.5930e-01, -3.5636e-01,
         -4.2568e-01,  3.4648e-01,  3.0902e-01],
        [-4.4069e-01,  2.2887e-01, -2.7858e-01,  2.9805e-01,  3.0120e-02,
         -1.8267e-01,  4.7830e-02, -2.3703e-01],
        [ 1.6560e-01, -3.9347e-01, -1.5621e-01, -7.4060e-02, -4.7593e-01,
         -2.9508e-01, -4.7205e-02, -3.1585e-01],
        [ 4.8253e-01,  1.6393e-01, -2.7680e-01, -3.0742e-01,  3.4217e-01,
          2.8088e-01,  7.9270e-03,  4.1555e-02],
        [-4.0581e-02, -5.4133e-02,  1.4810e-02, -9.0167e-02, -2.9838e-01,
          1.0859e-01, -2.4039e-02, -2.5320e-01],
        [-3.8352e-01,  4.7281e-01, -2.0216e-01, -4.2485e-01,  3.5160e-01,
         -3.6317e-01, -1.8242e-01, -1.6885e-01],
        [-3.4973e-01, -1.3115e-01, -4.9215e-01, -3.0751e-01, -3.0007e-01,
         -3.9759e-01,  3.7730e-01,  2.9348e-01],
        [ 1.8084e-01, -4.3669e-01, -1.7375e-01,  3.2108e-01,  3.1741e-01,
          1.5175e-01, -4.1881e-01, -3.0161e-01],
        [ 3.2600e-01, -6.7508e-02,  1.1908e-01,  4.9437e-01,  2.5420e-01,
         -3.1964e-02, -5.5197e-02,  1.5475e-01],
        [-1.2575e-01, -2.2463e-01,  1.8694e-01,  2.5097e-01, -1.2909e-01,
          1.5817e-01, -6.7702e-02, -1.6119e-01],
        [ 3.7534e-01,  3.6966e-01,  4.3056e-01, -3.4284e-01,  2.4259e-01,
         -9.6645e-02, -4.7147e-02, -2.4931e-01],
        [-1.9530e-01, -4.1402e-01, -4.0880e-01, -3.6032e-01,  5.3089e-03,
          1.7237e-01,  1.3188e-01,  1.7112e-01],
        [ 2.0171e-01,  3.0668e-01,  1.2067e-02, -1.2997e-01, -4.6105e-01,
          9.1161e-02,  3.0934e-02, -9.0938e-02],
        [-9.1223e-02, -1.2751e-01, -3.1900e-01, -2.2704e-01, -1.7249e-01,
          3.3192e-01, -1.3609e-01,  2.0413e-01],
        [-3.0823e-01,  4.8354e-01, -4.0271e-01,  2.7635e-01, -7.1984e-02,
          3.8145e-02,  3.3917e-01,  4.9370e-01],
        [ 4.8517e-01, -3.1871e-01, -1.6700e-01, -3.8554e-01, -3.4095e-01,
         -3.5571e-01, -4.9841e-01, -3.5627e-01],
        [-1.0353e-01,  4.5409e-01, -1.9339e-04, -4.0730e-01,  3.3234e-01,
          8.0021e-02,  3.5921e-01,  2.8297e-01],
        [ 5.3276e-02, -3.5597e-01, -6.6298e-02,  3.1664e-01, -2.2763e-01,
         -4.9504e-01,  1.6677e-01, -1.8754e-02],
        [ 4.0065e-01,  3.3636e-01, -2.7679e-01, -2.0668e-01,  4.4267e-01,
         -1.8897e-02, -2.4028e-01,  3.9419e-01],
        [-2.3750e-01, -1.7520e-01, -3.3269e-01, -4.9066e-01, -1.5546e-01,
         -2.6583e-01, -4.3690e-01, -8.9235e-02],
        [ 2.9683e-01,  3.7650e-01, -1.8502e-01, -4.5714e-01, -2.9762e-01,
          1.6978e-01, -3.3652e-01, -3.5343e-01]])
2025-03-18 18:06:16.360255 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([2281701379],"float32"),Tensor([64],"float32"),], )

[not compare]  None tensor([[ 0.0484,  0.0281,  0.4848, -0.0823, -0.4193,  0.0938, -0.3171,  0.3393],
        [ 0.0511, -0.2967,  0.3439, -0.4212, -0.3133,  0.0268, -0.1807, -0.0925],
        [-0.0037, -0.2223, -0.3443, -0.1512,  0.1677, -0.1244, -0.2737, -0.0465],
        [-0.0645, -0.2428, -0.2034,  0.0895,  0.2852, -0.4458,  0.2768,  0.0377],
        [-0.4529, -0.2788, -0.2556, -0.4018, -0.4018, -0.4285,  0.0876,  0.0899],
        [-0.2690,  0.1765,  0.0108,  0.0275, -0.0992, -0.2401,  0.1633,  0.3845],
        [-0.2912,  0.4370, -0.0304, -0.4121,  0.3065, -0.0720, -0.0408,  0.2474],
        [-0.3877,  0.0487,  0.3768,  0.0740, -0.0110, -0.2156,  0.1263,  0.1000],
        [ 0.0952, -0.0632,  0.4541,  0.3298, -0.0306, -0.4336, -0.3849,  0.3670],
        [ 0.0492,  0.4617,  0.1594,  0.0136, -0.3000,  0.3054, -0.2758, -0.1845],
        [ 0.1690,  0.1869, -0.4699,  0.2972,  0.1792, -0.0884, -0.3819, -0.0929],
        [ 0.4620,  0.0031,  0.0006,  0.0840, -0.4168, -0.0244,  0.1268,  0.0438],
        [ 0.1872,  0.1066, -0.1214,  0.3996, -0.4279,  0.2485,  0.4721, -0.3411],
        [ 0.4908,  0.3959, -0.3138, -0.0048, -0.2946,  0.3133, -0.2034,  0.4568],
        [-0.0391, -0.4032, -0.0915,  0.4351,  0.1699, -0.0348, -0.2171,  0.3340],
        [ 0.2714, -0.4197,  0.4182, -0.2406, -0.0463,  0.1279, -0.3931,  0.4416],
        [ 0.4735,  0.0931,  0.2053, -0.4340, -0.3020,  0.0616, -0.2981, -0.4571],
        [ 0.3038, -0.2089, -0.3432,  0.2915,  0.3432, -0.2437, -0.3871, -0.4010],
        [ 0.2476, -0.0188, -0.0904, -0.2635, -0.2521, -0.0813, -0.1598,  0.1720],
        [ 0.3897, -0.4393,  0.1769, -0.0807, -0.0796, -0.3808,  0.2053,  0.4491],
        [ 0.1310, -0.1263,  0.3298, -0.3903,  0.4158,  0.3685, -0.3886,  0.3485],
        [ 0.4611,  0.2004, -0.3821,  0.1484, -0.2185,  0.0728, -0.3881, -0.0473],
        [-0.0684,  0.2234, -0.0432, -0.4509, -0.3426,  0.1713,  0.4235,  0.4872],
        [-0.4578,  0.1196,  0.3732, -0.2135, -0.3189, -0.3496, -0.0678, -0.2504],
        [ 0.4072,  0.4311,  0.3120,  0.1567,  0.2731, -0.3681,  0.4249, -0.0081],
        [-0.0885, -0.0801,  0.4117,  0.3465,  0.0609,  0.4675, -0.2372,  0.0151],
        [ 0.4155, -0.4434, -0.3104, -0.0373, -0.0309, -0.1794, -0.3408, -0.4078],
        [-0.1882, -0.2508, -0.4529, -0.3509, -0.2879, -0.3444, -0.1852,  0.1615],
        [ 0.2833, -0.4100,  0.2345, -0.2945, -0.2654, -0.3231, -0.0526, -0.1874],
        [ 0.2057, -0.1900,  0.4153, -0.2166, -0.4624,  0.3620, -0.3664,  0.3834],
        [ 0.0831, -0.2485, -0.1811, -0.1992,  0.0619,  0.1468, -0.0554,  0.4171],
        [ 0.4627,  0.2674,  0.1010,  0.4793,  0.0583,  0.1749, -0.0764,  0.4447],
        [-0.4045, -0.2482,  0.1692, -0.4222,  0.1645, -0.3963,  0.4584, -0.4814],
        [ 0.0219,  0.2733, -0.0345,  0.2788,  0.0546, -0.4789,  0.2071,  0.3227],
        [ 0.3341, -0.4302,  0.1611,  0.3230, -0.1126,  0.3693,  0.4078,  0.3942],
        [ 0.1986, -0.3042, -0.1310,  0.0209, -0.4932, -0.1722,  0.3226, -0.4065],
        [-0.3192,  0.1801,  0.3844, -0.1755, -0.3510,  0.0006,  0.1494, -0.3149],
        [ 0.2617,  0.2831,  0.1154,  0.3832,  0.4216,  0.1614,  0.2396, -0.3041],
        [-0.4898, -0.3428, -0.1433, -0.1431,  0.3236,  0.4465, -0.4018,  0.2070],
        [ 0.2644, -0.2715, -0.2988, -0.0062,  0.4077, -0.1788, -0.3469, -0.1315],
        [-0.0797,  0.2082,  0.3957,  0.2769,  0.4804,  0.0075, -0.2175,  0.0728],
        [ 0.1514,  0.0999,  0.2309,  0.4697, -0.1997,  0.4845,  0.1972,  0.2727],
        [-0.4444, -0.1034,  0.3220,  0.3217, -0.3285,  0.3297,  0.0187, -0.1905],
        [ 0.0386,  0.0222,  0.0146,  0.2265,  0.3433, -0.2324, -0.3379, -0.3655],
        [ 0.1398,  0.0100,  0.0319,  0.4771,  0.3792, -0.2047, -0.0432,  0.4152],
        [ 0.1764,  0.2256,  0.0946,  0.0659, -0.4122,  0.1064, -0.2190, -0.4973],
        [-0.4895,  0.4696, -0.1786, -0.2320,  0.0515, -0.0591, -0.4939,  0.1803],
        [-0.2361,  0.3905, -0.2794, -0.1901, -0.1845,  0.1833,  0.0210,  0.2258],
        [-0.0852, -0.4885, -0.0798, -0.3649, -0.0898,  0.1966,  0.1483, -0.1092],
        [-0.3413,  0.3677, -0.4923,  0.1847, -0.3271, -0.4958,  0.2175,  0.0819],
        [ 0.0276,  0.2426,  0.1521, -0.3781,  0.1799,  0.4497,  0.1167,  0.1008],
        [-0.3544, -0.4906,  0.1791, -0.1431,  0.2752,  0.1275,  0.2872, -0.0951],
        [-0.3635, -0.2387,  0.3172,  0.3078,  0.3437, -0.4825, -0.2914, -0.2958],
        [-0.3017,  0.2449, -0.2864,  0.3556,  0.2643, -0.1189,  0.4108, -0.2328],
        [ 0.1696, -0.3831, -0.1946, -0.0861,  0.4633,  0.2293,  0.1139,  0.0629],
        [-0.4153,  0.4917,  0.2332, -0.3192, -0.3879,  0.1156,  0.2559, -0.1805],
        [ 0.3868,  0.4448, -0.2183, -0.4870, -0.4654,  0.2346,  0.4006,  0.3014],
        [-0.4909, -0.3741,  0.2582, -0.2665,  0.3944,  0.4137,  0.0950, -0.4809],
        [ 0.0700,  0.1928, -0.1743, -0.0955,  0.3018, -0.2648,  0.2804,  0.2612],
        [-0.1509, -0.1870,  0.3609, -0.0725, -0.4939,  0.2522, -0.3936, -0.3682],
        [ 0.2358, -0.3674, -0.4952, -0.1037, -0.0790, -0.4860, -0.1878,  0.2472],
        [ 0.1218,  0.0856,  0.3096,  0.2121,  0.4666,  0.3289, -0.1356,  0.1884],
        [ 0.2232,  0.1750,  0.2961,  0.0658, -0.4525, -0.2007, -0.1615,  0.2962],
        [ 0.2784,  0.1854,  0.0553,  0.2008, -0.1206,  0.0012, -0.1181, -0.3035]])
2025-03-18 18:08:06.373168 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([2281701379],"float32"),], )

[not compare]  None tensor([[ 0.0484,  0.0281,  0.4848, -0.0823, -0.4193,  0.0938, -0.3171,  0.3393],
        [ 0.0511, -0.2967,  0.3439, -0.4212, -0.3133,  0.0268, -0.1807, -0.0925],
        [-0.0037, -0.2223, -0.3443, -0.1512,  0.1677, -0.1244, -0.2737, -0.0465],
        [-0.0645, -0.2428, -0.2034,  0.0895,  0.2852, -0.4458,  0.2768,  0.0377],
        [-0.4529, -0.2788, -0.2556, -0.4018, -0.4018, -0.4285,  0.0876,  0.0899],
        [-0.2690,  0.1765,  0.0108,  0.0275, -0.0992, -0.2401,  0.1633,  0.3845],
        [-0.2912,  0.4370, -0.0304, -0.4121,  0.3065, -0.0720, -0.0408,  0.2474],
        [-0.3877,  0.0487,  0.3768,  0.0740, -0.0110, -0.2156,  0.1263,  0.1000],
        [ 0.0952, -0.0632,  0.4541,  0.3298, -0.0306, -0.4336, -0.3849,  0.3670],
        [ 0.0492,  0.4617,  0.1594,  0.0136, -0.3000,  0.3054, -0.2758, -0.1845],
        [ 0.1690,  0.1869, -0.4699,  0.2972,  0.1792, -0.0884, -0.3819, -0.0929],
        [ 0.4620,  0.0031,  0.0006,  0.0840, -0.4168, -0.0244,  0.1268,  0.0438],
        [ 0.1872,  0.1066, -0.1214,  0.3996, -0.4279,  0.2485,  0.4721, -0.3411],
        [ 0.4908,  0.3959, -0.3138, -0.0048, -0.2946,  0.3133, -0.2034,  0.4568],
        [-0.0391, -0.4032, -0.0915,  0.4351,  0.1699, -0.0348, -0.2171,  0.3340],
        [ 0.2714, -0.4197,  0.4182, -0.2406, -0.0463,  0.1279, -0.3931,  0.4416],
        [ 0.4735,  0.0931,  0.2053, -0.4340, -0.3020,  0.0616, -0.2981, -0.4571],
        [ 0.3038, -0.2089, -0.3432,  0.2915,  0.3432, -0.2437, -0.3871, -0.4010],
        [ 0.2476, -0.0188, -0.0904, -0.2635, -0.2521, -0.0813, -0.1598,  0.1720],
        [ 0.3897, -0.4393,  0.1769, -0.0807, -0.0796, -0.3808,  0.2053,  0.4491],
        [ 0.1310, -0.1263,  0.3298, -0.3903,  0.4158,  0.3685, -0.3886,  0.3485],
        [ 0.4611,  0.2004, -0.3821,  0.1484, -0.2185,  0.0728, -0.3881, -0.0473],
        [-0.0684,  0.2234, -0.0432, -0.4509, -0.3426,  0.1713,  0.4235,  0.4872],
        [-0.4578,  0.1196,  0.3732, -0.2135, -0.3189, -0.3496, -0.0678, -0.2504],
        [ 0.4072,  0.4311,  0.3120,  0.1567,  0.2731, -0.3681,  0.4249, -0.0081],
        [-0.0885, -0.0801,  0.4117,  0.3465,  0.0609,  0.4675, -0.2372,  0.0151],
        [ 0.4155, -0.4434, -0.3104, -0.0373, -0.0309, -0.1794, -0.3408, -0.4078],
        [-0.1882, -0.2508, -0.4529, -0.3509, -0.2879, -0.3444, -0.1852,  0.1615],
        [ 0.2833, -0.4100,  0.2345, -0.2945, -0.2654, -0.3231, -0.0526, -0.1874],
        [ 0.2057, -0.1900,  0.4153, -0.2166, -0.4624,  0.3620, -0.3664,  0.3834],
        [ 0.0831, -0.2485, -0.1811, -0.1992,  0.0619,  0.1468, -0.0554,  0.4171],
        [ 0.4627,  0.2674,  0.1010,  0.4793,  0.0583,  0.1749, -0.0764,  0.4447],
        [-0.4045, -0.2482,  0.1692, -0.4222,  0.1645, -0.3963,  0.4584, -0.4814],
        [ 0.0219,  0.2733, -0.0345,  0.2788,  0.0546, -0.4789,  0.2071,  0.3227],
        [ 0.3341, -0.4302,  0.1611,  0.3230, -0.1126,  0.3693,  0.4078,  0.3942],
        [ 0.1986, -0.3042, -0.1310,  0.0209, -0.4932, -0.1722,  0.3226, -0.4065],
        [-0.3192,  0.1801,  0.3844, -0.1755, -0.3510,  0.0006,  0.1494, -0.3149],
        [ 0.2617,  0.2831,  0.1154,  0.3832,  0.4216,  0.1614,  0.2396, -0.3041],
        [-0.4898, -0.3428, -0.1433, -0.1431,  0.3236,  0.4465, -0.4018,  0.2070],
        [ 0.2644, -0.2715, -0.2988, -0.0062,  0.4077, -0.1788, -0.3469, -0.1315],
        [-0.0797,  0.2082,  0.3957,  0.2769,  0.4804,  0.0075, -0.2175,  0.0728],
        [ 0.1514,  0.0999,  0.2309,  0.4697, -0.1997,  0.4845,  0.1972,  0.2727],
        [-0.4444, -0.1034,  0.3220,  0.3217, -0.3285,  0.3297,  0.0187, -0.1905],
        [ 0.0386,  0.0222,  0.0146,  0.2265,  0.3433, -0.2324, -0.3379, -0.3655],
        [ 0.1398,  0.0100,  0.0319,  0.4771,  0.3792, -0.2047, -0.0432,  0.4152],
        [ 0.1764,  0.2256,  0.0946,  0.0659, -0.4122,  0.1064, -0.2190, -0.4973],
        [-0.4895,  0.4696, -0.1786, -0.2320,  0.0515, -0.0591, -0.4939,  0.1803],
        [-0.2361,  0.3905, -0.2794, -0.1901, -0.1845,  0.1833,  0.0210,  0.2258],
        [-0.0852, -0.4885, -0.0798, -0.3649, -0.0898,  0.1966,  0.1483, -0.1092],
        [-0.3413,  0.3677, -0.4923,  0.1847, -0.3271, -0.4958,  0.2175,  0.0819],
        [ 0.0276,  0.2426,  0.1521, -0.3781,  0.1799,  0.4497,  0.1167,  0.1008],
        [-0.3544, -0.4906,  0.1791, -0.1431,  0.2752,  0.1275,  0.2872, -0.0951],
        [-0.3635, -0.2387,  0.3172,  0.3078,  0.3437, -0.4825, -0.2914, -0.2958],
        [-0.3017,  0.2449, -0.2864,  0.3556,  0.2643, -0.1189,  0.4108, -0.2328],
        [ 0.1696, -0.3831, -0.1946, -0.0861,  0.4633,  0.2293,  0.1139,  0.0629],
        [-0.4153,  0.4917,  0.2332, -0.3192, -0.3879,  0.1156,  0.2559, -0.1805],
        [ 0.3868,  0.4448, -0.2183, -0.4870, -0.4654,  0.2346,  0.4006,  0.3014],
        [-0.4909, -0.3741,  0.2582, -0.2665,  0.3944,  0.4137,  0.0950, -0.4809],
        [ 0.0700,  0.1928, -0.1743, -0.0955,  0.3018, -0.2648,  0.2804,  0.2612],
        [-0.1509, -0.1870,  0.3609, -0.0725, -0.4939,  0.2522, -0.3936, -0.3682],
        [ 0.2358, -0.3674, -0.4952, -0.1037, -0.0790, -0.4860, -0.1878,  0.2472],
        [ 0.1218,  0.0856,  0.3096,  0.2121,  0.4666,  0.3289, -0.1356,  0.1884],
        [ 0.2232,  0.1750,  0.2961,  0.0658, -0.4525, -0.2007, -0.1615,  0.2962],
        [ 0.2784,  0.1854,  0.0553,  0.2008, -0.1206,  0.0012, -0.1181, -0.3035]])
2025-03-18 18:09:58.855728 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 35651585],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[not compare]  None tensor([[-0.3652,  0.3295,  0.3397, -0.1253,  0.4130, -0.3423, -0.2474, -0.3317],
        [-0.2226,  0.3959,  0.3861, -0.4334, -0.2120, -0.4087, -0.1983,  0.1799],
        [-0.0648, -0.4729,  0.1894, -0.2525,  0.0654,  0.3680, -0.3794, -0.3400],
        [-0.0691, -0.0437,  0.4174, -0.3748, -0.4074,  0.0234, -0.1224,  0.4664],
        [ 0.0285,  0.1657,  0.1125, -0.1161,  0.3120,  0.0970,  0.0398, -0.1148],
        [-0.0183, -0.0496, -0.0237, -0.4589,  0.3347, -0.0372, -0.4361, -0.3755],
        [ 0.2082,  0.0346,  0.1829, -0.2872,  0.2302, -0.2721, -0.2252, -0.0805],
        [ 0.1670,  0.2779,  0.4490, -0.1109, -0.1896,  0.4440, -0.0911,  0.3861],
        [ 0.1138,  0.2960,  0.0955,  0.4091,  0.4683,  0.4966, -0.4560, -0.2645],
        [ 0.0455, -0.4307, -0.1242,  0.1781, -0.1539, -0.0828,  0.3690,  0.3446],
        [-0.1205, -0.1902, -0.1266, -0.1974, -0.3179,  0.0779, -0.1539,  0.1904],
        [ 0.4242, -0.4555, -0.2741, -0.3668, -0.2246, -0.3389, -0.3306, -0.3351],
        [-0.3904, -0.4631, -0.4981, -0.2836,  0.0795,  0.1997,  0.3252, -0.2195],
        [ 0.4920,  0.1590, -0.1666,  0.1603, -0.0590,  0.4110,  0.3595,  0.1949],
        [-0.3568, -0.0082, -0.3976,  0.1620, -0.3262, -0.4269,  0.2231,  0.4087],
        [-0.2084, -0.0739, -0.3792,  0.0627, -0.1287, -0.1398,  0.2823,  0.1534],
        [ 0.2706,  0.2614,  0.1931, -0.4682, -0.2043,  0.2567,  0.0005,  0.2640],
        [ 0.1807, -0.2019, -0.4829, -0.4538,  0.0216, -0.4008, -0.2951,  0.3589],
        [ 0.1860, -0.3273, -0.3131,  0.1339, -0.4619,  0.2490,  0.2637,  0.1881],
        [ 0.4550,  0.0007,  0.4825,  0.1627, -0.0712, -0.3970,  0.1599,  0.3025],
        [-0.1037, -0.3589,  0.3129,  0.3726, -0.2790,  0.0907, -0.3790,  0.2606],
        [-0.3659,  0.4499, -0.0732,  0.4978, -0.0607,  0.0887, -0.1145,  0.3577],
        [ 0.2392,  0.0412, -0.0357, -0.2875, -0.2923,  0.3180,  0.1469,  0.2894],
        [ 0.2007, -0.1344, -0.3721,  0.1521, -0.1810,  0.1652, -0.4759,  0.1323],
        [ 0.0850, -0.0501,  0.4218, -0.1979,  0.0983,  0.1556,  0.2690, -0.0533],
        [-0.0802, -0.0493,  0.2728,  0.3394,  0.2200,  0.4126, -0.3183,  0.1013],
        [-0.4693,  0.1854,  0.3110,  0.1743, -0.2411, -0.0987,  0.4821,  0.4218],
        [ 0.1600, -0.3737,  0.0856,  0.3606,  0.0279, -0.3711, -0.2190,  0.2529],
        [-0.4708,  0.2660,  0.2054, -0.3908,  0.4011, -0.2795,  0.2463,  0.0465],
        [-0.2408, -0.1926,  0.3313,  0.0363,  0.2083, -0.1764, -0.1444,  0.4457],
        [ 0.3767,  0.1743, -0.3119, -0.2285, -0.3084, -0.3070,  0.1567, -0.3245],
        [-0.4091, -0.1182,  0.0833, -0.0107, -0.1737, -0.0137, -0.0621,  0.2921],
        [ 0.2913,  0.1191, -0.1671,  0.3269,  0.3537,  0.3134, -0.1665,  0.4791],
        [-0.3418,  0.4152, -0.0060,  0.1076,  0.4867, -0.3185, -0.2656,  0.0564],
        [-0.3084, -0.3664,  0.0411, -0.0787,  0.3425,  0.3226, -0.2670, -0.3910],
        [-0.1752,  0.3057,  0.1430, -0.1469, -0.3863,  0.4202, -0.4832, -0.4889],
        [-0.3943, -0.0184,  0.4235,  0.3025, -0.4843,  0.3772,  0.2081,  0.0857],
        [ 0.2520,  0.1115, -0.2595,  0.2708,  0.3600,  0.3390, -0.1593, -0.4807],
        [-0.0704, -0.4454,  0.2271, -0.1281,  0.1581, -0.0647,  0.4230,  0.3777],
        [ 0.4369,  0.2871,  0.3171,  0.0577,  0.2174, -0.2672, -0.3422,  0.0847],
        [ 0.1245, -0.2525,  0.3133,  0.0283,  0.1959, -0.3637,  0.4368,  0.2714],
        [-0.3598,  0.0012, -0.1758, -0.2610, -0.2156,  0.4413,  0.0265,  0.2671],
        [ 0.3995,  0.1018, -0.4748,  0.4709,  0.0195,  0.0813,  0.4882,  0.2632],
        [-0.0532, -0.0115,  0.0263, -0.1886, -0.3101, -0.1762,  0.2320, -0.1651],
        [ 0.0516, -0.4857, -0.1165,  0.1032, -0.1769, -0.0786, -0.2890,  0.3183],
        [-0.0375, -0.1844,  0.1287, -0.3335,  0.1180,  0.3273,  0.2581, -0.1516],
        [ 0.1070, -0.1246, -0.4044,  0.4655, -0.4782,  0.1274,  0.3500, -0.2248],
        [-0.1004, -0.2077,  0.4216,  0.0053,  0.2730, -0.0467, -0.1527, -0.4243],
        [ 0.0620, -0.1061,  0.2113,  0.4427, -0.0711, -0.1655,  0.3286,  0.0771],
        [ 0.4784, -0.0048, -0.0387,  0.3593, -0.3564, -0.4257,  0.3465,  0.3090],
        [-0.4407,  0.2289, -0.2786,  0.2981,  0.0301, -0.1827,  0.0478, -0.2370],
        [ 0.1656, -0.3935, -0.1562, -0.0741, -0.4759, -0.2951, -0.0472, -0.3158],
        [ 0.4825,  0.1639, -0.2768, -0.3074,  0.3422,  0.2809,  0.0079,  0.0416],
        [-0.0406, -0.0541,  0.0148, -0.0902, -0.2984,  0.1086, -0.0240, -0.2532],
        [-0.3835,  0.4728, -0.2022, -0.4248,  0.3516, -0.3632, -0.1824, -0.1688],
        [-0.3497, -0.1312, -0.4921, -0.3075, -0.3001, -0.3976,  0.3773,  0.2935],
        [ 0.1808, -0.4367, -0.1738,  0.3211,  0.3174,  0.1518, -0.4188, -0.3016],
        [ 0.3260, -0.0675,  0.1191,  0.4944,  0.2542, -0.0320, -0.0552,  0.1547],
        [-0.1257, -0.2246,  0.1869,  0.2510, -0.1291,  0.1582, -0.0677, -0.1612],
        [ 0.3753,  0.3697,  0.4306, -0.3428,  0.2426, -0.0966, -0.0471, -0.2493],
        [-0.1953, -0.4140, -0.4088, -0.3603,  0.0053,  0.1724,  0.1319,  0.1711],
        [ 0.2017,  0.3067,  0.0121, -0.1300, -0.4610,  0.0912,  0.0309, -0.0909],
        [-0.0912, -0.1275, -0.3190, -0.2270, -0.1725,  0.3319, -0.1361,  0.2041],
        [-0.3082,  0.4835, -0.4027,  0.2764, -0.0720,  0.0381,  0.3392,  0.4937]])
2025-03-18 18:11:48.204806 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 35651585],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[not compare]  None tensor([[-0.3652,  0.3295,  0.3397, -0.1253,  0.4130, -0.3423, -0.2474, -0.3317],
        [-0.2226,  0.3959,  0.3861, -0.4334, -0.2120, -0.4087, -0.1983,  0.1799],
        [-0.0648, -0.4729,  0.1894, -0.2525,  0.0654,  0.3680, -0.3794, -0.3400],
        [-0.0691, -0.0437,  0.4174, -0.3748, -0.4074,  0.0234, -0.1224,  0.4664],
        [ 0.0285,  0.1657,  0.1125, -0.1161,  0.3120,  0.0970,  0.0398, -0.1148],
        [-0.0183, -0.0496, -0.0237, -0.4589,  0.3347, -0.0372, -0.4361, -0.3755],
        [ 0.2082,  0.0346,  0.1829, -0.2872,  0.2302, -0.2721, -0.2252, -0.0805],
        [ 0.1670,  0.2779,  0.4490, -0.1109, -0.1896,  0.4440, -0.0911,  0.3861],
        [ 0.1138,  0.2960,  0.0955,  0.4091,  0.4683,  0.4966, -0.4560, -0.2645],
        [ 0.0455, -0.4307, -0.1242,  0.1781, -0.1539, -0.0828,  0.3690,  0.3446],
        [-0.1205, -0.1902, -0.1266, -0.1974, -0.3179,  0.0779, -0.1539,  0.1904],
        [ 0.4242, -0.4555, -0.2741, -0.3668, -0.2246, -0.3389, -0.3306, -0.3351],
        [-0.3904, -0.4631, -0.4981, -0.2836,  0.0795,  0.1997,  0.3252, -0.2195],
        [ 0.4920,  0.1590, -0.1666,  0.1603, -0.0590,  0.4110,  0.3595,  0.1949],
        [-0.3568, -0.0082, -0.3976,  0.1620, -0.3262, -0.4269,  0.2231,  0.4087],
        [-0.2084, -0.0739, -0.3792,  0.0627, -0.1287, -0.1398,  0.2823,  0.1534],
        [ 0.2706,  0.2614,  0.1931, -0.4682, -0.2043,  0.2567,  0.0005,  0.2640],
        [ 0.1807, -0.2019, -0.4829, -0.4538,  0.0216, -0.4008, -0.2951,  0.3589],
        [ 0.1860, -0.3273, -0.3131,  0.1339, -0.4619,  0.2490,  0.2637,  0.1881],
        [ 0.4550,  0.0007,  0.4825,  0.1627, -0.0712, -0.3970,  0.1599,  0.3025],
        [-0.1037, -0.3589,  0.3129,  0.3726, -0.2790,  0.0907, -0.3790,  0.2606],
        [-0.3659,  0.4499, -0.0732,  0.4978, -0.0607,  0.0887, -0.1145,  0.3577],
        [ 0.2392,  0.0412, -0.0357, -0.2875, -0.2923,  0.3180,  0.1469,  0.2894],
        [ 0.2007, -0.1344, -0.3721,  0.1521, -0.1810,  0.1652, -0.4759,  0.1323],
        [ 0.0850, -0.0501,  0.4218, -0.1979,  0.0983,  0.1556,  0.2690, -0.0533],
        [-0.0802, -0.0493,  0.2728,  0.3394,  0.2200,  0.4126, -0.3183,  0.1013],
        [-0.4693,  0.1854,  0.3110,  0.1743, -0.2411, -0.0987,  0.4821,  0.4218],
        [ 0.1600, -0.3737,  0.0856,  0.3606,  0.0279, -0.3711, -0.2190,  0.2529],
        [-0.4708,  0.2660,  0.2054, -0.3908,  0.4011, -0.2795,  0.2463,  0.0465],
        [-0.2408, -0.1926,  0.3313,  0.0363,  0.2083, -0.1764, -0.1444,  0.4457],
        [ 0.3767,  0.1743, -0.3119, -0.2285, -0.3084, -0.3070,  0.1567, -0.3245],
        [-0.4091, -0.1182,  0.0833, -0.0107, -0.1737, -0.0137, -0.0621,  0.2921],
        [ 0.2913,  0.1191, -0.1671,  0.3269,  0.3537,  0.3134, -0.1665,  0.4791],
        [-0.3418,  0.4152, -0.0060,  0.1076,  0.4867, -0.3185, -0.2656,  0.0564],
        [-0.3084, -0.3664,  0.0411, -0.0787,  0.3425,  0.3226, -0.2670, -0.3910],
        [-0.1752,  0.3057,  0.1430, -0.1469, -0.3863,  0.4202, -0.4832, -0.4889],
        [-0.3943, -0.0184,  0.4235,  0.3025, -0.4843,  0.3772,  0.2081,  0.0857],
        [ 0.2520,  0.1115, -0.2595,  0.2708,  0.3600,  0.3390, -0.1593, -0.4807],
        [-0.0704, -0.4454,  0.2271, -0.1281,  0.1581, -0.0647,  0.4230,  0.3777],
        [ 0.4369,  0.2871,  0.3171,  0.0577,  0.2174, -0.2672, -0.3422,  0.0847],
        [ 0.1245, -0.2525,  0.3133,  0.0283,  0.1959, -0.3637,  0.4368,  0.2714],
        [-0.3598,  0.0012, -0.1758, -0.2610, -0.2156,  0.4413,  0.0265,  0.2671],
        [ 0.3995,  0.1018, -0.4748,  0.4709,  0.0195,  0.0813,  0.4882,  0.2632],
        [-0.0532, -0.0115,  0.0263, -0.1886, -0.3101, -0.1762,  0.2320, -0.1651],
        [ 0.0516, -0.4857, -0.1165,  0.1032, -0.1769, -0.0786, -0.2890,  0.3183],
        [-0.0375, -0.1844,  0.1287, -0.3335,  0.1180,  0.3273,  0.2581, -0.1516],
        [ 0.1070, -0.1246, -0.4044,  0.4655, -0.4782,  0.1274,  0.3500, -0.2248],
        [-0.1004, -0.2077,  0.4216,  0.0053,  0.2730, -0.0467, -0.1527, -0.4243],
        [ 0.0620, -0.1061,  0.2113,  0.4427, -0.0711, -0.1655,  0.3286,  0.0771],
        [ 0.4784, -0.0048, -0.0387,  0.3593, -0.3564, -0.4257,  0.3465,  0.3090],
        [-0.4407,  0.2289, -0.2786,  0.2981,  0.0301, -0.1827,  0.0478, -0.2370],
        [ 0.1656, -0.3935, -0.1562, -0.0741, -0.4759, -0.2951, -0.0472, -0.3158],
        [ 0.4825,  0.1639, -0.2768, -0.3074,  0.3422,  0.2809,  0.0079,  0.0416],
        [-0.0406, -0.0541,  0.0148, -0.0902, -0.2984,  0.1086, -0.0240, -0.2532],
        [-0.3835,  0.4728, -0.2022, -0.4248,  0.3516, -0.3632, -0.1824, -0.1688],
        [-0.3497, -0.1312, -0.4921, -0.3075, -0.3001, -0.3976,  0.3773,  0.2935],
        [ 0.1808, -0.4367, -0.1738,  0.3211,  0.3174,  0.1518, -0.4188, -0.3016],
        [ 0.3260, -0.0675,  0.1191,  0.4944,  0.2542, -0.0320, -0.0552,  0.1547],
        [-0.1257, -0.2246,  0.1869,  0.2510, -0.1291,  0.1582, -0.0677, -0.1612],
        [ 0.3753,  0.3697,  0.4306, -0.3428,  0.2426, -0.0966, -0.0471, -0.2493],
        [-0.1953, -0.4140, -0.4088, -0.3603,  0.0053,  0.1724,  0.1319,  0.1711],
        [ 0.2017,  0.3067,  0.0121, -0.1300, -0.4610,  0.0912,  0.0309, -0.0909],
        [-0.0912, -0.1275, -0.3190, -0.2270, -0.1725,  0.3319, -0.1361,  0.2041],
        [-0.3082,  0.4835, -0.4027,  0.2764, -0.0720,  0.0381,  0.3392,  0.4937]])
2025-03-18 18:13:30.675687 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([64, 8],"float32"),Tensor([64, 35651585],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[not compare]  None tensor([[-0.3652,  0.3295,  0.3397, -0.1253,  0.4130, -0.3423, -0.2474, -0.3317],
        [-0.2226,  0.3959,  0.3861, -0.4334, -0.2120, -0.4087, -0.1983,  0.1799],
        [-0.0648, -0.4729,  0.1894, -0.2525,  0.0654,  0.3680, -0.3794, -0.3400],
        [-0.0691, -0.0437,  0.4174, -0.3748, -0.4074,  0.0234, -0.1224,  0.4664],
        [ 0.0285,  0.1657,  0.1125, -0.1161,  0.3120,  0.0970,  0.0398, -0.1148],
        [-0.0183, -0.0496, -0.0237, -0.4589,  0.3347, -0.0372, -0.4361, -0.3755],
        [ 0.2082,  0.0346,  0.1829, -0.2872,  0.2302, -0.2721, -0.2252, -0.0805],
        [ 0.1670,  0.2779,  0.4490, -0.1109, -0.1896,  0.4440, -0.0911,  0.3861],
        [ 0.1138,  0.2960,  0.0955,  0.4091,  0.4683,  0.4966, -0.4560, -0.2645],
        [ 0.0455, -0.4307, -0.1242,  0.1781, -0.1539, -0.0828,  0.3690,  0.3446],
        [-0.1205, -0.1902, -0.1266, -0.1974, -0.3179,  0.0779, -0.1539,  0.1904],
        [ 0.4242, -0.4555, -0.2741, -0.3668, -0.2246, -0.3389, -0.3306, -0.3351],
        [-0.3904, -0.4631, -0.4981, -0.2836,  0.0795,  0.1997,  0.3252, -0.2195],
        [ 0.4920,  0.1590, -0.1666,  0.1603, -0.0590,  0.4110,  0.3595,  0.1949],
        [-0.3568, -0.0082, -0.3976,  0.1620, -0.3262, -0.4269,  0.2231,  0.4087],
        [-0.2084, -0.0739, -0.3792,  0.0627, -0.1287, -0.1398,  0.2823,  0.1534],
        [ 0.2706,  0.2614,  0.1931, -0.4682, -0.2043,  0.2567,  0.0005,  0.2640],
        [ 0.1807, -0.2019, -0.4829, -0.4538,  0.0216, -0.4008, -0.2951,  0.3589],
        [ 0.1860, -0.3273, -0.3131,  0.1339, -0.4619,  0.2490,  0.2637,  0.1881],
        [ 0.4550,  0.0007,  0.4825,  0.1627, -0.0712, -0.3970,  0.1599,  0.3025],
        [-0.1037, -0.3589,  0.3129,  0.3726, -0.2790,  0.0907, -0.3790,  0.2606],
        [-0.3659,  0.4499, -0.0732,  0.4978, -0.0607,  0.0887, -0.1145,  0.3577],
        [ 0.2392,  0.0412, -0.0357, -0.2875, -0.2923,  0.3180,  0.1469,  0.2894],
        [ 0.2007, -0.1344, -0.3721,  0.1521, -0.1810,  0.1652, -0.4759,  0.1323],
        [ 0.0850, -0.0501,  0.4218, -0.1979,  0.0983,  0.1556,  0.2690, -0.0533],
        [-0.0802, -0.0493,  0.2728,  0.3394,  0.2200,  0.4126, -0.3183,  0.1013],
        [-0.4693,  0.1854,  0.3110,  0.1743, -0.2411, -0.0987,  0.4821,  0.4218],
        [ 0.1600, -0.3737,  0.0856,  0.3606,  0.0279, -0.3711, -0.2190,  0.2529],
        [-0.4708,  0.2660,  0.2054, -0.3908,  0.4011, -0.2795,  0.2463,  0.0465],
        [-0.2408, -0.1926,  0.3313,  0.0363,  0.2083, -0.1764, -0.1444,  0.4457],
        [ 0.3767,  0.1743, -0.3119, -0.2285, -0.3084, -0.3070,  0.1567, -0.3245],
        [-0.4091, -0.1182,  0.0833, -0.0107, -0.1737, -0.0137, -0.0621,  0.2921],
        [ 0.2913,  0.1191, -0.1671,  0.3269,  0.3537,  0.3134, -0.1665,  0.4791],
        [-0.3418,  0.4152, -0.0060,  0.1076,  0.4867, -0.3185, -0.2656,  0.0564],
        [-0.3084, -0.3664,  0.0411, -0.0787,  0.3425,  0.3226, -0.2670, -0.3910],
        [-0.1752,  0.3057,  0.1430, -0.1469, -0.3863,  0.4202, -0.4832, -0.4889],
        [-0.3943, -0.0184,  0.4235,  0.3025, -0.4843,  0.3772,  0.2081,  0.0857],
        [ 0.2520,  0.1115, -0.2595,  0.2708,  0.3600,  0.3390, -0.1593, -0.4807],
        [-0.0704, -0.4454,  0.2271, -0.1281,  0.1581, -0.0647,  0.4230,  0.3777],
        [ 0.4369,  0.2871,  0.3171,  0.0577,  0.2174, -0.2672, -0.3422,  0.0847],
        [ 0.1245, -0.2525,  0.3133,  0.0283,  0.1959, -0.3637,  0.4368,  0.2714],
        [-0.3598,  0.0012, -0.1758, -0.2610, -0.2156,  0.4413,  0.0265,  0.2671],
        [ 0.3995,  0.1018, -0.4748,  0.4709,  0.0195,  0.0813,  0.4882,  0.2632],
        [-0.0532, -0.0115,  0.0263, -0.1886, -0.3101, -0.1762,  0.2320, -0.1651],
        [ 0.0516, -0.4857, -0.1165,  0.1032, -0.1769, -0.0786, -0.2890,  0.3183],
        [-0.0375, -0.1844,  0.1287, -0.3335,  0.1180,  0.3273,  0.2581, -0.1516],
        [ 0.1070, -0.1246, -0.4044,  0.4655, -0.4782,  0.1274,  0.3500, -0.2248],
        [-0.1004, -0.2077,  0.4216,  0.0053,  0.2730, -0.0467, -0.1527, -0.4243],
        [ 0.0620, -0.1061,  0.2113,  0.4427, -0.0711, -0.1655,  0.3286,  0.0771],
        [ 0.4784, -0.0048, -0.0387,  0.3593, -0.3564, -0.4257,  0.3465,  0.3090],
        [-0.4407,  0.2289, -0.2786,  0.2981,  0.0301, -0.1827,  0.0478, -0.2370],
        [ 0.1656, -0.3935, -0.1562, -0.0741, -0.4759, -0.2951, -0.0472, -0.3158],
        [ 0.4825,  0.1639, -0.2768, -0.3074,  0.3422,  0.2809,  0.0079,  0.0416],
        [-0.0406, -0.0541,  0.0148, -0.0902, -0.2984,  0.1086, -0.0240, -0.2532],
        [-0.3835,  0.4728, -0.2022, -0.4248,  0.3516, -0.3632, -0.1824, -0.1688],
        [-0.3497, -0.1312, -0.4921, -0.3075, -0.3001, -0.3976,  0.3773,  0.2935],
        [ 0.1808, -0.4367, -0.1738,  0.3211,  0.3174,  0.1518, -0.4188, -0.3016],
        [ 0.3260, -0.0675,  0.1191,  0.4944,  0.2542, -0.0320, -0.0552,  0.1547],
        [-0.1257, -0.2246,  0.1869,  0.2510, -0.1291,  0.1582, -0.0677, -0.1612],
        [ 0.3753,  0.3697,  0.4306, -0.3428,  0.2426, -0.0966, -0.0471, -0.2493],
        [-0.1953, -0.4140, -0.4088, -0.3603,  0.0053,  0.1724,  0.1319,  0.1711],
        [ 0.2017,  0.3067,  0.0121, -0.1300, -0.4610,  0.0912,  0.0309, -0.0909],
        [-0.0912, -0.1275, -0.3190, -0.2270, -0.1725,  0.3319, -0.1361,  0.2041],
        [-0.3082,  0.4835, -0.4027,  0.2764, -0.0720,  0.0381,  0.3392,  0.4937]])
2025-03-18 18:15:10.877352 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([71303169, 2, 4, 4],"float32"),Tensor([3],"float32"),], )

[not compare]  None tensor([[[[-2.3352e-01, -6.3720e-02, -2.7855e-01, -2.4608e-01],
          [-6.2878e-02,  3.3746e-01, -4.8186e-01, -4.1635e-02],
          [ 3.3076e-01,  4.0787e-01,  4.2444e-02,  1.8086e-01],
          [ 2.3930e-01,  1.3805e-01, -3.7408e-01,  4.2218e-01]],

         [[-4.5966e-01, -4.6613e-01, -4.2367e-01, -4.9590e-01],
          [ 3.5007e-01, -1.2753e-01, -2.2997e-01,  1.2884e-01],
          [ 2.4104e-01,  1.6759e-01, -1.4640e-01,  1.3786e-01],
          [ 8.0144e-02,  1.9655e-01, -6.3195e-02, -3.3585e-01]]],


        [[[ 1.6061e-01, -4.3819e-01, -4.1684e-01,  4.9294e-01],
          [ 8.5180e-02,  3.9437e-02, -1.0828e-01, -4.7164e-01],
          [ 2.5115e-02, -3.3898e-02,  3.2395e-01, -1.7367e-01],
          [-1.4246e-02,  3.5848e-01, -3.8309e-01,  1.8092e-01]],

         [[ 4.3723e-02,  9.6416e-02, -2.6700e-01,  4.5616e-01],
          [ 3.4744e-01,  4.9839e-01,  8.1663e-02,  4.1400e-02],
          [ 2.7579e-01,  1.2553e-01, -4.7814e-01,  1.0214e-01],
          [ 5.0357e-02,  1.6999e-02,  3.3284e-01,  7.0862e-02]]],


        [[[-9.5449e-02, -2.3550e-01, -4.0060e-01,  3.2601e-01],
          [-3.7589e-01,  4.9191e-01, -2.5112e-01,  2.4502e-01],
          [ 3.4502e-01,  7.0843e-02,  1.2678e-01, -4.9709e-01],
          [ 1.3311e-01,  1.3725e-01, -4.2097e-02, -1.3387e-01]],

         [[ 1.2105e-01,  4.4664e-01,  4.6134e-01, -4.1379e-01],
          [ 2.4025e-01, -2.9732e-01,  1.0366e-01,  1.6342e-01],
          [ 5.9875e-02, -2.7612e-01, -2.0925e-02, -4.9011e-01],
          [ 2.1154e-01,  4.5075e-01,  1.2705e-01,  1.6395e-01]]],


        ...,


        [[[ 3.3308e-01,  2.0528e-01,  3.3004e-01, -3.2347e-01],
          [-2.4480e-01, -4.6874e-02,  8.0368e-02, -3.1465e-02],
          [-1.5781e-01, -1.1402e-01,  3.7851e-01, -1.0087e-01],
          [ 4.9838e-01,  1.1901e-01,  2.5920e-01, -3.0172e-01]],

         [[-3.4181e-01,  2.3603e-01,  4.3538e-01, -2.9547e-01],
          [-5.0516e-02, -1.7126e-01, -2.4809e-01, -1.5076e-01],
          [-2.4838e-01, -4.7253e-01,  5.5365e-02,  2.8138e-01],
          [ 1.9273e-01, -5.7875e-02, -1.0314e-01, -2.1692e-01]]],


        [[[ 1.6946e-01,  2.0824e-01,  2.7541e-02,  4.0874e-01],
          [-6.5547e-02, -2.5866e-01, -3.2855e-04,  9.0284e-02],
          [-3.4100e-01,  3.6056e-01, -1.1090e-01,  1.6948e-01],
          [-1.8190e-01, -1.0407e-01, -4.4904e-02,  6.2364e-02]],

         [[ 4.5383e-02, -4.0807e-01,  4.4441e-01, -2.0194e-01],
          [ 2.2768e-01,  1.4779e-01,  4.9682e-01,  2.5804e-02],
          [-1.3775e-01, -2.2515e-01,  3.9518e-01,  1.0166e-01],
          [ 2.8747e-01, -2.0259e-01, -5.2554e-02, -3.0652e-01]]],


        [[[-1.9806e-01, -4.4153e-01, -3.6861e-01, -4.8663e-01],
          [-4.9562e-02, -5.5540e-02, -2.2558e-01,  4.1513e-01],
          [ 3.5180e-01, -3.5563e-01, -1.9798e-01,  8.4594e-02],
          [-1.8029e-01, -4.2807e-01, -4.2619e-01,  2.9464e-01]],

         [[-1.6814e-01,  2.2974e-01, -4.3483e-01, -4.7804e-02],
          [ 3.2146e-01, -3.6948e-04, -3.2227e-01, -2.8674e-01],
          [-2.8261e-01,  1.4893e-01,  4.1823e-01, -3.7154e-01],
          [ 5.9622e-02, -4.3501e-01, -6.1434e-02, -2.2014e-01]]]])
2025-03-18 18:17:08.140510 test begin: paddle.nn.utils.parameters_to_vector(list[Tensor([760567127, 3],"float32"),], )

[not compare]  None tensor([[-0.0632, -0.3358,  0.1606],
        [-0.4382, -0.4168,  0.4929],
        [ 0.0852,  0.0394, -0.1083],
        ...,
        [ 0.4182, -0.3715,  0.0596],
        [-0.4350, -0.0614, -0.2201],
        [-0.3204, -0.1164,  0.3856]])
2025-03-18 18:19:14.880095 test begin: paddle.nn.utils.vector_to_parameters(Tensor([165],"float32"), list[Tensor([10, 15],"float32"),Tensor([2281701379],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([165],"float32"), list[Tensor([10, 15],"float32"),Tensor([2281701379],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 15
2025-03-18 18:19:18.946004 test begin: paddle.nn.utils.vector_to_parameters(Tensor([165],"float32"), list[Tensor([10, 228170138],"float32"),Tensor([15],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([165],"float32"), list[Tensor([10, 228170138],"float32"),Tensor([15],"float32"),], ) 
 shape '[10, 228170138]' is invalid for input of size 165
2025-03-18 18:19:20.897398 test begin: paddle.nn.utils.vector_to_parameters(Tensor([165],"float32"), list[Tensor([152113426, 15],"float32"),Tensor([15],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([165],"float32"), list[Tensor([152113426, 15],"float32"),Tensor([15],"float32"),], ) 
 shape '[152113426, 15]' is invalid for input of size 165
2025-03-18 18:19:23.040659 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([10, 15],"float32"),Tensor([15],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([10, 15],"float32"),Tensor([15],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [150, 15], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:165 != input_axis_dim:2281701379.] (at ../paddle/phi/infermeta/unary.cc:4438)

2025-03-18 18:19:27.195544 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([10, 3],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([10, 3],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [30, 0], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:30 != input_axis_dim:2281701379.] (at ../paddle/phi/infermeta/unary.cc:4438)

2025-03-18 18:19:31.720901 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [128, 256, 16, 16, 256, 256, 16, 16], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:960 != input_axis_dim:2281701379.] (at ../paddle/phi/infermeta/unary.cc:4438)

2025-03-18 18:19:36.411003 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([2, 2],"float32"),Tensor([2],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([2, 2],"float32"),Tensor([2],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [4, 2], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:6 != input_axis_dim:2281701379.] (at ../paddle/phi/infermeta/unary.cc:4438)

2025-03-18 18:19:41.074097 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([3, 2, 4, 4, 4],"float32"),Tensor([3],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([3, 2, 4, 4, 4],"float32"),Tensor([3],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [384, 3], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:387 != input_axis_dim:2281701379.] (at ../paddle/phi/infermeta/unary.cc:4438)

2025-03-18 18:19:45.398877 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([3, 2, 4, 4],"float32"),Tensor([3],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([3, 2, 4, 4],"float32"),Tensor([3],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [96, 3], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:99 != input_axis_dim:2281701379.] (at ../paddle/phi/infermeta/unary.cc:4438)

2025-03-18 18:19:50.037395 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([3, 2, 4],"float32"),Tensor([3],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([3, 2, 4],"float32"),Tensor([3],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [24, 3], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:27 != input_axis_dim:2281701379.] (at ../paddle/phi/infermeta/unary.cc:4438)

2025-03-18 18:19:54.402803 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [384, 768, 48, 48, 768, 768, 48, 48], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:2880 != input_axis_dim:2281701379.] (at ../paddle/phi/infermeta/unary.cc:4438)

2025-03-18 18:19:59.056618 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[paddle error] paddle.nn.utils.vector_to_parameters(Tensor([2281701379],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 (InvalidArgument) Sum of Attr(num_or_sections) must be equal to the input's size along the split dimension. But received Attr(num_or_sections) = [512, 1024, 64, 64, 1024, 1024, 64, 64], input(X)'s shape = [2281701379], Attr(dim) = 0.
  [Hint: Expected sum_of_section == input_axis_dim, but received sum_of_section:3840 != input_axis_dim:2281701379.] (at ../paddle/phi/infermeta/unary.cc:4438)

2025-03-18 18:20:04.018663 test begin: paddle.nn.utils.vector_to_parameters(Tensor([27],"float32"), list[Tensor([285212673, 2, 4],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([27],"float32"), list[Tensor([285212673, 2, 4],"float32"),Tensor([3],"float32"),], ) 
 shape '[285212673, 2, 4]' is invalid for input of size 27
2025-03-18 18:20:07.431427 test begin: paddle.nn.utils.vector_to_parameters(Tensor([27],"float32"), list[Tensor([3, 190141782, 4],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([27],"float32"), list[Tensor([3, 190141782, 4],"float32"),Tensor([3],"float32"),], ) 
 shape '[3, 190141782, 4]' is invalid for input of size 27
2025-03-18 18:20:09.966768 test begin: paddle.nn.utils.vector_to_parameters(Tensor([27],"float32"), list[Tensor([3, 2, 380283564],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([27],"float32"), list[Tensor([3, 2, 380283564],"float32"),Tensor([3],"float32"),], ) 
 shape '[3, 2, 380283564]' is invalid for input of size 27
2025-03-18 18:20:12.007945 test begin: paddle.nn.utils.vector_to_parameters(Tensor([27],"float32"), list[Tensor([3, 2, 4],"float32"),Tensor([2281701379],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([27],"float32"), list[Tensor([3, 2, 4],"float32"),Tensor([2281701379],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 3
2025-03-18 18:20:14.023725 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([285212673, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([285212673, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 shape '[285212673, 8]' is invalid for input of size 2880
2025-03-18 18:20:16.058512 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 47535446],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 47535446],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 shape '[48, 47535446]' is invalid for input of size 2880
2025-03-18 18:20:18.099574 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([142606337, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([142606337, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 shape '[142606337, 16]' is invalid for input of size 2496
2025-03-18 18:20:20.123499 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([2281701379],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([2281701379],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 1728
2025-03-18 18:20:22.083181 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([2281701379],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([2281701379],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 1680
2025-03-18 18:20:24.043579 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([142606337, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([142606337, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 shape '[142606337, 16]' is invalid for input of size 1632
2025-03-18 18:20:26.398462 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([142606337, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([142606337, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 shape '[142606337, 16]' is invalid for input of size 864
2025-03-18 18:20:27.792829 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([2281701379],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([2281701379],"float32"),Tensor([48],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 96
2025-03-18 18:20:29.730276 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([2281701379],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([2281701379],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 48
2025-03-18 18:20:31.878092 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 47535446],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 47535446],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 shape '[48, 47535446]' is invalid for input of size 864
2025-03-18 18:20:34.176145 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 47535446],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 47535446],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 shape '[48, 47535446]' is invalid for input of size 1632
2025-03-18 18:20:35.678320 test begin: paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 47535446],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([2880],"float32"), list[Tensor([48, 8],"float32"),Tensor([48, 47535446],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),Tensor([48, 16],"float32"),Tensor([48, 16],"float32"),Tensor([48],"float32"),Tensor([48],"float32"),], ) 
 shape '[48, 47535446]' is invalid for input of size 2496
2025-03-18 18:20:37.841890 test begin: paddle.nn.utils.vector_to_parameters(Tensor([30],"float32"), list[Tensor([10, 228170138],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([30],"float32"), list[Tensor([10, 228170138],"float32"),], ) 
 shape '[10, 228170138]' is invalid for input of size 30
2025-03-18 18:20:40.005012 test begin: paddle.nn.utils.vector_to_parameters(Tensor([30],"float32"), list[Tensor([760567127, 3],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([30],"float32"), list[Tensor([760567127, 3],"float32"),], ) 
 shape '[760567127, 3]' is invalid for input of size 30
2025-03-18 18:20:41.953662 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([285212673, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([285212673, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 shape '[285212673, 8]' is invalid for input of size 3840
2025-03-18 18:20:44.018896 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 35651585],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 35651585],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 shape '[64, 35651585]' is invalid for input of size 3840
2025-03-18 18:20:46.064839 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([142606337, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([142606337, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 shape '[142606337, 16]' is invalid for input of size 3328
2025-03-18 18:20:48.118409 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([2281701379],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([2281701379],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 2304
2025-03-18 18:20:50.383317 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([2281701379],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([2281701379],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 2240
2025-03-18 18:20:53.173879 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([142606337, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([142606337, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 shape '[142606337, 16]' is invalid for input of size 2176
2025-03-18 18:20:54.950663 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([142606337, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([142606337, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 shape '[142606337, 16]' is invalid for input of size 1152
2025-03-18 18:20:57.192545 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([2281701379],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([2281701379],"float32"),Tensor([64],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 128
2025-03-18 18:20:59.286590 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([2281701379],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([2281701379],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 64
2025-03-18 18:21:01.505584 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 35651585],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 35651585],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 shape '[64, 35651585]' is invalid for input of size 1152
2025-03-18 18:21:03.703199 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 35651585],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 35651585],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 shape '[64, 35651585]' is invalid for input of size 2176
2025-03-18 18:21:05.973470 test begin: paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 35651585],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([3840],"float32"), list[Tensor([64, 8],"float32"),Tensor([64, 35651585],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),Tensor([64, 16],"float32"),Tensor([64, 16],"float32"),Tensor([64],"float32"),Tensor([64],"float32"),], ) 
 shape '[64, 35651585]' is invalid for input of size 3328
2025-03-18 18:21:08.233765 test begin: paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([17825793, 2, 4, 4, 4],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([17825793, 2, 4, 4, 4],"float32"),Tensor([3],"float32"),], ) 
 shape '[17825793, 2, 4, 4, 4]' is invalid for input of size 387
2025-03-18 18:21:10.322577 test begin: paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([3, 11883862, 4, 4, 4],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([3, 11883862, 4, 4, 4],"float32"),Tensor([3],"float32"),], ) 
 shape '[3, 11883862, 4, 4, 4]' is invalid for input of size 387
2025-03-18 18:21:12.512302 test begin: paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([3, 2, 23767723, 4, 4],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([3, 2, 23767723, 4, 4],"float32"),Tensor([3],"float32"),], ) 
 shape '[3, 2, 23767723, 4, 4]' is invalid for input of size 387
2025-03-18 18:21:14.616634 test begin: paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([3, 2, 4, 23767723, 4],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([3, 2, 4, 23767723, 4],"float32"),Tensor([3],"float32"),], ) 
 shape '[3, 2, 4, 23767723, 4]' is invalid for input of size 387
2025-03-18 18:21:16.821640 test begin: paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([3, 2, 4, 4, 23767723],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([3, 2, 4, 4, 23767723],"float32"),Tensor([3],"float32"),], ) 
 shape '[3, 2, 4, 4, 23767723]' is invalid for input of size 387
2025-03-18 18:21:18.937408 test begin: paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([3, 2, 4, 4, 4],"float32"),Tensor([2281701379],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([387],"float32"), list[Tensor([3, 2, 4, 4, 4],"float32"),Tensor([2281701379],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 3
2025-03-18 18:21:20.961032 test begin: paddle.nn.utils.vector_to_parameters(Tensor([6],"float32"), list[Tensor([1140850690, 2],"float32"),Tensor([2],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([6],"float32"), list[Tensor([1140850690, 2],"float32"),Tensor([2],"float32"),], ) 
 shape '[1140850690, 2]' is invalid for input of size 6
2025-03-18 18:21:23.356437 test begin: paddle.nn.utils.vector_to_parameters(Tensor([6],"float32"), list[Tensor([2, 1140850690],"float32"),Tensor([2],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([6],"float32"), list[Tensor([2, 1140850690],"float32"),Tensor([2],"float32"),], ) 
 shape '[2, 1140850690]' is invalid for input of size 6
2025-03-18 18:21:25.051694 test begin: paddle.nn.utils.vector_to_parameters(Tensor([6],"float32"), list[Tensor([2, 2],"float32"),Tensor([2281701379],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([6],"float32"), list[Tensor([2, 2],"float32"),Tensor([2281701379],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 2
2025-03-18 18:21:27.379465 test begin: paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 142606337],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 142606337],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 shape '[16, 142606337]' is invalid for input of size 960
2025-03-18 18:21:29.094265 test begin: paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([142606337, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([142606337, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 shape '[142606337, 16]' is invalid for input of size 832
2025-03-18 18:21:31.037673 test begin: paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 142606337],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 142606337],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 shape '[16, 142606337]' is invalid for input of size 832
2025-03-18 18:21:32.984726 test begin: paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([142606337, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([142606337, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 shape '[142606337, 16]' is invalid for input of size 544
2025-03-18 18:21:35.322960 test begin: paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 142606337],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 142606337],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 shape '[16, 142606337]' is invalid for input of size 544
2025-03-18 18:21:37.038361 test begin: paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([142606337, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([142606337, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 shape '[142606337, 16]' is invalid for input of size 288
2025-03-18 18:21:39.226613 test begin: paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 142606337],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 142606337],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 shape '[16, 142606337]' is invalid for input of size 288
2025-03-18 18:21:41.509547 test begin: paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([2281701379],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([2281701379],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 16
2025-03-18 18:21:43.769783 test begin: paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([2281701379],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([2281701379],"float32"),Tensor([16],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 32
2025-03-18 18:21:46.204088 test begin: paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([2281701379],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([2281701379],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 560
2025-03-18 18:21:48.192177 test begin: paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([2281701379],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([16, 8],"float32"),Tensor([16, 16],"float32"),Tensor([2281701379],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 576
2025-03-18 18:21:50.395209 test begin: paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([285212673, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([960],"float32"), list[Tensor([285212673, 8],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),Tensor([16, 16],"float32"),Tensor([16, 16],"float32"),Tensor([16],"float32"),Tensor([16],"float32"),], ) 
 shape '[285212673, 8]' is invalid for input of size 960
2025-03-18 18:21:52.661391 test begin: paddle.nn.utils.vector_to_parameters(Tensor([99],"float32"), list[Tensor([3, 2, 4, 4],"float32"),Tensor([2281701379],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([99],"float32"), list[Tensor([3, 2, 4, 4],"float32"),Tensor([2281701379],"float32"),], ) 
 shape '[2281701379]' is invalid for input of size 3
2025-03-18 18:21:54.939650 test begin: paddle.nn.utils.vector_to_parameters(Tensor([99],"float32"), list[Tensor([3, 2, 4, 95070891],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([99],"float32"), list[Tensor([3, 2, 4, 95070891],"float32"),Tensor([3],"float32"),], ) 
 shape '[3, 2, 4, 95070891]' is invalid for input of size 99
2025-03-18 18:21:57.269390 test begin: paddle.nn.utils.vector_to_parameters(Tensor([99],"float32"), list[Tensor([3, 2, 95070891, 4],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([99],"float32"), list[Tensor([3, 2, 95070891, 4],"float32"),Tensor([3],"float32"),], ) 
 shape '[3, 2, 95070891, 4]' is invalid for input of size 99
2025-03-18 18:21:59.447812 test begin: paddle.nn.utils.vector_to_parameters(Tensor([99],"float32"), list[Tensor([3, 47535446, 4, 4],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([99],"float32"), list[Tensor([3, 47535446, 4, 4],"float32"),Tensor([3],"float32"),], ) 
 shape '[3, 47535446, 4, 4]' is invalid for input of size 99
2025-03-18 18:22:01.737490 test begin: paddle.nn.utils.vector_to_parameters(Tensor([99],"float32"), list[Tensor([71303169, 2, 4, 4],"float32"),Tensor([3],"float32"),], )

[torch error] paddle.nn.utils.vector_to_parameters(Tensor([99],"float32"), list[Tensor([71303169, 2, 4, 4],"float32"),Tensor([3],"float32"),], ) 
 shape '[71303169, 2, 4, 4]' is invalid for input of size 99
2025-03-18 18:22:03.995716 test begin: paddle.ones_like(Tensor([1, 10, 114085069, 2],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 10, 114085069, 2],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701380 / 2281701380 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0.],
         [0., 0.],
         [0., 0.],...
 y: array([[[[1., 1.],
         [1., 1.],
         [1., 1.],...
2025-03-18 18:24:16.974041 test begin: paddle.ones_like(Tensor([1, 10, 8, 28521268],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 10, 8, 28521268],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701440 / 2281701440 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-03-18 18:26:52.036974 test begin: paddle.ones_like(Tensor([1, 1024, 2228225],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 1024, 2228225],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281702400 / 2281702400 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],...
2025-03-18 18:29:06.121725 test begin: paddle.ones_like(Tensor([1, 12, 9, 21126865],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 12, 9, 21126865],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701420 / 2281701420 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-03-18 18:31:10.954029 test begin: paddle.ones_like(Tensor([1, 12, 95070891, 2],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 12, 95070891, 2],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701384 / 2281701384 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0.],
         [0., 0.],
         [0., 0.],...
 y: array([[[[1., 1.],
         [1., 1.],
         [1., 1.],...
2025-03-18 18:33:21.461977 test begin: paddle.ones_like(Tensor([1, 126761188, 9, 2],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 126761188, 9, 2],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701384 / 2281701384 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0.],
         [0., 0.],
         [0., 0.],...
 y: array([[[[1., 1.],
         [1., 1.],
         [1., 1.],...
2025-03-18 18:35:59.367902 test begin: paddle.ones_like(Tensor([1, 128, 1114113, 16],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 128, 1114113, 16],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281703424 / 2281703424 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-03-18 18:38:09.524016 test begin: paddle.ones_like(Tensor([1, 128, 17825793],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 128, 17825793],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701504 / 2281701504 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],...
2025-03-18 18:40:12.531377 test begin: paddle.ones_like(Tensor([1, 128, 8, 2228225],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 128, 8, 2228225],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281702400 / 2281702400 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-03-18 18:42:48.118417 test begin: paddle.ones_like(Tensor([1, 142606337, 8, 2],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 142606337, 8, 2],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701392 / 2281701392 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0.],
         [0., 0.],
         [0., 0.],...
 y: array([[[[1., 1.],
         [1., 1.],
         [1., 1.],...
2025-03-18 18:45:20.814851 test begin: paddle.ones_like(Tensor([1, 144, 200, 79226],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 144, 200, 79226],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281708800 / 2281708800 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-03-18 18:47:30.353008 test begin: paddle.ones_like(Tensor([1, 144, 7922575, 2],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 144, 7922575, 2],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701600 / 2281701600 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0.],
         [0., 0.],
         [0., 0.],...
 y: array([[[[1., 1.],
         [1., 1.],
         [1., 1.],...
2025-03-18 18:49:45.771894 test begin: paddle.ones_like(Tensor([1, 15, 15, 10140896],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 15, 15, 10140896],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701600 / 2281701600 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-03-18 18:51:59.133047 test begin: paddle.ones_like(Tensor([1, 15, 76056713, 2],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 15, 76056713, 2],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701390 / 2281701390 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0.],
         [0., 0.],
         [0., 0.],...
 y: array([[[[1., 1.],
         [1., 1.],
         [1., 1.],...
2025-03-18 18:54:06.667214 test begin: paddle.ones_like(Tensor([1, 17825793, 128],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 17825793, 128],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701504 / 2281701504 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],...
2025-03-18 18:56:12.046474 test begin: paddle.ones_like(Tensor([1, 17825793, 8, 16],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 17825793, 8, 16],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701504 / 2281701504 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-03-18 18:58:09.149588 test begin: paddle.ones_like(Tensor([1, 2281701379, 1],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 2281701379, 1],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0.],
        [0.],
        [0.],...
 y: array([[[1.],
        [1.],
        [1.],...
2025-03-18 19:00:06.003599 test begin: paddle.ones_like(Tensor([1, 2281701379],"int32"), )

[accuracy error] paddle.ones_like(Tensor([1, 2281701379],"int32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0]], dtype=int32)
 y: array([[1, 1, 1, ..., 1, 1, 1]], dtype=int32)
2025-03-18 19:04:56.600805 test begin: paddle.ones_like(Tensor([1, 4096, 557057],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 4096, 557057],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281705472 / 2281705472 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],...
2025-03-18 19:07:05.520083 test begin: paddle.ones_like(Tensor([1, 5704254, 200, 2],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 5704254, 200, 2],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701600 / 2281701600 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0.],
         [0., 0.],
         [0., 0.],...
 y: array([[[[1., 1.],
         [1., 1.],
         [1., 1.],...
2025-03-18 19:08:59.945780 test begin: paddle.ones_like(Tensor([1, 58, 39339679],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 58, 39339679],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701382 / 2281701382 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],...
2025-03-18 19:10:46.925862 test begin: paddle.ones_like(Tensor([1, 76056713, 15, 2],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 76056713, 15, 2],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701390 / 2281701390 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0.],
         [0., 0.],
         [0., 0.],...
 y: array([[[[1., 1.],
         [1., 1.],
         [1., 1.],...
2025-03-18 19:12:47.781798 test begin: paddle.ones_like(Tensor([1, 8912897, 256],"float32"), )

[accuracy error] paddle.ones_like(Tensor([1, 8912897, 256],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701632 / 2281701632 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],...
2025-03-18 19:14:39.403434 test begin: paddle.ones_like(Tensor([10563433, 12, 9, 2],"float32"), )

[accuracy error] paddle.ones_like(Tensor([10563433, 12, 9, 2],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701528 / 2281701528 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0.],
         [0., 0.],
         [0., 0.],...
 y: array([[[[1., 1.],
         [1., 1.],
         [1., 1.],...
2025-03-18 19:16:48.448041 test begin: paddle.ones_like(Tensor([139265, 128, 128],"float32"), )

[accuracy error] paddle.ones_like(Tensor([139265, 128, 128],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281717760 / 2281717760 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],...
2025-03-18 19:18:39.761689 test begin: paddle.ones_like(Tensor([139265, 128, 8, 16],"float32"), )

[accuracy error] paddle.ones_like(Tensor([139265, 128, 8, 16],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281717760 / 2281717760 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],
         [1., 1., 1., ..., 1., 1., 1.],...
2025-03-18 19:20:49.947749 test begin: paddle.ones_like(Tensor([14260634, 10, 8, 2],"float32"), )

[accuracy error] paddle.ones_like(Tensor([14260634, 10, 8, 2],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701440 / 2281701440 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0.],
         [0., 0.],
         [0., 0.],...
 y: array([[[[1., 1.],
         [1., 1.],
         [1., 1.],...
2025-03-18 19:22:40.659431 test begin: paddle.ones_like(Tensor([2228225, 1024, 1],"float32"), )

[accuracy error] paddle.ones_like(Tensor([2228225, 1024, 1],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281702400 / 2281702400 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0.],
        [0.],
        [0.],...
 y: array([[[1.],
        [1.],
        [1.],...
2025-03-18 19:24:50.234838 test begin: paddle.ones_like(Tensor([2270350, 1005],"int32"), )

[accuracy error] paddle.ones_like(Tensor([2270350, 1005],"int32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701750 / 2281701750 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1],...
2025-03-18 19:28:24.027400 test begin: paddle.ones_like(Tensor([2272611, 1004],"int32"), )

[accuracy error] paddle.ones_like(Tensor([2272611, 1004],"int32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701444 / 2281701444 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1],...
2025-03-18 19:32:01.226284 test begin: paddle.ones_like(Tensor([2274877, 1003],"int32"), )

[accuracy error] paddle.ones_like(Tensor([2274877, 1003],"int32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701631 / 2281701631 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1],...
2025-03-18 19:34:44.693214 test begin: paddle.ones_like(Tensor([2277148, 1002],"int32"), )

[accuracy error] paddle.ones_like(Tensor([2277148, 1002],"int32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281702296 / 2281702296 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1],...
2025-03-18 19:37:39.611118 test begin: paddle.ones_like(Tensor([2279422, 1001],"int32"), )

[accuracy error] paddle.ones_like(Tensor([2279422, 1001],"int32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701422 / 2281701422 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],...
 y: array([[1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1],
       [1, 1, 1, ..., 1, 1, 1],...
2025-03-18 19:40:20.945522 test begin: paddle.ones_like(Tensor([2281701379],"int32"), )

[accuracy error] paddle.ones_like(Tensor([2281701379],"int32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701379 / 2281701379 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([0, 0, 0, ..., 0, 0, 0], dtype=int32)
 y: array([1, 1, 1, ..., 1, 1, 1], dtype=int32)
2025-03-18 19:43:02.442081 test begin: paddle.ones_like(Tensor([39339679, 58, 1],"float32"), )

[accuracy error] paddle.ones_like(Tensor([39339679, 58, 1],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701382 / 2281701382 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0.],
        [0.],
        [0.],...
 y: array([[[1.],
        [1.],
        [1.],...
2025-03-18 19:45:07.056035 test begin: paddle.ones_like(Tensor([39613, 144, 200, 2],"float32"), )

[accuracy error] paddle.ones_like(Tensor([39613, 144, 200, 2],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281708800 / 2281708800 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0.],
         [0., 0.],
         [0., 0.],...
 y: array([[[[1., 1.],
         [1., 1.],
         [1., 1.],...
2025-03-18 19:47:14.626487 test begin: paddle.ones_like(Tensor([4, 280, 376, 25, 217],"float32"), )

[accuracy error] paddle.ones_like(Tensor([4, 280, 376, 25, 217],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2284576000 / 2284576000 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[[0., 0., 0., ..., 0., 0., 0.],
          [0., 0., 0., ..., 0., 0., 0.],
          [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[[[1., 1., 1., ..., 1., 1., 1.],
          [1., 1., 1., ..., 1., 1., 1.],
          [1., 1., 1., ..., 1., 1., 1.],...
2025-03-18 19:49:30.610896 test begin: paddle.ones_like(Tensor([4, 280, 376, 5419, 1],"float32"), )

[accuracy error] paddle.ones_like(Tensor([4, 280, 376, 5419, 1],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2282049280 / 2282049280 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[[0.],
          [0.],
          [0.],...
 y: array([[[[[1.],
          [1.],
          [1.],...
2025-03-18 19:52:15.706841 test begin: paddle.ones_like(Tensor([4, 280, 81490, 25, 1],"float32"), )

[accuracy error] paddle.ones_like(Tensor([4, 280, 81490, 25, 1],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281720000 / 2281720000 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[[0.],
          [0.],
          [0.],...
 y: array([[[[[1.],
          [1.],
          [1.],...
2025-03-18 19:54:57.280804 test begin: paddle.ones_like(Tensor([4, 60684, 376, 25, 1],"float32"), )

[accuracy error] paddle.ones_like(Tensor([4, 60684, 376, 25, 1],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281718400 / 2281718400 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[[0.],
          [0.],
          [0.],...
 y: array([[[[[1.],
          [1.],
          [1.],...
2025-03-18 19:57:17.809636 test begin: paddle.ones_like(Tensor([5070448, 15, 15, 2],"float32"), )

[accuracy error] paddle.ones_like(Tensor([5070448, 15, 15, 2],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701600 / 2281701600 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[[0., 0.],
         [0., 0.],
         [0., 0.],...
 y: array([[[[1., 1.],
         [1., 1.],
         [1., 1.],...
2025-03-18 19:59:13.966297 test begin: paddle.ones_like(Tensor([557057, 4096, 1],"float32"), )

[accuracy error] paddle.ones_like(Tensor([557057, 4096, 1],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281705472 / 2281705472 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0.],
        [0.],
        [0.],...
 y: array([[[1.],
        [1.],
        [1.],...
2025-03-18 20:01:37.360315 test begin: paddle.ones_like(Tensor([69633, 128, 256],"float32"), )

[accuracy error] paddle.ones_like(Tensor([69633, 128, 256],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

x and y +inf location mismatch:
 x: array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],...
 y: array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],...
2025-03-18 20:02:07.208245 test begin: paddle.ones_like(Tensor([867, 280, 376, 25, 1],"float32"), )

[accuracy error] paddle.ones_like(Tensor([867, 280, 376, 25, 1],"float32"), ) 
 Unable to allocate 2.13 GiB for an array with shape (2281944000,) and data type bool
2025-03-18 20:03:28.499929 test begin: paddle.ones_like(x=Tensor([17674763, 3, 3, 3, 3, 3],"float16"), )

[Pass] paddle.ones_like(x=Tensor([17674763, 3, 3, 3, 3, 3],"float16"), )
2025-03-18 20:13:00.598594 test begin: paddle.ones_like(x=Tensor([253522376, 3, 3],"bool"), )

[accuracy error] paddle.ones_like(x=Tensor([253522376, 3, 3],"bool"), ) 
 
Arrays are not equal

Mismatched elements: 2281701384 / 2281701384 (100%)
 x: array([[[False, False, False],
        [False, False, False],
        [False, False, False]],...
 y: array([[[ True,  True,  True],
        [ True,  True,  True],
        [ True,  True,  True]],...
2025-03-18 20:14:35.530682 test begin: paddle.ones_like(x=Tensor([253522376, 3, 3],"float32"), )

[accuracy error] paddle.ones_like(x=Tensor([253522376, 3, 3],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701384 / 2281701384 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]],...
 y: array([[[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]],...
2025-03-18 20:16:29.789293 test begin: paddle.ones_like(x=Tensor([253522376, 3, 3],"int32"), )

[accuracy error] paddle.ones_like(x=Tensor([253522376, 3, 3],"int32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701384 / 2281701384 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([[[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]],...
 y: array([[[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]],...
2025-03-18 20:19:14.174425 test begin: paddle.ones_like(x=Tensor([3, 17674763, 3, 3, 3, 3],"float16"), )

[Pass] paddle.ones_like(x=Tensor([3, 17674763, 3, 3, 3, 3],"float16"), )
2025-03-18 20:27:07.525754 test begin: paddle.ones_like(x=Tensor([3, 253522376, 3],"bool"), )

[accuracy error] paddle.ones_like(x=Tensor([3, 253522376, 3],"bool"), ) 
 
Arrays are not equal

Mismatched elements: 2281701384 / 2281701384 (100%)
 x: array([[[False, False, False],
        [False, False, False],
        [False, False, False],...
 y: array([[[ True,  True,  True],
        [ True,  True,  True],
        [ True,  True,  True],...
2025-03-18 20:27:47.203806 test begin: paddle.ones_like(x=Tensor([3, 253522376, 3],"float32"), )

[accuracy error] paddle.ones_like(x=Tensor([3, 253522376, 3],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701384 / 2281701384 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],...
 y: array([[[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],...
2025-03-18 20:29:44.067822 test begin: paddle.ones_like(x=Tensor([3, 253522376, 3],"int32"), )

[accuracy error] paddle.ones_like(x=Tensor([3, 253522376, 3],"int32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701384 / 2281701384 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([[[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],...
 y: array([[[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],...
2025-03-18 20:32:33.395787 test begin: paddle.ones_like(x=Tensor([3, 3, 17674763, 3, 3, 3],"float16"), )

[Pass] paddle.ones_like(x=Tensor([3, 3, 17674763, 3, 3, 3],"float16"), )
2025-03-18 20:40:36.953013 test begin: paddle.ones_like(x=Tensor([3, 3, 253522376],"bool"), )

[accuracy error] paddle.ones_like(x=Tensor([3, 3, 253522376],"bool"), ) 
 
Arrays are not equal

Mismatched elements: 2281701384 / 2281701384 (100%)
 x: array([[[False, False, False, ..., False, False, False],
        [False, False, False, ..., False, False, False],
        [False, False, False, ..., False, False, False]],...
 y: array([[[ True,  True,  True, ...,  True,  True,  True],
        [ True,  True,  True, ...,  True,  True,  True],
        [ True,  True,  True, ...,  True,  True,  True]],...
2025-03-18 20:41:20.419401 test begin: paddle.ones_like(x=Tensor([3, 3, 253522376],"float32"), )

[accuracy error] paddle.ones_like(x=Tensor([3, 3, 253522376],"float32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701384 / 2281701384 (100%)
Max absolute difference: 1.
Max relative difference: 1.
 x: array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],...
 y: array([[[1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.],
        [1., 1., 1., ..., 1., 1., 1.]],...
2025-03-18 20:43:30.660753 test begin: paddle.ones_like(x=Tensor([3, 3, 253522376],"int32"), )

[accuracy error] paddle.ones_like(x=Tensor([3, 3, 253522376],"int32"), ) 
 
Not equal to tolerance rtol=0.01, atol=0.01

Mismatched elements: 2281701384 / 2281701384 (100%)
Max absolute difference: 1
Max relative difference: 1.
 x: array([[[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]],...
 y: array([[[1, 1, 1, ..., 1, 1, 1],
        [1, 1, 1, ..., 1, 1, 1],
        [1, 1, 1, ..., 1, 1, 1]],...
2025-03-18 20:46:40.753859 test begin: paddle.ones_like(x=Tensor([3, 3, 3, 17674763, 3, 3],"float16"), )

[Pass] paddle.ones_like(x=Tensor([3, 3, 3, 17674763, 3, 3],"float16"), )
2025-03-18 20:54:26.751521 test begin: paddle.ones_like(x=Tensor([3, 3, 3, 3, 17674763, 3],"float16"), )

[Pass] paddle.ones_like(x=Tensor([3, 3, 3, 3, 17674763, 3],"float16"), )
2025-03-18 21:02:11.140301 test begin: paddle.ones_like(x=Tensor([3, 3, 3, 3, 3, 17674763],"float16"), )

[Pass] paddle.ones_like(x=Tensor([3, 3, 3, 3, 3, 17674763],"float16"), )
2025-03-18 21:09:58.645777 test begin: paddle.ones_like(x=Tensor([3, 3, 477218589],"float16"), )

[Pass] paddle.ones_like(x=Tensor([3, 3, 477218589],"float16"), )
2025-03-18 21:17:55.546122 test begin: paddle.ones_like(x=Tensor([3, 477218589, 3],"float16"), )

[Pass] paddle.ones_like(x=Tensor([3, 477218589, 3],"float16"), )
2025-03-18 21:25:49.051070 test begin: paddle.ones_like(x=Tensor([477218589, 3, 3],"float16"), )

[Pass] paddle.ones_like(x=Tensor([477218589, 3, 3],"float16"), )
2025-03-18 21:33:44.901328 test begin: paddle.outer(Tensor([10],"float32"), Tensor([2281701379],"float32"), )

[torch error] paddle.outer(Tensor([10],"float32"), Tensor([2281701379],"float32"), ) 
 CUDA out of memory. Tried to allocate 85.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 67.97 GiB is free. Process 121515 has 10.09 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 4.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:33:48.868860 test begin: paddle.outer(Tensor([142],"float32"), Tensor([2281701379],"float32"), )

[torch error] paddle.outer(Tensor([142],"float32"), Tensor([2281701379],"float32"), ) 
 CUDA out of memory. Tried to allocate 1207.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 67.97 GiB is free. Process 121515 has 10.09 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 4.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:33:50.419821 test begin: paddle.outer(Tensor([16],"float32"), Tensor([2281701379],"float32"), )

[torch error] paddle.outer(Tensor([16],"float32"), Tensor([2281701379],"float32"), ) 
 CUDA out of memory. Tried to allocate 136.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 67.97 GiB is free. Process 121515 has 10.09 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 4.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:33:52.050158 test begin: paddle.outer(Tensor([2048],"float32"), Tensor([2281701379],"float32"), )

[torch error] paddle.outer(Tensor([2048],"float32"), Tensor([2281701379],"float32"), ) 
 CUDA out of memory. Tried to allocate 17408.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 67.97 GiB is free. Process 121515 has 10.09 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 3.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:33:53.582118 test begin: paddle.outer(Tensor([2281701379],"float32"), Tensor([1],"float32"), )

[paddle error] paddle.outer(Tensor([2281701379],"float32"), Tensor([1],"float32"), ) 
 (PreconditionNotMet) The meta data must be valid when call the mutable data function.
  [Hint: Expected valid() == true, but received valid():0 != true:1.] (at ../paddle/phi/core/dense_tensor.cc:117)

2025-03-18 21:34:21.487947 test begin: paddle.outer(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), )

[torch error] paddle.outer(Tensor([2281701379],"float32"), Tensor([2281701379],"float32"), ) 
 Storage size calculation overflowed with sizes=[2281701379, 2281701379]
2025-03-18 21:34:24.634424 test begin: paddle.outer(Tensor([2281701379],"float32"), Tensor([2],"float32"), )

2025-03-18 21:34:25.765692 test begin: paddle.outer(Tensor([2281701379],"float32"), Tensor([32],"float32"), )

[torch error] paddle.outer(Tensor([2281701379],"float32"), Tensor([32],"float32"), ) 
 CUDA out of memory. Tried to allocate 272.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 42.46 GiB is free. Process 121515 has 35.59 GiB memory in use. Of the allocated memory 34.00 GiB is allocated by PyTorch, and 8.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:27.339191 test begin: paddle.outer(Tensor([2281701379],"float32"), Tensor([4],"float32"), )

2025-03-18 21:34:28.778202 test begin: paddle.outer(x=Tensor([4294967297],"float16"), y=Tensor([4294967297],"float16"), )

[torch error] paddle.outer(x=Tensor([4294967297],"float16"), y=Tensor([4294967297],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:32.436943 test begin: paddle.outer(x=Tensor([4294967297],"float16"), y=Tensor([4],"float16"), )

[torch error] paddle.outer(x=Tensor([4294967297],"float16"), y=Tensor([4],"float16"), ) 
 CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:33.416670 test begin: paddle.outer(x=Tensor([4],"float16"), y=Tensor([4294967297],"float16"), )

[torch error] paddle.outer(x=Tensor([4],"float16"), y=Tensor([4294967297],"float16"), ) 
 CUDA out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:34.410240 test begin: paddle.pdist(Tensor([10, 228170138],"float32"), 0, )

[torch error] paddle.pdist(Tensor([10, 228170138],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:34.656269 test begin: paddle.pdist(Tensor([10, 228170138],"float32"), 1.0, )

[torch error] paddle.pdist(Tensor([10, 228170138],"float32"), 1.0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:34.669324 test begin: paddle.pdist(Tensor([10, 228170138],"float32"), 1.5, )

[torch error] paddle.pdist(Tensor([10, 228170138],"float32"), 1.5, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:34.681612 test begin: paddle.pdist(Tensor([10, 228170138],"float32"), 2.0, )

[torch error] paddle.pdist(Tensor([10, 228170138],"float32"), 2.0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:34.693620 test begin: paddle.pdist(Tensor([10, 228170138],"float32"), 2.5, )

[torch error] paddle.pdist(Tensor([10, 228170138],"float32"), 2.5, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:34.705414 test begin: paddle.pdist(Tensor([10, 228170138],"float32"), 3.0, )

[torch error] paddle.pdist(Tensor([10, 228170138],"float32"), 3.0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:34.718015 test begin: paddle.pdist(Tensor([10, 228170138],"float32"), math.inf, )

[torch error] paddle.pdist(Tensor([10, 228170138],"float32"), math.inf, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:34.730408 test begin: paddle.pdist(Tensor([114085069, 20],"float32"), 0, )

[torch error] paddle.pdist(Tensor([114085069, 20],"float32"), 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:34.742745 test begin: paddle.pdist(Tensor([114085069, 20],"float32"), 1.0, )

[torch error] paddle.pdist(Tensor([114085069, 20],"float32"), 1.0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:34.754955 test begin: paddle.pdist(Tensor([114085069, 20],"float32"), 1.5, )

[torch error] paddle.pdist(Tensor([114085069, 20],"float32"), 1.5, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:34.767214 test begin: paddle.pdist(Tensor([114085069, 20],"float32"), 2.0, )

[torch error] paddle.pdist(Tensor([114085069, 20],"float32"), 2.0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:34.778995 test begin: paddle.pdist(Tensor([114085069, 20],"float32"), 2.5, )

[torch error] paddle.pdist(Tensor([114085069, 20],"float32"), 2.5, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:34.790674 test begin: paddle.pdist(Tensor([114085069, 20],"float32"), 3.0, )

[torch error] paddle.pdist(Tensor([114085069, 20],"float32"), 3.0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:34.802400 test begin: paddle.pdist(Tensor([114085069, 20],"float32"), math.inf, )

[torch error] paddle.pdist(Tensor([114085069, 20],"float32"), math.inf, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:34.814098 test begin: paddle.pdist(Tensor([214748365, 20],"float16"), 2.0, )

[torch error] paddle.pdist(Tensor([214748365, 20],"float16"), 2.0, ) 
 CUDA out of memory. Tried to allocate 42949672.84 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:35.797480 test begin: paddle.pdist(Tensor([50, 85899346],"float16"), 2.0, )

[torch error] paddle.pdist(Tensor([50, 85899346],"float16"), 2.0, ) 
 "pdist_cuda" not implemented for 'Half'
2025-03-18 21:34:36.800698 test begin: paddle.polygamma(Tensor([10, 20, 11408507],"float32"), 1, )

[torch error] paddle.polygamma(Tensor([10, 20, 11408507],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:37.046356 test begin: paddle.polygamma(Tensor([10, 228170138, 1],"float32"), 1, )

[torch error] paddle.polygamma(Tensor([10, 228170138, 1],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:37.058389 test begin: paddle.polygamma(Tensor([114085069, 20, 1],"float32"), 1, )

[torch error] paddle.polygamma(Tensor([114085069, 20, 1],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:37.069802 test begin: paddle.polygamma(Tensor([2, 2, 1073741825],"float16"), 2, )

[torch error] paddle.polygamma(Tensor([2, 2, 1073741825],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:38.054530 test begin: paddle.polygamma(Tensor([2, 2147483649],"float16"), 1, )

[torch error] paddle.polygamma(Tensor([2, 2147483649],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:39.320318 test begin: paddle.polygamma(Tensor([2, 357913942, 6],"float16"), 2, )

[torch error] paddle.polygamma(Tensor([2, 357913942, 6],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:40.859652 test begin: paddle.polygamma(Tensor([2281701379],"float32"), 1, )

[torch error] paddle.polygamma(Tensor([2281701379],"float32"), 1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:41.104237 test begin: paddle.polygamma(Tensor([2281701379],"float32"), 2, )

[torch error] paddle.polygamma(Tensor([2281701379],"float32"), 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:41.117176 test begin: paddle.polygamma(Tensor([2281701379],"float32"), 3, )

[torch error] paddle.polygamma(Tensor([2281701379],"float32"), 3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:41.129483 test begin: paddle.polygamma(Tensor([357913942, 2, 6],"float16"), 2, )

[torch error] paddle.polygamma(Tensor([357913942, 2, 6],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:42.388673 test begin: paddle.polygamma(Tensor([4294967297],"float16"), 1, )

[torch error] paddle.polygamma(Tensor([4294967297],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:44.221072 test begin: paddle.polygamma(Tensor([4294967297],"float16"), 2, )

[torch error] paddle.polygamma(Tensor([4294967297],"float16"), 2, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:46.055074 test begin: paddle.polygamma(Tensor([4294967297],"float16"), 3, )

[torch error] paddle.polygamma(Tensor([4294967297],"float16"), 3, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:47.889548 test begin: paddle.polygamma(Tensor([715827883, 6],"float16"), 1, )

[torch error] paddle.polygamma(Tensor([715827883, 6],"float16"), 1, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:49.717049 test begin: paddle.quantile(Tensor([102261127, 7, 6],"float16"), q=0, axis=1, )

[torch error] paddle.quantile(Tensor([102261127, 7, 6],"float16"), q=0, axis=1, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:34:51.521808 test begin: paddle.quantile(Tensor([102261127, 7, 6],"float16"), q=0.35, )

[torch error] paddle.quantile(Tensor([102261127, 7, 6],"float16"), q=0.35, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:34:53.044517 test begin: paddle.quantile(Tensor([102261127, 7, 6],"float16"), q=0.35, axis=2, keepdim=True, )

[torch error] paddle.quantile(Tensor([102261127, 7, 6],"float16"), q=0.35, axis=2, keepdim=True, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:34:54.022934 test begin: paddle.quantile(Tensor([102261127, 7, 6],"float16"), q=0.5, axis=2, )

[torch error] paddle.quantile(Tensor([102261127, 7, 6],"float16"), q=0.5, axis=2, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:34:55.291379 test begin: paddle.quantile(Tensor([1124, 2029984],"float32"), 0.30000000000000004, )

[torch error] paddle.quantile(Tensor([1124, 2029984],"float32"), 0.30000000000000004, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:55.538333 test begin: paddle.quantile(Tensor([16, 142606337],"float32"), 0.30000000000000004, )

[torch error] paddle.quantile(Tensor([16, 142606337],"float32"), 0.30000000000000004, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:55.551110 test begin: paddle.quantile(Tensor([2, 1140850690],"float32"), 0.5, axis=None, )

[torch error] paddle.quantile(Tensor([2, 1140850690],"float32"), 0.5, axis=None, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:34:55.562993 test begin: paddle.quantile(Tensor([2, 3, 715827883],"float16"), q=0.35, axis=0, )

[torch error] paddle.quantile(Tensor([2, 3, 715827883],"float16"), q=0.35, axis=0, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:34:57.374014 test begin: paddle.quantile(Tensor([2, 3, 715827883],"float16"), q=0.35, axis=0, interpolation="higher", )

[torch error] paddle.quantile(Tensor([2, 3, 715827883],"float16"), q=0.35, axis=0, interpolation="higher", ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:34:59.180274 test begin: paddle.quantile(Tensor([2, 3, 715827883],"float16"), q=0.35, axis=0, interpolation="lower", )

[torch error] paddle.quantile(Tensor([2, 3, 715827883],"float16"), q=0.35, axis=0, interpolation="lower", ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:00.713754 test begin: paddle.quantile(Tensor([2, 3, 715827883],"float16"), q=0.35, axis=0, interpolation="midpoint", )

[torch error] paddle.quantile(Tensor([2, 3, 715827883],"float16"), q=0.35, axis=0, interpolation="midpoint", ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:01.964611 test begin: paddle.quantile(Tensor([2, 3, 715827883],"float16"), q=0.35, axis=0, interpolation="nearest", )

[torch error] paddle.quantile(Tensor([2, 3, 715827883],"float16"), q=0.35, axis=0, interpolation="nearest", ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:03.770662 test begin: paddle.quantile(Tensor([2, 536870913, 4],"float16"), q=0.35, axis=0, )

[torch error] paddle.quantile(Tensor([2, 536870913, 4],"float16"), q=0.35, axis=0, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:05.578855 test begin: paddle.quantile(Tensor([2, 536870913, 4],"float16"), q=0.35, axis=0, interpolation="higher", )

[torch error] paddle.quantile(Tensor([2, 536870913, 4],"float16"), q=0.35, axis=0, interpolation="higher", ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:07.111542 test begin: paddle.quantile(Tensor([2, 536870913, 4],"float16"), q=0.35, axis=0, interpolation="lower", )

[torch error] paddle.quantile(Tensor([2, 536870913, 4],"float16"), q=0.35, axis=0, interpolation="lower", ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:08.322783 test begin: paddle.quantile(Tensor([2, 536870913, 4],"float16"), q=0.35, axis=0, interpolation="midpoint", )

[torch error] paddle.quantile(Tensor([2, 536870913, 4],"float16"), q=0.35, axis=0, interpolation="midpoint", ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:09.748850 test begin: paddle.quantile(Tensor([2, 536870913, 4],"float16"), q=0.35, axis=0, interpolation="nearest", )

[torch error] paddle.quantile(Tensor([2, 536870913, 4],"float16"), q=0.35, axis=0, interpolation="nearest", ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:11.003896 test begin: paddle.quantile(Tensor([2281701379],"float32"), 0.30000000000000004, )

[torch error] paddle.quantile(Tensor([2281701379],"float32"), 0.30000000000000004, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:35:11.250084 test begin: paddle.quantile(Tensor([2281701379],"float32"), 0.5, 0, )

[torch error] paddle.quantile(Tensor([2281701379],"float32"), 0.5, 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:35:11.261881 test begin: paddle.quantile(Tensor([3, 11930465, 3, 4, 2, 5],"float16"), q=0.5, )

[torch error] paddle.quantile(Tensor([3, 11930465, 3, 4, 2, 5],"float16"), q=0.5, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:13.071245 test begin: paddle.quantile(Tensor([3, 6, 3, 4, 2, 9942054],"float16"), q=0.5, )

[torch error] paddle.quantile(Tensor([3, 6, 3, 4, 2, 9942054],"float16"), q=0.5, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:14.870265 test begin: paddle.quantile(Tensor([3, 6, 3, 4, 3976822, 5],"float16"), q=0.5, )

[torch error] paddle.quantile(Tensor([3, 6, 3, 4, 3976822, 5],"float16"), q=0.5, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:16.707069 test begin: paddle.quantile(Tensor([3, 6, 3, 7953644, 2, 5],"float16"), q=0.5, )

[torch error] paddle.quantile(Tensor([3, 6, 3, 7953644, 2, 5],"float16"), q=0.5, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:18.494168 test begin: paddle.quantile(Tensor([3, 6, 5965233, 4, 2, 5],"float16"), q=0.5, )

[torch error] paddle.quantile(Tensor([3, 6, 5965233, 4, 2, 5],"float16"), q=0.5, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:20.310295 test begin: paddle.quantile(Tensor([32, 71303169],"float32"), 0.30000000000000004, )

[torch error] paddle.quantile(Tensor([32, 71303169],"float32"), 0.30000000000000004, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:35:20.556320 test begin: paddle.quantile(Tensor([325957340, 7],"float32"), q=0.5, axis=1, )

[torch error] paddle.quantile(Tensor([325957340, 7],"float32"), q=0.5, axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:35:20.572052 test begin: paddle.quantile(Tensor([357913942, 3, 4],"float16"), q=0.35, axis=0, )

[torch error] paddle.quantile(Tensor([357913942, 3, 4],"float16"), q=0.35, axis=0, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:22.403586 test begin: paddle.quantile(Tensor([357913942, 3, 4],"float16"), q=0.35, axis=0, interpolation="higher", )

[torch error] paddle.quantile(Tensor([357913942, 3, 4],"float16"), q=0.35, axis=0, interpolation="higher", ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:24.217698 test begin: paddle.quantile(Tensor([357913942, 3, 4],"float16"), q=0.35, axis=0, interpolation="lower", )

[torch error] paddle.quantile(Tensor([357913942, 3, 4],"float16"), q=0.35, axis=0, interpolation="lower", ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:25.762955 test begin: paddle.quantile(Tensor([357913942, 3, 4],"float16"), q=0.35, axis=0, interpolation="midpoint", )

[torch error] paddle.quantile(Tensor([357913942, 3, 4],"float16"), q=0.35, axis=0, interpolation="midpoint", ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:27.051695 test begin: paddle.quantile(Tensor([357913942, 3, 4],"float16"), q=0.35, axis=0, interpolation="nearest", )

[torch error] paddle.quantile(Tensor([357913942, 3, 4],"float16"), q=0.35, axis=0, interpolation="nearest", ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:28.601215 test begin: paddle.quantile(Tensor([37, 61667605],"float32"), 0.30000000000000004, )

[torch error] paddle.quantile(Tensor([37, 61667605],"float32"), 0.30000000000000004, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:35:28.847358 test begin: paddle.quantile(Tensor([4, 1073741825],"float16"), q=0.5, axis=1, )

[torch error] paddle.quantile(Tensor([4, 1073741825],"float16"), q=0.5, axis=1, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:29.920674 test begin: paddle.quantile(Tensor([4, 178956971, 6],"float16"), q=0, axis=1, )

[torch error] paddle.quantile(Tensor([4, 178956971, 6],"float16"), q=0, axis=1, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:30.977883 test begin: paddle.quantile(Tensor([4, 178956971, 6],"float16"), q=0.35, )

[torch error] paddle.quantile(Tensor([4, 178956971, 6],"float16"), q=0.35, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:32.271981 test begin: paddle.quantile(Tensor([4, 178956971, 6],"float16"), q=0.35, axis=2, keepdim=True, )

[torch error] paddle.quantile(Tensor([4, 178956971, 6],"float16"), q=0.35, axis=2, keepdim=True, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:34.121551 test begin: paddle.quantile(Tensor([4, 178956971, 6],"float16"), q=0.5, axis=2, )

[torch error] paddle.quantile(Tensor([4, 178956971, 6],"float16"), q=0.5, axis=2, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:35.655730 test begin: paddle.quantile(Tensor([4, 570425345],"float32"), q=0.5, axis=1, )

[torch error] paddle.quantile(Tensor([4, 570425345],"float32"), q=0.5, axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:35:35.901250 test begin: paddle.quantile(Tensor([4, 7, 153391690],"float16"), q=0, axis=1, )

[torch error] paddle.quantile(Tensor([4, 7, 153391690],"float16"), q=0, axis=1, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:37.195102 test begin: paddle.quantile(Tensor([4, 7, 153391690],"float16"), q=0.35, )

[torch error] paddle.quantile(Tensor([4, 7, 153391690],"float16"), q=0.35, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:38.721370 test begin: paddle.quantile(Tensor([4, 7, 153391690],"float16"), q=0.35, axis=2, keepdim=True, )

[torch error] paddle.quantile(Tensor([4, 7, 153391690],"float16"), q=0.35, axis=2, keepdim=True, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:40.774706 test begin: paddle.quantile(Tensor([4, 7, 153391690],"float16"), q=0.5, axis=2, )

[torch error] paddle.quantile(Tensor([4, 7, 153391690],"float16"), q=0.5, axis=2, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:42.584966 test begin: paddle.quantile(Tensor([5965233, 6, 3, 4, 2, 5],"float16"), q=0.5, )

[torch error] paddle.quantile(Tensor([5965233, 6, 3, 4, 2, 5],"float16"), q=0.5, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:44.402520 test begin: paddle.quantile(Tensor([613566757, 7],"float16"), q=0.5, axis=1, )

[torch error] paddle.quantile(Tensor([613566757, 7],"float16"), q=0.5, axis=1, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:46.298161 test begin: paddle.quantile(Tensor([61667605, 37],"float32"), 0.30000000000000004, )

[torch error] paddle.quantile(Tensor([61667605, 37],"float32"), 0.30000000000000004, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:35:46.546048 test begin: paddle.quantile(Tensor([71303169, 32],"float32"), 0.30000000000000004, )

[torch error] paddle.quantile(Tensor([71303169, 32],"float32"), 0.30000000000000004, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:35:46.558650 test begin: paddle.quantile(Tensor([760567127, 3],"float32"), 0.5, axis=None, )

[torch error] paddle.quantile(Tensor([760567127, 3],"float32"), 0.5, axis=None, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:35:46.571138 test begin: paddle.quantile(x=Tensor([253522376, 3, 3],"float32"), q=0.5, axis=0, )

[torch error] paddle.quantile(x=Tensor([253522376, 3, 3],"float32"), q=0.5, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:35:46.582657 test begin: paddle.quantile(x=Tensor([3, 11930465, 3, 4, 2, 5],"float16"), q=0.75, axis=3, keepdim=True, )

[torch error] paddle.quantile(x=Tensor([3, 11930465, 3, 4, 2, 5],"float16"), q=0.75, axis=3, keepdim=True, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:48.195760 test begin: paddle.quantile(x=Tensor([3, 11930465, 3, 4, 2, 5],"float16"), q=0.75, axis=5, )

[torch error] paddle.quantile(x=Tensor([3, 11930465, 3, 4, 2, 5],"float16"), q=0.75, axis=5, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:49.243466 test begin: paddle.quantile(x=Tensor([3, 253522376, 3],"float32"), q=0.5, axis=0, )

[torch error] paddle.quantile(x=Tensor([3, 253522376, 3],"float32"), q=0.5, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:35:49.489067 test begin: paddle.quantile(x=Tensor([3, 3, 253522376],"float32"), q=0.5, axis=0, )

[torch error] paddle.quantile(x=Tensor([3, 3, 253522376],"float32"), q=0.5, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:35:49.500695 test begin: paddle.quantile(x=Tensor([3, 3, 477218589],"float16"), q=0.5, axis=0, )

[torch error] paddle.quantile(x=Tensor([3, 3, 477218589],"float16"), q=0.5, axis=0, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:50.853097 test begin: paddle.quantile(x=Tensor([3, 477218589, 3],"float16"), q=0.5, axis=0, )

[torch error] paddle.quantile(x=Tensor([3, 477218589, 3],"float16"), q=0.5, axis=0, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:52.745526 test begin: paddle.quantile(x=Tensor([3, 6, 3, 4, 2, 9942054],"float16"), q=0.75, axis=3, keepdim=True, )

[torch error] paddle.quantile(x=Tensor([3, 6, 3, 4, 2, 9942054],"float16"), q=0.75, axis=3, keepdim=True, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:54.372392 test begin: paddle.quantile(x=Tensor([3, 6, 3, 4, 2, 9942054],"float16"), q=0.75, axis=5, )

[torch error] paddle.quantile(x=Tensor([3, 6, 3, 4, 2, 9942054],"float16"), q=0.75, axis=5, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:55.443255 test begin: paddle.quantile(x=Tensor([3, 6, 3, 4, 3976822, 5],"float16"), q=0.75, axis=3, keepdim=True, )

[torch error] paddle.quantile(x=Tensor([3, 6, 3, 4, 3976822, 5],"float16"), q=0.75, axis=3, keepdim=True, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:56.804712 test begin: paddle.quantile(x=Tensor([3, 6, 3, 4, 3976822, 5],"float16"), q=0.75, axis=5, )

[torch error] paddle.quantile(x=Tensor([3, 6, 3, 4, 3976822, 5],"float16"), q=0.75, axis=5, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:58.410794 test begin: paddle.quantile(x=Tensor([3, 6, 3, 7953644, 2, 5],"float16"), q=0.75, axis=3, keepdim=True, )

[torch error] paddle.quantile(x=Tensor([3, 6, 3, 7953644, 2, 5],"float16"), q=0.75, axis=3, keepdim=True, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:35:59.461606 test begin: paddle.quantile(x=Tensor([3, 6, 3, 7953644, 2, 5],"float16"), q=0.75, axis=5, )

[torch error] paddle.quantile(x=Tensor([3, 6, 3, 7953644, 2, 5],"float16"), q=0.75, axis=5, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:36:00.811518 test begin: paddle.quantile(x=Tensor([3, 6, 5965233, 4, 2, 5],"float16"), q=0.75, axis=3, keepdim=True, )

[torch error] paddle.quantile(x=Tensor([3, 6, 5965233, 4, 2, 5],"float16"), q=0.75, axis=3, keepdim=True, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:36:02.677902 test begin: paddle.quantile(x=Tensor([3, 6, 5965233, 4, 2, 5],"float16"), q=0.75, axis=5, )

[torch error] paddle.quantile(x=Tensor([3, 6, 5965233, 4, 2, 5],"float16"), q=0.75, axis=5, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:36:04.590070 test begin: paddle.quantile(x=Tensor([477218589, 3, 3],"float16"), q=0.5, axis=0, )

[torch error] paddle.quantile(x=Tensor([477218589, 3, 3],"float16"), q=0.5, axis=0, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:36:06.192744 test begin: paddle.quantile(x=Tensor([5965233, 6, 3, 4, 2, 5],"float16"), q=0.75, axis=3, keepdim=True, )

[torch error] paddle.quantile(x=Tensor([5965233, 6, 3, 4, 2, 5],"float16"), q=0.75, axis=3, keepdim=True, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:36:07.249557 test begin: paddle.quantile(x=Tensor([5965233, 6, 3, 4, 2, 5],"float16"), q=0.75, axis=5, )

[torch error] paddle.quantile(x=Tensor([5965233, 6, 3, 4, 2, 5],"float16"), q=0.75, axis=5, ) 
 quantile() input tensor must be either float or double dtype
2025-03-18 21:36:08.292813 test begin: paddle.rad2deg(Tensor([2281701379],"float32"), )

[torch error] paddle.rad2deg(Tensor([2281701379],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:36:08.538152 test begin: paddle.rad2deg(Tensor([2281701379],"int64"), )

[torch error] paddle.rad2deg(Tensor([2281701379],"int64"), ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:28.534109 test begin: paddle.rad2deg(Tensor([4456449, 16, 32],"float32"), )

[torch error] paddle.rad2deg(Tensor([4456449, 16, 32],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:28.547066 test begin: paddle.rad2deg(Tensor([8, 16, 17825793],"float32"), )

[torch error] paddle.rad2deg(Tensor([8, 16, 17825793],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:28.562401 test begin: paddle.rad2deg(Tensor([8, 8912897, 32],"float32"), )

[torch error] paddle.rad2deg(Tensor([8, 8912897, 32],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:28.574807 test begin: paddle.rad2deg(x=Tensor([1073741825, 4],"float16"), )

[torch error] paddle.rad2deg(x=Tensor([1073741825, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:31.152856 test begin: paddle.rad2deg(x=Tensor([2281701379],"float32"), )

[torch error] paddle.rad2deg(x=Tensor([2281701379],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:31.411779 test begin: paddle.rad2deg(x=Tensor([2281701379],"int32"), )

[torch error] paddle.rad2deg(x=Tensor([2281701379],"int32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:31.423252 test begin: paddle.rad2deg(x=Tensor([2281701379],"int64"), )

[torch error] paddle.rad2deg(x=Tensor([2281701379],"int64"), ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:31.434185 test begin: paddle.rad2deg(x=Tensor([268435457, 4, 4],"float16"), )

[torch error] paddle.rad2deg(x=Tensor([268435457, 4, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:33.229954 test begin: paddle.rad2deg(x=Tensor([4, 1073741825],"float16"), )

[torch error] paddle.rad2deg(x=Tensor([4, 1073741825],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:34.785258 test begin: paddle.rad2deg(x=Tensor([4, 268435457, 4],"float16"), )

[torch error] paddle.rad2deg(x=Tensor([4, 268435457, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:36.031091 test begin: paddle.rad2deg(x=Tensor([4, 4, 268435457],"float16"), )

[torch error] paddle.rad2deg(x=Tensor([4, 4, 268435457],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:37.850934 test begin: paddle.rad2deg(x=Tensor([4, 4, 4, 67108865],"float16"), )

[torch error] paddle.rad2deg(x=Tensor([4, 4, 4, 67108865],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:39.662280 test begin: paddle.rad2deg(x=Tensor([4, 4, 67108865, 4],"float16"), )

[torch error] paddle.rad2deg(x=Tensor([4, 4, 67108865, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:41.188092 test begin: paddle.rad2deg(x=Tensor([4, 67108865, 4, 4],"float16"), )

[torch error] paddle.rad2deg(x=Tensor([4, 67108865, 4, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:42.137464 test begin: paddle.rad2deg(x=Tensor([4294967297],"float16"), )

[torch error] paddle.rad2deg(x=Tensor([4294967297],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:43.109406 test begin: paddle.rad2deg(x=Tensor([67108865, 4, 4, 4],"float16"), )

[torch error] paddle.rad2deg(x=Tensor([67108865, 4, 4, 4],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:44.366489 test begin: paddle.reciprocal(Tensor([10486, 1, 640, 640],"float16"), )

[torch error] paddle.reciprocal(Tensor([10486, 1, 640, 640],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 11.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:46.153828 test begin: paddle.reciprocal(Tensor([16, 1, 222823, 640],"float32"), )

[torch error] paddle.reciprocal(Tensor([16, 1, 222823, 640],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:46.399777 test begin: paddle.reciprocal(Tensor([16, 1, 419431, 640],"float16"), )

[torch error] paddle.reciprocal(Tensor([16, 1, 419431, 640],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 11.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:48.295984 test begin: paddle.reciprocal(Tensor([16, 1, 640, 222823],"float32"), )

[torch error] paddle.reciprocal(Tensor([16, 1, 640, 222823],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:48.541702 test begin: paddle.reciprocal(Tensor([16, 1, 640, 419431],"float16"), )

[torch error] paddle.reciprocal(Tensor([16, 1, 640, 419431],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 11.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:50.159238 test begin: paddle.reciprocal(Tensor([16, 349, 640, 640],"float32"), )

[torch error] paddle.reciprocal(Tensor([16, 349, 640, 640],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.52 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:50.406049 test begin: paddle.reciprocal(Tensor([16, 656, 640, 640],"float16"), )

[torch error] paddle.reciprocal(Tensor([16, 656, 640, 640],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.01 GiB. GPU 0 has a total capacity of 79.18 GiB of which 462.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.01 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:51.477680 test begin: paddle.reciprocal(Tensor([2, 300, 3802836],"float32"), )

[torch error] paddle.reciprocal(Tensor([2, 300, 3802836],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:51.723094 test begin: paddle.reciprocal(Tensor([2, 557057, 2048],"float32"), )

[torch error] paddle.reciprocal(Tensor([2, 557057, 2048],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:51.736817 test begin: paddle.reciprocal(Tensor([2476, 1, 960, 960],"float32"), )

[torch error] paddle.reciprocal(Tensor([2476, 1, 960, 960],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:51.748865 test begin: paddle.reciprocal(Tensor([3714, 300, 2048],"float32"), )

[torch error] paddle.reciprocal(Tensor([3714, 300, 2048],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:51.760682 test begin: paddle.reciprocal(Tensor([4, 1, 1677722, 640],"float16"), )

[torch error] paddle.reciprocal(Tensor([4, 1, 1677722, 640],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:53.128822 test begin: paddle.reciprocal(Tensor([4, 1, 594194, 960],"float32"), )

[torch error] paddle.reciprocal(Tensor([4, 1, 594194, 960],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:53.373912 test begin: paddle.reciprocal(Tensor([4, 1, 640, 1677722],"float16"), )

[torch error] paddle.reciprocal(Tensor([4, 1, 640, 1677722],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:55.257426 test begin: paddle.reciprocal(Tensor([4, 1, 640, 891290],"float32"), )

[torch error] paddle.reciprocal(Tensor([4, 1, 640, 891290],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:55.503787 test begin: paddle.reciprocal(Tensor([4, 1, 891290, 640],"float32"), )

[torch error] paddle.reciprocal(Tensor([4, 1, 891290, 640],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:55.516403 test begin: paddle.reciprocal(Tensor([4, 1, 960, 594194],"float32"), )

[torch error] paddle.reciprocal(Tensor([4, 1, 960, 594194],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:55.527712 test begin: paddle.reciprocal(Tensor([4, 1393, 640, 640],"float32"), )

[torch error] paddle.reciprocal(Tensor([4, 1393, 640, 640],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:55.538672 test begin: paddle.reciprocal(Tensor([4, 2622, 640, 640],"float16"), )

[torch error] paddle.reciprocal(Tensor([4, 2622, 640, 640],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:57.174910 test begin: paddle.reciprocal(Tensor([4, 619, 960, 960],"float32"), )

[torch error] paddle.reciprocal(Tensor([4, 619, 960, 960],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:57.419241 test begin: paddle.reciprocal(Tensor([5571, 1, 640, 640],"float32"), )

[torch error] paddle.reciprocal(Tensor([5571, 1, 640, 640],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:57.431660 test begin: paddle.reciprocal(x=Tensor([253522376, 3, 3],"float32"), )

[torch error] paddle.reciprocal(x=Tensor([253522376, 3, 3],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:57.443067 test begin: paddle.reciprocal(x=Tensor([3, 253522376, 3],"float32"), )

[torch error] paddle.reciprocal(x=Tensor([3, 253522376, 3],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:57.454191 test begin: paddle.reciprocal(x=Tensor([3, 3, 253522376],"float32"), )

[torch error] paddle.reciprocal(x=Tensor([3, 3, 253522376],"float32"), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:57.465258 test begin: paddle.reciprocal(x=Tensor([3, 3, 477218589],"float16"), )

[torch error] paddle.reciprocal(x=Tensor([3, 3, 477218589],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:37:58.844319 test begin: paddle.reciprocal(x=Tensor([3, 477218589, 3],"float16"), )

[torch error] paddle.reciprocal(x=Tensor([3, 477218589, 3],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:00.475487 test begin: paddle.reciprocal(x=Tensor([477218589, 3, 3],"float16"), )

[torch error] paddle.reciprocal(x=Tensor([477218589, 3, 3],"float16"), ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:01.795786 test begin: paddle.renorm(Tensor([10, 20, 11408507],"float32"), 1.0, -1, 2.05, )

[torch error] paddle.renorm(Tensor([10, 20, 11408507],"float32"), 1.0, -1, 2.05, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:02.041067 test begin: paddle.renorm(Tensor([10, 228170138, 1],"float32"), 1.0, -1, 2.05, )

[torch error] paddle.renorm(Tensor([10, 228170138, 1],"float32"), 1.0, -1, 2.05, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:02.053678 test begin: paddle.renorm(Tensor([114085069, 20, 1],"float32"), 1.0, -1, 2.05, )

[torch error] paddle.renorm(Tensor([114085069, 20, 1],"float32"), 1.0, -1, 2.05, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:02.065332 test begin: paddle.renorm(Tensor([2, 2, 570425345],"float32"), 1.0, -1, 2.05, )

[torch error] paddle.renorm(Tensor([2, 2, 570425345],"float32"), 1.0, -1, 2.05, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:02.076882 test begin: paddle.renorm(Tensor([2, 2, 570425345],"float32"), 1.0, 2, 2.05, )

[torch error] paddle.renorm(Tensor([2, 2, 570425345],"float32"), 1.0, 2, 2.05, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:02.089098 test begin: paddle.renorm(Tensor([2, 380283564, 3],"float32"), 1.0, -1, 2.05, )

[torch error] paddle.renorm(Tensor([2, 380283564, 3],"float32"), 1.0, -1, 2.05, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:02.101213 test begin: paddle.renorm(Tensor([2, 380283564, 3],"float32"), 1.0, 2, 2.05, )

[torch error] paddle.renorm(Tensor([2, 380283564, 3],"float32"), 1.0, 2, 2.05, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:02.112405 test begin: paddle.renorm(Tensor([380283564, 2, 3],"float32"), 1.0, -1, 2.05, )

[torch error] paddle.renorm(Tensor([380283564, 2, 3],"float32"), 1.0, -1, 2.05, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:02.123320 test begin: paddle.renorm(Tensor([380283564, 2, 3],"float32"), 1.0, 2, 2.05, )

[torch error] paddle.renorm(Tensor([380283564, 2, 3],"float32"), 1.0, 2, 2.05, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:02.134151 test begin: paddle.renorm(x=Tensor([3, 2, 380283564],"float32"), p=1, axis=0, max_norm=5, )

[torch error] paddle.renorm(x=Tensor([3, 2, 380283564],"float32"), p=1, axis=0, max_norm=5, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:02.144983 test begin: paddle.renorm(x=Tensor([3, 2, 715827883],"float16"), p=1, axis=0, max_norm=5, )

[torch error] paddle.renorm(x=Tensor([3, 2, 715827883],"float16"), p=1, axis=0, max_norm=5, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:03.755208 test begin: paddle.renorm(x=Tensor([3, 2, 715827883],"float16"), p=1.2, axis=2, max_norm=6.5, )

[torch error] paddle.renorm(x=Tensor([3, 2, 715827883],"float16"), p=1.2, axis=2, max_norm=6.5, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:05.121348 test begin: paddle.renorm(x=Tensor([3, 2, 715827883],"float16"), p=1.5, axis=2, max_norm=20, )

[torch error] paddle.renorm(x=Tensor([3, 2, 715827883],"float16"), p=1.5, axis=2, max_norm=20, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:06.757218 test begin: paddle.renorm(x=Tensor([3, 2, 715827883],"float16"), p=2, axis=1, max_norm=20, )

[torch error] paddle.renorm(x=Tensor([3, 2, 715827883],"float16"), p=2, axis=1, max_norm=20, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:08.099771 test begin: paddle.renorm(x=Tensor([3, 2, 715827883],"float16"), p=2, axis=1, max_norm=40, )

[torch error] paddle.renorm(x=Tensor([3, 2, 715827883],"float16"), p=2, axis=1, max_norm=40, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:10.000651 test begin: paddle.renorm(x=Tensor([3, 2, 715827883],"float16"), p=2, axis=1, max_norm=50, )

[torch error] paddle.renorm(x=Tensor([3, 2, 715827883],"float16"), p=2, axis=1, max_norm=50, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:11.868871 test begin: paddle.renorm(x=Tensor([3, 253522376, 3],"float32"), p=1, axis=0, max_norm=5, )

[torch error] paddle.renorm(x=Tensor([3, 253522376, 3],"float32"), p=1, axis=0, max_norm=5, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:12.113892 test begin: paddle.renorm(x=Tensor([3, 477218589, 3],"float16"), p=1, axis=0, max_norm=5, )

[torch error] paddle.renorm(x=Tensor([3, 477218589, 3],"float16"), p=1, axis=0, max_norm=5, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:13.733948 test begin: paddle.renorm(x=Tensor([3, 477218589, 3],"float16"), p=1.2, axis=2, max_norm=6.5, )

[torch error] paddle.renorm(x=Tensor([3, 477218589, 3],"float16"), p=1.2, axis=2, max_norm=6.5, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:15.670605 test begin: paddle.renorm(x=Tensor([3, 477218589, 3],"float16"), p=1.5, axis=2, max_norm=20, )

[torch error] paddle.renorm(x=Tensor([3, 477218589, 3],"float16"), p=1.5, axis=2, max_norm=20, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:17.269086 test begin: paddle.renorm(x=Tensor([3, 477218589, 3],"float16"), p=2, axis=1, max_norm=20, )

[torch error] paddle.renorm(x=Tensor([3, 477218589, 3],"float16"), p=2, axis=1, max_norm=20, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:19.473674 test begin: paddle.renorm(x=Tensor([3, 477218589, 3],"float16"), p=2, axis=1, max_norm=40, )

[torch error] paddle.renorm(x=Tensor([3, 477218589, 3],"float16"), p=2, axis=1, max_norm=40, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:21.468431 test begin: paddle.renorm(x=Tensor([3, 477218589, 3],"float16"), p=2, axis=1, max_norm=50, )

[torch error] paddle.renorm(x=Tensor([3, 477218589, 3],"float16"), p=2, axis=1, max_norm=50, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:23.377352 test begin: paddle.renorm(x=Tensor([380283564, 2, 3],"float32"), p=1, axis=0, max_norm=5, )

[torch error] paddle.renorm(x=Tensor([380283564, 2, 3],"float32"), p=1, axis=0, max_norm=5, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:23.622812 test begin: paddle.renorm(x=Tensor([715827883, 2, 3],"float16"), p=1, axis=0, max_norm=5, )

[torch error] paddle.renorm(x=Tensor([715827883, 2, 3],"float16"), p=1, axis=0, max_norm=5, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:25.501744 test begin: paddle.renorm(x=Tensor([715827883, 2, 3],"float16"), p=1.2, axis=2, max_norm=6.5, )

[torch error] paddle.renorm(x=Tensor([715827883, 2, 3],"float16"), p=1.2, axis=2, max_norm=6.5, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:27.412742 test begin: paddle.renorm(x=Tensor([715827883, 2, 3],"float16"), p=1.5, axis=2, max_norm=20, )

[torch error] paddle.renorm(x=Tensor([715827883, 2, 3],"float16"), p=1.5, axis=2, max_norm=20, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:29.313433 test begin: paddle.renorm(x=Tensor([715827883, 2, 3],"float16"), p=2, axis=1, max_norm=20, )

[torch error] paddle.renorm(x=Tensor([715827883, 2, 3],"float16"), p=2, axis=1, max_norm=20, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:30.903152 test begin: paddle.renorm(x=Tensor([715827883, 2, 3],"float16"), p=2, axis=1, max_norm=40, )

[torch error] paddle.renorm(x=Tensor([715827883, 2, 3],"float16"), p=2, axis=1, max_norm=40, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:32.234367 test begin: paddle.renorm(x=Tensor([715827883, 2, 3],"float16"), p=2, axis=1, max_norm=50, )

[torch error] paddle.renorm(x=Tensor([715827883, 2, 3],"float16"), p=2, axis=1, max_norm=50, ) 
 CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:33.841651 test begin: paddle.repeat_interleave(Tensor([1, 1500, 1521135],"float32"), 5, axis=0, )

[torch error] paddle.repeat_interleave(Tensor([1, 1500, 1521135],"float32"), 5, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.089650 test begin: paddle.repeat_interleave(Tensor([1, 1782580, 1280],"float32"), 5, axis=0, )

[torch error] paddle.repeat_interleave(Tensor([1, 1782580, 1280],"float32"), 5, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.105914 test begin: paddle.repeat_interleave(Tensor([1, 2228225, 1024],"float32"), 5, axis=0, )

[torch error] paddle.repeat_interleave(Tensor([1, 2228225, 1024],"float32"), 5, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.127267 test begin: paddle.repeat_interleave(Tensor([1, 2281701379],"float32"), 128, 0, )

[torch error] paddle.repeat_interleave(Tensor([1, 2281701379],"float32"), 128, 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.139254 test begin: paddle.repeat_interleave(Tensor([1, 2281701379],"float32"), 2, axis=1, )

[torch error] paddle.repeat_interleave(Tensor([1, 2281701379],"float32"), 2, axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.151301 test begin: paddle.repeat_interleave(Tensor([1, 2281701379],"int64"), 1, axis=0, )

[torch error] paddle.repeat_interleave(Tensor([1, 2281701379],"int64"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.163198 test begin: paddle.repeat_interleave(Tensor([1, 2281701379],"int64"), 2, axis=0, )

[torch error] paddle.repeat_interleave(Tensor([1, 2281701379],"int64"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.174057 test begin: paddle.repeat_interleave(Tensor([1, 2281701379],"int64"), 3, 1, )

[torch error] paddle.repeat_interleave(Tensor([1, 2281701379],"int64"), 3, 1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.185142 test begin: paddle.repeat_interleave(Tensor([1, 2281701379],"int64"), 5, axis=0, )

[torch error] paddle.repeat_interleave(Tensor([1, 2281701379],"int64"), 5, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.198966 test begin: paddle.repeat_interleave(Tensor([1, 2970966, 768],"float32"), 5, axis=0, )

[torch error] paddle.repeat_interleave(Tensor([1, 2970966, 768],"float32"), 5, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.214293 test begin: paddle.repeat_interleave(Tensor([1, 4456449, 512],"float32"), 5, axis=0, )

[torch error] paddle.repeat_interleave(Tensor([1, 4456449, 512],"float32"), 5, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.230672 test begin: paddle.repeat_interleave(Tensor([1, 5941931, 384],"float32"), 5, axis=0, )

[torch error] paddle.repeat_interleave(Tensor([1, 5941931, 384],"float32"), 5, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.244799 test begin: paddle.repeat_interleave(Tensor([10, 228170138],"float32"), 2, axis=1, )

[torch error] paddle.repeat_interleave(Tensor([10, 228170138],"float32"), 2, axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.257371 test begin: paddle.repeat_interleave(Tensor([1140850690, 2],"float32"), 2, axis=1, )

[torch error] paddle.repeat_interleave(Tensor([1140850690, 2],"float32"), 2, axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.277022 test begin: paddle.repeat_interleave(Tensor([1140850690, 2],"int64"), 1, axis=0, )

[torch error] paddle.repeat_interleave(Tensor([1140850690, 2],"int64"), 1, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.289528 test begin: paddle.repeat_interleave(Tensor([1140850690, 2],"int64"), 2, axis=0, )

[torch error] paddle.repeat_interleave(Tensor([1140850690, 2],"int64"), 2, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.302825 test begin: paddle.repeat_interleave(Tensor([1189, 1500, 1280],"float32"), 5, axis=0, )

[torch error] paddle.repeat_interleave(Tensor([1189, 1500, 1280],"float32"), 5, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.314539 test begin: paddle.repeat_interleave(Tensor([13, 175515491, 1],"float32"), 1, 2, )

[torch error] paddle.repeat_interleave(Tensor([13, 175515491, 1],"float32"), 1, 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.325836 test begin: paddle.repeat_interleave(Tensor([13, 384, 457072],"float32"), 1, 2, )

[torch error] paddle.repeat_interleave(Tensor([13, 384, 457072],"float32"), 1, 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.337488 test begin: paddle.repeat_interleave(Tensor([14, 1, 384, 424424],"float32"), repeats=3, axis=1, )

[torch error] paddle.repeat_interleave(Tensor([14, 1, 384, 424424],"float32"), repeats=3, axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.350578 test begin: paddle.repeat_interleave(Tensor([14, 1, 424424, 384],"float32"), repeats=3, axis=1, )

[torch error] paddle.repeat_interleave(Tensor([14, 1, 424424, 384],"float32"), repeats=3, axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.369687 test begin: paddle.repeat_interleave(Tensor([14, 1106, 384, 384],"float32"), repeats=3, axis=1, )

[torch error] paddle.repeat_interleave(Tensor([14, 1106, 384, 384],"float32"), repeats=3, axis=1, ) 
 CUDA out of memory. Tried to allocate 8.51 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.384665 test begin: paddle.repeat_interleave(Tensor([1431655766, 3],"bfloat16"), 2, None, )

[torch error] paddle.repeat_interleave(Tensor([1431655766, 3],"bfloat16"), 2, None, ) 
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.398352 test begin: paddle.repeat_interleave(Tensor([1486, 1500, 1024],"float32"), 5, axis=0, )

[torch error] paddle.repeat_interleave(Tensor([1486, 1500, 1024],"float32"), 5, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.420166 test begin: paddle.repeat_interleave(Tensor([15474, 1, 384, 384],"float32"), repeats=3, axis=1, )

[torch error] paddle.repeat_interleave(Tensor([15474, 1, 384, 384],"float32"), repeats=3, axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.431784 test begin: paddle.repeat_interleave(Tensor([16, 142606337, 1],"float32"), 1, 2, )

[torch error] paddle.repeat_interleave(Tensor([16, 142606337, 1],"float32"), 1, 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.447424 test begin: paddle.repeat_interleave(Tensor([16, 384, 371371],"float32"), 1, 2, )

[torch error] paddle.repeat_interleave(Tensor([16, 384, 371371],"float32"), 1, 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.459351 test begin: paddle.repeat_interleave(Tensor([17685, 1, 192, 672],"float32"), repeats=3, axis=1, )

[torch error] paddle.repeat_interleave(Tensor([17685, 1, 192, 672],"float32"), repeats=3, axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.471147 test begin: paddle.repeat_interleave(Tensor([17825793, 128],"float32"), 128, 0, )

[torch error] paddle.repeat_interleave(Tensor([17825793, 128],"float32"), 128, 0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.484185 test begin: paddle.repeat_interleave(Tensor([1981, 1500, 768],"float32"), 5, axis=0, )

[torch error] paddle.repeat_interleave(Tensor([1981, 1500, 768],"float32"), 5, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.500930 test begin: paddle.repeat_interleave(Tensor([2, 1140850690],"float32"), 2, axis=1, )

[torch error] paddle.repeat_interleave(Tensor([2, 1140850690],"float32"), 2, axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.518120 test begin: paddle.repeat_interleave(Tensor([2, 1140850690],"int32"), 2, None, )

[torch error] paddle.repeat_interleave(Tensor([2, 1140850690],"int32"), 2, None, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.533251 test begin: paddle.repeat_interleave(Tensor([2, 2147483649],"bfloat16"), 2, None, )

[torch error] paddle.repeat_interleave(Tensor([2, 2147483649],"bfloat16"), 2, None, ) 
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.544846 test begin: paddle.repeat_interleave(Tensor([2971, 1500, 512],"float32"), 5, axis=0, )

[torch error] paddle.repeat_interleave(Tensor([2971, 1500, 512],"float32"), 5, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.564373 test begin: paddle.repeat_interleave(Tensor([3, 384, 1980644],"float32"), 1, 2, )

[torch error] paddle.repeat_interleave(Tensor([3, 384, 1980644],"float32"), 1, 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.576299 test begin: paddle.repeat_interleave(Tensor([3, 760567127, 1],"float32"), 1, 2, )

[torch error] paddle.repeat_interleave(Tensor([3, 760567127, 1],"float32"), 1, 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.588272 test begin: paddle.repeat_interleave(Tensor([3, 760567127],"float32"), 2, axis=1, )

[torch error] paddle.repeat_interleave(Tensor([3, 760567127],"float32"), 2, axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.598974 test begin: paddle.repeat_interleave(Tensor([32595734, 70],"int64"), 3, 1, )

[torch error] paddle.repeat_interleave(Tensor([32595734, 70],"int64"), 3, 1, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.609788 test begin: paddle.repeat_interleave(Tensor([3869, 1, 768, 768],"float32"), repeats=3, axis=1, )

[torch error] paddle.repeat_interleave(Tensor([3869, 1, 768, 768],"float32"), repeats=3, axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.622730 test begin: paddle.repeat_interleave(Tensor([3962, 1500, 384],"float32"), 5, axis=0, )

[torch error] paddle.repeat_interleave(Tensor([3962, 1500, 384],"float32"), 5, axis=0, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.636879 test begin: paddle.repeat_interleave(Tensor([4, 570425345],"float32"), 2, axis=1, )

[torch error] paddle.repeat_interleave(Tensor([4, 570425345],"float32"), 2, axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.654038 test begin: paddle.repeat_interleave(Tensor([5, 1, 594194, 768],"float32"), repeats=3, axis=1, )

[torch error] paddle.repeat_interleave(Tensor([5, 1, 594194, 768],"float32"), repeats=3, axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.670071 test begin: paddle.repeat_interleave(Tensor([5, 1, 768, 594194],"float32"), repeats=3, axis=1, )

[torch error] paddle.repeat_interleave(Tensor([5, 1, 768, 594194],"float32"), repeats=3, axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.695071 test begin: paddle.repeat_interleave(Tensor([5, 774, 768, 768],"float32"), repeats=3, axis=1, )

[torch error] paddle.repeat_interleave(Tensor([5, 774, 768, 768],"float32"), repeats=3, axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.721090 test begin: paddle.repeat_interleave(Tensor([5941931, 384, 1],"float32"), 1, 2, )

[torch error] paddle.repeat_interleave(Tensor([5941931, 384, 1],"float32"), 1, 2, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.735206 test begin: paddle.repeat_interleave(Tensor([7, 1, 192, 1697695],"float32"), repeats=3, axis=1, )

[torch error] paddle.repeat_interleave(Tensor([7, 1, 192, 1697695],"float32"), repeats=3, axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.746269 test begin: paddle.repeat_interleave(Tensor([7, 1, 485056, 672],"float32"), repeats=3, axis=1, )

[torch error] paddle.repeat_interleave(Tensor([7, 1, 485056, 672],"float32"), repeats=3, axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.757431 test begin: paddle.repeat_interleave(Tensor([7, 2527, 192, 672],"float32"), repeats=3, axis=1, )

[torch error] paddle.repeat_interleave(Tensor([7, 2527, 192, 672],"float32"), repeats=3, axis=1, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.768380 test begin: paddle.repeat_interleave(Tensor([760567127, 3],"int32"), 2, None, )

[torch error] paddle.repeat_interleave(Tensor([760567127, 3],"int32"), 2, None, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.782505 test begin: paddle.repeat_interleave(Tensor([760567127, 3],"int64"), 5, axis=0, )

[torch error] paddle.repeat_interleave(Tensor([760567127, 3],"int64"), 5, axis=0, ) 
 CUDA out of memory. Tried to allocate 17.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:34.801292 test begin: paddle.repeat_interleave(x=Tensor([107374183, 2, 4, 5],"float16"), repeats=2, )

[torch error] paddle.repeat_interleave(x=Tensor([107374183, 2, 4, 5],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:36.188252 test begin: paddle.repeat_interleave(x=Tensor([14260634, 2, 4, 4, 5],"int32"), repeats=2, axis=3, )

[torch error] paddle.repeat_interleave(x=Tensor([14260634, 2, 4, 4, 5],"int32"), repeats=2, axis=3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:36.434410 test begin: paddle.repeat_interleave(x=Tensor([2147483649, 2],"float16"), repeats=2, )

[torch error] paddle.repeat_interleave(x=Tensor([2147483649, 2],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:38.058154 test begin: paddle.repeat_interleave(x=Tensor([2281701379],"float32"), repeats=3, )

[torch error] paddle.repeat_interleave(x=Tensor([2281701379],"float32"), repeats=3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:38.304006 test begin: paddle.repeat_interleave(x=Tensor([26843546, 2, 4, 4, 5],"float16"), repeats=2, )

[torch error] paddle.repeat_interleave(x=Tensor([26843546, 2, 4, 4, 5],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:39.268726 test begin: paddle.repeat_interleave(x=Tensor([26843546, 2, 4, 4, 5],"float16"), repeats=2, axis=1, )

[torch error] paddle.repeat_interleave(x=Tensor([26843546, 2, 4, 4, 5],"float16"), repeats=2, axis=1, ) 
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:40.227783 test begin: paddle.repeat_interleave(x=Tensor([4, 1073741825],"float16"), repeats=2, )

[torch error] paddle.repeat_interleave(x=Tensor([4, 1073741825],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:41.195874 test begin: paddle.repeat_interleave(x=Tensor([4, 13421773, 4, 4, 5],"float16"), repeats=2, )

[torch error] paddle.repeat_interleave(x=Tensor([4, 13421773, 4, 4, 5],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:42.155304 test begin: paddle.repeat_interleave(x=Tensor([4, 13421773, 4, 4, 5],"float16"), repeats=2, axis=1, )

[torch error] paddle.repeat_interleave(x=Tensor([4, 13421773, 4, 4, 5],"float16"), repeats=2, axis=1, ) 
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:43.165036 test begin: paddle.repeat_interleave(x=Tensor([4, 2, 107374183, 5],"float16"), repeats=2, )

[torch error] paddle.repeat_interleave(x=Tensor([4, 2, 107374183, 5],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:44.207456 test begin: paddle.repeat_interleave(x=Tensor([4, 2, 14260634, 4, 5],"int32"), repeats=2, axis=3, )

[torch error] paddle.repeat_interleave(x=Tensor([4, 2, 14260634, 4, 5],"int32"), repeats=2, axis=3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:44.454036 test begin: paddle.repeat_interleave(x=Tensor([4, 2, 26843546, 4, 5],"float16"), repeats=2, )

[torch error] paddle.repeat_interleave(x=Tensor([4, 2, 26843546, 4, 5],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:45.720519 test begin: paddle.repeat_interleave(x=Tensor([4, 2, 26843546, 4, 5],"float16"), repeats=2, axis=1, )

[torch error] paddle.repeat_interleave(x=Tensor([4, 2, 26843546, 4, 5],"float16"), repeats=2, axis=1, ) 
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:47.495255 test begin: paddle.repeat_interleave(x=Tensor([4, 2, 4, 134217729],"float16"), repeats=2, )

[torch error] paddle.repeat_interleave(x=Tensor([4, 2, 4, 134217729],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:49.226558 test begin: paddle.repeat_interleave(x=Tensor([4, 2, 4, 14260634, 5],"int32"), repeats=2, axis=3, )

[torch error] paddle.repeat_interleave(x=Tensor([4, 2, 4, 14260634, 5],"int32"), repeats=2, axis=3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:49.471620 test begin: paddle.repeat_interleave(x=Tensor([4, 2, 4, 26843546, 5],"float16"), repeats=2, )

[torch error] paddle.repeat_interleave(x=Tensor([4, 2, 4, 26843546, 5],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:51.059339 test begin: paddle.repeat_interleave(x=Tensor([4, 2, 4, 26843546, 5],"float16"), repeats=2, axis=1, )

[torch error] paddle.repeat_interleave(x=Tensor([4, 2, 4, 26843546, 5],"float16"), repeats=2, axis=1, ) 
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:52.252576 test begin: paddle.repeat_interleave(x=Tensor([4, 2, 4, 4, 17825793],"int32"), repeats=2, axis=3, )

[torch error] paddle.repeat_interleave(x=Tensor([4, 2, 4, 4, 17825793],"int32"), repeats=2, axis=3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:52.499198 test begin: paddle.repeat_interleave(x=Tensor([4, 2, 4, 4, 33554433],"float16"), repeats=2, )

[torch error] paddle.repeat_interleave(x=Tensor([4, 2, 4, 4, 33554433],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:53.974418 test begin: paddle.repeat_interleave(x=Tensor([4, 2, 4, 4, 33554433],"float16"), repeats=2, axis=1, )

[torch error] paddle.repeat_interleave(x=Tensor([4, 2, 4, 4, 33554433],"float16"), repeats=2, axis=1, ) 
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:55.380368 test begin: paddle.repeat_interleave(x=Tensor([4, 2, 536870913],"float16"), repeats=2, )

[torch error] paddle.repeat_interleave(x=Tensor([4, 2, 536870913],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:56.586605 test begin: paddle.repeat_interleave(x=Tensor([4, 268435457, 4],"float16"), repeats=2, )

[torch error] paddle.repeat_interleave(x=Tensor([4, 268435457, 4],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:58.232810 test begin: paddle.repeat_interleave(x=Tensor([4, 53687092, 4, 5],"float16"), repeats=2, )

[torch error] paddle.repeat_interleave(x=Tensor([4, 53687092, 4, 5],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:59.724055 test begin: paddle.repeat_interleave(x=Tensor([4, 7130317, 4, 4, 5],"int32"), repeats=2, axis=3, )

[torch error] paddle.repeat_interleave(x=Tensor([4, 7130317, 4, 4, 5],"int32"), repeats=2, axis=3, ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:38:59.972016 test begin: paddle.repeat_interleave(x=Tensor([4294967297],"float16"), repeats=3, )

[torch error] paddle.repeat_interleave(x=Tensor([4294967297],"float16"), repeats=3, ) 
 CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:01.266613 test begin: paddle.repeat_interleave(x=Tensor([536870913, 2, 4],"float16"), repeats=2, )

[torch error] paddle.repeat_interleave(x=Tensor([536870913, 2, 4],"float16"), repeats=2, ) 
 CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 468.38 MiB is free. Process 121515 has 77.60 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 12.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:02.794537 test begin: paddle.reshape(Tensor([1, 1, 1, 2147483649, 2],"float16"), list[1,1,1,64,], )

[torch error] paddle.reshape(Tensor([1, 1, 1, 2147483649, 2],"float16"), list[1,1,1,64,], ) 
 shape '[1, 1, 1, 64]' is invalid for input of size 4294967298
2025-03-18 21:39:04.855786 test begin: paddle.reshape(Tensor([1, 1, 1, 2281701379],"float32"), list[-1,3,], )

[torch error] paddle.reshape(Tensor([1, 1, 1, 2281701379],"float32"), list[-1,3,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:05.120241 test begin: paddle.reshape(Tensor([1, 1, 1, 2281701379],"float32"), tuple(1,-1,), )

[torch error] paddle.reshape(Tensor([1, 1, 1, 2281701379],"float32"), tuple(1,-1,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:05.132721 test begin: paddle.reshape(Tensor([1, 1, 1, 32, 134217729],"float16"), list[1,1,1,64,], )

[torch error] paddle.reshape(Tensor([1, 1, 1, 32, 134217729],"float16"), list[1,1,1,64,], ) 
 shape '[1, 1, 1, 64]' is invalid for input of size 4294967328
2025-03-18 21:39:06.671036 test begin: paddle.reshape(Tensor([1, 1, 207427399, 11],"float32"), tuple(1,-1,), )

[torch error] paddle.reshape(Tensor([1, 1, 207427399, 11],"float32"), tuple(1,-1,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:06.916138 test begin: paddle.reshape(Tensor([1, 1, 2281701379],"float32"), list[1,1,4,-1,], )

[torch error] paddle.reshape(Tensor([1, 1, 2281701379],"float32"), list[1,1,4,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:06.929275 test begin: paddle.reshape(Tensor([1, 1, 253522376, 9],"float32"), tuple(1,-1,), )

[torch error] paddle.reshape(Tensor([1, 1, 253522376, 9],"float32"), tuple(1,-1,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:06.941301 test begin: paddle.reshape(Tensor([1, 1, 3, 3, 253522376],"float32"), list[1,3,3,1,1,], )

[torch error] paddle.reshape(Tensor([1, 1, 3, 3, 253522376],"float32"), list[1,3,3,1,1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:06.953120 test begin: paddle.reshape(Tensor([1, 1, 3, 760567127, 1],"float32"), list[1,3,3,1,1,], )

[torch error] paddle.reshape(Tensor([1, 1, 3, 760567127, 1],"float32"), list[1,3,3,1,1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:06.964803 test begin: paddle.reshape(Tensor([1, 1, 325957340, 7],"float32"), tuple(1,-1,), )

[torch error] paddle.reshape(Tensor([1, 1, 325957340, 7],"float32"), tuple(1,-1,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:06.976270 test begin: paddle.reshape(Tensor([1, 1, 64, 32, 2097153],"float16"), list[1,1,64,64,], )

[torch error] paddle.reshape(Tensor([1, 1, 64, 32, 2097153],"float16"), list[1,1,64,64,], ) 
 shape '[1, 1, 64, 64]' is invalid for input of size 4294969344
2025-03-18 21:39:07.953707 test begin: paddle.reshape(Tensor([1, 1, 64, 33554433, 2],"float16"), list[1,1,64,64,], )

[torch error] paddle.reshape(Tensor([1, 1, 64, 33554433, 2],"float16"), list[1,1,64,64,], ) 
 shape '[1, 1, 64, 64]' is invalid for input of size 4294967424
2025-03-18 21:39:09.230598 test begin: paddle.reshape(Tensor([1, 1, 67108865, 32, 2],"float16"), list[1,1,1,64,], )

[torch error] paddle.reshape(Tensor([1, 1, 67108865, 32, 2],"float16"), list[1,1,1,64,], ) 
 shape '[1, 1, 1, 64]' is invalid for input of size 4294967360
2025-03-18 21:39:10.742137 test begin: paddle.reshape(Tensor([1, 1, 67108865, 32, 2],"float16"), list[1,1,64,64,], )

[torch error] paddle.reshape(Tensor([1, 1, 67108865, 32, 2],"float16"), list[1,1,64,64,], ) 
 shape '[1, 1, 64, 64]' is invalid for input of size 4294967360
2025-03-18 21:39:11.945228 test begin: paddle.reshape(Tensor([1, 1, 760567127, 3, 1],"float32"), list[1,3,3,1,1,], )

[torch error] paddle.reshape(Tensor([1, 1, 760567127, 3, 1],"float32"), list[1,3,3,1,1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:12.189979 test begin: paddle.reshape(Tensor([1, 1, 760567127, 3],"float32"), list[-1,3,], )

[torch error] paddle.reshape(Tensor([1, 1, 760567127, 3],"float32"), list[-1,3,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:12.202533 test begin: paddle.reshape(Tensor([1, 10, 1, 228170138],"float32"), list[-1,10,], )

[torch error] paddle.reshape(Tensor([1, 10, 1, 228170138],"float32"), list[-1,10,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:12.213788 test begin: paddle.reshape(Tensor([1, 10, 14, 16297867],"float32"), shape=tuple(1,-1,1,), )

[torch error] paddle.reshape(Tensor([1, 10, 14, 16297867],"float32"), shape=tuple(1,-1,1,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:12.224677 test begin: paddle.reshape(Tensor([1, 10, 14, 16297867],"float32"), shape=tuple(1,-1,4,), )

[torch error] paddle.reshape(Tensor([1, 10, 14, 16297867],"float32"), shape=tuple(1,-1,4,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:12.235480 test begin: paddle.reshape(Tensor([1, 10, 15, 15211343],"float32"), shape=tuple(1,-1,1,), )

[torch error] paddle.reshape(Tensor([1, 10, 15, 15211343],"float32"), shape=tuple(1,-1,1,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:12.246195 test begin: paddle.reshape(Tensor([1, 10, 15, 15211343],"float32"), shape=tuple(1,-1,4,), )

[torch error] paddle.reshape(Tensor([1, 10, 15, 15211343],"float32"), shape=tuple(1,-1,4,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:12.258032 test begin: paddle.reshape(Tensor([1, 10, 16, 14260634],"float32"), shape=tuple(1,-1,1,), )

[torch error] paddle.reshape(Tensor([1, 10, 16, 14260634],"float32"), shape=tuple(1,-1,1,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:12.270008 test begin: paddle.reshape(Tensor([1, 10, 16, 14260634],"float32"), shape=tuple(1,-1,4,), )

[torch error] paddle.reshape(Tensor([1, 10, 16, 14260634],"float32"), shape=tuple(1,-1,4,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:12.281085 test begin: paddle.reshape(Tensor([1, 10, 19014179, 12],"float32"), shape=tuple(1,-1,4,), )

[torch error] paddle.reshape(Tensor([1, 10, 19014179, 12],"float32"), shape=tuple(1,-1,4,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:12.291873 test begin: paddle.reshape(Tensor([1, 10, 228170138, 1],"float32"), list[-1,10,], )

[torch error] paddle.reshape(Tensor([1, 10, 228170138, 1],"float32"), list[-1,10,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:12.302584 test begin: paddle.reshape(Tensor([1, 10, 25352238, 9],"float32"), shape=tuple(1,-1,1,), )

[torch error] paddle.reshape(Tensor([1, 10, 25352238, 9],"float32"), shape=tuple(1,-1,1,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:12.313215 test begin: paddle.reshape(Tensor([1, 10, 6338060, 36],"float32"), shape=tuple(1,-1,4,), )

[torch error] paddle.reshape(Tensor([1, 10, 6338060, 36],"float32"), shape=tuple(1,-1,4,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:12.323849 test begin: paddle.reshape(Tensor([1, 10, 76056713, 3],"float32"), shape=tuple(1,-1,1,), )

[torch error] paddle.reshape(Tensor([1, 10, 76056713, 3],"float32"), shape=tuple(1,-1,1,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:12.334431 test begin: paddle.reshape(Tensor([1, 10, 8, 28521268],"float32"), shape=tuple(1,-1,1,), )

[torch error] paddle.reshape(Tensor([1, 10, 8, 28521268],"float32"), shape=tuple(1,-1,1,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:12.344971 test begin: paddle.reshape(Tensor([1, 10, 8, 28521268],"float32"), shape=tuple(1,-1,4,), )

[torch error] paddle.reshape(Tensor([1, 10, 8, 28521268],"float32"), shape=tuple(1,-1,4,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:12.355645 test begin: paddle.reshape(Tensor([1, 100, 152, 150112],"float32"), shape=tuple(1,-1,1,), )

[torch error] paddle.reshape(Tensor([1, 100, 152, 150112],"float32"), shape=tuple(1,-1,1,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:12.366244 test begin: paddle.reshape(Tensor([1, 100, 152, 150112],"float32"), shape=tuple(1,-1,4,), )

[torch error] paddle.reshape(Tensor([1, 100, 152, 150112],"float32"), shape=tuple(1,-1,4,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:12.376906 test begin: paddle.reshape(Tensor([1, 100, 1901418, 12],"float32"), shape=tuple(1,-1,4,), )

[torch error] paddle.reshape(Tensor([1, 100, 1901418, 12],"float32"), shape=tuple(1,-1,4,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:12.387505 test begin: paddle.reshape(Tensor([1, 100, 22817014],"float32"), list[100,10,], )

[torch error] paddle.reshape(Tensor([1, 100, 22817014],"float32"), list[100,10,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:12.398199 test begin: paddle.reshape(Tensor([1, 100, 7605672, 3],"float32"), shape=tuple(1,-1,1,), )

[torch error] paddle.reshape(Tensor([1, 100, 7605672, 3],"float32"), shape=tuple(1,-1,1,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:12.408800 test begin: paddle.reshape(Tensor([1, 1024, 1, 2228225],"float32"), shape=list[-1,1024,], )

[torch error] paddle.reshape(Tensor([1, 1024, 1, 2228225],"float32"), shape=list[-1,1024,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:12.419427 test begin: paddle.reshape(Tensor([1, 1024, 2228225, 1],"float32"), shape=list[-1,1024,], )

[torch error] paddle.reshape(Tensor([1, 1024, 2228225, 1],"float32"), shape=list[-1,1024,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:12.430068 test begin: paddle.reshape(Tensor([1, 1024, 2228225],"float32"), list[1024,256,1,1,], )

[torch error] paddle.reshape(Tensor([1, 1024, 2228225],"float32"), list[1024,256,1,1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:12.441880 test begin: paddle.reshape(Tensor([1, 1024, 2228225],"float32"), list[1024,512,1,1,], )

[torch error] paddle.reshape(Tensor([1, 1024, 2228225],"float32"), list[1024,512,1,1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:12.453169 test begin: paddle.reshape(Tensor([1, 1048577, 64, 32, 2],"float16"), list[1,1,64,64,], )

[torch error] paddle.reshape(Tensor([1, 1048577, 64, 32, 2],"float16"), list[1,1,64,64,], ) 
 shape '[1, 1, 64, 64]' is invalid for input of size 4294971392
2025-03-18 21:39:14.149216 test begin: paddle.reshape(Tensor([1, 107374183, 5, 2, 4],"float16"), shape=list[15,8,], )

[torch error] paddle.reshape(Tensor([1, 107374183, 5, 2, 4],"float16"), shape=list[15,8,], ) 
 shape '[15, 8]' is invalid for input of size 4294967320
2025-03-18 21:39:15.686089 test begin: paddle.reshape(Tensor([1, 107374183, 5, 8],"float16"), shape=list[1,3,5,2,4,], )

[torch error] paddle.reshape(Tensor([1, 107374183, 5, 8],"float16"), shape=list[1,3,5,2,4,], ) 
 shape '[1, 3, 5, 2, 4]' is invalid for input of size 4294967320
2025-03-18 21:39:16.964166 test begin: paddle.reshape(Tensor([1, 1140850690, 2],"float32"), list[-1,], )

[torch error] paddle.reshape(Tensor([1, 1140850690, 2],"float32"), list[-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:17.209772 test begin: paddle.reshape(Tensor([1, 11408507, 2, 10, 10],"float32"), shape=list[1,24,10,10,], )

[torch error] paddle.reshape(Tensor([1, 11408507, 2, 10, 10],"float32"), shape=list[1,24,10,10,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:17.222007 test begin: paddle.reshape(Tensor([1, 11883862, 16, 12],"float32"), shape=tuple(1,-1,4,), )

[torch error] paddle.reshape(Tensor([1, 11883862, 16, 12],"float32"), shape=tuple(1,-1,4,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:17.233388 test begin: paddle.reshape(Tensor([1, 12, 12, 15845149],"float32"), shape=tuple(-1,2,), )

[torch error] paddle.reshape(Tensor([1, 12, 12, 15845149],"float32"), shape=tuple(-1,2,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:17.244728 test begin: paddle.reshape(Tensor([1, 12, 1901418, 10, 10],"float32"), shape=list[1,24,10,10,], )

[torch error] paddle.reshape(Tensor([1, 12, 1901418, 10, 10],"float32"), shape=list[1,24,10,10,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:17.255720 test begin: paddle.reshape(Tensor([1, 12, 2, 10, 9507090],"float32"), shape=list[1,24,10,10,], )

[torch error] paddle.reshape(Tensor([1, 12, 2, 10, 9507090],"float32"), shape=list[1,24,10,10,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:17.266538 test begin: paddle.reshape(Tensor([1, 12, 2, 9507090, 10],"float32"), shape=list[1,24,10,10,], )

[torch error] paddle.reshape(Tensor([1, 12, 2, 9507090, 10],"float32"), shape=list[1,24,10,10,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:17.277485 test begin: paddle.reshape(Tensor([1, 12, 95070891, 2],"float32"), shape=tuple(-1,2,), )

[torch error] paddle.reshape(Tensor([1, 12, 95070891, 2],"float32"), shape=tuple(-1,2,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:17.288169 test begin: paddle.reshape(Tensor([1, 1250933, 152, 12],"float32"), shape=tuple(1,-1,4,), )

[torch error] paddle.reshape(Tensor([1, 1250933, 152, 12],"float32"), shape=tuple(1,-1,4,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:17.298907 test begin: paddle.reshape(Tensor([1, 126761188, 2, 3, 3],"float32"), shape=list[1,96,3,3,], )

[torch error] paddle.reshape(Tensor([1, 126761188, 2, 3, 3],"float32"), shape=list[1,96,3,3,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:17.309660 test begin: paddle.reshape(Tensor([1, 12676119, 15, 12],"float32"), shape=tuple(1,-1,4,), )

[torch error] paddle.reshape(Tensor([1, 12676119, 15, 12],"float32"), shape=tuple(1,-1,4,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:17.320326 test begin: paddle.reshape(Tensor([1, 128, 17825793],"float32"), list[128,128,3,3,], )

[torch error] paddle.reshape(Tensor([1, 128, 17825793],"float32"), list[128,128,3,3,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:17.330952 test begin: paddle.reshape(Tensor([1, 128, 17825793],"float32"), list[128,256,1,1,], )

[torch error] paddle.reshape(Tensor([1, 128, 17825793],"float32"), list[128,256,1,1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:17.341566 test begin: paddle.reshape(Tensor([1, 128, 17825793],"float32"), list[128,512,1,1,], )

[torch error] paddle.reshape(Tensor([1, 128, 17825793],"float32"), list[128,512,1,1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:17.352209 test begin: paddle.reshape(Tensor([1, 1280, 1, 1782580],"float32"), shape=list[-1,1280,], )

[torch error] paddle.reshape(Tensor([1, 1280, 1, 1782580],"float32"), shape=list[-1,1280,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:17.362816 test begin: paddle.reshape(Tensor([1, 1280, 1782580, 1],"float32"), shape=list[-1,1280,], )

[torch error] paddle.reshape(Tensor([1, 1280, 1782580, 1],"float32"), shape=list[-1,1280,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:17.373374 test begin: paddle.reshape(Tensor([1, 13581556, 14, 12],"float32"), shape=tuple(1,-1,4,), )

[torch error] paddle.reshape(Tensor([1, 13581556, 14, 12],"float32"), shape=tuple(1,-1,4,), ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:17.384013 test begin: paddle.reshape(Tensor([1, 14, 162978670],"float32"), list[1,14,4,-1,], )

[torch error] paddle.reshape(Tensor([1, 14, 162978670],"float32"), list[1,14,4,-1,], ) 
 CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.46 GiB is free. Process 121515 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 10.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-18 21:39:17.394642 test begin: paddle.reshape(Tensor([1, 1431655766, 3],"float16"), list[-1,], )

2025-03-18 21:39:31.648618 test begin: paddle.reshape(Tensor([1, 144, 15845149],"float32"), shape=tuple(-1,256,), )

W0318 21:40:38.181864 126095 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0318 21:40:38.183291 126095 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[torch error] paddle.reshape(Tensor([1, 144, 15845149],"float32"), shape=tuple(-1,256,), ) 
 shape '[-1, 256]' is invalid for input of size 2281701456
2025-03-18 21:40:40.158477 test begin: paddle.reshape(Tensor([1, 15845149, 12, 12],"float32"), shape=tuple(1,256,-1,), )

[torch error] paddle.reshape(Tensor([1, 15845149, 12, 12],"float32"), shape=tuple(1,256,-1,), ) 
 shape '[1, 256, -1]' is invalid for input of size 2281701456
2025-03-18 21:40:42.387580 test begin: paddle.reshape(Tensor([1, 16, 16, 8912897],"float32"), shape=tuple(-1,2,), )

[Pass] paddle.reshape(Tensor([1, 16, 16, 8912897],"float32"), shape=tuple(-1,2,), )
2025-03-18 21:43:59.233692 test begin: paddle.reshape(Tensor([1, 16, 71303169, 2],"float32"), shape=tuple(-1,2,), )

[Pass] paddle.reshape(Tensor([1, 16, 71303169, 2],"float32"), shape=tuple(-1,2,), )
2025-03-18 21:46:58.273180 test begin: paddle.reshape(Tensor([1, 17825793, 128],"float32"), list[512,128,1,1,], )

[torch error] paddle.reshape(Tensor([1, 17825793, 128],"float32"), list[512,128,1,1,], ) 
 shape '[512, 128, 1, 1]' is invalid for input of size 2281701504
2025-03-18 21:47:01.399786 test begin: paddle.reshape(Tensor([1, 1980644, 1152],"float32"), list[128,128,3,3,], )

[torch error] paddle.reshape(Tensor([1, 1980644, 1152],"float32"), list[128,128,3,3,], ) 
 shape '[128, 128, 3, 3]' is invalid for input of size 2281701888
2025-03-18 21:47:03.540110 test begin: paddle.reshape(Tensor([1, 2, 1140850690],"float32"), list[-1,], )

[Pass] paddle.reshape(Tensor([1, 2, 1140850690],"float32"), list[-1,], )
2025-03-18 21:50:40.833188 test begin: paddle.reshape(Tensor([1, 2, 2147483649],"float16"), list[-1,], )

[Pass] paddle.reshape(Tensor([1, 2, 2147483649],"float16"), list[-1,], )
2025-03-18 22:08:15.166209 test begin: paddle.reshape(Tensor([1, 20, 114085069],"float32"), list[1,20,4,-1,], )

[torch error] paddle.reshape(Tensor([1, 20, 114085069],"float32"), list[1,20,4,-1,], ) 
 shape '[1, 20, 4, -1]' is invalid for input of size 2281701380
2025-03-18 22:08:19.793565 test begin: paddle.reshape(Tensor([1, 20, 14260634, 8],"float32"), list[1,20,-1,], )

[Pass] paddle.reshape(Tensor([1, 20, 14260634, 8],"float32"), list[1,20,-1,], )
2025-03-18 22:11:02.196817 test begin: paddle.reshape(Tensor([1, 20, 4, 28521268],"float32"), list[1,20,-1,], )

[Pass] paddle.reshape(Tensor([1, 20, 4, 28521268],"float32"), list[1,20,-1,], )
2025-03-18 22:14:20.339005 test begin: paddle.reshape(Tensor([1, 2048, 1, 1114113],"float32"), shape=list[-1,2048,], )

[Pass] paddle.reshape(Tensor([1, 2048, 1, 1114113],"float32"), shape=list[-1,2048,], )
2025-03-18 22:17:21.041621 test begin: paddle.reshape(Tensor([1, 2048, 1114113, 1],"float32"), shape=list[-1,2048,], )

[Pass] paddle.reshape(Tensor([1, 2048, 1114113, 1],"float32"), shape=list[-1,2048,], )
2025-03-18 22:20:22.908972 test begin: paddle.reshape(Tensor([1, 207427399, 1, 11],"float32"), tuple(1,-1,), )

[Pass] paddle.reshape(Tensor([1, 207427399, 1, 11],"float32"), tuple(1,-1,), )
2025-03-18 22:23:23.142725 test begin: paddle.reshape(Tensor([1, 2147483649, 2],"float16"), list[-1,], )

[Pass] paddle.reshape(Tensor([1, 2147483649, 2],"float16"), list[-1,], )
2025-03-18 22:39:23.939741 test begin: paddle.reshape(Tensor([1, 2228225, 1024],"float32"), list[256,1024,1,1,], )

[torch error] paddle.reshape(Tensor([1, 2228225, 1024],"float32"), list[256,1024,1,1,], ) 
 shape '[256, 1024, 1, 1]' is invalid for input of size 2281702400
2025-03-18 22:39:28.104511 test begin: paddle.reshape(Tensor([1, 2228225, 32, 32],"float32"), list[-1,3072,], )

[torch error] paddle.reshape(Tensor([1, 2228225, 32, 32],"float32"), list[-1,3072,], ) 
 shape '[-1, 3072]' is invalid for input of size 2281702400
2025-03-18 22:39:29.490833 test begin: paddle.reshape(Tensor([1, 2281701379, 1, 1],"float32"), list[-1,10,], )

[torch error] paddle.reshape(Tensor([1, 2281701379, 1, 1],"float32"), list[-1,10,], ) 
 shape '[-1, 10]' is invalid for input of size 2281701379
2025-03-18 22:39:31.836631 test begin: paddle.reshape(Tensor([1, 2281701379, 1, 1],"float32"), shape=list[-1,1024,], )

[torch error] paddle.reshape(Tensor([1, 2281701379, 1, 1],"float32"), shape=list[-1,1024,], ) 
 shape '[-1, 1024]' is invalid for input of size 2281701379
2025-03-18 22:39:33.846081 test begin: paddle.reshape(Tensor([1, 2281701379, 1, 1],"float32"), shape=list[-1,1280,], )

[torch error] paddle.reshape(Tensor([1, 2281701379, 1, 1],"float32"), shape=list[-1,1280,], ) 
 shape '[-1, 1280]' is invalid for input of size 2281701379
2025-03-18 22:39:35.231218 test begin: paddle.reshape(Tensor([1, 2281701379, 1, 1],"float32"), shape=list[-1,2048,], )

[torch error] paddle.reshape(Tensor([1, 2281701379, 1, 1],"float32"), shape=list[-1,2048,], ) 
 shape '[-1, 2048]' is invalid for input of size 2281701379
2025-03-18 22:39:37.181550 test begin: paddle.reshape(Tensor([1, 2281701379, 1, 1],"float32"), shape=list[-1,256,], )

[torch error] paddle.reshape(Tensor([1, 2281701379, 1, 1],"float32"), shape=list[-1,256,], ) 
 shape '[-1, 256]' is invalid for input of size 2281701379
2025-03-18 22:39:39.500851 test begin: paddle.reshape(Tensor([1, 2281701379, 1, 1],"float32"), shape=list[-1,512,], )

[torch error] paddle.reshape(Tensor([1, 2281701379, 1, 1],"float32"), shape=list[-1,512,], ) 
 shape '[-1, 512]' is invalid for input of size 2281701379
2025-03-18 22:39:41.516850 test begin: paddle.reshape(Tensor([1, 2281701379, 1],"float32"), tuple(1,-1,), )

[Pass] paddle.reshape(Tensor([1, 2281701379, 1],"float32"), tuple(1,-1,), )
2025-03-18 22:42:32.441482 test begin: paddle.reshape(Tensor([1, 2281701379],"float32"), list[-1,10,1,1,], )

[torch error] paddle.reshape(Tensor([1, 2281701379],"float32"), list[-1,10,1,1,], ) 
 shape '[-1, 10, 1, 1]' is invalid for input of size 2281701379
2025-03-18 22:42:36.812203 test begin: paddle.reshape(Tensor([1, 2281701379],"float32"), list[-1,], )

[Pass] paddle.reshape(Tensor([1, 2281701379],"float32"), list[-1,], )
2025-03-18 22:45:14.178890 test begin: paddle.reshape(Tensor([1, 2281701379],"float32"), list[1,-1,4,], )

[torch error] paddle.reshape(Tensor([1, 2281701379],"float32"), list[1,-1,4,], ) 
 shape '[1, -1, 4]' is invalid for input of size 2281701379
2025-03-18 22:45:18.372648 test begin: paddle.reshape(Tensor([1, 2281701379],"float32"), list[1,1,1,-1,1,3,], )

[torch error] paddle.reshape(Tensor([1, 2281701379],"float32"), list[1,1,1,-1,1,3,], ) 
 shape '[1, 1, 1, -1, 1, 3]' is invalid for input of size 2281701379
2025-03-18 22:45:20.567392 test begin: paddle.reshape(Tensor([1, 2281701379],"float32"), list[1,1,1,11,], )

[torch error] paddle.reshape(Tensor([1, 2281701379],"float32"), list[1,1,1,11,], ) 
 shape '[1, 1, 1, 11]' is invalid for input of size 2281701379
2025-03-18 22:45:21.929129 test begin: paddle.reshape(Tensor([1, 2281701379],"float32"), list[1,1,1,7,], )

[torch error] paddle.reshape(Tensor([1, 2281701379],"float32"), list[1,1,1,7,], ) 
 shape '[1, 1, 1, 7]' is invalid for input of size 2281701379
2025-03-18 22:45:24.267854 test begin: paddle.reshape(Tensor([1, 2281701379],"float32"), list[1,1,1,9,], )

[torch error] paddle.reshape(Tensor([1, 2281701379],"float32"), list[1,1,1,9,], ) 
 shape '[1, 1, 1, 9]' is invalid for input of size 2281701379
2025-03-18 22:45:26.329187 test begin: paddle.reshape(Tensor([1, 2281701379],"float32"), list[1,1024,1,1,], )

[torch error] paddle.reshape(Tensor([1, 2281701379],"float32"), list[1,1024,1,1,], ) 
 shape '[1, 1024, 1, 1]' is invalid for input of size 2281701379
2025-03-18 22:45:27.716201 test begin: paddle.reshape(Tensor([1, 2281701379],"float32"), list[1,2048,1,1,], )

[torch error] paddle.reshape(Tensor([1, 2281701379],"float32"), list[1,2048,1,1,], ) 
 shape '[1, 2048, 1, 1]' is invalid for input of size 2281701379
2025-03-18 22:45:29.744931 test begin: paddle.reshape(Tensor([1, 2281701379],"float32"), list[1,256,1,1,], )

[torch error] paddle.reshape(Tensor([1, 2281701379],"float32"), list[1,256,1,1,], ) 
 shape '[1, 256, 1, 1]' is invalid for input of size 2281701379
2025-03-18 22:45:31.798328 test begin: paddle.reshape(Tensor([1, 2281701379],"float32"), list[1,512,1,1,], )

[torch error] paddle.reshape(Tensor([1, 2281701379],"float32"), list[1,512,1,1,], ) 
 shape '[1, 512, 1, 1]' is invalid for input of size 2281701379
2025-03-18 22:45:33.846826 test begin: paddle.reshape(Tensor([1, 2281701379],"float32"), list[1,64,1,], )

[torch error] paddle.reshape(Tensor([1, 2281701379],"float32"), list[1,64,1,], ) 
 shape '[1, 64, 1]' is invalid for input of size 2281701379
2025-03-18 22:45:35.907965 test begin: paddle.reshape(Tensor([1, 2281701379],"float32"), list[1,64,3,], )

[torch error] paddle.reshape(Tensor([1, 2281701379],"float32"), list[1,64,3,], ) 
 shape '[1, 64, 3]' is invalid for input of size 2281701379
2025-03-18 22:45:38.297132 test begin: paddle.reshape(Tensor([1, 2281701379],"float32"), list[], )

[torch error] paddle.reshape(Tensor([1, 2281701379],"float32"), list[], ) 
 shape '[]' is invalid for input of size 2281701379
2025-03-18 22:45:40.430671 test begin: paddle.reshape(Tensor([1, 2281701379],"float32"), shape=list[-1,6,], )

[torch error] paddle.reshape(Tensor([1, 2281701379],"float32"), shape=list[-1,6,], ) 
 shape '[-1, 6]' is invalid for input of size 2281701379
2025-03-18 22:45:41.960145 test begin: paddle.reshape(Tensor([1, 2281701379],"int64"), shape=list[-1,], )

[Pass] paddle.reshape(Tensor([1, 2281701379],"int64"), shape=list[-1,], )
2025-03-18 22:49:32.117351 test begin: paddle.reshape(Tensor([1, 228170138, 10],"float32"), list[100,10,], )

[torch error] paddle.reshape(Tensor([1, 228170138, 10],"float32"), list[100,10,], ) 
 shape '[100, 10]' is invalid for input of size 2281701380
2025-03-18 22:49:36.655576 test begin: paddle.reshape(Tensor([1, 228170138, 10],"float32"), shape=list[-1,10,], )

[Pass] paddle.reshape(Tensor([1, 228170138, 10],"float32"), shape=list[-1,10,], )
2025-03-18 22:52:15.628679 test begin: paddle.reshape(Tensor([1, 228170138, 10],"float32"), shape=list[1,-1,10,], )

[Pass] paddle.reshape(Tensor([1, 228170138, 10],"float32"), shape=list[1,-1,10,], )
2025-03-18 22:55:08.403735 test begin: paddle.reshape(Tensor([1, 22817014, 10, 10],"float32"), shape=list[1,2,12,10,10,], )

[torch error] paddle.reshape(Tensor([1, 22817014, 10, 10],"float32"), shape=list[1,2,12,10,10,], ) 
 shape '[1, 2, 12, 10, 10]' is invalid for input of size 2281701400
2025-03-18 22:55:12.546100 test begin: paddle.reshape(Tensor([1, 23767723, 96],"float32"), list[1,20,4,-1,], )

[torch error] paddle.reshape(Tensor([1, 23767723, 96],"float32"), list[1,20,4,-1,], ) 
 shape '[1, 20, 4, -1]' is invalid for input of size 2281701408
2025-03-18 22:55:14.597012 test begin: paddle.reshape(Tensor([1, 24, 10, 9507090],"float32"), shape=list[1,2,12,10,10,], )

[torch error] paddle.reshape(Tensor([1, 24, 10, 9507090],"float32"), shape=list[1,2,12,10,10,], ) 
 shape '[1, 2, 12, 10, 10]' is invalid for input of size 2281701600
2025-03-18 22:55:16.597528 test begin: paddle.reshape(Tensor([1, 24, 2, 5, 9507090],"float32"), shape=list[1,48,5,5,], )

[torch error] paddle.reshape(Tensor([1, 24, 2, 5, 9507090],"float32"), shape=list[1,48,5,5,], ) 
 shape '[1, 48, 5, 5]' is invalid for input of size 2281701600
2025-03-18 22:55:18.637488 test begin: paddle.reshape(Tensor([1, 24, 2, 9507090, 5],"float32"), shape=list[1,48,5,5,], )

[torch error] paddle.reshape(Tensor([1, 24, 2, 9507090, 5],"float32"), shape=list[1,48,5,5,], ) 
 shape '[1, 48, 5, 5]' is invalid for input of size 2281701600
2025-03-18 22:55:20.655545 test begin: paddle.reshape(Tensor([1, 24, 24, 3961288],"float32"), shape=tuple(-1,2,), )

[Pass] paddle.reshape(Tensor([1, 24, 24, 3961288],"float32"), shape=tuple(-1,2,), )
2025-03-18 22:58:04.788877 test begin: paddle.reshape(Tensor([1, 24, 3802836, 5, 5],"float32"), shape=list[1,48,5,5,], )

[torch error] paddle.reshape(Tensor([1, 24, 3802836, 5, 5],"float32"), shape=list[1,48,5,5,], ) 
 shape '[1, 48, 5, 5]' is invalid for input of size 2281701600
2025-03-18 22:58:09.008450 test begin: paddle.reshape(Tensor([1, 24, 47535446, 2],"float32"), shape=tuple(-1,2,), )

[Pass] paddle.reshape(Tensor([1, 24, 47535446, 2],"float32"), shape=tuple(-1,2,), )
2025-03-18 23:01:16.568192 test begin: paddle.reshape(Tensor([1, 24, 9507090, 10],"float32"), shape=list[1,2,12,10,10,], )

[torch error] paddle.reshape(Tensor([1, 24, 9507090, 10],"float32"), shape=list[1,2,12,10,10,], ) 
 shape '[1, 2, 12, 10, 10]' is invalid for input of size 2281701600
2025-03-18 23:01:20.732512 test begin: paddle.reshape(Tensor([1, 253522376, 1, 9],"float32"), tuple(1,-1,), )

[Pass] paddle.reshape(Tensor([1, 253522376, 1, 9],"float32"), tuple(1,-1,), )
2025-03-18 23:04:16.400011 test begin: paddle.reshape(Tensor([1, 253522376, 3, 3, 1],"float32"), list[1,3,3,1,1,], )

[torch error] paddle.reshape(Tensor([1, 253522376, 3, 3, 1],"float32"), list[1,3,3,1,1,], ) 
 shape '[1, 3, 3, 1, 1]' is invalid for input of size 2281701384
2025-03-18 23:04:20.512586 test begin: paddle.reshape(Tensor([1, 253522376, 3, 3],"float32"), shape=list[1,2,48,3,3,], )

[torch error] paddle.reshape(Tensor([1, 253522376, 3, 3],"float32"), shape=list[1,2,48,3,3,], ) 
 shape '[1, 2, 48, 3, 3]' is invalid for input of size 2281701384
2025-03-18 23:04:22.538015 test begin: paddle.reshape(Tensor([1, 256, 1, 8912897],"float32"), shape=list[-1,256,], )

[Pass] paddle.reshape(Tensor([1, 256, 1, 8912897],"float32"), shape=list[-1,256,], )
2025-03-18 23:07:18.509958 test begin: paddle.reshape(Tensor([1, 256, 12, 742742],"float32"), shape=tuple(1,256,-1,), )

[Pass] paddle.reshape(Tensor([1, 256, 12, 742742],"float32"), shape=tuple(1,256,-1,), )
2025-03-18 23:09:59.346882 test begin: paddle.reshape(Tensor([1, 256, 16, 557057],"float32"), shape=tuple(1,256,-1,), )

[Pass] paddle.reshape(Tensor([1, 256, 16, 557057],"float32"), shape=tuple(1,256,-1,), )
2025-03-18 23:12:40.591712 test begin: paddle.reshape(Tensor([1, 256, 200, 44565],"float32"), shape=tuple(1,256,-1,), )

[Pass] paddle.reshape(Tensor([1, 256, 200, 44565],"float32"), shape=tuple(1,256,-1,), )
2025-03-18 23:15:34.864741 test begin: paddle.reshape(Tensor([1, 256, 28567, 312],"float32"), shape=tuple(1,256,-1,), )

[Pass] paddle.reshape(Tensor([1, 256, 28567, 312],"float32"), shape=tuple(1,256,-1,), )
2025-03-18 23:18:16.079986 test begin: paddle.reshape(Tensor([1, 256, 29319, 304],"float32"), shape=tuple(1,256,-1,), )

[Pass] paddle.reshape(Tensor([1, 256, 29319, 304],"float32"), shape=tuple(1,256,-1,), )
2025-03-18 23:21:17.596863 test begin: paddle.reshape(Tensor([1, 256, 32769, 272],"float32"), shape=tuple(1,256,-1,), )

[Pass] paddle.reshape(Tensor([1, 256, 32769, 272],"float32"), shape=tuple(1,256,-1,), )
2025-03-18 23:24:05.468893 test begin: paddle.reshape(Tensor([1, 256, 557057, 16],"float32"), shape=tuple(1,256,-1,), )

[Pass] paddle.reshape(Tensor([1, 256, 557057, 16],"float32"), shape=tuple(1,256,-1,), )
2025-03-18 23:26:50.890528 test begin: paddle.reshape(Tensor([1, 256, 742742, 12],"float32"), shape=tuple(1,256,-1,), )

[Pass] paddle.reshape(Tensor([1, 256, 742742, 12],"float32"), shape=tuple(1,256,-1,), )
2025-03-18 23:29:50.916206 test begin: paddle.reshape(Tensor([1, 256, 8912897, 1],"float32"), shape=list[-1,256,], )

[Pass] paddle.reshape(Tensor([1, 256, 8912897, 1],"float32"), shape=list[-1,256,], )
2025-03-18 23:33:16.738612 test begin: paddle.reshape(Tensor([1, 256, 8912897],"float32"), list[256,1024,1,1,], )

[torch error] paddle.reshape(Tensor([1, 256, 8912897],"float32"), list[256,1024,1,1,], ) 
 shape '[256, 1024, 1, 1]' is invalid for input of size 2281701632
2025-03-18 23:33:20.958730 test begin: paddle.reshape(Tensor([1, 256, 8912897],"float32"), list[256,256,3,3,], )

[torch error] paddle.reshape(Tensor([1, 256, 8912897],"float32"), list[256,256,3,3,], ) 
 shape '[256, 256, 3, 3]' is invalid for input of size 2281701632
2025-03-18 23:33:23.152900 test begin: paddle.reshape(Tensor([1, 256, 8912897],"float32"), list[256,512,1,1,], )

[torch error] paddle.reshape(Tensor([1, 256, 8912897],"float32"), list[256,512,1,1,], ) 
 shape '[256, 512, 1, 1]' is invalid for input of size 2281701632
2025-03-18 23:33:24.540143 test begin: paddle.reshape(Tensor([1, 256, 8912897],"float32"), list[256,64,1,1,], )

[torch error] paddle.reshape(Tensor([1, 256, 8912897],"float32"), list[256,64,1,1,], ) 
 shape '[256, 64, 1, 1]' is invalid for input of size 2281701632
2025-03-18 23:33:26.606419 test begin: paddle.reshape(Tensor([1, 256, 8912897],"float32"), shape=tuple(-1,256,), )

[Pass] paddle.reshape(Tensor([1, 256, 8912897],"float32"), shape=tuple(-1,256,), )
2025-03-18 23:36:10.399763 test begin: paddle.reshape(Tensor([1, 28521268, 40, 2],"float32"), shape=tuple(-1,2,), )

[Pass] paddle.reshape(Tensor([1, 28521268, 40, 2],"float32"), shape=tuple(-1,2,), )
2025-03-18 23:39:06.546702 test begin: paddle.reshape(Tensor([1, 3, 1, 16, 1, 32, 1485483],"float32"), list[1,6,1,8,1,4,1,8,1,], )

[torch error] paddle.reshape(Tensor([1, 3, 1, 16, 1, 32, 1485483],"float32"), list[1,6,1,8,1,4,1,8,1,], ) 
 shape '[1, 6, 1, 8, 1, 4, 1, 8, 1]' is invalid for input of size 2281701888
2025-03-18 23:39:11.121359 test begin: paddle.reshape(Tensor([1, 3, 1, 16, 1, 47535446, 1],"float32"), list[1,6,1,8,1,4,1,8,1,], )

[torch error] paddle.reshape(Tensor([1, 3, 1, 16, 1, 47535446, 1],"float32"), list[1,6,1,8,1,4,1,8,1,], ) 
 shape '[1, 6, 1, 8, 1, 4, 1, 8, 1]' is invalid for input of size 2281701408
2025-03-18 23:39:12.997846 test begin: paddle.reshape(Tensor([1, 3, 1, 16, 1485483, 32, 1],"float32"), list[1,6,1,8,1,4,1,8,1,], )

[torch error] paddle.reshape(Tensor([1, 3, 1, 16, 1485483, 32, 1],"float32"), list[1,6,1,8,1,4,1,8,1,], ) 
 shape '[1, 6, 1, 8, 1, 4, 1, 8, 1]' is invalid for input of size 2281701888
2025-03-18 23:39:15.318759 test begin: paddle.reshape(Tensor([1, 3, 1, 23767723, 1, 32, 1],"float32"), list[1,6,1,8,1,4,1,8,1,], )

[torch error] paddle.reshape(Tensor([1, 3, 1, 23767723, 1, 32, 1],"float32"), list[1,6,1,8,1,4,1,8,1,], ) 
 shape '[1, 6, 1, 8, 1, 4, 1, 8, 1]' is invalid for input of size 2281701408
2025-03-18 23:39:17.346796 test begin: paddle.reshape(Tensor([1, 3, 1431655766],"float16"), list[-1,], )

[Pass] paddle.reshape(Tensor([1, 3, 1431655766],"float16"), list[-1,], )
2025-03-18 23:56:29.313573 test begin: paddle.reshape(Tensor([1, 3, 1485483, 16, 1, 32, 1],"float32"), list[1,6,1,8,1,4,1,8,1,], )

[torch error] paddle.reshape(Tensor([1, 3, 1485483, 16, 1, 32, 1],"float32"), list[1,6,1,8,1,4,1,8,1,], ) 
 shape '[1, 6, 1, 8, 1, 4, 1, 8, 1]' is invalid for input of size 2281701888
2025-03-18 23:56:33.593013 test begin: paddle.reshape(Tensor([1, 3, 178956971, 2, 4],"float16"), shape=list[15,8,], )

[torch error] paddle.reshape(Tensor([1, 3, 178956971, 2, 4],"float16"), shape=list[15,8,], ) 
 shape '[15, 8]' is invalid for input of size 4294967304
2025-03-18 23:56:37.988918 test begin: paddle.reshape(Tensor([1, 3, 178956971, 8],"float16"), shape=list[1,3,5,2,4,], )

[torch error] paddle.reshape(Tensor([1, 3, 178956971, 8],"float16"), shape=list[1,3,5,2,4,], ) 
 shape '[1, 3, 5, 2, 4]' is invalid for input of size 4294967304
2025-03-18 23:56:39.638974 test begin: paddle.reshape(Tensor([1, 3, 23767723, 32],"float32"), list[-1,3072,], )

[torch error] paddle.reshape(Tensor([1, 3, 23767723, 32],"float32"), list[-1,3072,], ) 
 shape '[-1, 3072]' is invalid for input of size 2281701408
2025-03-18 23:56:41.068210 test begin: paddle.reshape(Tensor([1, 3, 256, 2970966],"float32"), list[-1,196608,], )

[torch error] paddle.reshape(Tensor([1, 3, 256, 2970966],"float32"), list[-1,196608,], ) 
 shape '[-1, 196608]' is invalid for input of size 2281701888
2025-03-18 23:56:43.117167 test begin: paddle.reshape(Tensor([1, 3, 2970966, 256],"float32"), list[-1,196608,], )

[torch error] paddle.reshape(Tensor([1, 3, 2970966, 256],"float32"), list[-1,196608,], ) 
 shape '[-1, 196608]' is invalid for input of size 2281701888
2025-03-18 23:56:45.504668 test begin: paddle.reshape(Tensor([1, 3, 3, 253522376],"float32"), list[3,3,1,1,], )

[torch error] paddle.reshape(Tensor([1, 3, 3, 253522376],"float32"), list[3,3,1,1,], ) 
 shape '[3, 3, 1, 1]' is invalid for input of size 2281701384
2025-03-18 23:56:47.685844 test begin: paddle.reshape(Tensor([1, 3, 32, 23767723],"float32"), list[-1,3072,], )

[torch error] paddle.reshape(Tensor([1, 3, 32, 23767723],"float32"), list[-1,3072,], ) 
 shape '[-1, 3072]' is invalid for input of size 2281701408
2025-03-18 23:56:49.866008 test begin: paddle.reshape(Tensor([1, 3, 5, 152113426],"float32"), shape=list[1,3,5,2,4,], )

[torch error] paddle.reshape(Tensor([1, 3, 5, 152113426],"float32"), shape=list[1,3,5,2,4,], ) 
 shape '[1, 3, 5, 2, 4]' is invalid for input of size 2281701390
2025-03-18 23:56:52.056036 test begin: paddle.reshape(Tensor([1, 3, 5, 2, 143165577],"float16"), shape=list[15,8,], )

[torch error] paddle.reshape(Tensor([1, 3, 5, 2, 143165577],"float16"), shape=list[15,8,], ) 
 shape '[15, 8]' is invalid for input of size 4294967310
2025-03-18 23:56:53.977975 test begin: paddle.reshape(Tensor([1, 3, 5, 2, 76056713],"float32"), shape=list[15,8,], )

[torch error] paddle.reshape(Tensor([1, 3, 5, 2, 76056713],"float32"), shape=list[15,8,], ) 
 shape '[15, 8]' is invalid for input of size 2281701390
2025-03-18 23:56:55.708100 test begin: paddle.reshape(Tensor([1, 3, 5, 286331154],"float16"), shape=list[1,3,5,2,4,], )

[torch error] paddle.reshape(Tensor([1, 3, 5, 286331154],"float16"), shape=list[1,3,5,2,4,], ) 
 shape '[1, 3, 5, 2, 4]' is invalid for input of size 4294967310
2025-03-18 23:56:57.951606 test begin: paddle.reshape(Tensor([1, 3, 5, 38028357, 4],"float32"), shape=list[15,8,], )

[torch error] paddle.reshape(Tensor([1, 3, 5, 38028357, 4],"float32"), shape=list[15,8,], ) 
 shape '[15, 8]' is invalid for input of size 2281701420
2025-03-18 23:57:00.570320 test begin: paddle.reshape(Tensor([1, 3, 5, 71582789, 4],"float16"), shape=list[15,8,], )

[torch error] paddle.reshape(Tensor([1, 3, 5, 71582789, 4],"float16"), shape=list[15,8,], ) 
 shape '[15, 8]' is invalid for input of size 4294967340
2025-03-18 23:57:02.479582 test begin: paddle.reshape(Tensor([1, 3, 760567127, 1],"float32"), list[3,3,1,1,], )

[torch error] paddle.reshape(Tensor([1, 3, 760567127, 1],"float32"), list[3,3,1,1,], ) 
 shape '[3, 3, 1, 1]' is invalid for input of size 2281701381
2025-03-18 23:57:04.729631 test begin: paddle.reshape(Tensor([1, 3, 760567127],"float32"), list[-1,], )

[Pass] paddle.reshape(Tensor([1, 3, 760567127],"float32"), list[-1,], )
2025-03-19 00:00:47.559532 test begin: paddle.reshape(Tensor([1, 3, 95070891, 2, 4],"float32"), shape=list[15,8,], )

[torch error] paddle.reshape(Tensor([1, 3, 95070891, 2, 4],"float32"), shape=list[15,8,], ) 
 shape '[15, 8]' is invalid for input of size 2281701384
2025-03-19 00:00:51.803884 test begin: paddle.reshape(Tensor([1, 3, 95070891, 8],"float32"), shape=list[1,3,5,2,4,], )

[torch error] paddle.reshape(Tensor([1, 3, 95070891, 8],"float32"), shape=list[1,3,5,2,4,], ) 
 shape '[1, 3, 5, 2, 4]' is invalid for input of size 2281701384
2025-03-19 00:00:53.205612 test begin: paddle.reshape(Tensor([1, 31690297, 36, 2],"float32"), shape=tuple(-1,2,), )

[Pass] paddle.reshape(Tensor([1, 31690297, 36, 2],"float32"), shape=tuple(-1,2,), )
2025-03-19 00:03:58.919611 test begin: paddle.reshape(Tensor([1, 31690297, 8, 9],"float32"), shape=tuple(1,-1,1,), )

[Pass] paddle.reshape(Tensor([1, 31690297, 8, 9],"float32"), shape=tuple(1,-1,1,), )
2025-03-19 00:06:45.610959 test begin: paddle.reshape(Tensor([1, 325957340, 1, 7],"float32"), tuple(1,-1,), )

[Pass] paddle.reshape(Tensor([1, 325957340, 1, 7],"float32"), tuple(1,-1,), )
2025-03-19 00:09:42.706859 test begin: paddle.reshape(Tensor([1, 34817, 256, 256],"float32"), list[-1,196608,], )

[torch error] paddle.reshape(Tensor([1, 34817, 256, 256],"float32"), list[-1,196608,], ) 
 shape '[-1, 196608]' is invalid for input of size 2281766912
2025-03-19 00:09:46.953357 test begin: paddle.reshape(Tensor([1, 35651585, 64],"float32"), list[256,64,1,1,], )

[torch error] paddle.reshape(Tensor([1, 35651585, 64],"float32"), list[256,64,1,1,], ) 
 shape '[256, 64, 1, 1]' is invalid for input of size 2281701440
2025-03-19 00:09:49.477711 test begin: paddle.reshape(Tensor([1, 35651585, 64],"float32"), list[64,64,1,1,], )

[torch error] paddle.reshape(Tensor([1, 35651585, 64],"float32"), list[64,64,1,1,], ) 
 shape '[64, 64, 1, 1]' is invalid for input of size 2281701440
2025-03-19 00:09:50.990129 test begin: paddle.reshape(Tensor([1, 36, 31690297, 2],"float32"), shape=tuple(-1,2,), )

[Pass] paddle.reshape(Tensor([1, 36, 31690297, 2],"float32"), shape=tuple(-1,2,), )
2025-03-19 00:13:07.272721 test begin: paddle.reshape(Tensor([1, 36, 36, 1760573],"float32"), shape=tuple(-1,2,), )

[Pass] paddle.reshape(Tensor([1, 36, 36, 1760573],"float32"), shape=tuple(-1,2,), )
2025-03-19 00:16:11.376047 test begin: paddle.reshape(Tensor([1, 36566, 200, 312],"float32"), shape=list[-1,200,312,], )

[Pass] paddle.reshape(Tensor([1, 36566, 200, 312],"float32"), shape=list[-1,200,312,], )
2025-03-19 00:19:34.526426 test begin: paddle.reshape(Tensor([1, 36566, 200, 312],"float32"), shape=tuple(1,256,-1,), )

[torch error] paddle.reshape(Tensor([1, 36566, 200, 312],"float32"), shape=tuple(1,256,-1,), ) 
 shape '[1, 256, -1]' is invalid for input of size 2281718400
2025-03-19 00:19:38.723561 test begin: paddle.reshape(Tensor([1, 36566, 62400],"float32"), shape=tuple(-1,200,312,), )

[Pass] paddle.reshape(Tensor([1, 36566, 62400],"float32"), shape=tuple(-1,200,312,), )
2025-03-19 00:23:32.985843 test begin: paddle.reshape(Tensor([1, 37528, 200, 304],"float32"), shape=list[-1,200,304,], )

[Pass] paddle.reshape(Tensor([1, 37528, 200, 304],"float32"), shape=list[-1,200,304,], )
2025-03-19 00:26:51.585148 test begin: paddle.reshape(Tensor([1, 37528, 200, 304],"float32"), shape=tuple(1,256,-1,), )

[Pass] paddle.reshape(Tensor([1, 37528, 200, 304],"float32"), shape=tuple(1,256,-1,), )
2025-03-19 00:30:04.223948 test begin: paddle.reshape(Tensor([1, 37528, 304, 200],"float32"), shape=list[-1,304,200,], )

[Pass] paddle.reshape(Tensor([1, 37528, 304, 200],"float32"), shape=list[-1,304,200,], )
2025-03-19 00:33:39.219753 test begin: paddle.reshape(Tensor([1, 37528, 60800],"float32"), shape=tuple(-1,200,304,), )

[Pass] paddle.reshape(Tensor([1, 37528, 60800],"float32"), shape=tuple(-1,200,304,), )
2025-03-19 00:36:50.214227 test begin: paddle.reshape(Tensor([1, 37528, 60800],"float32"), shape=tuple(-1,304,200,), )

[Pass] paddle.reshape(Tensor([1, 37528, 60800],"float32"), shape=tuple(-1,304,200,), )
2025-03-19 00:39:57.836547 test begin: paddle.reshape(Tensor([1, 3961288, 576],"float32"), list[64,64,3,3,], )

[torch error] paddle.reshape(Tensor([1, 3961288, 576],"float32"), list[64,64,3,3,], ) 
 shape '[64, 64, 3, 3]' is invalid for input of size 2281701888
2025-03-19 00:40:02.729406 test begin: paddle.reshape(Tensor([1, 4, 570425345],"float32"), shape=list[-1,10,], )

[Pass] paddle.reshape(Tensor([1, 4, 570425345],"float32"), shape=list[-1,10,], )
2025-03-19 00:43:09.191120 test begin: paddle.reshape(Tensor([1, 4, 570425345],"float32"), shape=list[1,-1,10,], )

[Pass] paddle.reshape(Tensor([1, 4, 570425345],"float32"), shape=list[1,-1,10,], )
2025-03-19 00:46:31.335217 test begin: paddle.reshape(Tensor([1, 40, 28521268, 2],"float32"), shape=tuple(-1,2,), )

[Pass] paddle.reshape(Tensor([1, 40, 28521268, 2],"float32"), shape=tuple(-1,2,), )
2025-03-19 00:49:35.328141 test begin: paddle.reshape(Tensor([1, 40, 40, 1426064],"float32"), shape=tuple(-1,2,), )

[Pass] paddle.reshape(Tensor([1, 40, 40, 1426064],"float32"), shape=tuple(-1,2,), )
2025-03-19 00:52:15.700865 test begin: paddle.reshape(Tensor([1, 40, 57042535],"float32"), list[1960,], )

[torch error] paddle.reshape(Tensor([1, 40, 57042535],"float32"), list[1960,], ) 
 shape '[1960]' is invalid for input of size 2281701400
2025-03-19 00:52:19.871044 test begin: paddle.reshape(Tensor([1, 41944, 200, 272],"float32"), shape=list[-1,200,272,], )

[Pass] paddle.reshape(Tensor([1, 41944, 200, 272],"float32"), shape=list[-1,200,272,], )
2025-03-19 00:55:18.775923 test begin: paddle.reshape(Tensor([1, 41944, 200, 272],"float32"), shape=tuple(1,256,-1,), )

[Pass] paddle.reshape(Tensor([1, 41944, 200, 272],"float32"), shape=tuple(1,256,-1,), )
2025-03-19 00:57:58.128667 test begin: paddle.reshape(Tensor([1, 41944, 54400],"float32"), shape=tuple(-1,200,272,), )

[Pass] paddle.reshape(Tensor([1, 41944, 54400],"float32"), shape=tuple(-1,200,272,), )
2025-03-19 01:00:53.651908 test begin: paddle.reshape(Tensor([1, 4294967297],"float16"), list[-1,], )

[Pass] paddle.reshape(Tensor([1, 4294967297],"float16"), list[-1,], )
2025-03-19 01:16:48.928796 test begin: paddle.reshape(Tensor([1, 4456449, 1, 16, 1, 32, 1],"float32"), list[1,6,1,8,1,4,1,8,1,], )

[torch error] paddle.reshape(Tensor([1, 4456449, 1, 16, 1, 32, 1],"float32"), list[1,6,1,8,1,4,1,8,1,], ) 
 shape '[1, 6, 1, 8, 1, 4, 1, 8, 1]' is invalid for input of size 2281701888
2025-03-19 01:16:53.062697 test begin: paddle.reshape(Tensor([1, 4456449, 512],"float32"), list[1024,512,1,1,], )

[torch error] paddle.reshape(Tensor([1, 4456449, 512],"float32"), list[1024,512,1,1,], ) 
 shape '[1024, 512, 1, 1]' is invalid for input of size 2281701888
2025-03-19 01:16:54.451398 test begin: paddle.reshape(Tensor([1, 4456449, 512],"float32"), list[128,512,1,1,], )

[torch error] paddle.reshape(Tensor([1, 4456449, 512],"float32"), list[128,512,1,1,], ) 
 shape '[128, 512, 1, 1]' is invalid for input of size 2281701888
2025-03-19 01:16:56.777836 test begin: paddle.reshape(Tensor([1, 4456449, 512],"float32"), list[256,512,1,1,], )

[torch error] paddle.reshape(Tensor([1, 4456449, 512],"float32"), list[256,512,1,1,], ) 
 shape '[256, 512, 1, 1]' is invalid for input of size 2281701888
2025-03-19 01:16:58.833711 test begin: paddle.reshape(Tensor([1, 45634028, 2, 5, 5],"float32"), shape=list[1,48,5,5,], )

[torch error] paddle.reshape(Tensor([1, 45634028, 2, 5, 5],"float32"), shape=list[1,48,5,5,], ) 
 shape '[1, 48, 5, 5]' is invalid for input of size 2281701400
2025-03-19 01:17:00.230928 test begin: paddle.reshape(Tensor([1, 46565335, 49],"float32"), list[1960,], )

[torch error] paddle.reshape(Tensor([1, 46565335, 49],"float32"), list[1960,], ) 
 shape '[1960]' is invalid for input of size 2281701415
2025-03-19 01:17:02.206406 test begin: paddle.reshape(Tensor([1, 46565335, 49],"float32"), list[64,1,7,7,], )

[torch error] paddle.reshape(Tensor([1, 46565335, 49],"float32"), list[64,1,7,7,], ) 
 shape '[64, 1, 7, 7]' is invalid for input of size 2281701415
2025-03-19 01:17:04.534312 test begin: paddle.reshape(Tensor([1, 47535446, 16, 3],"float32"), shape=tuple(1,-1,1,), )

[Pass] paddle.reshape(Tensor([1, 47535446, 16, 3],"float32"), shape=tuple(1,-1,1,), )
2025-03-19 01:19:41.767458 test begin: paddle.reshape(Tensor([1, 47535446, 24, 2],"float32"), shape=tuple(-1,2,), )

[Pass] paddle.reshape(Tensor([1, 47535446, 24, 2],"float32"), shape=tuple(-1,2,), )
2025-03-19 01:22:34.792371 test begin: paddle.reshape(Tensor([1, 48, 2, 3, 7922575],"float32"), shape=list[1,96,3,3,], )

[torch error] paddle.reshape(Tensor([1, 48, 2, 3, 7922575],"float32"), shape=list[1,96,3,3,], ) 
 shape '[1, 96, 3, 3]' is invalid for input of size 2281701600
2025-03-19 01:22:39.053724 test begin: paddle.reshape(Tensor([1, 48, 2, 7922575, 3],"float32"), shape=list[1,96,3,3,], )

[torch error] paddle.reshape(Tensor([1, 48, 2, 7922575, 3],"float32"), shape=list[1,96,3,3,], ) 
 shape '[1, 96, 3, 3]' is invalid for input of size 2281701600
2025-03-19 01:22:41.381653 test begin: paddle.reshape(Tensor([1, 48, 5, 9507090],"float32"), shape=list[1,2,24,5,5,], )

[torch error] paddle.reshape(Tensor([1, 48, 5, 9507090],"float32"), shape=list[1,2,24,5,5,], ) 
 shape '[1, 2, 24, 5, 5]' is invalid for input of size 2281701600
2025-03-19 01:22:42.775792 test begin: paddle.reshape(Tensor([1, 48, 5281717, 3, 3],"float32"), shape=list[1,96,3,3,], )

[torch error] paddle.reshape(Tensor([1, 48, 5281717, 3, 3],"float32"), shape=list[1,96,3,3,], ) 
 shape '[1, 96, 3, 3]' is invalid for input of size 2281701744
2025-03-19 01:22:45.017180 test begin: paddle.reshape(Tensor([1, 48, 9507090, 5],"float32"), shape=list[1,2,24,5,5,], )

[torch error] paddle.reshape(Tensor([1, 48, 9507090, 5],"float32"), shape=list[1,2,24,5,5,], ) 
 shape '[1, 2, 24, 5, 5]' is invalid for input of size 2281701600
2025-03-19 01:22:46.524348 test begin: paddle.reshape(Tensor([1, 50, 53, 861020],"float32"), shape=list[-1,140450,], )

[torch error] paddle.reshape(Tensor([1, 50, 53, 861020],"float32"), shape=list[-1,140450,], ) 
 shape '[-1, 140450]' is invalid for input of size 2281703000
2025-03-19 01:22:48.468491 test begin: paddle.reshape(Tensor([1, 50, 861020, 53],"float32"), shape=list[-1,140450,], )

[torch error] paddle.reshape(Tensor([1, 50, 861020, 53],"float32"), shape=list[-1,140450,], ) 
 shape '[-1, 140450]' is invalid for input of size 2281703000
2025-03-19 01:22:50.514630 test begin: paddle.reshape(Tensor([1, 5003732, 152, 3],"float32"), shape=tuple(1,-1,1,), )

[Pass] paddle.reshape(Tensor([1, 5003732, 152, 3],"float32"), shape=tuple(1,-1,1,), )
2025-03-19 01:25:33.298610 test begin: paddle.reshape(Tensor([1, 50704476, 15, 3],"float32"), shape=tuple(1,-1,1,), )

[Pass] paddle.reshape(Tensor([1, 50704476, 15, 3],"float32"), shape=tuple(1,-1,1,), )
2025-03-19 01:29:12.235314 test begin: paddle.reshape(Tensor([1, 512, 1, 4456449],"float32"), shape=list[-1,512,], )

[Pass] paddle.reshape(Tensor([1, 512, 1, 4456449],"float32"), shape=list[-1,512,], )
2025-03-19 01:32:04.195783 test begin: paddle.reshape(Tensor([1, 512, 4456449, 1],"float32"), shape=list[-1,512,], )

[Pass] paddle.reshape(Tensor([1, 512, 4456449, 1],"float32"), shape=list[-1,512,], )
2025-03-19 01:35:10.742781 test begin: paddle.reshape(Tensor([1, 512, 4456449],"float32"), list[512,128,1,1,], )

[torch error] paddle.reshape(Tensor([1, 512, 4456449],"float32"), list[512,128,1,1,], ) 
 shape '[512, 128, 1, 1]' is invalid for input of size 2281701888
2025-03-19 01:35:15.037015 test begin: paddle.reshape(Tensor([1, 512, 4456449],"float32"), list[512,256,1,1,], )

[torch error] paddle.reshape(Tensor([1, 512, 4456449],"float32"), list[512,256,1,1,], ) 
 shape '[512, 256, 1, 1]' is invalid for input of size 2281701888
2025-03-19 01:35:17.242485 test begin: paddle.reshape(Tensor([1, 54326224, 14, 3],"float32"), shape=tuple(1,-1,1,), )

[Pass] paddle.reshape(Tensor([1, 54326224, 14, 3],"float32"), shape=tuple(1,-1,1,), )
2025-03-19 01:38:34.479081 test begin: paddle.reshape(Tensor([1, 57042535, 5, 2, 4],"float32"), shape=list[15,8,], )

[torch error] paddle.reshape(Tensor([1, 57042535, 5, 2, 4],"float32"), shape=list[15,8,], ) 
 shape '[15, 8]' is invalid for input of size 2281701400
2025-03-19 01:38:39.273368 test begin: paddle.reshape(Tensor([1, 57042535, 5, 8],"float32"), shape=list[1,3,5,2,4,], )

[torch error] paddle.reshape(Tensor([1, 57042535, 5, 8],"float32"), shape=list[1,3,5,2,4,], ) 
 shape '[1, 3, 5, 2, 4]' is invalid for input of size 2281701400
2025-03-19 01:38:41.363937 test begin: paddle.reshape(Tensor([1, 6, 1250933, 304],"float32"), shape=list[-1,200,304,], )

[torch error] paddle.reshape(Tensor([1, 6, 1250933, 304],"float32"), shape=list[-1,200,304,], ) 
 shape '[-1, 200, 304]' is invalid for input of size 2281701792
2025-03-19 01:38:43.458071 test begin: paddle.reshape(Tensor([1, 6, 1901418, 200],"float32"), shape=list[-1,304,200,], )

[torch error] paddle.reshape(Tensor([1, 6, 1901418, 200],"float32"), shape=list[-1,304,200,], ) 
 shape '[-1, 304, 200]' is invalid for input of size 2281701600
2025-03-19 01:38:45.519804 test begin: paddle.reshape(Tensor([1, 6, 200, 1901418],"float32"), shape=list[-1,200,304,], )

[torch error] paddle.reshape(Tensor([1, 6, 200, 1901418],"float32"), shape=list[-1,200,304,], ) 
 shape '[-1, 200, 304]' is invalid for input of size 2281701600
2025-03-19 01:38:47.837793 test begin: paddle.reshape(Tensor([1, 6, 304, 1250933],"float32"), shape=list[-1,304,200,], )

[torch error] paddle.reshape(Tensor([1, 6, 304, 1250933],"float32"), shape=list[-1,304,200,], ) 
 shape '[-1, 304, 200]' is invalid for input of size 2281701792
2025-03-19 01:38:49.271604 test begin: paddle.reshape(Tensor([1, 6, 380283564],"float32"), shape=tuple(-1,200,304,), )

[torch error] paddle.reshape(Tensor([1, 6, 380283564],"float32"), shape=tuple(-1,200,304,), ) 
 shape '[-1, 200, 304]' is invalid for input of size 2281701384
2025-03-19 01:38:51.345689 test begin: paddle.reshape(Tensor([1, 6, 380283564],"float32"), shape=tuple(-1,304,200,), )

[torch error] paddle.reshape(Tensor([1, 6, 380283564],"float32"), shape=tuple(-1,304,200,), ) 
 shape '[-1, 304, 200]' is invalid for input of size 2281701384
2025-03-19 01:38:53.425203 test begin: paddle.reshape(Tensor([1, 64, 35651585],"float32"), list[64,1,7,7,], )

[torch error] paddle.reshape(Tensor([1, 64, 35651585],"float32"), list[64,1,7,7,], ) 
 shape '[64, 1, 7, 7]' is invalid for input of size 2281701440
2025-03-19 01:38:55.490329 test begin: paddle.reshape(Tensor([1, 64, 35651585],"float32"), list[64,256,1,1,], )

[torch error] paddle.reshape(Tensor([1, 64, 35651585],"float32"), list[64,256,1,1,], ) 
 shape '[64, 256, 1, 1]' is invalid for input of size 2281701440
2025-03-19 01:38:57.527172 test begin: paddle.reshape(Tensor([1, 64, 35651585],"float32"), list[64,64,1,1,], )

[torch error] paddle.reshape(Tensor([1, 64, 35651585],"float32"), list[64,64,1,1,], ) 
 shape '[64, 64, 1, 1]' is invalid for input of size 2281701440
2025-03-19 01:38:59.921002 test begin: paddle.reshape(Tensor([1, 64, 35651585],"float32"), list[64,64,3,3,], )

[torch error] paddle.reshape(Tensor([1, 64, 35651585],"float32"), list[64,64,3,3,], ) 
 shape '[64, 64, 3, 3]' is invalid for input of size 2281701440
2025-03-19 01:39:02.101943 test begin: paddle.reshape(Tensor([1, 64, 35651585],"float32"), tuple(1,-1,), )

[Pass] paddle.reshape(Tensor([1, 64, 35651585],"float32"), tuple(1,-1,), )
2025-03-19 01:41:48.858166 test begin: paddle.reshape(Tensor([1, 67108865, 1, 32, 2],"float16"), list[1,1,1,64,], )

[torch error] paddle.reshape(Tensor([1, 67108865, 1, 32, 2],"float16"), list[1,1,1,64,], ) 
 shape '[1, 1, 1, 64]' is invalid for input of size 4294967360
2025-03-19 01:41:52.883331 test begin: paddle.reshape(Tensor([1, 71303169, 16, 2],"float32"), shape=tuple(-1,2,), )

[Pass] paddle.reshape(Tensor([1, 71303169, 16, 2],"float32"), shape=tuple(-1,2,), )
2025-03-19 01:45:05.990482 test begin: paddle.reshape(Tensor([1, 71303169, 32],"float32"), list[1,1,4,-1,], )

[Pass] paddle.reshape(Tensor([1, 71303169, 32],"float32"), list[1,1,4,-1,], )
2025-03-19 01:48:05.362880 test begin: paddle.reshape(Tensor([1, 71303169, 32],"float32"), list[1,14,4,-1,], )

[Pass] paddle.reshape(Tensor([1, 71303169, 32],"float32"), list[1,14,4,-1,], )
2025-03-19 01:50:52.690009 test begin: paddle.reshape(Tensor([1, 71303169, 4, 8],"float32"), list[1,20,-1,], )

[torch error] paddle.reshape(Tensor([1, 71303169, 4, 8],"float32"), list[1,20,-1,], ) 
 shape '[1, 20, -1]' is invalid for input of size 2281701408
2025-03-19 01:50:56.977802 test begin: paddle.reshape(Tensor([1, 760567127, 1, 3],"float32"), list[-1,3,], )

[Pass] paddle.reshape(Tensor([1, 760567127, 1, 3],"float32"), list[-1,3,], )
2025-03-19 01:53:50.592094 test begin: paddle.reshape(Tensor([1, 760567127, 3, 1],"float32"), list[3,3,1,1,], )

[torch error] paddle.reshape(Tensor([1, 760567127, 3, 1],"float32"), list[3,3,1,1,], ) 
 shape '[3, 3, 1, 1]' is invalid for input of size 2281701381
2025-03-19 01:53:54.950009 test begin: paddle.reshape(Tensor([1, 760567127, 3],"float32"), list[-1,], )

[Pass] paddle.reshape(Tensor([1, 760567127, 3],"float32"), list[-1,], )
2025-03-19 01:56:44.230707 test begin: paddle.reshape(Tensor([1, 760567127, 3],"float32"), tuple(1,-1,), )

[Pass] paddle.reshape(Tensor([1, 760567127, 3],"float32"), tuple(1,-1,), )
2025-03-19 01:59:24.775467 test begin: paddle.reshape(Tensor([1, 7922575, 8, 36],"float32"), shape=tuple(1,-1,4,), )

[Pass] paddle.reshape(Tensor([1, 7922575, 8, 36],"float32"), shape=tuple(1,-1,4,), )
2025-03-19 02:02:17.133208 test begin: paddle.reshape(Tensor([1, 812283, 53, 53],"float32"), shape=list[-1,140450,], )

[torch error] paddle.reshape(Tensor([1, 812283, 53, 53],"float32"), shape=list[-1,140450,], ) 
 shape '[-1, 140450]' is invalid for input of size 2281702947
2025-03-19 02:02:21.421181 test begin: paddle.reshape(Tensor([1, 8912897, 16, 16],"float32"), shape=tuple(1,256,-1,), )

[Pass] paddle.reshape(Tensor([1, 8912897, 16, 16],"float32"), shape=tuple(1,256,-1,), )
2025-03-19 02:05:43.709107 test begin: paddle.reshape(Tensor([1, 8912897, 256],"float32"), list[1024,256,1,1,], )

[torch error] paddle.reshape(Tensor([1, 8912897, 256],"float32"), list[1024,256,1,1,], ) 
 shape '[1024, 256, 1, 1]' is invalid for input of size 2281701632
2025-03-19 02:05:48.120725 test begin: paddle.reshape(Tensor([1, 8912897, 256],"float32"), list[128,256,1,1,], )

[torch error] paddle.reshape(Tensor([1, 8912897, 256],"float32"), list[128,256,1,1,], ) 
 shape '[128, 256, 1, 1]' is invalid for input of size 2281701632
2025-03-19 02:05:50.430688 test begin: paddle.reshape(Tensor([1, 8912897, 256],"float32"), list[512,256,1,1,], )

[torch error] paddle.reshape(Tensor([1, 8912897, 256],"float32"), list[512,256,1,1,], ) 
 shape '[512, 256, 1, 1]' is invalid for input of size 2281701632
2025-03-19 02:05:51.936016 test begin: paddle.reshape(Tensor([1, 8912897, 256],"float32"), list[64,256,1,1,], )

[torch error] paddle.reshape(Tensor([1, 8912897, 256],"float32"), list[64,256,1,1,], ) 
 shape '[64, 256, 1, 1]' is invalid for input of size 2281701632
2025-03-19 02:05:54.216348 test begin: paddle.reshape(Tensor([1, 8912897, 256],"float32"), shape=tuple(-1,256,), )

[Pass] paddle.reshape(Tensor([1, 8912897, 256],"float32"), shape=tuple(-1,256,), )
2025-03-19 02:08:40.070570 test begin: paddle.reshape(Tensor([1, 9, 1267612, 200],"float32"), shape=list[-1,304,200,], )

[torch error] paddle.reshape(Tensor([1, 9, 1267612, 200],"float32"), shape=list[-1,304,200,], ) 
 shape '[-1, 304, 200]' is invalid for input of size 2281701600
2025-03-19 02:08:44.619755 test begin: paddle.reshape(Tensor([1, 9, 200, 1267612],"float32"), shape=list[-1,200,272,], )

[torch error] paddle.reshape(Tensor([1, 9, 200, 1267612],"float32"), shape=list[-1,200,272,], ) 
 shape '[-1, 200, 272]' is invalid for input of size 2281701600
2025-03-19 02:08:46.981471 test begin: paddle.reshape(Tensor([1, 9, 200, 1267612],"float32"), shape=list[-1,200,304,], )

[torch error] paddle.reshape(Tensor([1, 9, 200, 1267612],"float32"), shape=list[-1,200,304,], ) 
 shape '[-1, 200, 304]' is invalid for input of size 2281701600
2025-03-19 02:08:49.186728 test begin: paddle.reshape(Tensor([1, 9, 200, 1267612],"float32"), shape=list[-1,200,312,], )

[torch error] paddle.reshape(Tensor([1, 9, 200, 1267612],"float32"), shape=list[-1,200,312,], ) 
 shape '[-1, 200, 312]' is invalid for input of size 2281701600
2025-03-19 02:08:51.385781 test begin: paddle.reshape(Tensor([1, 9, 253522376],"float32"), shape=tuple(-1,200,272,), )

[torch error] paddle.reshape(Tensor([1, 9, 253522376],"float32"), shape=tuple(-1,200,272,), ) 
 shape '[-1, 200, 272]' is invalid for input of size 2281701384
2025-03-19 02:08:53.583406 test begin: paddle.reshape(Tensor([1, 9, 253522376],"float32"), shape=tuple(-1,200,304,), )

[torch error] paddle.reshape(Tensor([1, 9, 253522376],"float32"), shape=tuple(-1,200,304,), ) 
 shape '[-1, 200, 304]' is invalid for input of size 2281701384
2025-03-19 02:08:55.065179 test begin: paddle.reshape(Tensor([1, 9, 253522376],"float32"), shape=tuple(-1,200,312,), )

[torch error] paddle.reshape(Tensor([1, 9, 253522376],"float32"), shape=tuple(-1,200,312,), ) 
 shape '[-1, 200, 312]' is invalid for input of size 2281701384
2025-03-19 02:08:56.656459 test begin: paddle.reshape(Tensor([1, 9, 253522376],"float32"), shape=tuple(-1,304,200,), )

[torch error] paddle.reshape(Tensor([1, 9, 253522376],"float32"), shape=tuple(-1,304,200,), ) 
 shape '[-1, 304, 200]' is invalid for input of size 2281701384
2025-03-19 02:08:58.910484 test begin: paddle.reshape(Tensor([1, 9, 304, 833956],"float32"), shape=list[-1,304,200,], )

[torch error] paddle.reshape(Tensor([1, 9, 304, 833956],"float32"), shape=list[-1,304,200,], ) 
 shape '[-1, 304, 200]' is invalid for input of size 2281703616
2025-03-19 02:09:00.581987 test begin: paddle.reshape(Tensor([1, 9, 812572, 312],"float32"), shape=list[-1,200,312,], )

[torch error] paddle.reshape(Tensor([1, 9, 812572, 312],"float32"), shape=list[-1,200,312,], ) 
 shape '[-1, 200, 312]' is invalid for input of size 2281702176
2025-03-19 02:09:02.666045 test begin: paddle.reshape(Tensor([1, 9, 833956, 304],"float32"), shape=list[-1,200,304,], )

[torch error] paddle.reshape(Tensor([1, 9, 833956, 304],"float32"), shape=list[-1,200,304,], ) 
 shape '[-1, 200, 304]' is invalid for input of size 2281703616
2025-03-19 02:09:04.348434 test begin: paddle.reshape(Tensor([1, 9, 932068, 272],"float32"), shape=list[-1,200,272,], )

[torch error] paddle.reshape(Tensor([1, 9, 932068, 272],"float32"), shape=list[-1,200,272,], ) 
 shape '[-1, 200, 272]' is invalid for input of size 2281702464
2025-03-19 02:09:06.648052 test begin: paddle.reshape(Tensor([1, 91268056, 5, 5],"float32"), shape=list[1,2,24,5,5,], )

[torch error] paddle.reshape(Tensor([1, 91268056, 5, 5],"float32"), shape=list[1,2,24,5,5,], ) 
 shape '[1, 2, 24, 5, 5]' is invalid for input of size 2281701400
2025-03-19 02:09:08.297119 test begin: paddle.reshape(Tensor([1, 96, 3, 7922575],"float32"), shape=list[1,2,48,3,3,], )

[torch error] paddle.reshape(Tensor([1, 96, 3, 7922575],"float32"), shape=list[1,2,48,3,3,], ) 
 shape '[1, 2, 48, 3, 3]' is invalid for input of size 2281701600
2025-03-19 02:09:09.960737 test begin: paddle.reshape(Tensor([1, 96, 7922575, 3],"float32"), shape=list[1,2,48,3,3,], )

[torch error] paddle.reshape(Tensor([1, 96, 7922575, 3],"float32"), shape=list[1,2,48,3,3,], ) 
 shape '[1, 2, 48, 3, 3]' is invalid for input of size 2281701600
2025-03-19 02:09:12.279670 test begin: paddle.reshape(Tensor([1, 990322, 2304],"float32"), list[256,256,3,3,], )

[torch error] paddle.reshape(Tensor([1, 990322, 2304],"float32"), list[256,256,3,3,], ) 
 shape '[256, 256, 3, 3]' is invalid for input of size 2281701888
2025-03-19 02:09:14.597046 test begin: paddle.reshape(Tensor([10, 1, 228170138],"float32"), list[10,10,], )

[torch error] paddle.reshape(Tensor([10, 1, 228170138],"float32"), list[10,10,], ) 
 shape '[10, 10]' is invalid for input of size 2281701380
2025-03-19 02:09:16.373132 test begin: paddle.reshape(Tensor([10, 10, 10, 2281702],"float32"), list[10,100,20,], )

[torch error] paddle.reshape(Tensor([10, 10, 10, 2281702],"float32"), list[10,100,20,], ) 
 shape '[10, 100, 20]' is invalid for input of size 2281702000
2025-03-19 02:09:18.274607 test begin: paddle.reshape(Tensor([10, 10, 1140851, 20],"float32"), list[10,100,20,], )

[torch error] paddle.reshape(Tensor([10, 10, 1140851, 20],"float32"), list[10,100,20,], ) 
 shape '[10, 100, 20]' is invalid for input of size 2281702000
2025-03-19 02:09:20.635915 test begin: paddle.reshape(Tensor([10, 1140851, 10, 20],"float32"), list[10,100,20,], )

[torch error] paddle.reshape(Tensor([10, 1140851, 10, 20],"float32"), list[10,100,20,], ) 
 shape '[10, 100, 20]' is invalid for input of size 2281702000
2025-03-19 02:09:22.328199 test begin: paddle.reshape(Tensor([10, 228170138],"float32"), list[2,25,], )

[torch error] paddle.reshape(Tensor([10, 228170138],"float32"), list[2,25,], ) 
 shape '[2, 25]' is invalid for input of size 2281701380
2025-03-19 02:09:24.314587 test begin: paddle.reshape(Tensor([10, 228170138],"int64"), list[2,25,], )

[torch error] paddle.reshape(Tensor([10, 228170138],"int64"), list[2,25,], ) 
 shape '[2, 25]' is invalid for input of size 2281701380
2025-03-19 02:09:33.273317 test begin: paddle.reshape(Tensor([10, 22817014, 10],"float32"), list[10,10,], )

[torch error] paddle.reshape(Tensor([10, 22817014, 10],"float32"), list[10,10,], ) 
 shape '[10, 10]' is invalid for input of size 2281701400
2025-03-19 02:09:35.830404 test begin: paddle.reshape(Tensor([10, 2328267, 7, 14],"float32"), list[10,4,14,7,], )

[torch error] paddle.reshape(Tensor([10, 2328267, 7, 14],"float32"), list[10,4,14,7,], ) 
 shape '[10, 4, 14, 7]' is invalid for input of size 2281701660
2025-03-19 02:09:37.495760 test begin: paddle.reshape(Tensor([10, 2507365, 13, 7],"float32"), list[10,4,7,13,], )

[torch error] paddle.reshape(Tensor([10, 2507365, 13, 7],"float32"), list[10,4,7,13,], ) 
 shape '[10, 4, 7, 13]' is invalid for input of size 2281702150
2025-03-19 02:09:39.148529 test begin: paddle.reshape(Tensor([10, 4, 13, 4387888],"float32"), list[10,4,7,13,], )

[torch error] paddle.reshape(Tensor([10, 4, 13, 4387888],"float32"), list[10,4,7,13,], ) 
 shape '[10, 4, 7, 13]' is invalid for input of size 2281701760
2025-03-19 02:09:40.914764 test begin: paddle.reshape(Tensor([10, 4, 4074467, 14],"float32"), list[10,4,14,7,], )

[torch error] paddle.reshape(Tensor([10, 4, 4074467, 14],"float32"), list[10,4,14,7,], ) 
 shape '[10, 4, 14, 7]' is invalid for input of size 2281701520
2025-03-19 02:09:43.411211 test begin: paddle.reshape(Tensor([10, 4, 7, 8148934],"float32"), list[10,4,14,7,], )

[torch error] paddle.reshape(Tensor([10, 4, 7, 8148934],"float32"), list[10,4,14,7,], ) 
 shape '[10, 4, 14, 7]' is invalid for input of size 2281701520
2025-03-19 02:09:44.950914 test begin: paddle.reshape(Tensor([10, 4, 8148934, 7],"float32"), list[10,4,7,13,], )

[torch error] paddle.reshape(Tensor([10, 4, 8148934, 7],"float32"), list[10,4,7,13,], ) 
 shape '[10, 4, 7, 13]' is invalid for input of size 2281701520
2025-03-19 02:09:46.519486 test begin: paddle.reshape(Tensor([10, 429496730],"float16"), list[10,20,], )

[torch error] paddle.reshape(Tensor([10, 429496730],"float16"), list[10,20,], ) 
 shape '[10, 20]' is invalid for input of size 4294967300
2025-03-19 02:09:50.307873 test begin: paddle.reshape(Tensor([100, 127827, 336],"float16"), shape=tuple(100,-1,), )

[Pass] paddle.reshape(Tensor([100, 127827, 336],"float16"), shape=tuple(100,-1,), )
2025-03-19 02:26:29.292978 test begin: paddle.reshape(Tensor([100, 137660, 312],"float16"), shape=tuple(100,-1,), )

[Pass] paddle.reshape(Tensor([100, 137660, 312],"float16"), shape=tuple(100,-1,), )
2025-03-19 02:42:55.894340 test begin: paddle.reshape(Tensor([100, 141282, 304],"float16"), shape=tuple(100,-1,), )

[Pass] paddle.reshape(Tensor([100, 141282, 304],"float16"), shape=tuple(100,-1,), )
2025-03-19 02:59:11.528551 test begin: paddle.reshape(Tensor([100, 200, 214749],"float16"), shape=tuple(100,-1,), )

[Pass] paddle.reshape(Tensor([100, 200, 214749],"float16"), shape=tuple(100,-1,), )
2025-03-19 03:15:46.078235 test begin: paddle.reshape(Tensor([100, 22817014],"float32"), shape=list[4,-1,256,], )

[torch error] paddle.reshape(Tensor([100, 22817014],"float32"), shape=list[4,-1,256,], ) 
 shape '[4, -1, 256]' is invalid for input of size 2281701400
2025-03-19 03:15:50.441613 test begin: paddle.reshape(Tensor([100, 42949673],"float16"), shape=list[4,-1,256,], )

[torch error] paddle.reshape(Tensor([100, 42949673],"float16"), shape=list[4,-1,256,], ) 
 shape '[4, -1, 256]' is invalid for input of size 4294967300
2025-03-19 03:15:54.269788 test begin: paddle.reshape(Tensor([100, 73132, 312],"float32"), shape=tuple(100,-1,), )

[Pass] paddle.reshape(Tensor([100, 73132, 312],"float32"), shape=tuple(100,-1,), )
2025-03-19 03:18:41.388728 test begin: paddle.reshape(Tensor([1003, 26, 288, 304],"float32"), shape=list[-1,288,304,], )

[Pass] paddle.reshape(Tensor([1003, 26, 288, 304],"float32"), shape=list[-1,288,304,], )
2025-03-19 03:21:40.220509 test begin: paddle.reshape(Tensor([1003, 26, 304, 288],"float32"), shape=list[-1,304,288,], )

[Pass] paddle.reshape(Tensor([1003, 26, 304, 288],"float32"), shape=list[-1,304,288,], )
2025-03-19 03:24:35.188638 test begin: paddle.reshape(Tensor([100357, 232, 2, 7, 7],"float32"), shape=list[2,464,7,7,], )

[torch error] paddle.reshape(Tensor([100357, 232, 2, 7, 7],"float32"), shape=list[2,464,7,7,], ) 
 shape '[2, 464, 7, 7]' is invalid for input of size 2281716752
2025-03-19 03:24:39.546470 test begin: paddle.reshape(Tensor([100357, 464, 7, 7],"float32"), shape=list[2,2,232,7,7,], )

[torch error] paddle.reshape(Tensor([100357, 464, 7, 7],"float32"), shape=list[2,2,232,7,7,], ) 
 shape '[2, 2, 232, 7, 7]' is invalid for input of size 2281716752
2025-03-19 03:24:41.778112 test begin: paddle.reshape(Tensor([1005, 26, 280, 312],"float32"), shape=list[-1,280,312,], )

[Pass] paddle.reshape(Tensor([1005, 26, 280, 312],"float32"), shape=list[-1,280,312,], )
2025-03-19 03:27:40.903862 test begin: paddle.reshape(Tensor([10080, 226360],"float32"), shape=tuple(-1,4,), )

[Pass] paddle.reshape(Tensor([10080, 226360],"float32"), shape=tuple(-1,4,), )
2025-03-19 03:31:07.848973 test begin: paddle.reshape(Tensor([10186167, 4, 7, 8],"float32"), list[13,4,7,1,-1,], )

[torch error] paddle.reshape(Tensor([10186167, 4, 7, 8],"float32"), list[13,4,7,1,-1,], ) 
 shape '[13, 4, 7, 1, -1]' is invalid for input of size 2281701408
2025-03-19 03:31:12.054213 test begin: paddle.reshape(Tensor([10186167, 4, 7, 8],"float32"), list[52,4,7,1,-1,], )

[torch error] paddle.reshape(Tensor([10186167, 4, 7, 8],"float32"), list[52,4,7,1,-1,], ) 
 shape '[52, 4, 7, 1, -1]' is invalid for input of size 2281701408
2025-03-19 03:31:14.305020 test begin: paddle.reshape(Tensor([10186167, 4, 7, 8],"float32"), tuple(52,-1,8,), )

[torch error] paddle.reshape(Tensor([10186167, 4, 7, 8],"float32"), tuple(52,-1,8,), ) 
 shape '[52, -1, 8]' is invalid for input of size 2281701408
2025-03-19 03:31:16.322892 test begin: paddle.reshape(Tensor([10186167, 7, 32],"float32"), list[13,-1,4,8,], )

[torch error] paddle.reshape(Tensor([10186167, 7, 32],"float32"), list[13,-1,4,8,], ) 
 shape '[13, -1, 4, 8]' is invalid for input of size 2281701408
2025-03-19 03:31:18.356575 test begin: paddle.reshape(Tensor([10186167, 7, 32],"float32"), list[13,7,4,-1,], )

[torch error] paddle.reshape(Tensor([10186167, 7, 32],"float32"), list[13,7,4,-1,], ) 
 shape '[13, 7, 4, -1]' is invalid for input of size 2281701408
2025-03-19 03:31:20.379261 test begin: paddle.reshape(Tensor([10186167, 7, 32],"float32"), list[13,7,4,8,], )

[torch error] paddle.reshape(Tensor([10186167, 7, 32],"float32"), list[13,7,4,8,], ) 
 shape '[13, 7, 4, 8]' is invalid for input of size 2281701408
2025-03-19 03:31:22.411866 test begin: paddle.reshape(Tensor([10186167, 7, 32],"float32"), list[13,7,8,4,], )

[torch error] paddle.reshape(Tensor([10186167, 7, 32],"float32"), list[13,7,8,4,], ) 
 shape '[13, 7, 8, 4]' is invalid for input of size 2281701408
2025-03-19 03:31:24.437797 test begin: paddle.reshape(Tensor([10186167, 7, 32],"float32"), list[52,7,4,-1,], )

[torch error] paddle.reshape(Tensor([10186167, 7, 32],"float32"), list[52,7,4,-1,], ) 
 shape '[52, 7, 4, -1]' is invalid for input of size 2281701408
2025-03-19 03:31:26.458382 test begin: paddle.reshape(Tensor([10186167, 7, 32],"float32"), shape=list[7,7,4,8,], )

[torch error] paddle.reshape(Tensor([10186167, 7, 32],"float32"), shape=list[7,7,4,8,], ) 
 shape '[7, 7, 4, 8]' is invalid for input of size 2281701408
2025-03-19 03:31:28.555044 test begin: paddle.reshape(Tensor([10186167, 7, 32],"float32"), tuple(2,7,13,32,), )

[torch error] paddle.reshape(Tensor([10186167, 7, 32],"float32"), tuple(2,7,13,32,), ) 
 shape '[2, 7, 13, 32]' is invalid for input of size 2281701408
2025-03-19 03:31:30.037155 test begin: paddle.reshape(Tensor([10186167, 7, 4, 8],"float32"), list[13,7,-1,], )

[torch error] paddle.reshape(Tensor([10186167, 7, 4, 8],"float32"), list[13,7,-1,], ) 
 shape '[13, 7, -1]' is invalid for input of size 2281701408
2025-03-19 03:31:32.087851 test begin: paddle.reshape(Tensor([10186167, 7, 4, 8],"float32"), list[52,7,-1,], )

[torch error] paddle.reshape(Tensor([10186167, 7, 4, 8],"float32"), list[52,7,-1,], ) 
 shape '[52, 7, -1]' is invalid for input of size 2281701408
2025-03-19 03:31:34.199879 test begin: paddle.reshape(Tensor([10186167, 7, 4, 8],"float32"), tuple(13,1,7,32,), )

[torch error] paddle.reshape(Tensor([10186167, 7, 4, 8],"float32"), tuple(13,1,7,32,), ) 
 shape '[13, 1, 7, 32]' is invalid for input of size 2281701408
2025-03-19 03:31:36.589705 test begin: paddle.reshape(Tensor([10186167, 7, 4, 8],"float32"), tuple(13,7,32,), )

[torch error] paddle.reshape(Tensor([10186167, 7, 4, 8],"float32"), tuple(13,7,32,), ) 
 shape '[13, 7, 32]' is invalid for input of size 2281701408
2025-03-19 03:31:38.720254 test begin: paddle.reshape(Tensor([1019, 40, 200, 280],"float32"), shape=list[-1,200,280,], )

[Pass] paddle.reshape(Tensor([1019, 40, 200, 280],"float32"), shape=list[-1,200,280,], )
2025-03-19 03:34:56.121484 test begin: paddle.reshape(Tensor([1020, 2236963],"float32"), list[-1,1,4,], )

[Pass] paddle.reshape(Tensor([1020, 2236963],"float32"), list[-1,1,4,], )
2025-03-19 03:37:58.327097 test begin: paddle.reshape(Tensor([1020, 2236963],"float32"), shape=tuple(-1,4,), )

[Pass] paddle.reshape(Tensor([1020, 2236963],"float32"), shape=tuple(-1,4,), )
2025-03-19 03:41:26.026073 test begin: paddle.reshape(Tensor([1020, 3, 745655],"float32"), list[-1,4,], )

[Pass] paddle.reshape(Tensor([1020, 3, 745655],"float32"), list[-1,4,], )
2025-03-19 03:45:03.112015 test begin: paddle.reshape(Tensor([1020, 559241, 4],"float32"), list[-1,4,], )

[Pass] paddle.reshape(Tensor([1020, 559241, 4],"float32"), list[-1,4,], )
2025-03-19 03:48:34.982392 test begin: paddle.reshape(Tensor([10200, 223697],"float32"), shape=tuple(-1,4,), )

[Pass] paddle.reshape(Tensor([10200, 223697],"float32"), shape=tuple(-1,4,), )
2025-03-19 03:51:54.385294 test begin: paddle.reshape(Tensor([102144, 22339],"float32"), list[-1,1,4,], )

[Pass] paddle.reshape(Tensor([102144, 22339],"float32"), list[-1,1,4,], )
2025-03-19 03:55:19.957189 test begin: paddle.reshape(Tensor([102144, 3, 7447],"float32"), list[-1,4,], )

[Pass] paddle.reshape(Tensor([102144, 3, 7447],"float32"), list[-1,4,], )
2025-03-19 03:58:40.077179 test begin: paddle.reshape(Tensor([102144, 5585, 4],"float32"), list[-1,4,], )

[Pass] paddle.reshape(Tensor([102144, 5585, 4],"float32"), list[-1,4,], )
2025-03-19 04:01:53.590941 test begin: paddle.reshape(Tensor([102262, 100, 140, 3],"float16"), shape=tuple(8,-1,1,), )

[Pass] paddle.reshape(Tensor([102262, 100, 140, 3],"float16"), shape=tuple(8,-1,1,), )
2025-03-19 04:18:36.614292 test begin: paddle.reshape(Tensor([1023, 2230403],"float32"), list[-1,1,4,], )

[torch error] paddle.reshape(Tensor([1023, 2230403],"float32"), list[-1,1,4,], ) 
 shape '[-1, 1, 4]' is invalid for input of size 2281702269
2025-03-19 04:18:41.215220 test begin: paddle.reshape(Tensor([1023, 3, 743468],"float32"), list[-1,4,], )

[Pass] paddle.reshape(Tensor([1023, 3, 743468],"float32"), list[-1,4,], )
2025-03-19 04:21:19.932340 test begin: paddle.reshape(Tensor([1023, 557601, 4],"float32"), list[-1,4,], )

[Pass] paddle.reshape(Tensor([1023, 557601, 4],"float32"), list[-1,4,], )
2025-03-19 04:24:20.763620 test begin: paddle.reshape(Tensor([1024, 1, 2228225],"float32"), shape=list[-1,1000,], )

[torch error] paddle.reshape(Tensor([1024, 1, 2228225],"float32"), shape=list[-1,1000,], ) 
 shape '[-1, 1000]' is invalid for input of size 2281702400
2025-03-19 04:24:25.001928 test begin: paddle.reshape(Tensor([1024, 1, 2228225],"float32"), shape=list[-1,32,128,], )

[torch error] paddle.reshape(Tensor([1024, 1, 2228225],"float32"), shape=list[-1,32,128,], ) 
 shape '[-1, 32, 128]' is invalid for input of size 2281702400
2025-03-19 04:24:26.452113 test begin: paddle.reshape(Tensor([1024, 1, 4194305],"float16"), shape=list[-1,1000,], )

[torch error] paddle.reshape(Tensor([1024, 1, 4194305],"float16"), shape=list[-1,1000,], ) 
 shape '[-1, 1000]' is invalid for input of size 4294968320
2025-03-19 04:24:30.328917 test begin: paddle.reshape(Tensor([1024, 34, 328, 200],"float32"), shape=list[-1,328,200,], )

[Pass] paddle.reshape(Tensor([1024, 34, 328, 200],"float32"), shape=list[-1,328,200,], )
2025-03-19 04:28:03.055245 test begin: paddle.reshape(Tensor([1024, 4195, 1000],"float16"), shape=list[-1,1000,], )

[Pass] paddle.reshape(Tensor([1024, 4195, 1000],"float16"), shape=list[-1,1000,], )
2025-03-19 04:44:35.684283 test begin: paddle.reshape(Tensor([1037, 21, 312, 336],"float32"), shape=list[-1,312,336,], )

[Pass] paddle.reshape(Tensor([1037, 21, 312, 336],"float32"), shape=list[-1,312,336,], )
2025-03-19 04:48:09.447370 test begin: paddle.reshape(Tensor([103884, 4, 38, 68, 4],"float16"), list[-1,4,], )

[Pass] paddle.reshape(Tensor([103884, 4, 38, 68, 4],"float16"), list[-1,4,], )
2025-03-19 05:05:03.277839 test begin: paddle.reshape(Tensor([1039, 26, 264, 320],"float32"), shape=list[-1,264,320,], )

[Pass] paddle.reshape(Tensor([1039, 26, 264, 320],"float32"), shape=list[-1,264,320,], )
2025-03-19 05:08:16.453393 test begin: paddle.reshape(Tensor([104, 122911, 336],"float16"), shape=tuple(104,-1,), )

[Pass] paddle.reshape(Tensor([104, 122911, 336],"float16"), shape=tuple(104,-1,), )
2025-03-19 05:24:45.760764 test begin: paddle.reshape(Tensor([104, 135848, 304],"float16"), shape=tuple(104,-1,), )

[Pass] paddle.reshape(Tensor([104, 135848, 304],"float16"), shape=tuple(104,-1,), )
2025-03-19 05:41:47.571471 test begin: paddle.reshape(Tensor([104, 151831, 272],"float16"), shape=tuple(104,-1,), )

[Pass] paddle.reshape(Tensor([104, 151831, 272],"float16"), shape=tuple(104,-1,), )
2025-03-19 05:58:22.873060 test begin: paddle.reshape(Tensor([104, 200, 206489],"float16"), shape=tuple(104,-1,), )

[Pass] paddle.reshape(Tensor([104, 200, 206489],"float16"), shape=tuple(104,-1,), )
2025-03-19 06:14:57.258685 test begin: paddle.reshape(Tensor([104, 21939437],"float32"), shape=list[4,-1,256,], )

[torch error] paddle.reshape(Tensor([104, 21939437],"float32"), shape=list[4,-1,256,], ) 
 shape '[4, -1, 256]' is invalid for input of size 2281701448
2025-03-19 06:15:01.592038 test begin: paddle.reshape(Tensor([104, 2742430, 8],"float32"), tuple(2,-1,7,8,), )

[torch error] paddle.reshape(Tensor([104, 2742430, 8],"float32"), tuple(2,-1,7,8,), ) 
 shape '[2, -1, 7, 8]' is invalid for input of size 2281701760
2025-03-19 06:15:03.912612 test begin: paddle.reshape(Tensor([104, 41297763],"float16"), shape=list[4,-1,256,], )

[torch error] paddle.reshape(Tensor([104, 41297763],"float16"), shape=list[4,-1,256,], ) 
 shape '[4, -1, 256]' is invalid for input of size 4294967352
2025-03-19 06:15:07.393368 test begin: paddle.reshape(Tensor([104, 72170, 304],"float32"), shape=tuple(104,-1,), )

[Pass] paddle.reshape(Tensor([104, 72170, 304],"float32"), shape=tuple(104,-1,), )
2025-03-19 06:17:54.368776 test begin: paddle.reshape(Tensor([1043, 25, 87552],"float32"), shape=tuple(-1,288,304,), )

[Pass] paddle.reshape(Tensor([1043, 25, 87552],"float32"), shape=tuple(-1,288,304,), )
2025-03-19 06:21:06.434798 test begin: paddle.reshape(Tensor([1045, 39, 200, 280],"float32"), shape=list[-1,200,280,], )

[Pass] paddle.reshape(Tensor([1045, 39, 200, 280],"float32"), shape=list[-1,200,280,], )
2025-03-19 06:24:27.192021 test begin: paddle.reshape(Tensor([1048577, 1, 64, 32, 2],"float16"), list[1,1,64,64,], )

[torch error] paddle.reshape(Tensor([1048577, 1, 64, 32, 2],"float16"), list[1,1,64,64,], ) 
 shape '[1, 1, 64, 64]' is invalid for input of size 4294971392
2025-03-19 06:24:30.815498 test begin: paddle.reshape(Tensor([10486, 4, 102400],"float16"), shape=tuple(-1,320,320,), )

[Pass] paddle.reshape(Tensor([10486, 4, 102400],"float16"), shape=tuple(-1,320,320,), )
2025-03-19 06:40:46.868564 test begin: paddle.reshape(Tensor([10486, 4, 272, 200],"float32"), shape=list[-1,272,200,], )

[Pass] paddle.reshape(Tensor([10486, 4, 272, 200],"float32"), shape=list[-1,272,200,], )
2025-03-19 06:44:15.688433 test begin: paddle.reshape(Tensor([10513, 6, 68096],"float16"), shape=tuple(-1,224,304,), )

[Pass] paddle.reshape(Tensor([10513, 6, 68096],"float16"), shape=tuple(-1,224,304,), )
2025-03-19 07:00:33.759464 test begin: paddle.reshape(Tensor([1052, 38, 107520],"float16"), shape=tuple(-1,336,320,), )

[Pass] paddle.reshape(Tensor([1052, 38, 107520],"float16"), shape=tuple(-1,336,320,), )
2025-03-19 07:17:03.346973 test begin: paddle.reshape(Tensor([105269, 100, 136, 3],"float16"), shape=tuple(8,-1,1,), )

[Pass] paddle.reshape(Tensor([105269, 100, 136, 3],"float16"), shape=tuple(8,-1,1,), )
2025-03-19 07:33:25.463586 test begin: paddle.reshape(Tensor([1057, 24, 304, 296],"float32"), shape=list[-1,304,296,], )

[Pass] paddle.reshape(Tensor([1057, 24, 304, 296],"float32"), shape=list[-1,304,296,], )
2025-03-19 07:36:16.350812 test begin: paddle.reshape(Tensor([10700, 56, 56, 128],"float16"), list[128,56,56,128,], )

[torch error] paddle.reshape(Tensor([10700, 56, 56, 128],"float16"), list[128,56,56,128,], ) 
 shape '[128, 56, 56, 128]' is invalid for input of size 4295065600
2025-03-19 07:36:20.299107 test begin: paddle.reshape(Tensor([1073741825, 4],"float16"), list[1,-1,4,], )

[Pass] paddle.reshape(Tensor([1073741825, 4],"float16"), list[1,-1,4,], )
2025-03-19 07:53:30.776898 test begin: paddle.reshape(Tensor([107374183, 40],"float16"), shape=list[-1,20,2,], )

[Pass] paddle.reshape(Tensor([107374183, 40],"float16"), shape=list[-1,20,2,], )
2025-03-19 08:10:42.729295 test begin: paddle.reshape(Tensor([10737419, 1, 1, 1, 400],"float16"), shape=tuple(8,-1,), )

[Pass] paddle.reshape(Tensor([10737419, 1, 1, 1, 400],"float16"), shape=tuple(8,-1,), )
2025-03-19 08:27:51.515954 test begin: paddle.reshape(Tensor([10737419, 4, 100],"float16"), list[-1,400,], )

[Pass] paddle.reshape(Tensor([10737419, 4, 100],"float16"), list[-1,400,], )
2025-03-19 08:44:08.038341 test begin: paddle.reshape(Tensor([10737419, 400],"float16"), list[-1,8,400,], )

[torch error] paddle.reshape(Tensor([10737419, 400],"float16"), list[-1,8,400,], ) 
 shape '[-1, 8, 400]' is invalid for input of size 4294967600
2025-03-19 08:44:11.984972 test begin: paddle.reshape(Tensor([1079, 29, 240, 304],"float32"), shape=list[-1,240,304,], )

[Pass] paddle.reshape(Tensor([1079, 29, 240, 304],"float32"), shape=list[-1,240,304,], )
2025-03-19 08:47:29.173263 test begin: paddle.reshape(Tensor([108, 130817, 304],"float16"), shape=tuple(108,-1,), )

[Pass] paddle.reshape(Tensor([108, 130817, 304],"float16"), shape=tuple(108,-1,), )
2025-03-19 09:04:22.276545 test begin: paddle.reshape(Tensor([108, 138085, 288],"float16"), shape=tuple(108,-1,), )

[Pass] paddle.reshape(Tensor([108, 138085, 288],"float16"), shape=tuple(108,-1,), )
2025-03-19 09:20:47.785439 test begin: paddle.reshape(Tensor([108, 146207, 272],"float16"), shape=tuple(108,-1,), )

[Pass] paddle.reshape(Tensor([108, 146207, 272],"float16"), shape=tuple(108,-1,), )
2025-03-19 09:37:15.585215 test begin: paddle.reshape(Tensor([108, 200, 198842],"float16"), shape=tuple(108,-1,), )

[Pass] paddle.reshape(Tensor([108, 200, 198842],"float16"), shape=tuple(108,-1,), )
2025-03-19 09:53:24.776545 test begin: paddle.reshape(Tensor([108, 39768216],"float16"), shape=list[4,-1,256,], )

[torch error] paddle.reshape(Tensor([108, 39768216],"float16"), shape=list[4,-1,256,], ) 
 shape '[4, -1, 256]' is invalid for input of size 4294967328
2025-03-19 09:53:28.674279 test begin: paddle.reshape(Tensor([108, 73358, 288],"float32"), shape=tuple(108,-1,), )

[Pass] paddle.reshape(Tensor([108, 73358, 288],"float32"), shape=tuple(108,-1,), )
2025-03-19 09:56:59.034287 test begin: paddle.reshape(Tensor([1081, 23, 280, 328],"float32"), shape=list[-1,280,328,], )

[Pass] paddle.reshape(Tensor([1081, 23, 280, 328],"float32"), shape=list[-1,280,328,], )
2025-03-19 09:59:53.764491 test begin: paddle.reshape(Tensor([10969719, 4, 13, 4],"float32"), list[13,4,4,13,], )

[torch error] paddle.reshape(Tensor([10969719, 4, 13, 4],"float32"), list[13,4,4,13,], ) 
 shape '[13, 4, 4, 13]' is invalid for input of size 2281701552
2025-03-19 09:59:58.287263 test begin: paddle.reshape(Tensor([11, 2116607, 7, 14],"float32"), list[11,4,14,7,], )

[torch error] paddle.reshape(Tensor([11, 2116607, 7, 14],"float32"), list[11,4,14,7,], ) 
 shape '[11, 4, 14, 7]' is invalid for input of size 2281702346
2025-03-19 09:59:59.688072 test begin: paddle.reshape(Tensor([11, 2279422, 13, 7],"float32"), list[11,4,7,13,], )

[torch error] paddle.reshape(Tensor([11, 2279422, 13, 7],"float32"), list[11,4,7,13,], ) 
 shape '[11, 4, 7, 13]' is invalid for input of size 2281701422
2025-03-19 10:00:01.662470 test begin: paddle.reshape(Tensor([11, 4, 13, 3988989],"float32"), list[11,4,7,13,], )

[torch error] paddle.reshape(Tensor([11, 4, 13, 3988989],"float32"), list[11,4,7,13,], ) 
 shape '[11, 4, 7, 13]' is invalid for input of size 2281701708
2025-03-19 10:00:03.797623 test begin: paddle.reshape(Tensor([11, 4, 3704061, 14],"float32"), list[11,4,14,7,], )

[torch error] paddle.reshape(Tensor([11, 4, 3704061, 14],"float32"), list[11,4,14,7,], ) 
 shape '[11, 4, 14, 7]' is invalid for input of size 2281701576
2025-03-19 10:00:05.510874 test begin: paddle.reshape(Tensor([11, 4, 7, 7408122],"float32"), list[11,4,14,7,], )

[torch error] paddle.reshape(Tensor([11, 4, 7, 7408122],"float32"), list[11,4,14,7,], ) 
 shape '[11, 4, 14, 7]' is invalid for input of size 2281701576
2025-03-19 10:00:07.470403 test begin: paddle.reshape(Tensor([11, 4, 7408122, 7],"float32"), list[11,4,7,13,], )

[torch error] paddle.reshape(Tensor([11, 4, 7408122, 7],"float32"), list[11,4,7,13,], ) 
 shape '[11, 4, 7, 13]' is invalid for input of size 2281701576
2025-03-19 10:00:09.423562 test begin: paddle.reshape(Tensor([110377, 4, 38, 68, 2],"float32"), list[-1,2,], )

[Pass] paddle.reshape(Tensor([110377, 4, 38, 68, 2],"float32"), list[-1,2,], )
2025-03-19 10:03:19.545853 test begin: paddle.reshape(Tensor([11097, 4, 96768],"float16"), shape=tuple(-1,288,336,), )

[Pass] paddle.reshape(Tensor([11097, 4, 96768],"float16"), shape=tuple(-1,288,336,), )
2025-03-19 10:19:36.948891 test begin: paddle.reshape(Tensor([1114113, 2, 64, 16],"float32"), shape=list[13,2,4,16,16,], )

[torch error] paddle.reshape(Tensor([1114113, 2, 64, 16],"float32"), shape=list[13,2,4,16,16,], ) 
 shape '[13, 2, 4, 16, 16]' is invalid for input of size 2281703424
2025-03-19 10:19:41.656757 test begin: paddle.reshape(Tensor([1114113, 2048, 1, 1],"float32"), list[-1,8,2048,], )

[torch error] paddle.reshape(Tensor([1114113, 2048, 1, 1],"float32"), list[-1,8,2048,], ) 
 shape '[-1, 8, 2048]' is invalid for input of size 2281703424
2025-03-19 10:19:43.866836 test begin: paddle.reshape(Tensor([1114113, 2048, 1, 1],"float32"), list[240,2048,], )

[torch error] paddle.reshape(Tensor([1114113, 2048, 1, 1],"float32"), list[240,2048,], ) 
 shape '[240, 2048]' is invalid for input of size 2281703424
2025-03-19 10:19:46.331477 test begin: paddle.reshape(Tensor([1114113, 2048],"float32"), list[1,2048,1,1,], )

[torch error] paddle.reshape(Tensor([1114113, 2048],"float32"), list[1,2048,1,1,], ) 
 shape '[1, 2048, 1, 1]' is invalid for input of size 2281703424
2025-03-19 10:19:48.455997 test begin: paddle.reshape(Tensor([1114113, 2048],"float32"), list[8,2048,1,1,], )

[torch error] paddle.reshape(Tensor([1114113, 2048],"float32"), list[8,2048,1,1,], ) 
 shape '[8, 2048, 1, 1]' is invalid for input of size 2281703424
2025-03-19 10:19:50.014011 test begin: paddle.reshape(Tensor([111412, 16, 16, 80],"float32"), shape=tuple(-1,80,), )

[Pass] paddle.reshape(Tensor([111412, 16, 16, 80],"float32"), shape=tuple(-1,80,), )
2025-03-19 10:23:45.521619 test begin: paddle.reshape(Tensor([11142, 2, 102400],"float32"), shape=tuple(-1,320,320,), )

[Pass] paddle.reshape(Tensor([11142, 2, 102400],"float32"), shape=tuple(-1,320,320,), )
2025-03-19 10:27:23.434121 test begin: paddle.reshape(Tensor([111849, 100, 128, 3],"float16"), shape=tuple(4,-1,1,), )

[Pass] paddle.reshape(Tensor([111849, 100, 128, 3],"float16"), shape=tuple(4,-1,1,), )
2025-03-19 10:43:55.597019 test begin: paddle.reshape(Tensor([112, 122911, 312],"float16"), shape=tuple(112,-1,), )

[Pass] paddle.reshape(Tensor([112, 122911, 312],"float16"), shape=tuple(112,-1,), )
2025-03-19 11:01:26.218962 test begin: paddle.reshape(Tensor([112, 126145, 304],"float16"), shape=tuple(112,-1,), )

[Pass] paddle.reshape(Tensor([112, 126145, 304],"float16"), shape=tuple(112,-1,), )
2025-03-19 11:18:13.288494 test begin: paddle.reshape(Tensor([112, 140986, 272],"float16"), shape=tuple(112,-1,), )

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 86, in func_timeout
    thread.join(timeout)
  File "/usr/lib/python3.9/threading.py", line 1064, in join
    self._wait_for_tstate_lock(timeout=max(timeout, 0))
  File "/usr/lib/python3.9/threading.py", line 1080, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt
2025-03-19 12:15:38.800458 test begin: paddle.reshape(Tensor([112, 200, 191740],"float16"), shape=tuple(112,-1,), )

W0319 12:17:16.724745 73589 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0319 12:17:16.725999 73589 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.reshape(Tensor([112, 200, 191740],"float16"), shape=tuple(112,-1,), )
2025-03-19 12:32:27.817463 test begin: paddle.reshape(Tensor([112, 20372334],"int64"), list[14,2,4,2,], name="Categorical_sample", )

[torch error] paddle.reshape(Tensor([112, 20372334],"int64"), list[14,2,4,2,], name="Categorical_sample", ) 
 shape '[14, 2, 4, 2]' is invalid for input of size 2281701408
2025-03-19 12:33:33.598690 test begin: paddle.reshape(Tensor([112, 67015, 304],"float32"), shape=tuple(112,-1,), )

[Pass] paddle.reshape(Tensor([112, 67015, 304],"float32"), shape=tuple(112,-1,), )
2025-03-19 12:38:05.425039 test begin: paddle.reshape(Tensor([1123, 18, 336, 336],"float32"), shape=list[-1,336,336,], )

[Pass] paddle.reshape(Tensor([1123, 18, 336, 336],"float32"), shape=list[-1,336,336,], )
2025-03-19 12:41:20.452087 test begin: paddle.reshape(Tensor([1128, 30, 248, 272],"float32"), shape=list[-1,248,272,], )

[Pass] paddle.reshape(Tensor([1128, 30, 248, 272],"float32"), shape=list[-1,248,272,], )
2025-03-19 12:44:14.147575 test begin: paddle.reshape(Tensor([1129, 30, 216, 312],"float32"), shape=list[-1,216,312,], )

[Pass] paddle.reshape(Tensor([1129, 30, 216, 312],"float32"), shape=list[-1,216,312,], )
2025-03-19 12:47:44.724950 test begin: paddle.reshape(Tensor([11369, 28, 28, 256],"float32"), list[12,28,28,256,], )

[torch error] paddle.reshape(Tensor([11369, 28, 28, 256],"float32"), list[12,28,28,256,], ) 
 shape '[12, 28, 28, 256]' is invalid for input of size 2281803776
2025-03-19 12:47:48.679489 test begin: paddle.reshape(Tensor([11369, 28, 28, 256],"float32"), list[16,28,28,256,], )

[torch error] paddle.reshape(Tensor([11369, 28, 28, 256],"float32"), list[16,28,28,256,], ) 
 shape '[16, 28, 28, 256]' is invalid for input of size 2281803776
2025-03-19 12:47:50.100902 test begin: paddle.reshape(Tensor([11369, 28, 28, 256],"float32"), list[60,28,28,256,], )

[torch error] paddle.reshape(Tensor([11369, 28, 28, 256],"float32"), list[60,28,28,256,], ) 
 shape '[60, 28, 28, 256]' is invalid for input of size 2281803776
2025-03-19 12:47:52.432988 test begin: paddle.reshape(Tensor([11369, 28, 28, 256],"float32"), list[64,28,28,256,], )

[torch error] paddle.reshape(Tensor([11369, 28, 28, 256],"float32"), list[64,28,28,256,], ) 
 shape '[64, 28, 28, 256]' is invalid for input of size 2281803776
2025-03-19 12:47:54.130776 test begin: paddle.reshape(Tensor([114085069, 20],"float32"), list[4,5,4,], )

[torch error] paddle.reshape(Tensor([114085069, 20],"float32"), list[4,5,4,], ) 
 shape '[4, 5, 4]' is invalid for input of size 2281701380
2025-03-19 12:47:56.045506 test begin: paddle.reshape(Tensor([114085069, 4, 5],"float32"), list[3,-1,], )

[torch error] paddle.reshape(Tensor([114085069, 4, 5],"float32"), list[3,-1,], ) 
 shape '[3, -1]' is invalid for input of size 2281701380
2025-03-19 12:47:58.424577 test begin: paddle.vstack(list[Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 4294967297],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 4294967297],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 1 but got size 4294967297 for tensor number 2 in the list.
2025-03-19 12:48:02.115353 test begin: paddle.vstack(list[Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 4294967297],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 4294967297],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 1 but got size 4294967297 for tensor number 2 in the list.
2025-03-19 12:48:04.588490 test begin: paddle.vstack(list[Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 4294967297, 1],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 4294967297, 1],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 1 but got size 4294967297 for tensor number 2 in the list.
2025-03-19 12:48:07.729959 test begin: paddle.vstack(list[Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 4294967297, 1],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 4294967297, 1],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 1 but got size 4294967297 for tensor number 2 in the list.
2025-03-19 12:48:10.006750 test begin: paddle.vstack(list[Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),Tensor([1, 4294967297, 1, 1],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),Tensor([1, 4294967297, 1, 1],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 1 but got size 4294967297 for tensor number 2 in the list.
2025-03-19 12:48:11.994249 test begin: paddle.vstack(list[Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),Tensor([1, 4294967297, 1, 1],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),Tensor([1, 4294967297, 1, 1],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 1 but got size 4294967297 for tensor number 2 in the list.
2025-03-19 12:48:13.950889 test begin: paddle.vstack(list[Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),Tensor([4294967297, 1, 1, 1],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),Tensor([4294967297, 1, 1, 1],"float16"),], )      
2025-03-19 13:04:31.262196 test begin: paddle.vstack(list[Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),Tensor([4294967297, 1, 1, 1],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),Tensor([4294967297, 1, 1, 1],"float16"),], name=None, )      
2025-03-19 13:21:09.203957 test begin: paddle.vstack(list[Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 4294967297],"float16"),Tensor([1, 1, 1, 1],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 4294967297],"float16"),Tensor([1, 1, 1, 1],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 1 but got size 4294967297 for tensor number 1 in the list.
2025-03-19 13:21:13.427611 test begin: paddle.vstack(list[Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 4294967297],"float16"),Tensor([1, 1, 1, 1],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 4294967297],"float16"),Tensor([1, 1, 1, 1],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 1 but got size 4294967297 for tensor number 1 in the list.
2025-03-19 13:21:15.446967 test begin: paddle.vstack(list[Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 4294967297, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 4294967297, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 1 but got size 4294967297 for tensor number 1 in the list.
2025-03-19 13:21:17.371303 test begin: paddle.vstack(list[Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 4294967297, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 4294967297, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 1 but got size 4294967297 for tensor number 1 in the list.
2025-03-19 13:21:19.282582 test begin: paddle.vstack(list[Tensor([1, 1, 1, 1],"float16"),Tensor([1, 4294967297, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([1, 1, 1, 1],"float16"),Tensor([1, 4294967297, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 1 but got size 4294967297 for tensor number 1 in the list.
2025-03-19 13:21:21.194872 test begin: paddle.vstack(list[Tensor([1, 1, 1, 1],"float16"),Tensor([1, 4294967297, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([1, 1, 1, 1],"float16"),Tensor([1, 4294967297, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 1 but got size 4294967297 for tensor number 1 in the list.
2025-03-19 13:21:23.124983 test begin: paddle.vstack(list[Tensor([1, 1, 1, 1],"float16"),Tensor([4294967297, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([1, 1, 1, 1],"float16"),Tensor([4294967297, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),], )      
2025-03-19 13:37:40.165403 test begin: paddle.vstack(list[Tensor([1, 1, 1, 1],"float16"),Tensor([4294967297, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([1, 1, 1, 1],"float16"),Tensor([4294967297, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),], name=None, )      
2025-03-19 13:54:13.499440 test begin: paddle.vstack(list[Tensor([1, 1, 1, 4294967297],"float16"),Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([1, 1, 1, 4294967297],"float16"),Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 4294967297 but got size 1 for tensor number 1 in the list.
2025-03-19 13:54:17.628516 test begin: paddle.vstack(list[Tensor([1, 1, 1, 4294967297],"float16"),Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([1, 1, 1, 4294967297],"float16"),Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 4294967297 but got size 1 for tensor number 1 in the list.
2025-03-19 13:54:19.225409 test begin: paddle.vstack(list[Tensor([1, 1, 1, 4294967297],"float16"),Tensor([1, 1, 1, 4294967297],"float16"),Tensor([1, 1, 1, 4294967297],"float16"),], )      

2025-03-19 13:54:24.594746 test begin: paddle.vstack(list[Tensor([1, 1, 1, 4294967297],"float16"),Tensor([1, 1, 1, 4294967297],"float16"),Tensor([1, 1, 1, 4294967297],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([1, 1, 1, 4294967297],"float16"),Tensor([1, 1, 1, 4294967297],"float16"),Tensor([1, 1, 1, 4294967297],"float16"),], name=None, )       
 CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 4.46 GiB is free. Process 75462 has 73.60 GiB memory in use. Of the allocated memory 72.00 GiB is allocated by PyTorch, and 14.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-19 13:54:31.052166 test begin: paddle.vstack(list[Tensor([1, 1, 1, 4294967297],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([1, 1, 1, 4294967297],"float16"),], )      
2025-03-19 14:11:24.758413 test begin: paddle.vstack(list[Tensor([1, 1, 1, 4294967297],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([1, 1, 1, 4294967297],"float16"),], name=None, )      
2025-03-19 14:28:41.340739 test begin: paddle.vstack(list[Tensor([1, 1, 1],"float16"),Tensor([1, 1, 1],"float16"),Tensor([1, 1, 4294967297],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([1, 1, 1],"float16"),Tensor([1, 1, 1],"float16"),Tensor([1, 1, 4294967297],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 1 but got size 4294967297 for tensor number 2 in the list.
2025-03-19 14:28:45.538559 test begin: paddle.vstack(list[Tensor([1, 1, 1],"float16"),Tensor([1, 1, 1],"float16"),Tensor([1, 1, 4294967297],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([1, 1, 1],"float16"),Tensor([1, 1, 1],"float16"),Tensor([1, 1, 4294967297],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 1 but got size 4294967297 for tensor number 2 in the list.
2025-03-19 14:28:47.573465 test begin: paddle.vstack(list[Tensor([1, 1, 1],"float16"),Tensor([1, 1, 1],"float16"),Tensor([1, 4294967297, 1],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([1, 1, 1],"float16"),Tensor([1, 1, 1],"float16"),Tensor([1, 4294967297, 1],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 1 but got size 4294967297 for tensor number 2 in the list.
2025-03-19 14:28:49.140011 test begin: paddle.vstack(list[Tensor([1, 1, 1],"float16"),Tensor([1, 1, 1],"float16"),Tensor([1, 4294967297, 1],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([1, 1, 1],"float16"),Tensor([1, 1, 1],"float16"),Tensor([1, 4294967297, 1],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 1 but got size 4294967297 for tensor number 2 in the list.
2025-03-19 14:28:50.953719 test begin: paddle.vstack(list[Tensor([1, 1, 1],"float16"),Tensor([1, 1, 1],"float16"),Tensor([4294967297, 1, 1],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([1, 1, 1],"float16"),Tensor([1, 1, 1],"float16"),Tensor([4294967297, 1, 1],"float16"),], )      
2025-03-19 14:45:03.333549 test begin: paddle.vstack(list[Tensor([1, 1, 1],"float16"),Tensor([1, 1, 1],"float16"),Tensor([4294967297, 1, 1],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([1, 1, 1],"float16"),Tensor([1, 1, 1],"float16"),Tensor([4294967297, 1, 1],"float16"),], name=None, )      
2025-03-19 15:01:31.850039 test begin: paddle.vstack(list[Tensor([1, 1, 1],"float16"),Tensor([1, 1, 4294967297],"float16"),Tensor([1, 1, 1],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([1, 1, 1],"float16"),Tensor([1, 1, 4294967297],"float16"),Tensor([1, 1, 1],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 1 but got size 4294967297 for tensor number 1 in the list.
2025-03-19 15:01:35.748103 test begin: paddle.vstack(list[Tensor([1, 1, 1],"float16"),Tensor([1, 1, 4294967297],"float16"),Tensor([1, 1, 1],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([1, 1, 1],"float16"),Tensor([1, 1, 4294967297],"float16"),Tensor([1, 1, 1],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 1 but got size 4294967297 for tensor number 1 in the list.
2025-03-19 15:01:37.959410 test begin: paddle.vstack(list[Tensor([1, 1, 1],"float16"),Tensor([1, 4294967297, 1],"float16"),Tensor([1, 1, 1],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([1, 1, 1],"float16"),Tensor([1, 4294967297, 1],"float16"),Tensor([1, 1, 1],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 1 but got size 4294967297 for tensor number 1 in the list.
2025-03-19 15:01:39.548597 test begin: paddle.vstack(list[Tensor([1, 1, 1],"float16"),Tensor([1, 4294967297, 1],"float16"),Tensor([1, 1, 1],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([1, 1, 1],"float16"),Tensor([1, 4294967297, 1],"float16"),Tensor([1, 1, 1],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 1 but got size 4294967297 for tensor number 1 in the list.
2025-03-19 15:01:41.500742 test begin: paddle.vstack(list[Tensor([1, 1, 1],"float16"),Tensor([4294967297, 1, 1],"float16"),Tensor([1, 1, 1],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([1, 1, 1],"float16"),Tensor([4294967297, 1, 1],"float16"),Tensor([1, 1, 1],"float16"),], )      
2025-03-19 15:17:54.846398 test begin: paddle.vstack(list[Tensor([1, 1, 1],"float16"),Tensor([4294967297, 1, 1],"float16"),Tensor([1, 1, 1],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([1, 1, 1],"float16"),Tensor([4294967297, 1, 1],"float16"),Tensor([1, 1, 1],"float16"),], name=None, )      
2025-03-19 15:34:50.398675 test begin: paddle.vstack(list[Tensor([1, 1, 4294967297, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([1, 1, 4294967297, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 4294967297 but got size 1 for tensor number 1 in the list.
2025-03-19 15:34:54.307386 test begin: paddle.vstack(list[Tensor([1, 1, 4294967297, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([1, 1, 4294967297, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 4294967297 but got size 1 for tensor number 1 in the list.
2025-03-19 15:34:55.631393 test begin: paddle.vstack(list[Tensor([1, 1, 4294967297, 1],"float16"),Tensor([1, 1, 4294967297, 1],"float16"),Tensor([1, 1, 4294967297, 1],"float16"),], )      

2025-03-19 15:35:02.837879 test begin: paddle.vstack(list[Tensor([1, 1, 4294967297, 1],"float16"),Tensor([1, 1, 4294967297, 1],"float16"),Tensor([1, 1, 4294967297, 1],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([1, 1, 4294967297, 1],"float16"),Tensor([1, 1, 4294967297, 1],"float16"),Tensor([1, 1, 4294967297, 1],"float16"),], name=None, )       
 CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 4.46 GiB is free. Process 75462 has 73.60 GiB memory in use. Of the allocated memory 72.00 GiB is allocated by PyTorch, and 14.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-19 15:35:09.242937 test begin: paddle.vstack(list[Tensor([1, 1, 4294967297, 1],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([1, 1, 4294967297, 1],"float16"),], )      
2025-03-19 15:52:14.981395 test begin: paddle.vstack(list[Tensor([1, 1, 4294967297, 1],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([1, 1, 4294967297, 1],"float16"),], name=None, )      
2025-03-19 16:09:03.154238 test begin: paddle.vstack(list[Tensor([1, 1, 4294967297],"float16"),Tensor([1, 1, 1],"float16"),Tensor([1, 1, 1],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([1, 1, 4294967297],"float16"),Tensor([1, 1, 1],"float16"),Tensor([1, 1, 1],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 4294967297 but got size 1 for tensor number 1 in the list.
2025-03-19 16:09:07.606529 test begin: paddle.vstack(list[Tensor([1, 1, 4294967297],"float16"),Tensor([1, 1, 1],"float16"),Tensor([1, 1, 1],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([1, 1, 4294967297],"float16"),Tensor([1, 1, 1],"float16"),Tensor([1, 1, 1],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 4294967297 but got size 1 for tensor number 1 in the list.
2025-03-19 16:09:09.111096 test begin: paddle.vstack(list[Tensor([1, 1, 4294967297],"float16"),Tensor([1, 1, 4294967297],"float16"),Tensor([1, 1, 4294967297],"float16"),], )      

2025-03-19 16:09:14.096127 test begin: paddle.vstack(list[Tensor([1, 1, 4294967297],"float16"),Tensor([1, 1, 4294967297],"float16"),Tensor([1, 1, 4294967297],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([1, 1, 4294967297],"float16"),Tensor([1, 1, 4294967297],"float16"),Tensor([1, 1, 4294967297],"float16"),], name=None, )       
 CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 4.46 GiB is free. Process 75462 has 73.60 GiB memory in use. Of the allocated memory 72.00 GiB is allocated by PyTorch, and 14.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-19 16:09:21.485658 test begin: paddle.vstack(list[Tensor([1, 1, 4294967297],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([1, 1, 4294967297],"float16"),], )      
2025-03-19 16:25:21.208467 test begin: paddle.vstack(list[Tensor([1, 1, 4294967297],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([1, 1, 4294967297],"float16"),], name=None, )      
2025-03-19 16:41:21.830626 test begin: paddle.vstack(list[Tensor([1, 1],"float16"),Tensor([1, 1],"float16"),Tensor([1, 4294967297],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([1, 1],"float16"),Tensor([1, 1],"float16"),Tensor([1, 4294967297],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 1 but got size 4294967297 for tensor number 2 in the list.
2025-03-19 16:41:25.714712 test begin: paddle.vstack(list[Tensor([1, 1],"float16"),Tensor([1, 1],"float16"),Tensor([1, 4294967297],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([1, 1],"float16"),Tensor([1, 1],"float16"),Tensor([1, 4294967297],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 1 but got size 4294967297 for tensor number 2 in the list.
2025-03-19 16:41:27.458188 test begin: paddle.vstack(list[Tensor([1, 1],"float16"),Tensor([1, 1],"float16"),Tensor([4294967297, 1],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([1, 1],"float16"),Tensor([1, 1],"float16"),Tensor([4294967297, 1],"float16"),], )      
2025-03-19 16:57:41.662257 test begin: paddle.vstack(list[Tensor([1, 1],"float16"),Tensor([1, 1],"float16"),Tensor([4294967297, 1],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([1, 1],"float16"),Tensor([1, 1],"float16"),Tensor([4294967297, 1],"float16"),], name=None, )      
2025-03-19 17:13:38.651577 test begin: paddle.vstack(list[Tensor([1, 1],"float16"),Tensor([1, 4294967297],"float16"),Tensor([1, 1],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([1, 1],"float16"),Tensor([1, 4294967297],"float16"),Tensor([1, 1],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 1 but got size 4294967297 for tensor number 1 in the list.
2025-03-19 17:13:42.711910 test begin: paddle.vstack(list[Tensor([1, 1],"float16"),Tensor([1, 4294967297],"float16"),Tensor([1, 1],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([1, 1],"float16"),Tensor([1, 4294967297],"float16"),Tensor([1, 1],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 1 but got size 4294967297 for tensor number 1 in the list.
2025-03-19 17:13:44.899670 test begin: paddle.vstack(list[Tensor([1, 1],"float16"),Tensor([4294967297, 1],"float16"),Tensor([1, 1],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([1, 1],"float16"),Tensor([4294967297, 1],"float16"),Tensor([1, 1],"float16"),], )      
2025-03-19 17:29:37.074881 test begin: paddle.vstack(list[Tensor([1, 1],"float16"),Tensor([4294967297, 1],"float16"),Tensor([1, 1],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([1, 1],"float16"),Tensor([4294967297, 1],"float16"),Tensor([1, 1],"float16"),], name=None, )      
2025-03-19 17:46:13.312906 test begin: paddle.vstack(list[Tensor([1, 4294967297, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([1, 4294967297, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 4294967297 but got size 1 for tensor number 1 in the list.
2025-03-19 17:46:17.252264 test begin: paddle.vstack(list[Tensor([1, 4294967297, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([1, 4294967297, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 4294967297 but got size 1 for tensor number 1 in the list.
2025-03-19 17:46:18.868916 test begin: paddle.vstack(list[Tensor([1, 4294967297, 1, 1],"float16"),Tensor([1, 4294967297, 1, 1],"float16"),Tensor([1, 4294967297, 1, 1],"float16"),], )      

2025-03-19 17:46:25.002492 test begin: paddle.vstack(list[Tensor([1, 4294967297, 1, 1],"float16"),Tensor([1, 4294967297, 1, 1],"float16"),Tensor([1, 4294967297, 1, 1],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([1, 4294967297, 1, 1],"float16"),Tensor([1, 4294967297, 1, 1],"float16"),Tensor([1, 4294967297, 1, 1],"float16"),], name=None, )       
 CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 4.46 GiB is free. Process 75462 has 73.60 GiB memory in use. Of the allocated memory 72.00 GiB is allocated by PyTorch, and 14.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-19 17:46:31.142739 test begin: paddle.vstack(list[Tensor([1, 4294967297, 1, 1],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([1, 4294967297, 1, 1],"float16"),], )      
2025-03-19 18:02:47.423153 test begin: paddle.vstack(list[Tensor([1, 4294967297, 1, 1],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([1, 4294967297, 1, 1],"float16"),], name=None, )      
2025-03-19 18:19:26.393453 test begin: paddle.vstack(list[Tensor([1, 4294967297, 1],"float16"),Tensor([1, 1, 1],"float16"),Tensor([1, 1, 1],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([1, 4294967297, 1],"float16"),Tensor([1, 1, 1],"float16"),Tensor([1, 1, 1],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 4294967297 but got size 1 for tensor number 1 in the list.
2025-03-19 18:19:30.783314 test begin: paddle.vstack(list[Tensor([1, 4294967297, 1],"float16"),Tensor([1, 1, 1],"float16"),Tensor([1, 1, 1],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([1, 4294967297, 1],"float16"),Tensor([1, 1, 1],"float16"),Tensor([1, 1, 1],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 4294967297 but got size 1 for tensor number 1 in the list.
2025-03-19 18:19:33.294059 test begin: paddle.vstack(list[Tensor([1, 4294967297, 1],"float16"),Tensor([1, 4294967297, 1],"float16"),Tensor([1, 4294967297, 1],"float16"),], )      

2025-03-19 18:19:40.968407 test begin: paddle.vstack(list[Tensor([1, 4294967297, 1],"float16"),Tensor([1, 4294967297, 1],"float16"),Tensor([1, 4294967297, 1],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([1, 4294967297, 1],"float16"),Tensor([1, 4294967297, 1],"float16"),Tensor([1, 4294967297, 1],"float16"),], name=None, )       
 CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 4.46 GiB is free. Process 75462 has 73.60 GiB memory in use. Of the allocated memory 72.00 GiB is allocated by PyTorch, and 14.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-19 18:19:47.587284 test begin: paddle.vstack(list[Tensor([1, 4294967297, 1],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([1, 4294967297, 1],"float16"),], )      
2025-03-19 18:35:49.560599 test begin: paddle.vstack(list[Tensor([1, 4294967297, 1],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([1, 4294967297, 1],"float16"),], name=None, )      
2025-03-19 18:52:22.079517 test begin: paddle.vstack(list[Tensor([1, 4294967297],"float16"),Tensor([1, 1],"float16"),Tensor([1, 1],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([1, 4294967297],"float16"),Tensor([1, 1],"float16"),Tensor([1, 1],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 4294967297 but got size 1 for tensor number 1 in the list.
2025-03-19 18:52:26.418752 test begin: paddle.vstack(list[Tensor([1, 4294967297],"float16"),Tensor([1, 1],"float16"),Tensor([1, 1],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([1, 4294967297],"float16"),Tensor([1, 1],"float16"),Tensor([1, 1],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 4294967297 but got size 1 for tensor number 1 in the list.
2025-03-19 18:52:28.573221 test begin: paddle.vstack(list[Tensor([1, 4294967297],"float16"),Tensor([1, 4294967297],"float16"),Tensor([1, 4294967297],"float16"),], )      

2025-03-19 18:52:35.056477 test begin: paddle.vstack(list[Tensor([1, 4294967297],"float16"),Tensor([1, 4294967297],"float16"),Tensor([1, 4294967297],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([1, 4294967297],"float16"),Tensor([1, 4294967297],"float16"),Tensor([1, 4294967297],"float16"),], name=None, )       
 CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 4.46 GiB is free. Process 75462 has 73.60 GiB memory in use. Of the allocated memory 72.00 GiB is allocated by PyTorch, and 14.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-19 18:52:41.529679 test begin: paddle.vstack(list[Tensor([1, 4294967297],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([1, 4294967297],"float16"),], )      
2025-03-19 19:09:08.517359 test begin: paddle.vstack(list[Tensor([1, 4294967297],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([1, 4294967297],"float16"),], name=None, )      
2025-03-19 19:25:13.635304 test begin: paddle.vstack(list[Tensor([107374183, 4, 2, 5],"float16"),Tensor([107374183, 4, 2, 5],"float16"),Tensor([107374183, 4, 2, 5],"float16"),], )      

2025-03-19 19:25:21.849805 test begin: paddle.vstack(list[Tensor([107374183, 4, 2, 5],"float16"),Tensor([107374183, 4, 2, 5],"float16"),Tensor([107374183, 4, 2, 5],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([107374183, 4, 2, 5],"float16"),Tensor([107374183, 4, 2, 5],"float16"),Tensor([107374183, 4, 2, 5],"float16"),], name=None, )       
 CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 4.46 GiB is free. Process 75462 has 73.60 GiB memory in use. Of the allocated memory 72.00 GiB is allocated by PyTorch, and 14.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-19 19:25:27.607961 test begin: paddle.vstack(list[Tensor([107374183, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([107374183, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),], )      
2025-03-19 19:42:09.424286 test begin: paddle.vstack(list[Tensor([107374183, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([107374183, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),], name=None, )      
2025-03-19 19:58:18.528945 test begin: paddle.vstack(list[Tensor([107374183, 4, 2, 5],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([107374183, 4, 2, 5],"float16"),], )      
2025-03-19 20:14:34.673341 test begin: paddle.vstack(list[Tensor([107374183, 4, 2, 5],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([107374183, 4, 2, 5],"float16"),], name=None, )      
2025-03-19 20:31:27.881152 test begin: paddle.vstack(list[Tensor([1],"float16"),Tensor([1],"float16"),Tensor([4294967297],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([1],"float16"),Tensor([1],"float16"),Tensor([4294967297],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 1 but got size 4294967297 for tensor number 2 in the list.
2025-03-19 20:31:31.803727 test begin: paddle.vstack(list[Tensor([1],"float16"),Tensor([1],"float16"),Tensor([4294967297],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([1],"float16"),Tensor([1],"float16"),Tensor([4294967297],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 1 but got size 4294967297 for tensor number 2 in the list.
2025-03-19 20:31:33.393678 test begin: paddle.vstack(list[Tensor([1],"float16"),Tensor([4294967297],"float16"),Tensor([1],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([1],"float16"),Tensor([4294967297],"float16"),Tensor([1],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 1 but got size 4294967297 for tensor number 1 in the list.
2025-03-19 20:31:35.374241 test begin: paddle.vstack(list[Tensor([1],"float16"),Tensor([4294967297],"float16"),Tensor([1],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([1],"float16"),Tensor([4294967297],"float16"),Tensor([1],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 1 but got size 4294967297 for tensor number 1 in the list.
2025-03-19 20:31:37.214105 test begin: paddle.vstack(list[Tensor([2147483649, 2],"float16"),Tensor([2147483649, 2],"float16"),Tensor([2147483649, 2],"float16"),], )      

2025-03-19 20:31:43.491679 test begin: paddle.vstack(list[Tensor([2147483649, 2],"float16"),Tensor([2147483649, 2],"float16"),Tensor([2147483649, 2],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([2147483649, 2],"float16"),Tensor([2147483649, 2],"float16"),Tensor([2147483649, 2],"float16"),], name=None, )       
 CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 4.46 GiB is free. Process 75462 has 73.60 GiB memory in use. Of the allocated memory 72.00 GiB is allocated by PyTorch, and 14.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-19 20:31:49.630502 test begin: paddle.vstack(list[Tensor([2147483649, 2],"float16"),Tensor([3, 2],"float16"),Tensor([3, 2],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([2147483649, 2],"float16"),Tensor([3, 2],"float16"),Tensor([3, 2],"float16"),], )      
2025-03-19 20:49:08.327792 test begin: paddle.vstack(list[Tensor([2147483649, 2],"float16"),Tensor([3, 2],"float16"),Tensor([3, 2],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([2147483649, 2],"float16"),Tensor([3, 2],"float16"),Tensor([3, 2],"float16"),], name=None, )      
2025-03-19 21:06:05.671881 test begin: paddle.vstack(list[Tensor([2147483649, 2],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([2147483649, 2],"float16"),], )      
2025-03-19 21:23:06.193308 test begin: paddle.vstack(list[Tensor([2147483649, 2],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([2147483649, 2],"float16"),], name=None, )      
2025-03-19 21:39:42.464496 test begin: paddle.vstack(list[Tensor([2],"float16"),Tensor([1, 4294967297],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([2],"float16"),Tensor([1, 4294967297],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 2 but got size 4294967297 for tensor number 1 in the list.
2025-03-19 21:39:46.423814 test begin: paddle.vstack(list[Tensor([2],"float16"),Tensor([1, 4294967297],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([2],"float16"),Tensor([1, 4294967297],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 2 but got size 4294967297 for tensor number 1 in the list.
2025-03-19 21:39:48.418503 test begin: paddle.vstack(list[Tensor([2],"float16"),Tensor([2147483649, 2],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([2],"float16"),Tensor([2147483649, 2],"float16"),], )      
2025-03-19 21:56:59.270586 test begin: paddle.vstack(list[Tensor([2],"float16"),Tensor([2147483649, 2],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([2],"float16"),Tensor([2147483649, 2],"float16"),], name=None, )      
2025-03-19 22:13:39.327306 test begin: paddle.vstack(list[Tensor([3, 1431655766],"float16"),Tensor([3, 1431655766],"float16"),Tensor([3, 1431655766],"float16"),], )      

2025-03-19 22:13:49.370570 test begin: paddle.vstack(list[Tensor([3, 1431655766],"float16"),Tensor([3, 1431655766],"float16"),Tensor([3, 1431655766],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([3, 1431655766],"float16"),Tensor([3, 1431655766],"float16"),Tensor([3, 1431655766],"float16"),], name=None, )       
 CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 4.46 GiB is free. Process 75462 has 73.60 GiB memory in use. Of the allocated memory 72.00 GiB is allocated by PyTorch, and 14.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-19 22:13:54.962138 test begin: paddle.vstack(list[Tensor([3, 1431655766],"float16"),Tensor([3, 2],"float16"),Tensor([3, 2],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([3, 1431655766],"float16"),Tensor([3, 2],"float16"),Tensor([3, 2],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 1431655766 but got size 2 for tensor number 1 in the list.
2025-03-19 22:13:57.161090 test begin: paddle.vstack(list[Tensor([3, 1431655766],"float16"),Tensor([3, 2],"float16"),Tensor([3, 2],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([3, 1431655766],"float16"),Tensor([3, 2],"float16"),Tensor([3, 2],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 1431655766 but got size 2 for tensor number 1 in the list.
2025-03-19 22:13:59.421621 test begin: paddle.vstack(list[Tensor([3, 1431655766],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([3, 1431655766],"float16"),], )      
2025-03-19 22:30:22.909290 test begin: paddle.vstack(list[Tensor([3, 1431655766],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([3, 1431655766],"float16"),], name=None, )      
2025-03-19 22:46:41.375378 test begin: paddle.vstack(list[Tensor([3, 143165577, 2, 5],"float16"),Tensor([3, 143165577, 2, 5],"float16"),Tensor([3, 143165577, 2, 5],"float16"),], )      

2025-03-19 22:46:49.206972 test begin: paddle.vstack(list[Tensor([3, 143165577, 2, 5],"float16"),Tensor([3, 143165577, 2, 5],"float16"),Tensor([3, 143165577, 2, 5],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([3, 143165577, 2, 5],"float16"),Tensor([3, 143165577, 2, 5],"float16"),Tensor([3, 143165577, 2, 5],"float16"),], name=None, )       
 CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 4.46 GiB is free. Process 75462 has 73.60 GiB memory in use. Of the allocated memory 72.00 GiB is allocated by PyTorch, and 14.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-19 22:46:54.838688 test begin: paddle.vstack(list[Tensor([3, 143165577, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([3, 143165577, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 143165577 but got size 4 for tensor number 1 in the list.
2025-03-19 22:46:57.040261 test begin: paddle.vstack(list[Tensor([3, 143165577, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([3, 143165577, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 143165577 but got size 4 for tensor number 1 in the list.
2025-03-19 22:46:59.276451 test begin: paddle.vstack(list[Tensor([3, 143165577, 2, 5],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([3, 143165577, 2, 5],"float16"),], )      
2025-03-19 23:03:18.173420 test begin: paddle.vstack(list[Tensor([3, 143165577, 2, 5],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([3, 143165577, 2, 5],"float16"),], name=None, )      
2025-03-19 23:19:39.358721 test begin: paddle.vstack(list[Tensor([3, 2],"float16"),Tensor([2147483649, 2],"float16"),Tensor([3, 2],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([3, 2],"float16"),Tensor([2147483649, 2],"float16"),Tensor([3, 2],"float16"),], )      
2025-03-19 23:36:18.168993 test begin: paddle.vstack(list[Tensor([3, 2],"float16"),Tensor([2147483649, 2],"float16"),Tensor([3, 2],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([3, 2],"float16"),Tensor([2147483649, 2],"float16"),Tensor([3, 2],"float16"),], name=None, )      
2025-03-19 23:52:45.936666 test begin: paddle.vstack(list[Tensor([3, 2],"float16"),Tensor([3, 1431655766],"float16"),Tensor([3, 2],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([3, 2],"float16"),Tensor([3, 1431655766],"float16"),Tensor([3, 2],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 2 but got size 1431655766 for tensor number 1 in the list.
2025-03-19 23:52:49.852505 test begin: paddle.vstack(list[Tensor([3, 2],"float16"),Tensor([3, 1431655766],"float16"),Tensor([3, 2],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([3, 2],"float16"),Tensor([3, 1431655766],"float16"),Tensor([3, 2],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 2 but got size 1431655766 for tensor number 1 in the list.
2025-03-19 23:52:51.779381 test begin: paddle.vstack(list[Tensor([3, 2],"float16"),Tensor([3, 2],"float16"),Tensor([2147483649, 2],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([3, 2],"float16"),Tensor([3, 2],"float16"),Tensor([2147483649, 2],"float16"),], )      
2025-03-20 00:09:10.809417 test begin: paddle.vstack(list[Tensor([3, 2],"float16"),Tensor([3, 2],"float16"),Tensor([2147483649, 2],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([3, 2],"float16"),Tensor([3, 2],"float16"),Tensor([2147483649, 2],"float16"),], name=None, )      
2025-03-20 00:25:15.071641 test begin: paddle.vstack(list[Tensor([3, 2],"float16"),Tensor([3, 2],"float16"),Tensor([3, 1431655766],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([3, 2],"float16"),Tensor([3, 2],"float16"),Tensor([3, 1431655766],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 2 but got size 1431655766 for tensor number 2 in the list.
2025-03-20 00:25:18.964639 test begin: paddle.vstack(list[Tensor([3, 2],"float16"),Tensor([3, 2],"float16"),Tensor([3, 1431655766],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([3, 2],"float16"),Tensor([3, 2],"float16"),Tensor([3, 1431655766],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 2 but got size 1431655766 for tensor number 2 in the list.
2025-03-20 00:25:20.893047 test begin: paddle.vstack(list[Tensor([3, 4, 2, 178956971],"float16"),Tensor([3, 4, 2, 178956971],"float16"),Tensor([3, 4, 2, 178956971],"float16"),], )      

2025-03-20 00:25:26.676941 test begin: paddle.vstack(list[Tensor([3, 4, 2, 178956971],"float16"),Tensor([3, 4, 2, 178956971],"float16"),Tensor([3, 4, 2, 178956971],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([3, 4, 2, 178956971],"float16"),Tensor([3, 4, 2, 178956971],"float16"),Tensor([3, 4, 2, 178956971],"float16"),], name=None, )       
 CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 4.46 GiB is free. Process 75462 has 73.60 GiB memory in use. Of the allocated memory 72.00 GiB is allocated by PyTorch, and 14.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-20 00:25:32.528883 test begin: paddle.vstack(list[Tensor([3, 4, 2, 178956971],"float16"),Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([3, 4, 2, 178956971],"float16"),Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 178956971 but got size 5 for tensor number 1 in the list.
2025-03-20 00:25:34.600269 test begin: paddle.vstack(list[Tensor([3, 4, 2, 178956971],"float16"),Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([3, 4, 2, 178956971],"float16"),Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 178956971 but got size 5 for tensor number 1 in the list.
2025-03-20 00:25:36.897892 test begin: paddle.vstack(list[Tensor([3, 4, 2, 178956971],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([3, 4, 2, 178956971],"float16"),], )      
2025-03-20 00:41:31.152294 test begin: paddle.vstack(list[Tensor([3, 4, 2, 178956971],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([3, 4, 2, 178956971],"float16"),], name=None, )      
2025-03-20 00:57:28.918305 test begin: paddle.vstack(list[Tensor([3, 4, 2, 5],"float16"),Tensor([107374183, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([3, 4, 2, 5],"float16"),Tensor([107374183, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),], )      
2025-03-20 01:13:30.369262 test begin: paddle.vstack(list[Tensor([3, 4, 2, 5],"float16"),Tensor([107374183, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([3, 4, 2, 5],"float16"),Tensor([107374183, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),], name=None, )      
2025-03-20 01:30:52.474585 test begin: paddle.vstack(list[Tensor([3, 4, 2, 5],"float16"),Tensor([3, 143165577, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([3, 4, 2, 5],"float16"),Tensor([3, 143165577, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 4 but got size 143165577 for tensor number 1 in the list.
2025-03-20 01:30:56.837775 test begin: paddle.vstack(list[Tensor([3, 4, 2, 5],"float16"),Tensor([3, 143165577, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([3, 4, 2, 5],"float16"),Tensor([3, 143165577, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 4 but got size 143165577 for tensor number 1 in the list.
2025-03-20 01:30:58.639762 test begin: paddle.vstack(list[Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 178956971],"float16"),Tensor([3, 4, 2, 5],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 178956971],"float16"),Tensor([3, 4, 2, 5],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 5 but got size 178956971 for tensor number 1 in the list.
2025-03-20 01:31:00.665351 test begin: paddle.vstack(list[Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 178956971],"float16"),Tensor([3, 4, 2, 5],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 178956971],"float16"),Tensor([3, 4, 2, 5],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 5 but got size 178956971 for tensor number 1 in the list.
2025-03-20 01:31:02.899499 test begin: paddle.vstack(list[Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),Tensor([107374183, 4, 2, 5],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),Tensor([107374183, 4, 2, 5],"float16"),], )      
2025-03-20 01:46:44.068463 test begin: paddle.vstack(list[Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),Tensor([107374183, 4, 2, 5],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),Tensor([107374183, 4, 2, 5],"float16"),], name=None, )      
2025-03-20 02:04:11.808870 test begin: paddle.vstack(list[Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),Tensor([3, 143165577, 2, 5],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),Tensor([3, 143165577, 2, 5],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 4 but got size 143165577 for tensor number 2 in the list.
2025-03-20 02:04:16.032186 test begin: paddle.vstack(list[Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),Tensor([3, 143165577, 2, 5],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),Tensor([3, 143165577, 2, 5],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 4 but got size 143165577 for tensor number 2 in the list.
2025-03-20 02:04:17.938011 test begin: paddle.vstack(list[Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 178956971],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 178956971],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 5 but got size 178956971 for tensor number 2 in the list.
2025-03-20 02:04:19.827137 test begin: paddle.vstack(list[Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 178956971],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 178956971],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 5 but got size 178956971 for tensor number 2 in the list.
2025-03-20 02:04:21.443661 test begin: paddle.vstack(list[Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 71582789, 5],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 71582789, 5],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 2 but got size 71582789 for tensor number 2 in the list.
2025-03-20 02:04:23.415257 test begin: paddle.vstack(list[Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 71582789, 5],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 71582789, 5],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 2 but got size 71582789 for tensor number 2 in the list.
2025-03-20 02:04:25.382727 test begin: paddle.vstack(list[Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 71582789, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 71582789, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 2 but got size 71582789 for tensor number 1 in the list.
2025-03-20 02:04:27.597054 test begin: paddle.vstack(list[Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 71582789, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 71582789, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 2 but got size 71582789 for tensor number 1 in the list.
2025-03-20 02:04:29.583151 test begin: paddle.vstack(list[Tensor([3, 4, 2],"float16"),Tensor([3, 4, 2],"float16"),Tensor([3, 4, 357913942],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([3, 4, 2],"float16"),Tensor([3, 4, 2],"float16"),Tensor([3, 4, 357913942],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 2 but got size 357913942 for tensor number 2 in the list.
2025-03-20 02:04:31.271273 test begin: paddle.vstack(list[Tensor([3, 4, 2],"float16"),Tensor([3, 4, 2],"float16"),Tensor([3, 4, 357913942],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([3, 4, 2],"float16"),Tensor([3, 4, 2],"float16"),Tensor([3, 4, 357913942],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 2 but got size 357913942 for tensor number 2 in the list.
2025-03-20 02:04:33.526829 test begin: paddle.vstack(list[Tensor([3, 4, 2],"float16"),Tensor([3, 4, 2],"float16"),Tensor([3, 715827883, 2],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([3, 4, 2],"float16"),Tensor([3, 4, 2],"float16"),Tensor([3, 715827883, 2],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 4 but got size 715827883 for tensor number 2 in the list.
2025-03-20 02:04:35.578688 test begin: paddle.vstack(list[Tensor([3, 4, 2],"float16"),Tensor([3, 4, 2],"float16"),Tensor([3, 715827883, 2],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([3, 4, 2],"float16"),Tensor([3, 4, 2],"float16"),Tensor([3, 715827883, 2],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 4 but got size 715827883 for tensor number 2 in the list.
2025-03-20 02:04:37.547542 test begin: paddle.vstack(list[Tensor([3, 4, 2],"float16"),Tensor([3, 4, 2],"float16"),Tensor([536870913, 4, 2],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([3, 4, 2],"float16"),Tensor([3, 4, 2],"float16"),Tensor([536870913, 4, 2],"float16"),], )      
2025-03-20 02:22:03.724783 test begin: paddle.vstack(list[Tensor([3, 4, 2],"float16"),Tensor([3, 4, 2],"float16"),Tensor([536870913, 4, 2],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([3, 4, 2],"float16"),Tensor([3, 4, 2],"float16"),Tensor([536870913, 4, 2],"float16"),], name=None, )      
2025-03-20 02:38:03.614545 test begin: paddle.vstack(list[Tensor([3, 4, 2],"float16"),Tensor([3, 4, 357913942],"float16"),Tensor([3, 4, 2],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([3, 4, 2],"float16"),Tensor([3, 4, 357913942],"float16"),Tensor([3, 4, 2],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 2 but got size 357913942 for tensor number 1 in the list.
2025-03-20 02:38:07.532787 test begin: paddle.vstack(list[Tensor([3, 4, 2],"float16"),Tensor([3, 4, 357913942],"float16"),Tensor([3, 4, 2],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([3, 4, 2],"float16"),Tensor([3, 4, 357913942],"float16"),Tensor([3, 4, 2],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 2 but got size 357913942 for tensor number 1 in the list.
2025-03-20 02:38:09.617708 test begin: paddle.vstack(list[Tensor([3, 4, 2],"float16"),Tensor([3, 715827883, 2],"float16"),Tensor([3, 4, 2],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([3, 4, 2],"float16"),Tensor([3, 715827883, 2],"float16"),Tensor([3, 4, 2],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 4 but got size 715827883 for tensor number 1 in the list.
2025-03-20 02:38:11.011704 test begin: paddle.vstack(list[Tensor([3, 4, 2],"float16"),Tensor([3, 715827883, 2],"float16"),Tensor([3, 4, 2],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([3, 4, 2],"float16"),Tensor([3, 715827883, 2],"float16"),Tensor([3, 4, 2],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 4 but got size 715827883 for tensor number 1 in the list.
2025-03-20 02:38:13.263674 test begin: paddle.vstack(list[Tensor([3, 4, 2],"float16"),Tensor([536870913, 4, 2],"float16"),Tensor([3, 4, 2],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([3, 4, 2],"float16"),Tensor([536870913, 4, 2],"float16"),Tensor([3, 4, 2],"float16"),], )      
2025-03-20 02:54:58.701393 test begin: paddle.vstack(list[Tensor([3, 4, 2],"float16"),Tensor([536870913, 4, 2],"float16"),Tensor([3, 4, 2],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([3, 4, 2],"float16"),Tensor([536870913, 4, 2],"float16"),Tensor([3, 4, 2],"float16"),], name=None, )      
2025-03-20 03:11:42.372535 test begin: paddle.vstack(list[Tensor([3, 4, 357913942],"float16"),Tensor([3, 4, 2],"float16"),Tensor([3, 4, 2],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([3, 4, 357913942],"float16"),Tensor([3, 4, 2],"float16"),Tensor([3, 4, 2],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 357913942 but got size 2 for tensor number 1 in the list.
2025-03-20 03:11:46.311330 test begin: paddle.vstack(list[Tensor([3, 4, 357913942],"float16"),Tensor([3, 4, 2],"float16"),Tensor([3, 4, 2],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([3, 4, 357913942],"float16"),Tensor([3, 4, 2],"float16"),Tensor([3, 4, 2],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 357913942 but got size 2 for tensor number 1 in the list.
2025-03-20 03:11:48.239018 test begin: paddle.vstack(list[Tensor([3, 4, 357913942],"float16"),Tensor([3, 4, 357913942],"float16"),Tensor([3, 4, 357913942],"float16"),], )      

2025-03-20 03:11:54.286705 test begin: paddle.vstack(list[Tensor([3, 4, 357913942],"float16"),Tensor([3, 4, 357913942],"float16"),Tensor([3, 4, 357913942],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([3, 4, 357913942],"float16"),Tensor([3, 4, 357913942],"float16"),Tensor([3, 4, 357913942],"float16"),], name=None, )       
 CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 4.46 GiB is free. Process 75462 has 73.60 GiB memory in use. Of the allocated memory 72.00 GiB is allocated by PyTorch, and 14.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-20 03:12:00.144808 test begin: paddle.vstack(list[Tensor([3, 4, 357913942],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([3, 4, 357913942],"float16"),], )      
2025-03-20 03:28:41.456647 test begin: paddle.vstack(list[Tensor([3, 4, 357913942],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([3, 4, 357913942],"float16"),], name=None, )      
2025-03-20 03:44:56.926777 test begin: paddle.vstack(list[Tensor([3, 4, 71582789, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([3, 4, 71582789, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 71582789 but got size 2 for tensor number 1 in the list.
2025-03-20 03:45:00.944695 test begin: paddle.vstack(list[Tensor([3, 4, 71582789, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([3, 4, 71582789, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),Tensor([3, 4, 2, 5],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 71582789 but got size 2 for tensor number 1 in the list.
2025-03-20 03:45:03.028580 test begin: paddle.vstack(list[Tensor([3, 4, 71582789, 5],"float16"),Tensor([3, 4, 71582789, 5],"float16"),Tensor([3, 4, 71582789, 5],"float16"),], )      

2025-03-20 03:45:08.799011 test begin: paddle.vstack(list[Tensor([3, 4, 71582789, 5],"float16"),Tensor([3, 4, 71582789, 5],"float16"),Tensor([3, 4, 71582789, 5],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([3, 4, 71582789, 5],"float16"),Tensor([3, 4, 71582789, 5],"float16"),Tensor([3, 4, 71582789, 5],"float16"),], name=None, )       
 CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 4.46 GiB is free. Process 75462 has 73.60 GiB memory in use. Of the allocated memory 72.00 GiB is allocated by PyTorch, and 14.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-20 03:45:14.873101 test begin: paddle.vstack(list[Tensor([3, 4, 71582789, 5],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([3, 4, 71582789, 5],"float16"),], )      
2025-03-20 04:01:46.745966 test begin: paddle.vstack(list[Tensor([3, 4, 71582789, 5],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([3, 4, 71582789, 5],"float16"),], name=None, )      
2025-03-20 04:18:08.312589 test begin: paddle.vstack(list[Tensor([3, 715827883, 2],"float16"),Tensor([3, 4, 2],"float16"),Tensor([3, 4, 2],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([3, 715827883, 2],"float16"),Tensor([3, 4, 2],"float16"),Tensor([3, 4, 2],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 715827883 but got size 4 for tensor number 1 in the list.
2025-03-20 04:18:12.663480 test begin: paddle.vstack(list[Tensor([3, 715827883, 2],"float16"),Tensor([3, 4, 2],"float16"),Tensor([3, 4, 2],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([3, 715827883, 2],"float16"),Tensor([3, 4, 2],"float16"),Tensor([3, 4, 2],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 715827883 but got size 4 for tensor number 1 in the list.
2025-03-20 04:18:14.261791 test begin: paddle.vstack(list[Tensor([3, 715827883, 2],"float16"),Tensor([3, 715827883, 2],"float16"),Tensor([3, 715827883, 2],"float16"),], )      

2025-03-20 04:18:20.686378 test begin: paddle.vstack(list[Tensor([3, 715827883, 2],"float16"),Tensor([3, 715827883, 2],"float16"),Tensor([3, 715827883, 2],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([3, 715827883, 2],"float16"),Tensor([3, 715827883, 2],"float16"),Tensor([3, 715827883, 2],"float16"),], name=None, )       
 CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 4.46 GiB is free. Process 75462 has 73.60 GiB memory in use. Of the allocated memory 72.00 GiB is allocated by PyTorch, and 14.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-20 04:18:26.700372 test begin: paddle.vstack(list[Tensor([3, 715827883, 2],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([3, 715827883, 2],"float16"),], )      
2025-03-20 04:34:35.720514 test begin: paddle.vstack(list[Tensor([3, 715827883, 2],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([3, 715827883, 2],"float16"),], name=None, )      
2025-03-20 04:50:36.701406 test begin: paddle.vstack(list[Tensor([4294967297, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([4294967297, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),], )      
2025-03-20 05:06:42.550000 test begin: paddle.vstack(list[Tensor([4294967297, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([4294967297, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),Tensor([1, 1, 1, 1],"float16"),], name=None, )      
2025-03-20 05:22:52.138815 test begin: paddle.vstack(list[Tensor([4294967297, 1, 1, 1],"float16"),Tensor([4294967297, 1, 1, 1],"float16"),Tensor([4294967297, 1, 1, 1],"float16"),], )      

2025-03-20 05:23:00.942241 test begin: paddle.vstack(list[Tensor([4294967297, 1, 1, 1],"float16"),Tensor([4294967297, 1, 1, 1],"float16"),Tensor([4294967297, 1, 1, 1],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([4294967297, 1, 1, 1],"float16"),Tensor([4294967297, 1, 1, 1],"float16"),Tensor([4294967297, 1, 1, 1],"float16"),], name=None, )       
 CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 4.46 GiB is free. Process 75462 has 73.60 GiB memory in use. Of the allocated memory 72.00 GiB is allocated by PyTorch, and 14.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-20 05:23:07.419383 test begin: paddle.vstack(list[Tensor([4294967297, 1, 1, 1],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([4294967297, 1, 1, 1],"float16"),], )      
2025-03-20 05:39:17.734428 test begin: paddle.vstack(list[Tensor([4294967297, 1, 1, 1],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([4294967297, 1, 1, 1],"float16"),], name=None, )      
2025-03-20 05:55:18.155224 test begin: paddle.vstack(list[Tensor([4294967297, 1, 1],"float16"),Tensor([1, 1, 1],"float16"),Tensor([1, 1, 1],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([4294967297, 1, 1],"float16"),Tensor([1, 1, 1],"float16"),Tensor([1, 1, 1],"float16"),], )      
2025-03-20 06:11:23.550847 test begin: paddle.vstack(list[Tensor([4294967297, 1, 1],"float16"),Tensor([1, 1, 1],"float16"),Tensor([1, 1, 1],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([4294967297, 1, 1],"float16"),Tensor([1, 1, 1],"float16"),Tensor([1, 1, 1],"float16"),], name=None, )      
2025-03-20 06:27:27.231637 test begin: paddle.vstack(list[Tensor([4294967297, 1, 1],"float16"),Tensor([4294967297, 1, 1],"float16"),Tensor([4294967297, 1, 1],"float16"),], )      

2025-03-20 06:27:35.517175 test begin: paddle.vstack(list[Tensor([4294967297, 1, 1],"float16"),Tensor([4294967297, 1, 1],"float16"),Tensor([4294967297, 1, 1],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([4294967297, 1, 1],"float16"),Tensor([4294967297, 1, 1],"float16"),Tensor([4294967297, 1, 1],"float16"),], name=None, )       
 CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 4.46 GiB is free. Process 75462 has 73.60 GiB memory in use. Of the allocated memory 72.00 GiB is allocated by PyTorch, and 14.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-20 06:27:41.726110 test begin: paddle.vstack(list[Tensor([4294967297, 1, 1],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([4294967297, 1, 1],"float16"),], )      
2025-03-20 06:43:44.885356 test begin: paddle.vstack(list[Tensor([4294967297, 1, 1],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([4294967297, 1, 1],"float16"),], name=None, )      
2025-03-20 06:59:54.230714 test begin: paddle.vstack(list[Tensor([4294967297, 1],"float16"),Tensor([1, 1],"float16"),Tensor([1, 1],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([4294967297, 1],"float16"),Tensor([1, 1],"float16"),Tensor([1, 1],"float16"),], )      
2025-03-20 07:15:57.786866 test begin: paddle.vstack(list[Tensor([4294967297, 1],"float16"),Tensor([1, 1],"float16"),Tensor([1, 1],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([4294967297, 1],"float16"),Tensor([1, 1],"float16"),Tensor([1, 1],"float16"),], name=None, )      
2025-03-20 07:32:08.643716 test begin: paddle.vstack(list[Tensor([4294967297, 1],"float16"),Tensor([4294967297, 1],"float16"),Tensor([4294967297, 1],"float16"),], )      

2025-03-20 07:32:16.488280 test begin: paddle.vstack(list[Tensor([4294967297, 1],"float16"),Tensor([4294967297, 1],"float16"),Tensor([4294967297, 1],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([4294967297, 1],"float16"),Tensor([4294967297, 1],"float16"),Tensor([4294967297, 1],"float16"),], name=None, )       
 CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 4.46 GiB is free. Process 75462 has 73.60 GiB memory in use. Of the allocated memory 72.00 GiB is allocated by PyTorch, and 14.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-20 07:32:21.861539 test begin: paddle.vstack(list[Tensor([4294967297, 1],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([4294967297, 1],"float16"),], )      
2025-03-20 07:48:28.523650 test begin: paddle.vstack(list[Tensor([4294967297, 1],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([4294967297, 1],"float16"),], name=None, )      
2025-03-20 08:04:34.671442 test begin: paddle.vstack(list[Tensor([4294967297],"float16"),Tensor([1, 2],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([4294967297],"float16"),Tensor([1, 2],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 4294967297 but got size 2 for tensor number 1 in the list.
2025-03-20 08:04:38.615659 test begin: paddle.vstack(list[Tensor([4294967297],"float16"),Tensor([1, 2],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([4294967297],"float16"),Tensor([1, 2],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 4294967297 but got size 2 for tensor number 1 in the list.
2025-03-20 08:04:40.547835 test begin: paddle.vstack(list[Tensor([4294967297],"float16"),Tensor([1],"float16"),Tensor([1],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([4294967297],"float16"),Tensor([1],"float16"),Tensor([1],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 4294967297 but got size 1 for tensor number 1 in the list.
2025-03-20 08:04:42.471908 test begin: paddle.vstack(list[Tensor([4294967297],"float16"),Tensor([1],"float16"),Tensor([1],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([4294967297],"float16"),Tensor([1],"float16"),Tensor([1],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 4294967297 but got size 1 for tensor number 1 in the list.
2025-03-20 08:04:43.805418 test begin: paddle.vstack(list[Tensor([4294967297],"float16"),Tensor([4294967297],"float16"),Tensor([4294967297],"float16"),], )      

2025-03-20 08:04:49.979918 test begin: paddle.vstack(list[Tensor([4294967297],"float16"),Tensor([4294967297],"float16"),Tensor([4294967297],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([4294967297],"float16"),Tensor([4294967297],"float16"),Tensor([4294967297],"float16"),], name=None, )       
 CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 4.46 GiB is free. Process 75462 has 73.60 GiB memory in use. Of the allocated memory 72.00 GiB is allocated by PyTorch, and 14.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-20 08:04:55.965235 test begin: paddle.vstack(list[Tensor([4294967297],"float16"),Tensor([5],"float16"),Tensor([5],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([4294967297],"float16"),Tensor([5],"float16"),Tensor([5],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 4294967297 but got size 5 for tensor number 1 in the list.
2025-03-20 08:04:58.298150 test begin: paddle.vstack(list[Tensor([4294967297],"float16"),Tensor([5],"float16"),Tensor([5],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([4294967297],"float16"),Tensor([5],"float16"),Tensor([5],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 4294967297 but got size 5 for tensor number 1 in the list.
2025-03-20 08:05:00.554052 test begin: paddle.vstack(list[Tensor([4294967297],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([4294967297],"float16"),], )      
2025-03-20 08:21:35.284491 test begin: paddle.vstack(list[Tensor([4294967297],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([4294967297],"float16"),], name=None, )      
2025-03-20 08:37:46.477652 test begin: paddle.vstack(list[Tensor([536870913, 4, 2],"float16"),Tensor([3, 4, 2],"float16"),Tensor([3, 4, 2],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([536870913, 4, 2],"float16"),Tensor([3, 4, 2],"float16"),Tensor([3, 4, 2],"float16"),], )      
2025-03-20 08:53:49.719890 test begin: paddle.vstack(list[Tensor([536870913, 4, 2],"float16"),Tensor([3, 4, 2],"float16"),Tensor([3, 4, 2],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([536870913, 4, 2],"float16"),Tensor([3, 4, 2],"float16"),Tensor([3, 4, 2],"float16"),], name=None, )      
2025-03-20 09:09:56.082250 test begin: paddle.vstack(list[Tensor([536870913, 4, 2],"float16"),Tensor([536870913, 4, 2],"float16"),Tensor([536870913, 4, 2],"float16"),], )      

2025-03-20 09:10:04.012082 test begin: paddle.vstack(list[Tensor([536870913, 4, 2],"float16"),Tensor([536870913, 4, 2],"float16"),Tensor([536870913, 4, 2],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([536870913, 4, 2],"float16"),Tensor([536870913, 4, 2],"float16"),Tensor([536870913, 4, 2],"float16"),], name=None, )       
 CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 4.46 GiB is free. Process 75462 has 73.60 GiB memory in use. Of the allocated memory 72.00 GiB is allocated by PyTorch, and 14.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-20 09:10:09.788036 test begin: paddle.vstack(list[Tensor([536870913, 4, 2],"float16"),], )      

[Pass] paddle.vstack(list[Tensor([536870913, 4, 2],"float16"),], )      
2025-03-20 09:26:21.239333 test begin: paddle.vstack(list[Tensor([536870913, 4, 2],"float16"),], name=None, )      

[Pass] paddle.vstack(list[Tensor([536870913, 4, 2],"float16"),], name=None, )      
2025-03-20 09:42:18.056740 test begin: paddle.vstack(list[Tensor([5],"float16"),Tensor([4294967297],"float16"),Tensor([5],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([5],"float16"),Tensor([4294967297],"float16"),Tensor([5],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 5 but got size 4294967297 for tensor number 1 in the list.
2025-03-20 09:42:21.989429 test begin: paddle.vstack(list[Tensor([5],"float16"),Tensor([4294967297],"float16"),Tensor([5],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([5],"float16"),Tensor([4294967297],"float16"),Tensor([5],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 5 but got size 4294967297 for tensor number 1 in the list.
2025-03-20 09:42:23.580802 test begin: paddle.vstack(list[Tensor([5],"float16"),Tensor([5],"float16"),Tensor([4294967297],"float16"),], )      

[torch error] paddle.vstack(list[Tensor([5],"float16"),Tensor([5],"float16"),Tensor([4294967297],"float16"),], )       
 Sizes of tensors must match except in dimension 0. Expected size 5 but got size 4294967297 for tensor number 2 in the list.
2025-03-20 09:42:25.573712 test begin: paddle.vstack(list[Tensor([5],"float16"),Tensor([5],"float16"),Tensor([4294967297],"float16"),], name=None, )      

[torch error] paddle.vstack(list[Tensor([5],"float16"),Tensor([5],"float16"),Tensor([4294967297],"float16"),], name=None, )       
 Sizes of tensors must match except in dimension 0. Expected size 5 but got size 4294967297 for tensor number 2 in the list.
2025-03-20 09:42:27.380031 test begin: paddle.zeros_like(Tensor([1, 1, 2281701379],"float32"), )      

[Pass] paddle.zeros_like(Tensor([1, 1, 2281701379],"float32"), )      
2025-03-20 09:43:50.023930 test begin: paddle.zeros_like(Tensor([1, 221848, 10285],"float32"), )      

[Pass] paddle.zeros_like(Tensor([1, 221848, 10285],"float32"), )      
2025-03-20 09:45:14.957739 test begin: paddle.zeros_like(Tensor([1, 2281701379],"float32"), )      

[Pass] paddle.zeros_like(Tensor([1, 2281701379],"float32"), )      
2025-03-20 09:46:53.497249 test begin: paddle.zeros_like(Tensor([1, 2281701379],"int32"), )      

[Pass] paddle.zeros_like(Tensor([1, 2281701379],"int32"), )      
2025-03-20 09:49:31.011124 test begin: paddle.zeros_like(Tensor([1, 547828, 4165],"float32"), )      

[Pass] paddle.zeros_like(Tensor([1, 547828, 4165],"float32"), )      
2025-03-20 09:50:57.224348 test begin: paddle.zeros_like(Tensor([1, 59417, 38402],"float32"), )      

[Pass] paddle.zeros_like(Tensor([1, 59417, 38402],"float32"), )      
2025-03-20 09:52:39.811737 test begin: paddle.zeros_like(Tensor([1, 61906, 36858],"float32"), )      

[Pass] paddle.zeros_like(Tensor([1, 61906, 36858],"float32"), )      
2025-03-20 09:54:03.391822 test begin: paddle.zeros_like(Tensor([1, 634159, 3598],"float32"), )      

[Pass] paddle.zeros_like(Tensor([1, 634159, 3598],"float32"), )      
2025-03-20 09:55:36.070751 test begin: paddle.zeros_like(Tensor([1024, 1024, 1, 2177],"float32"), )      

[Pass] paddle.zeros_like(Tensor([1024, 1024, 1, 2177],"float32"), )      
2025-03-20 09:57:01.914866 test begin: paddle.zeros_like(Tensor([1024, 1024, 1, 4097],"float16"), )      

[Pass] paddle.zeros_like(Tensor([1024, 1024, 1, 4097],"float16"), )      
2025-03-20 10:05:13.533041 test begin: paddle.zeros_like(Tensor([1024, 1024, 2177, 1],"float32"), )      

[Pass] paddle.zeros_like(Tensor([1024, 1024, 2177, 1],"float32"), )      
2025-03-20 10:06:54.774378 test begin: paddle.zeros_like(Tensor([1024, 1024, 4097, 1],"float16"), )      

[Pass] paddle.zeros_like(Tensor([1024, 1024, 4097, 1],"float16"), )      
2025-03-20 10:14:50.102300 test begin: paddle.zeros_like(Tensor([1024, 2048, 1, 1089],"float32"), )      

[Pass] paddle.zeros_like(Tensor([1024, 2048, 1, 1089],"float32"), )      
2025-03-20 10:16:20.428536 test begin: paddle.zeros_like(Tensor([1024, 2048, 1, 2049],"float16"), )      

[Pass] paddle.zeros_like(Tensor([1024, 2048, 1, 2049],"float16"), )      
2025-03-20 10:24:35.247583 test begin: paddle.zeros_like(Tensor([1024, 2048, 1089, 1],"float32"), )      

[Pass] paddle.zeros_like(Tensor([1024, 2048, 1089, 1],"float32"), )      
2025-03-20 10:25:57.475553 test begin: paddle.zeros_like(Tensor([1024, 2048, 2049, 1],"float16"), )      

[Pass] paddle.zeros_like(Tensor([1024, 2048, 2049, 1],"float16"), )      
2025-03-20 10:34:11.498456 test begin: paddle.zeros_like(Tensor([1024, 2228225, 1, 1],"float32"), )      

[Pass] paddle.zeros_like(Tensor([1024, 2228225, 1, 1],"float32"), )      
2025-03-20 10:35:36.020405 test begin: paddle.zeros_like(Tensor([1024, 4194305, 1, 1],"float16"), )      

[Pass] paddle.zeros_like(Tensor([1024, 4194305, 1, 1],"float16"), )      
2025-03-20 10:43:40.899534 test begin: paddle.zeros_like(Tensor([1024, 512, 1, 8193],"float16"), )      

[Pass] paddle.zeros_like(Tensor([1024, 512, 1, 8193],"float16"), )      
2025-03-20 10:51:50.922559 test begin: paddle.zeros_like(Tensor([1024, 512, 8193, 1],"float16"), )      

[Pass] paddle.zeros_like(Tensor([1024, 512, 8193, 1],"float16"), )      
2025-03-20 10:59:47.248202 test begin: paddle.zeros_like(Tensor([107374183, 40],"float16"), dtype=type(int), )      

[Pass] paddle.zeros_like(Tensor([107374183, 40],"float16"), dtype=type(int), )      
2025-03-20 11:05:10.352947 test begin: paddle.zeros_like(Tensor([1114113, 2048, 1, 1],"float32"), )      

[Pass] paddle.zeros_like(Tensor([1114113, 2048, 1, 1],"float32"), )      
2025-03-20 11:06:51.736453 test begin: paddle.zeros_like(Tensor([1760573, 8, 9, 18],"float32"), dtype=type(int), )      

[Pass] paddle.zeros_like(Tensor([1760573, 8, 9, 18],"float32"), dtype=type(int), )      
2025-03-20 11:09:30.890089 test begin: paddle.zeros_like(Tensor([17895698, 2, 3, 4, 5, 1, 2],"float16"), )      

[Pass] paddle.zeros_like(Tensor([17895698, 2, 3, 4, 5, 1, 2],"float16"), )      
2025-03-20 11:17:36.287847 test begin: paddle.zeros_like(Tensor([2097153, 2048, 1, 1],"float16"), )      

[Pass] paddle.zeros_like(Tensor([2097153, 2048, 1, 1],"float16"), )      
2025-03-20 11:25:57.445856 test begin: paddle.zeros_like(Tensor([221848, 1, 10285],"float32"), )      

[Pass] paddle.zeros_like(Tensor([221848, 1, 10285],"float32"), )      
2025-03-20 11:27:35.282411 test begin: paddle.zeros_like(Tensor([2228225, 1024, 1, 1],"float32"), )      

[Pass] paddle.zeros_like(Tensor([2228225, 1024, 1, 1],"float32"), )      
2025-03-20 11:28:57.074772 test begin: paddle.zeros_like(Tensor([2274877, 1003],"int32"), )      

[Pass] paddle.zeros_like(Tensor([2274877, 1003],"int32"), )      
2025-03-20 11:30:54.328895 test begin: paddle.zeros_like(Tensor([2277148, 1002],"float32"), )      

[Pass] paddle.zeros_like(Tensor([2277148, 1002],"float32"), )      
2025-03-20 11:32:35.625360 test begin: paddle.zeros_like(Tensor([2277148, 1002],"int32"), )      

[Pass] paddle.zeros_like(Tensor([2277148, 1002],"int32"), )      
2025-03-20 11:34:27.497658 test begin: paddle.zeros_like(Tensor([2279422, 1001],"float32"), )      

[Pass] paddle.zeros_like(Tensor([2279422, 1001],"float32"), )      
2025-03-20 11:36:02.109870 test begin: paddle.zeros_like(Tensor([2279422, 1001],"int32"), )      

[Pass] paddle.zeros_like(Tensor([2279422, 1001],"int32"), )      
2025-03-20 11:38:11.894792 test begin: paddle.zeros_like(Tensor([2281701379],"float32"), )      

[Pass] paddle.zeros_like(Tensor([2281701379],"float32"), )      
2025-03-20 11:39:40.916627 test begin: paddle.zeros_like(Tensor([2281701379],"float32"), dtype=type(int), )      

[Pass] paddle.zeros_like(Tensor([2281701379],"float32"), dtype=type(int), )      
2025-03-20 11:42:25.352359 test begin: paddle.zeros_like(Tensor([3, 11930465, 3, 4, 5, 1, 2],"float16"), )      

Traceback (most recent call last):
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 147, in <module>
    main()
  File "/host_home/wanghuan29/APItest3/PaddleAPITest/engine.py", line 122, in main
    case.test()
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 185, in <lambda>
    return wraps(func)(lambda *args, **kwargs : func_timeout(defaultTimeout, func, args=args, kwargs=kwargs))
  File "/usr/local/lib/python3.9/dist-packages/func_timeout/dafunc.py", line 86, in func_timeout
    thread.join(timeout)
  File "/usr/lib/python3.9/threading.py", line 1064, in join
    self._wait_for_tstate_lock(timeout=max(timeout, 0))
  File "/usr/lib/python3.9/threading.py", line 1080, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt
2025-03-20 11:49:27.095035 test begin: paddle.zeros_like(Tensor([3, 1431655766],"float16"), dtype=type(int), )      

W0320 11:51:17.785696 138817 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0320 11:51:17.786633 138817 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.zeros_like(Tensor([3, 1431655766],"float16"), dtype=type(int), )      
2025-03-20 11:55:53.079656 test begin: paddle.zeros_like(Tensor([3, 2, 17895698, 4, 5, 1, 2],"float16"), )      

[Pass] paddle.zeros_like(Tensor([3, 2, 17895698, 4, 5, 1, 2],"float16"), )      
2025-03-20 12:03:49.466060 test begin: paddle.zeros_like(Tensor([3, 2, 3, 23860930, 5, 1, 2],"float16"), )      

[Pass] paddle.zeros_like(Tensor([3, 2, 3, 23860930, 5, 1, 2],"float16"), )      
2025-03-20 12:11:45.502957 test begin: paddle.zeros_like(Tensor([3, 2, 3, 4, 29826162, 1, 2],"float16"), )      

[Pass] paddle.zeros_like(Tensor([3, 2, 3, 4, 29826162, 1, 2],"float16"), )      
2025-03-20 12:19:42.309690 test begin: paddle.zeros_like(Tensor([3, 2, 3, 4, 5, 1, 11930465],"float16"), )      

[Pass] paddle.zeros_like(Tensor([3, 2, 3, 4, 5, 1, 11930465],"float16"), )      
2025-03-20 12:27:37.424749 test begin: paddle.zeros_like(Tensor([3, 2, 3, 4, 5, 5965233, 2],"float16"), )      

[Pass] paddle.zeros_like(Tensor([3, 2, 3, 4, 5, 5965233, 2],"float16"), )      
2025-03-20 12:35:32.658885 test begin: paddle.zeros_like(Tensor([3, 760567127],"float32"), dtype=type(int), )      

[Pass] paddle.zeros_like(Tensor([3, 760567127],"float32"), dtype=type(int), )      
2025-03-20 12:39:35.941005 test begin: paddle.zeros_like(Tensor([30, 76056713],"float32"), dtype=type(int), )      

[Pass] paddle.zeros_like(Tensor([30, 76056713],"float32"), dtype=type(int), )      
2025-03-20 12:42:37.053666 test begin: paddle.zeros_like(Tensor([300, 14316558],"float16"), dtype=type(int), )      

[Pass] paddle.zeros_like(Tensor([300, 14316558],"float16"), dtype=type(int), )      
2025-03-20 12:47:34.834723 test begin: paddle.zeros_like(Tensor([3314018, 8, 9, 18],"float16"), dtype=type(int), )      

[Pass] paddle.zeros_like(Tensor([3314018, 8, 9, 18],"float16"), dtype=type(int), )      
2025-03-20 12:52:20.290515 test begin: paddle.zeros_like(Tensor([4194305, 1024, 1, 1],"float16"), )      

[Pass] paddle.zeros_like(Tensor([4194305, 1024, 1, 1],"float16"), )      
2025-03-20 13:00:16.912476 test begin: paddle.zeros_like(Tensor([4294967297, 1],"float16"), dtype=type(int), )      

[Pass] paddle.zeros_like(Tensor([4294967297, 1],"float16"), dtype=type(int), )      
2025-03-20 13:05:20.537472 test begin: paddle.zeros_like(Tensor([4294967297],"float16"), )      

[Pass] paddle.zeros_like(Tensor([4294967297],"float16"), )      
2025-03-20 13:13:16.997531 test begin: paddle.zeros_like(Tensor([4294967297],"float16"), dtype=type(int), )      

[Pass] paddle.zeros_like(Tensor([4294967297],"float16"), dtype=type(int), )      
2025-03-20 14:08:04.512544 test begin: paddle.sinc(Tensor([16, 142606337],"float32"), )

CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.45 GiB is free. Process 150929 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 16.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-20 14:09:53.775387 test begin: paddle.sinc(Tensor([16, 268435457],"float16"), )

CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 4.45 GiB is free. Process 65955 has 73.60 GiB memory in use. Of the allocated memory 72.00 GiB is allocated by PyTorch, and 18.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-20 14:12:23.677124 test begin: paddle.sinc(Tensor([2281701379],"float32"), )

CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.45 GiB is free. Process 1990 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 16.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-20 14:13:48.225608 test begin: paddle.sinc(Tensor([35651585, 64],"float32"), )

CUDA out of memory. Tried to allocate 8.50 GiB. GPU 0 has a total capacity of 79.18 GiB of which 8.45 GiB is free. Process 51391 has 69.60 GiB memory in use. Of the allocated memory 68.00 GiB is allocated by PyTorch, and 16.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-20 14:15:09.935079 test begin: paddle.sinc(Tensor([4294967297],"float16"), )

CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 4.45 GiB is free. Process 92023 has 73.60 GiB memory in use. Of the allocated memory 72.00 GiB is allocated by PyTorch, and 18.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-20 14:17:20.230586 test begin: paddle.sinc(Tensor([67108865, 64],"float16"), )

CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 79.18 GiB of which 4.45 GiB is free. Process 3688 has 73.60 GiB memory in use. Of the allocated memory 72.00 GiB is allocated by PyTorch, and 18.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-20 16:56:41.454035 test begin: paddle.squeeze(Tensor([2097153, 2048, 1, 1],"float16"), axis=list[2,3,], )

W0320 16:58:20.323730 62186 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0320 16:58:20.325110 62186 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([2097153, 2048, 1, 1],"float16"), axis=list[2,3,], )
2025-03-20 17:13:42.562152 test begin: paddle.squeeze(Tensor([2177, 1, 1024, 1024],"int32"), axis=1, )

[Pass] paddle.squeeze(Tensor([2177, 1, 1024, 1024],"int32"), axis=1, )
2025-03-20 17:16:55.318543 test begin: paddle.squeeze(Tensor([2177, 1, 1024, 1024],"int64"), axis=1, )

[Pass] paddle.squeeze(Tensor([2177, 1, 1024, 1024],"int64"), axis=1, )
2025-03-20 17:21:14.762709 test begin: paddle.squeeze(Tensor([2177, 1024, 1024, 1],"float32"), axis=-1, )

[Pass] paddle.squeeze(Tensor([2177, 1024, 1024, 1],"float32"), axis=-1, )
2025-03-20 17:25:08.064367 test begin: paddle.squeeze(Tensor([2177, 8, 16, 128, 64],"float32"), axis=0, )

[Pass] paddle.squeeze(Tensor([2177, 8, 16, 128, 64],"float32"), axis=0, )
2025-03-20 17:27:59.545513 test begin: paddle.squeeze(Tensor([22737, 2048, 1, 7, 7],"float32"), axis=2, )

[Pass] paddle.squeeze(Tensor([22737, 2048, 1, 7, 7],"float32"), axis=2, )
2025-03-20 17:30:55.139187 test begin: paddle.squeeze(Tensor([2281701379, 1],"float32"), axis=-1, )

[Pass] paddle.squeeze(Tensor([2281701379, 1],"float32"), axis=-1, )
2025-03-20 17:33:48.573369 test begin: paddle.squeeze(Tensor([2281701379, 1],"float32"), axis=1, )

[Pass] paddle.squeeze(Tensor([2281701379, 1],"float32"), axis=1, )
2025-03-20 17:36:45.870090 test begin: paddle.squeeze(Tensor([2281701379, 1],"float32"), axis=list[1,], )

[Pass] paddle.squeeze(Tensor([2281701379, 1],"float32"), axis=list[1,], )
2025-03-20 17:39:43.643106 test begin: paddle.squeeze(Tensor([2281701379, 1],"float32"), list[-1,], )

[Pass] paddle.squeeze(Tensor([2281701379, 1],"float32"), list[-1,], )
2025-03-20 17:42:41.928965 test begin: paddle.squeeze(Tensor([2281701379, 1],"int32"), axis=-1, )

[Pass] paddle.squeeze(Tensor([2281701379, 1],"int32"), axis=-1, )
2025-03-20 17:45:02.873135 test begin: paddle.squeeze(Tensor([2281701379],"float32"), )

[Pass] paddle.squeeze(Tensor([2281701379],"float32"), )
2025-03-20 17:47:51.593865 test begin: paddle.squeeze(Tensor([2281701379],"float32"), axis=-1, )

[Pass] paddle.squeeze(Tensor([2281701379],"float32"), axis=-1, )
2025-03-20 17:50:47.287514 test begin: paddle.squeeze(Tensor([2281701379],"float32"), axis=0, )

[Pass] paddle.squeeze(Tensor([2281701379],"float32"), axis=0, )
2025-03-20 17:53:43.007456 test begin: paddle.squeeze(Tensor([22817014, 100, 1],"float32"), -1, )

[Pass] paddle.squeeze(Tensor([22817014, 100, 1],"float32"), -1, )
2025-03-20 17:56:38.464073 test begin: paddle.squeeze(Tensor([228171, 1, 100, 100],"float32"), axis=list[1,], )

[Pass] paddle.squeeze(Tensor([228171, 1, 100, 100],"float32"), axis=list[1,], )
2025-03-20 17:59:36.466759 test begin: paddle.squeeze(Tensor([228171, 1, 10000],"float32"), )

[Pass] paddle.squeeze(Tensor([228171, 1, 10000],"float32"), )
2025-03-20 18:02:28.655627 test begin: paddle.squeeze(Tensor([23767723, 1, 96],"float32"), axis=1, )

[Pass] paddle.squeeze(Tensor([23767723, 1, 96],"float32"), axis=1, )
2025-03-20 18:05:26.737013 test begin: paddle.squeeze(Tensor([23860930, 6, 3, 1, 2, 5],"float16"), axis=3, )

[Pass] paddle.squeeze(Tensor([23860930, 6, 3, 1, 2, 5],"float16"), axis=3, )
2025-03-20 18:22:04.754155 test begin: paddle.squeeze(Tensor([24, 1, 95070891],"float32"), axis=1, )

[Pass] paddle.squeeze(Tensor([24, 1, 95070891],"float32"), axis=1, )
2025-03-20 18:25:05.142997 test begin: paddle.squeeze(Tensor([24, 990322, 96],"float32"), axis=1, )

[Pass] paddle.squeeze(Tensor([24, 990322, 96],"float32"), axis=1, )
2025-03-20 18:27:57.769996 test begin: paddle.squeeze(Tensor([24422, 1, 93431],"float32"), axis=list[1,], )

[Pass] paddle.squeeze(Tensor([24422, 1, 93431],"float32"), axis=list[1,], )
2025-03-20 18:30:51.910005 test begin: paddle.squeeze(Tensor([253522376, 3, 3],"float32"), axis=0, )

[Pass] paddle.squeeze(Tensor([253522376, 3, 3],"float32"), axis=0, )
2025-03-20 18:33:23.445895 test begin: paddle.squeeze(Tensor([256, 8912897],"float32"), list[-1,], )

[Pass] paddle.squeeze(Tensor([256, 8912897],"float32"), list[-1,], )
2025-03-20 18:36:06.469995 test begin: paddle.squeeze(Tensor([274878, 125, 125, 1],"float16"), -1, )

[Pass] paddle.squeeze(Tensor([274878, 125, 125, 1],"float16"), -1, )
2025-03-20 18:52:13.947433 test begin: paddle.squeeze(Tensor([285212673, 1, 8],"float32"), list[1,], )

[Pass] paddle.squeeze(Tensor([285212673, 1, 8],"float32"), list[1,], )
2025-03-20 18:55:07.721535 test begin: paddle.squeeze(Tensor([29826162, 6, 3, 4, 2, 1],"float16"), axis=5, )

[Pass] paddle.squeeze(Tensor([29826162, 6, 3, 4, 2, 1],"float16"), axis=5, )
2025-03-20 19:11:41.670243 test begin: paddle.squeeze(Tensor([3, 1, 158452, 1600, 3],"float32"), axis=1, )

[Pass] paddle.squeeze(Tensor([3, 1, 158452, 1600, 3],"float32"), axis=1, )
2025-03-20 19:14:41.044474 test begin: paddle.squeeze(Tensor([3, 1, 158452, 3, 1600],"float32"), axis=1, )

[Pass] paddle.squeeze(Tensor([3, 1, 158452, 3, 1600],"float32"), axis=1, )
2025-03-20 19:17:43.480869 test begin: paddle.squeeze(Tensor([3, 1, 3, 158452, 1600],"float32"), axis=1, )

[Pass] paddle.squeeze(Tensor([3, 1, 3, 158452, 1600],"float32"), axis=1, )
2025-03-20 19:21:08.885955 test begin: paddle.squeeze(Tensor([3, 6, 3, 4, 19884108, 1],"float16"), axis=5, )

W0320 19:22:54.103674 159864 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 12.0, Runtime API Version: 11.8
W0320 19:22:54.104795 159864 gpu_resources.cc:164] device: 0, cuDNN Version: 8.6.
[Pass] paddle.squeeze(Tensor([3, 6, 3, 4, 19884108, 1],"float16"), axis=5, )
2025-03-20 19:39:21.668354 test begin: paddle.squeeze(Tensor([3, 6, 3, 4, 2, 9942054],"float16"), axis=5, )

[Pass] paddle.squeeze(Tensor([3, 6, 3, 4, 2, 9942054],"float16"), axis=5, )
2025-03-20 19:55:56.483290 test begin: paddle.squeeze(Tensor([3, 6, 3, 4, 3976822, 5],"float16"), axis=4, )

[Pass] paddle.squeeze(Tensor([3, 6, 3, 4, 3976822, 5],"float16"), axis=4, )
2025-03-20 20:12:40.011575 test begin: paddle.squeeze(Tensor([3, 6, 3, 7953644, 2, 5],"float16"), axis=3, )

[Pass] paddle.squeeze(Tensor([3, 6, 3, 7953644, 2, 5],"float16"), axis=3, )
2025-03-20 20:29:16.173696 test begin: paddle.squeeze(Tensor([3, 760567127, 1],"float32"), axis=2, )

[Pass] paddle.squeeze(Tensor([3, 760567127, 1],"float32"), axis=2, )
