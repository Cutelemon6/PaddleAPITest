paddle.nn.functional.batch_norm(Tensor([2, 1, 2, 3],"float32"), Tensor([1],"float32"), Tensor([1],"float32"), Tensor([1],"float32"), Tensor([1],"float32"), training=True, momentum=0.9, epsilon=1e-05, data_format="NCHW", use_global_stats=True, )
paddle.nn.functional.batch_norm(Tensor([2, 4, 3, 3],"float64"), Tensor([3],"float64"), Tensor([3],"float64"), Tensor([3],"float64"), Tensor([3],"float64"), training=False, momentum=0.9, epsilon=1e-05, data_format="NHWC", use_global_stats=False, )
paddle.nn.functional.batch_norm(Tensor([2, 4, 3, 3],"float64"), Tensor([3],"float64"), Tensor([3],"float64"), Tensor([3],"float64"), Tensor([3],"float64"), training=True, momentum=0.9, epsilon=1e-05, data_format="NHWC", use_global_stats=True, )
paddle.nn.functional.batch_norm(Tensor([4, 6, 12, 24],"float32"), Tensor([6],"float32"), Tensor([6],"float32"), Tensor([6],"float32"), Tensor([6],"float32"), training=False, momentum=0.9, epsilon=1e-05, data_format="NCHW", use_global_stats=False, )
paddle.nn.functional.batch_norm(Tensor([4, 6, 12, 24],"float32"), Tensor([6],"float32"), Tensor([6],"float32"), Tensor([6],"float32"), Tensor([6],"float32"), training=True, momentum=0.9, epsilon=1e-05, data_format="NCHW", use_global_stats=True, )
paddle.nn.functional.batch_norm(Tensor([4, 6, 12, 24],"float64"), Tensor([6],"float64"), Tensor([6],"float64"), Tensor([6],"float64"), Tensor([6],"float64"), training=False, momentum=0.9, epsilon=1e-05, data_format="NCHW", use_global_stats=False, )
paddle.nn.functional.batch_norm(Tensor([4, 6, 12, 24],"float64"), Tensor([6],"float64"), Tensor([6],"float64"), Tensor([6],"float64"), Tensor([6],"float64"), training=True, momentum=0.9, epsilon=1e-05, data_format="NCHW", use_global_stats=True, )
paddle.nn.functional.batch_norm(Tensor([8, 8, 16, 16],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), Tensor([8],"float32"), training=True, momentum=0.9, epsilon=1e-05, data_format="NCHW", use_global_stats=True, )
paddle.nn.functional.batch_norm(Tensor([8, 8, 16, 16],"float64"), Tensor([16],"float64"), Tensor([16],"float64"), Tensor([16],"float64"), Tensor([16],"float64"), training=False, momentum=0.9, epsilon=1e-05, data_format="NHWC", use_global_stats=False, )
paddle.nn.functional.batch_norm(Tensor([8, 8, 16, 16],"float64"), Tensor([16],"float64"), Tensor([16],"float64"), Tensor([16],"float64"), Tensor([16],"float64"), training=True, momentum=0.9, epsilon=1e-05, data_format="NHWC", use_global_stats=True, )
paddle.nn.functional.gelu(Tensor([128, 96, 96, 768],"float32"), False, None, )
paddle.nn.functional.conv2d(Tensor([1, 1024, 256, 256],"float32"), Tensor([1024, 128, 3, 3],"float32"), padding=1, groups=8, )
paddle.nn.functional.conv2d(Tensor([10, 1, 37, 293],"float32"), Tensor([64, 1, 7, 7],"float32"), None, list[2,2,], 0, list[1,1,], 1, )
paddle.nn.functional.conv2d(Tensor([10, 1, 69, 357],"float32"), Tensor([64, 1, 7, 7],"float32"), None, list[2,2,], 0, list[1,1,], 1, )
paddle.nn.functional.conv2d(Tensor([10, 1, 69, 517],"float32"), Tensor([64, 1, 7, 7],"float32"), None, list[2,2,], 0, list[1,1,], 1, )
paddle.nn.functional.conv2d(Tensor([10, 1, 69, 549],"float32"), Tensor([64, 1, 7, 7],"float32"), None, list[2,2,], 0, list[1,1,], 1, )
paddle.nn.functional.conv2d(Tensor([3, 1, 69, 421],"float32"), Tensor([64, 1, 7, 7],"float32"), None, list[2,2,], 0, list[1,1,], 1, )
paddle.nn.functional.conv2d(Tensor([5, 1, 69, 581],"float32"), Tensor([64, 1, 7, 7],"float32"), None, list[2,2,], 0, list[1,1,], 1, )
paddle.nn.functional.conv2d(Tensor([7, 1, 69, 613],"float32"), Tensor([64, 1, 7, 7],"float32"), None, list[2,2,], 0, list[1,1,], 1, )
paddle.nn.functional.conv2d(Tensor([8, 1, 69, 453],"float32"), Tensor([64, 1, 7, 7],"float32"), None, list[2,2,], 0, list[1,1,], 1, )
paddle.nn.functional.conv2d(Tensor([8, 1, 69, 485],"float32"), Tensor([64, 1, 7, 7],"float32"), None, list[2,2,], 0, list[1,1,], 1, )
paddle.nn.functional.conv2d(Tensor([8, 128, 256, 256],"float32"), Tensor([128, 128, 3, 3],"float32"), bias=None, stride=1, padding=1, )
paddle.nn.functional.conv2d(Tensor([8, 3, 256, 256],"float32"), Tensor([128, 3, 1, 1],"float32"), bias=None, stride=1, padding=0, )
paddle.nn.functional.conv2d_transpose(Tensor([8, 2048, 16, 16],"float16"), Tensor([2048, 128, 4, 4],"float16"), bias=None, padding=1, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
